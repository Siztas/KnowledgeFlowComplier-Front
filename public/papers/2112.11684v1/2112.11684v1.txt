æ ‡é¢˜ï¼šHP-GNN: Generating High ThroughputGNNTraining Implementation on CPU-FPGAHeterogeneousPlatform

Graph Neural Networks (GNNs) have shown great success in many applications such as recommendation systems, molecular property prediction, traffic prediction, etc.
Recently, CPU-FPGA heterogeneous platforms have been used to accelerate many applications by exploiting customizable data path and abundant user-controllable on-chip memory resources of FPGAs.
Yet, accelerating and deploying GNN training on such platforms requires not only
expertise in hardware design but also substantial development efforts.

We propose HP-GNN, a novel framework that generates high throughput GNN training implementations on a given CPU-FPGA platform that can benefit both application developers and machine learning researchers.
HP-GNN takes GNN training algorithms, GNN models as the inputs, and automatically performs hardware mapping onto the target CPU-FPGA platform.
HP-GNN consists of: (1) data layout and internal representation that reduce the memory traffic and random memory accesses; (2) optimized hardware templates that support various GNN models; (3) a design space exploration engine for automatic hardware mapping; (4) high-level application programming interfaces (APIs) that allows users to specify GNN training with only a handful of lines of code.
To evaluate HP-GNN, we experiment with two well-known sampling-based GNN training algorithms and two GNN models. For each training algorithm and model, HP-GNN generates implementation on a state-of-the-art CPU-FPGA platform.
Compared with CPU-only and CPU-GPU platforms, experimental results show that the generated implementations achieve55.67Ã—55.67\timesand2.17Ã—2.17\timesspeedup on the average, respectively. Compared with the state-of-the-art GNN training implementations, HP-GNN achieves up to4.45Ã—4.45\timesspeedup.

Recently, Graph Neural Networks (GNNs) have shown great success in many fields including recommendation systems(Ying etÂ al.,2018; Zhu etÂ al.,2019), molecular property prediction(Hamilton
etÂ al.,2017), traffic prediction(Jiang and Luo,2021), etc.
Initially, GNN was trained using the full graph(Kipf and Welling,2017)as the input, directly. However, as graphs become larger, full-graph GNN training becomes inefficient because the model is only updated once for each epoch. Moreover, huge amount of intermediate results needs to be stored in full-graph GNN training, which leads to high memory footprint(Liu
etÂ al.,2021). To overcome the above issues, many sampling-based GNN training algorithms(Hamilton
etÂ al.,2017; Zeng etÂ al.,2020; Chen
etÂ al.,2018; Chiang
etÂ al.,2019)have been proposed for training the GNN models. Sampling-based methods first sample the full graph to produce mini-batches, and then take these mini-batches as input for training.
The sampling-based mini-batch training methods demonstrate great advantages compared with full-graph training in terms of accuracy, generalization, and scalability for large-scale graphs(Hamilton
etÂ al.,2017; Zeng etÂ al.,2020; Chen
etÂ al.,2018; Chiang
etÂ al.,2019). Therefore, state-of-the-art GNN frameworks(Fey and Lenssen,2019; Wang etÂ al.,2019)adopt sampling-based mini-batch training algorithms.

Compared with the CPU-only platforms or CPU-GPU platforms, CPU-FPGA platforms are promising platforms to deploy GNN training since this can support customized memory access pattern(Zhou etÂ al.,2019)and data layout to reduce the substantial memory traffic and random memory access in GNN training.
However, deploying GNN training on CPU-FPGA heterogeneous platform is challenging due to notorious development efforts that requires hardware design expertise.
Though there are many FPGA-based GNN accelerators(Yan etÂ al.,2020; Zhang
etÂ al.,2021,2020; Geng etÂ al.,2020; Lin
etÂ al.,2021), previous works either focus on full-graph GNN inference, or a specific GNN model, or a specific GNN training algorithm, and no general framework has been proposed.

Motivated by the challenges, we propose HP-GNN, a framework for mapping GNN training on CPU-FPGA heterogeneous platform.
We first formulate an high-level abstraction to describe the computation of sampling-based mini-batch GNN training. Then we develop optimized hardware templates based on the GNN abstraction.
In order to reduce development effort and eliminate the need for hardware expertise, HP-GNN provides easy-to-use software programming APIs that allow fast development without the need for hardware programming.
To achieve high throughput and automate the accelerator generation process, we develop a general design space exploration engine that optimizes the hardware configuration based on the selected GNN training algorithm. In addition, we propose a data layout and internal representation that reduces the memory traffic and random memory access. We summarize our contributions as follow:

We propose a general framework for mapping various sampling based mini-batch GNN training onto a CPU-FPGA platform. We demonstrate the applicability of HP-GNN to various sampling algorithms and GNN models.

We provide high-level and easy-to-use software programming interface that abstracts the hardware implementation details.

To enable hardware mapping and high throughput GNN training, the proposed framework consists of the following optimizations:

Data layout and internal representation that reduce the memory traffic and random memory access caused by the irregular computation pattern of GNN.

Optimized hardware templates that support various widely-used GNN models.

General design space exploration algorithm that generates hardware design configuration to optimize the training throughput for various sampling methods and algorithmic parameters.

We evaluate our framework using two state-of-the-art GNN models: GraphSAGE(Hamilton
etÂ al.,2017), GCN(Kipf and Welling,2017), along with two commonly used sampling methods: neighbor sampling(Hamilton
etÂ al.,2017), and subgraph sampling(Zeng etÂ al.,2020). Running on a Xilinx Alveo U250 board hosted by a 64-core AMD processor, the experimental results show that the accelerators produced by our framework can achieve2.17Ã—2.17\timesthroughput on the average compared with a state-of-the-art GNN framework on CPU-GPU platform.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1.Computation abstraction of a GNN layer

Given an input graphğ’¢â€‹(ğ’±,â„°,ğ‘¿)ğ’¢ğ’±â„°ğ‘¿\mathcal{G}(\mathcal{V},\mathcal{E},\bm{X}), a GNN model is specified by:

Lğ¿L: number of layers.

ğ’±tsuperscriptğ’±ğ‘¡\mathcal{V}^{t}: a set of target vertices to be inferred.

flsuperscriptğ‘“ğ‘™f^{l}: hidden dimension in layerlâ€‹(1â©½lâ©½L)ğ‘™1ğ‘™ğ¿l~{}(1\leqslant l\leqslant L).

A mechanism of how to construct:

ğ’±lsuperscriptğ’±ğ‘™\mathcal{V}^{l}: the set of vertices in layerlâ€‹(0â©½lâ©½L)ğ‘™0ğ‘™ğ¿l~{}(0\leqslant l\leqslant L).|ğ’±l|superscriptğ’±ğ‘™|\mathcal{V}^{l}|denotes the number of vertices in layerlğ‘™l. Moreover,ğ’±L=ğ’±tsuperscriptğ’±ğ¿superscriptğ’±ğ‘¡\mathcal{V}^{L}=\mathcal{V}^{t}.

ğ‘¨lâˆˆâ„|ğ’±lâˆ’1|Ã—|ğ’±l|superscriptğ‘¨ğ‘™superscriptâ„superscriptğ’±ğ‘™1superscriptğ’±ğ‘™\bm{A}^{l}\in\mathbb{R}^{{|\mathcal{V}^{l-1}|}\times{|\mathcal{V}^{l}|}}: adjacency matrix for feature aggregation in layerlâ€‹(1â©½lâ©½L)ğ‘™1ğ‘™ğ¿l~{}(1\leqslant l\leqslant L).ğ‘¨lsuperscriptğ‘¨ğ‘™\bm{A}^{l}defines the inter-layer connectivity betweenğ’±lâˆ’1superscriptğ’±ğ‘™1\mathcal{V}^{l-1}andğ’±lsuperscriptğ’±ğ‘™\mathcal{V}^{l}.

ğ‘¾lâˆˆâ„flâˆ’1Ã—flsuperscriptğ‘¾ğ‘™superscriptâ„superscriptğ‘“ğ‘™1superscriptğ‘“ğ‘™\bm{W}^{l}\in\mathbb{R}^{f^{l-1}\times f^{l}}: weight matrix of layerlâ€‹(1â©½lâ©½L)ğ‘™1ğ‘™ğ¿l~{}(1\leqslant l\leqslant L)that is used in update function to perform linear transformation of vertex features.

ğ‘¿lâˆˆâ„|ğ’±l|Ã—flsuperscriptğ‘¿ğ‘™superscriptâ„superscriptğ’±ğ‘™superscriptğ‘“ğ‘™\bm{X}^{l}\in\mathbb{R}^{{|\mathcal{V}^{l}|}\times f^{l}}: feature matrix of layerlâ€‹(1â©½lâ©½L)ğ‘™1ğ‘™ğ¿l~{}(1\leqslant l\leqslant L)

Aggregate() function that is used by each vertex to aggregate information from its neighbors.

Update() function including an one-layer multi-layer perception (MLP) and an activation functionÏƒğœ\sigma() that is used by each vertex to perform feature transformation.

Figure1depicts the computation abstraction of a GNN layer. The computations using a GNN model can also be expressed using the aggregate-update paradigm(Yan etÂ al.,2020), as shown in Algorithm1.ğ’‰vlâˆˆâ„flsuperscriptsubscriptğ’‰ğ‘£ğ‘™superscriptâ„superscriptğ‘“ğ‘™\bm{h}_{v}^{l}\in\mathbb{R}^{f^{l}}is the feature vector ofvâˆˆâ„¬lğ‘£superscriptâ„¬ğ‘™v\in\mathcal{B}^{l}in layerlğ‘™l, andğ’‚vlâˆˆâ„flsuperscriptsubscriptğ’‚ğ‘£ğ‘™superscriptâ„superscriptğ‘“ğ‘™\bm{a}_{v}^{l}\in\mathbb{R}^{f^{l}}is the intermediate result ofvâˆˆâ„¬lğ‘£superscriptâ„¬ğ‘™v\in\mathcal{B}^{l}.

There are several widely used GNN models proposed in the literature:

GCN: GCN is proposed in(Kipf and Welling,2017). Given the input graphğ’¢â€‹(ğ’±,â„°,ğ‘¿)ğ’¢ğ’±â„°ğ‘¿\mathcal{G}(\mathcal{V},\mathcal{E},\bm{X}), the GCN model is specified by:

ğ’±1=ğ’±2=â€¦=ğ’±L=ğ’±T=ğ’±superscriptğ’±1superscriptğ’±2â€¦superscriptğ’±ğ¿superscriptğ’±ğ‘‡ğ’±\mathcal{V}^{1}=\mathcal{V}^{2}=...=\mathcal{V}^{L}=\mathcal{V}^{T}=\mathcal{V}

ğ‘¨1=ğ‘¨2=â€¦=ğ‘¨L=ğ‘«âˆ’12â€‹(ğ‘¨+ğ‘°)â€‹ğ‘«âˆ’12superscriptğ‘¨1superscriptğ‘¨2â€¦superscriptğ‘¨ğ¿superscriptğ‘«12ğ‘¨ğ‘°superscriptğ‘«12\bm{A}^{1}=\bm{A}^{2}=...=\bm{A}^{L}=\bm{D}^{-\frac{1}{2}}(\bm{A}+\bm{I})\bm{D}^{-\frac{1}{2}}, whereğ‘¨ğ‘¨\bm{A}andğ‘«ğ‘«\bm{D}are the adjacency matrix and the Laplacian matrix of the input graph.ğ‘°ğ‘°\bm{I}is the identity matrix.

Lğ¿L: number of layers;flsuperscriptğ‘“ğ‘™f^{l}: feature size in layerlâ€‹(1â©½lâ©½L)ğ‘™1ğ‘™ğ¿l~{}(1\leqslant l\leqslant L);

The Aggregate() function and Update() function of GCN are expressed as:

whereğ’©â€‹(v)ğ’©ğ‘£\mathcal{N}(v)denotes the neighbor set ofvğ‘£vinğ’±lâˆ’1superscriptğ’±ğ‘™1\mathcal{V}^{l-1},Dâ€‹(v)ğ·ğ‘£D(v)denotes the degree of vertexvğ‘£v, andğ’ƒlsuperscriptğ’ƒğ‘™\bm{b}^{l}denotes the bias of the update function.

GraphSAGE: GraphSAGE is proposed in(Hamilton
etÂ al.,2017)for inductive representation learning on graphs. Starting from a set of target vertexğ’±Tsuperscriptğ’±ğ‘‡\mathcal{V}^{T}, GraphSAGE neighbor sampler recursively samples the neighbors to buildğ’±1,ğ’±2,â€¦,ğ’±Lsuperscriptğ’±1superscriptğ’±2â€¦superscriptğ’±ğ¿\mathcal{V}^{1},\mathcal{V}^{2},...,\mathcal{V}^{L}. The adjacency matrixğ‘¨lsuperscriptğ‘¨ğ‘™\bm{A}^{l}defines the edge connections betweenğ’±lâˆ’1superscriptğ’±ğ‘™1\mathcal{V}^{l-1}andğ’±lsuperscriptğ’±ğ‘™\mathcal{V}^{l}, and each edge has weight equal to one. The Aggregate() function and Update() function of GraphSAGE are expressed as:

Note: In the rest of the paper, we use GCN and GraphSAGE to refer to their GNN-layer operators Aggregate(), Update().

To train a GNN model, the GNN training process consists of five stages(Hamilton
etÂ al.,2017; Zeng etÂ al.,2020; Chen
etÂ al.,2018; Chiang
etÂ al.,2019): sampling, forward propagation, loss calculation, back propagation and weight update. In the sampling stage, a set of vertices and adjacency matrices are sampled from{ğ’±l:0â©½lâ©½L}conditional-setsuperscriptğ’±ğ‘™0ğ‘™ğ¿\{\mathcal{V}^{l}:0\leqslant l\leqslant L\}and{ğ‘¨l:1â©½lâ©½L}conditional-setsuperscriptğ‘¨ğ‘™1ğ‘™ğ¿\{\bm{A}^{l}:1\leqslant l\leqslant L\}to form a mini-batch. We useâ„¬lsuperscriptâ„¬ğ‘™\mathcal{B}^{l}to denote the vertices sampled fromğ’±lsuperscriptğ’±ğ‘™\mathcal{V}^{l}in layerlğ‘™l.ğ‘¨slsuperscriptsubscriptğ‘¨ğ‘ ğ‘™\bm{A}_{s}^{l}denotes the sampled adjacency matrix, which describes inter-layer connections (edges) betweenâ„¬lsuperscriptâ„¬ğ‘™\mathcal{B}^{l}andâ„¬lâˆ’1superscriptâ„¬ğ‘™1\mathcal{B}^{l-1}within the mini-batch. A mini-batch consists of target verticesâ„¬Lsuperscriptâ„¬ğ¿\mathcal{B}^{L}, sampled vertices for each layer{â„¬l:0â©½lâ©½Lâˆ’1}conditional-setsuperscriptâ„¬ğ‘™0ğ‘™ğ¿1\{\mathcal{B}^{l}:0\leqslant l\leqslant L-1\}, and sampled adjacency matrices{ğ‘¨s1:1â©½lâ©½Lâˆ’1}conditional-setsuperscriptsubscriptğ‘¨ğ‘ 11ğ‘™ğ¿1\{\bm{A}_{s}^{1}:1\leqslant l\leqslant L-1\}. In the forward propagation stage, the mini-batch is processed layer by layer. The output embeddings in the last layer{ğ’‰iL:viâˆˆâ„¬L}conditional-setsuperscriptsubscriptğ’‰ğ‘–ğ¿subscriptğ‘£ğ‘–superscriptâ„¬ğ¿\{\bm{h}_{i}^{L}:v_{i}\in\mathcal{B}^{L}\}are compared with the ground truth for loss calculation. The calculated loss will be used as input for back propagation, which performs a similar computation as forward propagation but in the reverse direction. Finally, the gradients ofğ‘¾lsuperscriptğ‘¾ğ‘™\bm{W}^{l}in each layer are derived and be used to update the weight. We show the steps of GNN training in Algorithm2. In Algorithm2,ğ’©sâ€‹(v)subscriptğ’©ğ‘ ğ‘£\mathcal{N}_{s}(v)denotes neighbors ofvğ‘£vinâ„¬lâˆ’1superscriptâ„¬ğ‘™1\mathcal{B}^{l-1}that are specified inğ‘¨slsuperscriptsubscriptğ‘¨ğ‘ ğ‘™\bm{A}_{s}^{l}. A GNN training algorithm is specified by a sampling algorithm (see Section2.3) to construct the mini-batch that consists of{â„¬l:0â©½lâ©½Lâˆ’1}conditional-setsuperscriptâ„¬ğ‘™0ğ‘™ğ¿1\{\mathcal{B}^{l}:0\leqslant l\leqslant L-1\}and{ğ‘¨s1:1â©½lâ©½Lâˆ’1}conditional-setsuperscriptsubscriptğ‘¨ğ‘ 11ğ‘™ğ¿1\{\bm{A}_{s}^{1}:1\leqslant l\leqslant L-1\}.

In HP-GNN, we exploit data parallelism within each mini-batch by aggregating and updating multiple vertices concurrently (shown in Line 5 of Algorithm2). The computation order within the same mini-batch does not affect the final results. Thus, training in our parallel framework leads to the same result and accuracy as training in serial fashion.

An algorithm to sample a mini-batch is specified by:

A method to sample the verticesâ„¬lâ€‹(0â©½lâ©½L)superscriptâ„¬ğ‘™0ğ‘™ğ¿\mathcal{B}^{l}~{}(0\leqslant l\leqslant L)fromğ’±lsuperscriptğ’±ğ‘™\mathcal{V}^{l}.

A method to construct the adjacency matrixğ‘¨slâ€‹(1â©½lâ©½L)superscriptsubscriptğ‘¨ğ‘ ğ‘™1ğ‘™ğ¿\bm{A}_{s}^{l}~{}(1\leqslant l\leqslant L)fromğ‘¨lsuperscriptğ‘¨ğ‘™\bm{A}^{l}.

By sampling the verticesâ„¬lâ€‹(0â©½lâ©½L)superscriptâ„¬ğ‘™0ğ‘™ğ¿\mathcal{B}^{l}~{}(0\leqslant l\leqslant L)and the adjacency matrixğ‘¨lâ€‹(1â©½lâ©½L)superscriptğ‘¨ğ‘™1ğ‘™ğ¿\bm{A}^{l}~{}(1\leqslant l\leqslant L), we construct the mini-batch as the input for each training iteration.
Various sampling methods(Zeng etÂ al.,2020; Hamilton
etÂ al.,2017; Chen
etÂ al.,2018; Ying etÂ al.,2018; Dai
etÂ al.,2018)are proposed to form a mini-batch from input graphs. These sampling methods(Liu
etÂ al.,2021)falls into three categories: neighbor sampling, layer-wise sampling and subgraph sampling. We only introduce neighbor sampling and subgraph sampling, since layer-wise sampling(Chen
etÂ al.,2018)has the similar computation pattern with subgraph sampling(Zeng etÂ al.,2020).

Neighbor Sampling:Neighbor sampling(Hamilton
etÂ al.,2017; Ying etÂ al.,2018; Chen
etÂ al.,2017)is a type of sampling strategy that recursively samples the neighbors from the target vertices. To perform neighbor sampling, users specify the size of target vertices|ğ’±t|superscriptğ’±ğ‘¡|\mathcal{V}^{t}|and the neighbor sample sizeNâ€‹Slğ‘superscriptğ‘†ğ‘™NS^{l}for each vertex in layerlğ‘™l.
The sampler first chooses a set of vertices as target verticesğ’±tsuperscriptğ’±ğ‘¡\mathcal{V}^{t}. Then, the sampler samplesNâ€‹SLğ‘superscriptğ‘†ğ¿NS^{L}neighbors for each vertex inğ’±tsuperscriptğ’±ğ‘¡\mathcal{V}^{t}based on a specific probability distribution (e.g., uniform distribution(Hamilton
etÂ al.,2017)). After the first iteration of neighbor sampling, the set of 1-hop sampled neighborsâ„¬Lâˆ’1superscriptâ„¬ğ¿1\mathcal{B}^{L-1}is obtained, where|â„¬Lâˆ’1|=|ğ’±t|Ã—Nâ€‹SLsuperscriptâ„¬ğ¿1superscriptğ’±ğ‘¡ğ‘superscriptğ‘†ğ¿|\mathcal{B}^{L-1}|=|\mathcal{V}^{t}|\times NS^{L}. Similarly, we obtain the 2-hop neighborsâ„¬Lâˆ’2superscriptâ„¬ğ¿2\mathcal{B}^{L-2}by sampling the neighbors of the 1-hop neighborsâ„¬Lâˆ’1superscriptâ„¬ğ¿1\mathcal{B}^{L-1}, where|â„¬Lâˆ’2|=|ğ’±t|Ã—Nâ€‹SLÃ—Nâ€‹SLâˆ’1superscriptâ„¬ğ¿2superscriptğ’±ğ‘¡ğ‘superscriptğ‘†ğ¿ğ‘superscriptğ‘†ğ¿1|\mathcal{B}^{L-2}|=|\mathcal{V}^{t}|\times NS^{L}\times NS^{L-1}. By performing neighbor sampling recursively, we obtainLğ¿L-hop neighbors of the target vertices. After obtaining{â„¬l:0â©½lâ©½Lâˆ’1}conditional-setsuperscriptâ„¬ğ‘™0ğ‘™ğ¿1\{\mathcal{B}^{l}:0\leqslant l\leqslant L-1\},ğ‘¨slâ€‹(1â©½lâ©½L)superscriptsubscriptğ‘¨ğ‘ ğ‘™1ğ‘™ğ¿\bm{A}_{s}^{l}~{}(1\leqslant l\leqslant L)is constructed by:

Subgraph Sampling:Subgraph sampling(Zeng etÂ al.,2020; Chiang
etÂ al.,2019)is a strategy to sample a subgraph from the input graph and perform GNN information propagation within the subgraph.
In the subgraph sampling-based method, users specify sampling budgetSâ€‹Bğ‘†ğµSBwhich denotes how many vertices to be sampled for the subgraph. Then, the sampler samplesSâ€‹Bğ‘†ğµSBof vertices or edges based on specific probability, and induce a subgraph based on the sampled vertices or edges. For subgraph sampling, the sampled vertices are identical for each layer, i.e.â„¬0=â„¬1=â€¦=â„¬Lsuperscriptâ„¬0superscriptâ„¬1â€¦superscriptâ„¬ğ¿\mathcal{B}^{0}=\mathcal{B}^{1}=...=\mathcal{B}^{L}. The construction ofğ‘¨slâ€‹(1â©½lâ©½L)superscriptsubscriptğ‘¨ğ‘ ğ‘™1ğ‘™ğ¿\bm{A}_{s}^{l}~{}(1\leqslant l\leqslant L)is the same as neighbor sampling.

Several software GNN frameworks(Fey and Lenssen,2019; Wang etÂ al.,2019; Zhu etÂ al.,2019; Wang
etÂ al.,2021)have been proposed in the literature. PyTorch Geometric (PyG)(Fey and Lenssen,2019)is one of the most commonly-used frameworks for GNN deployment that uses PyTorch(Paszke
etÂ al.,2019)as the backend. PyG users can describe various GNN models using PyGâ€™s message passing API. Deep Graph Library (DGL)(Wang etÂ al.,2019)adopts several parallelization strategies to achieve high performance and memory efficiency for GNN computations. Moreover, DGL offers several major frameworks(Paszke
etÂ al.,2019; Abadi etÂ al.,2016; Chen etÂ al.,2015)as the backend, allowing users to port their model across frameworks.
Aligraph(Zhu etÂ al.,2019)supports training on heterogeneous attributed graph (HAG), i.e. graphs that contain different types of vertices and edges.
These software frameworks share similar features: They are built on existing frameworks(Paszke
etÂ al.,2019; Abadi etÂ al.,2016; Chen etÂ al.,2015), and abstract away the detailed implementations by providing graph-oriented APIs so that users can describe GNN models easily.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2.Framework overview

GraphACT(Zeng and Prasanna,2020)performs sub-graph sampling based GNN training on CPU-FPGA heterogeneous platform. GraphACT exploits both task-level parallelism and data parallelism, and adopts a redundancy reduction technique to reduce the number of on-chip memory accesses. However, GraphACT optimizes the hardware design for inductive GNN models(Hamilton
etÂ al.,2017)using subgraph sampling, and does not support transductive model, such as GCN . As shown in(Hu etÂ al.,2021), different sampling algorithms perform well in different applications, so there is no one-size-fits-all sampling algorithms.
Rubik(Chen etÂ al.,2021b)decomposes GNN computations into graph-level and node-level computations, and proposes hierarchical task mapping strategies to exploit data reuse and parallelism in the two computation levels. However, Rubik is an ASIC accelerator that is hard to be optimized for various sampling algorithms.
DeepBuring-GL(Liang
etÂ al.,2020)is a FPGA framework to accelerate GNN inference. DeepBuring-GL provides various templates to support different GNN computations and memory access patterns. DeepBuring-GL analyzes the GNN model with the input graph to identify performance bottleneck and choose appropriate hardware templates for kernel implementation to accelerate GNN inference.
Though frameworks like DeepBuring-GL have been proposed, most of the hardware acceleration works still require significant hardware expertise to make use of them. Moreover, no framework has been proposed to support various mini-batch GNN training on CPU-FPGA platform, which motivates us to conduct this work. In this paper, we build a framework to accelerate GNN training on CPU-FPGA platform. Our framework provides infrastructures to support various GNN models and training algorithms.

We summarize the benefits of using CPU-FPGA platform: while CPU can support various sampling algorithms, FPGA platform allows customizable data path and memory access pattern that can be exploited to optimize the GNN training throughput.

Figure3depicts the mapping of graph data and various kernels onto the CPU-FPGA heterogeneous platform. Sampling is performed on the host CPU because CPU is flexible to support various sampling algorithms; GNN operations including feature aggregation and feature update are performed on the proposed FPGA accelerator.
Based on this task assignment, the structural information of input graph (ğ’±,â„°ğ’±â„°\mathcal{V},\mathcal{E}) is stored in the host memory for the host CPU to perform sampling. After sampling is done, the structural information of the mini-batch is generated and transferred to the FPGA local memory. The vertex featuresğ‘¿ğ‘¿\bm{X}are stored in the FPGA local memory to be directly accessed by the FPGA accelerator, which can reduce the overhead of data movement. The state-of-the-art FPGA boards(Gidel,2021)have up to 260 GB memory; this can support medium size graph.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3.System overview

Regarding very large graphs, we store the vertex features in host memory and transfer the vertex features of the mini-batch to the FPGA accelerator after sampling.

Figure2demonstrates the framework overview. The generated design by the framework consists of two major components: (1) a host program that manages CPU-FPGA communication, kernel task scheduling and mini-batch sampling; (2) an accelerator design that runs on the FPGA.
To generate the mini-batch GNN training implementation on CPU-FPGA platform, our framework takes the user program as the input and generates a high-level abstraction for mini-batch GNN training. In the input program, user specifies the following parameters:

GNN parameters: number of layersLğ¿L; hidden dimension of each layer:fl,(0â©½lâ©½L)superscriptğ‘“ğ‘™0ğ‘™ğ¿f^{l},(0\leqslant l\leqslant L). The hidden dimensions also define the dimension of weight matrixğ‘¾lâˆˆâ„flÃ—fl+1superscriptğ‘¾ğ‘™superscriptâ„superscriptğ‘“ğ‘™superscriptğ‘“ğ‘™1\bm{W}^{l}\in\mathbb{R}^{f^{l}\times f^{l+1}}.

Specify an off-the-shelf GNN model, or provide user-defined functions (UDFs) for scatter( ), gather( ) and update( ) to build custom GNN computation layer.

Sampling algorithm and its parameters. For example, a neighbor sampler for a 2-layer GNN model can be defined asSampler( â€™NeighborSamplerâ€™, L=2, budgets=[10, 25])through our high-level API described in Section3.3. We provide several off-the-shelf samplers for users to choose from.

The program parser extracts a GNN abstraction from user program, which serves as the intermediate representation for the software generator and hardware generator to generate the implementations on CPU-FPGA platform. The GNN abstraction consists of GNN model configuration (hidden dimensionsflsuperscriptğ‘“ğ‘™f^{l}, GNN operators, number of layersLğ¿L) and mini-batch configuration (number of vertices in each layer|â„¬l|,(0â©½lâ©½L)superscriptâ„¬ğ‘™0ğ‘™ğ¿|\mathcal{B}^{l}|,(0\leqslant l\leqslant L)and number of edges in each layer|â„°l|,(1â©½lâ©½L)superscriptâ„°ğ‘™1ğ‘™ğ¿|\mathcal{E}^{l}|,(1\leqslant l\leqslant L)). The mini-batch configuration is deduced from the sampling algorithm that implies number of vertices|â„¬l|,(0â©½lâ©½L)superscriptâ„¬ğ‘™0ğ‘™ğ¿|\mathcal{B}^{l}|,(0\leqslant l\leqslant L)in each layer and number of edges|â„°l|,(1â©½lâ©½L)superscriptâ„°ğ‘™1ğ‘™ğ¿|\mathcal{E}^{l}|,(1\leqslant l\leqslant L)in each layer.

DSE Engine: the DSE engine takes the GNN abstraction and the platform metadata as input and generates the accelerator configuration that optimizes the GNN training throughput (Section5).

HLS Template/Hardware Template: In the framework, we provide optimized hardware templates written in high-level synthesis (HLS). The key computation operators of the templates (e.g. scatter(), gather()) are obtained from the GNN abstraction. We describe the details of the HLS template design in Section4.

Accelerator generator: Given the generated accelerator configuration and hardware templates, the accelerator generator generates the hardware accelerators for the target FPGA board. The accelerator generator uses the available synthesis tools such as Xilinx Vitis as the backend.

Software generator: Given the input program, the software generator produces a runtime system that runs on the host processor. The runtime system performs the mini-batch sampling, CPU-FPGA communication management, and task scheduling.

API FunctionsDescriptionInit( )Initialization the platform with FPGA bitstreamGNN_Parameters( )Number of layerLğ¿L, feature lengthflsuperscriptğ‘“ğ‘™f^{l},Wlsuperscriptğ‘Šğ‘™W^{l}andXğ‘‹XGNN_Computation( )The layer operators in GNN modelSpecify an off-the-shelf GNN model or â€customizedâ€Scatter( )UDF, required if customized layer operator is specifiedGather( )UDF, required if customized layer operator is specifiedUpdate( )UDF, required if customized layer operator is specifiedGNN_Model( )Build GNN model using GNN parameters and computationPlatformParameters( )FPGA Memory bandwidth, Number of DSPs, LUTs, etc.LoadInputGraph( )Specify the input graph.Sampler( )Sampling method with algorithmic parametersDistributeData( )Distribute graph into host memory and FPGA local memoryGenerateDesign( )Generate hardware design and software designStart_training( )Run GNN trainingSave_model( )Save trained GNN modelPrepareEdges( )Prepare graph edge values that is used for training

Table1summarizes our provided high-level APIs for user to program the mini-batch training in Python. Listing1is an example for developing the GNN training using our proposed framework.

In the design phase, user specifies the mini-batch sampler, GNN model and parameters of platform. The framework automatically generates the optimized accelerator design and software design. In the runtime phase, user starts the GNN training on the target CPU-FPGA platform. Using our high-level APIs, a GNN training program only requires a few dozen lines of code.

Use cases: Our framework can serve application developers, who utilize existing GNN models to build GNN applications. Our framework provides off-the-shelf GNN training implementations for some of the commonly-used GNN models (GCN(Kipf and Welling,2017), GraphSAGE(Hamilton
etÂ al.,2017), GIN(Xu
etÂ al.,2019)) which can be directly deployed on the CPU-FPGA platform.
For machine learning (ML) researchers who develop novel GNN models, our framework allows them to customize their own GNN models. For both cases, our framework accelerates GNN training on CPU-FPGA platform without hardware expertise.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: Figure 4.Data layout and Internal Representation

We design hardware templates based on the computation abstraction of the GNN layer described in Section2.1. The hardware templates describe a general architecture of the GNN layer as in Figure5and Figure6. Then, the accelerator generator takes user-defined functions as input and integrates them into the hardware templates to generate the accelerator design.

Aggregating feature vector from neighbors incurs irregular memory access and large memory traffic. Figure4presents the data layout and internal representation used in our framework to reduce the memory traffic as well as the number of random memory accesses. The data layout is produced by the sampler throughrenamingandsorting.

Reducing Memory Traffic (RMT): During aggregation stage, the feature vector of the source vertex is sent to its destination for aggregation.
For the first layer of aggregation, the input feature matrixğ‘¿ğ‘¿\bm{X}is stored in the memory. Thus, loading feature vectors incurs a large number of random memory accesses.
Since the edges are represented in coordinate (COO) format sorted by source vertices in our framework, edges that share the same source vertex can reuse the feature vector that has been loaded, and thus reduce memory traffic. The total number of memory traffic can be reduced fromOâ€‹(|â„°1|â€‹f0)ğ‘‚subscriptâ„°1superscriptğ‘“0O(|\mathcal{E}_{1}|f^{0})toOâ€‹(|â„¬0|â€‹f0)ğ‘‚subscriptâ„¬0superscriptğ‘“0O(|\mathcal{B}_{0}|f^{0}), where|â„°1|subscriptâ„°1|\mathcal{E}_{1}|is usually larger than|â„¬0|subscriptâ„¬0|\mathcal{B}_{0}|. Fig4depicts an example: Â¡v1,v2subscriptğ‘£1subscriptğ‘£2v_{1},v_{2}Â¿ loads the feature vector ofv1subscriptğ‘£1v_{1}from memory, and the loaded feature vector ofv1subscriptğ‘£1v_{1}can be reused by (v1,v7subscriptğ‘£1subscriptğ‘£7v_{1},v_{7}).

Reducing Random Access (RRA):
Since the edges are sorted by the source index in the first layer, the destination vertex index of the edges are in a random order; thus, the hidden features are stored randomly as shown in the layer 1 and layer 2 of Figure4. To reduce random access, our framework performs vertex renaming, which labels the vertices based on the order they are stored, this step also renames the vertices in each edge. Next, we sort the renamed edges by source vertices, and then accessing hidden features become sequential since the source vertex number follows the order it is stored.

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: Figure 5.Architecture of aggregate kernel

Aggregate Kernel: The aggregate kernel adopts the scatter-gather paradigm(Chen
etÂ al.,2021a)as illustrated in Algorithm3.
Figure5depicts the detailed architecture of the aggregate kernel.
Multiple processing elements (PEs) process multiple edges concurrently in each clock cycle.
The vertex feature vectors are first streamed to a feature duplicator. The feature duplicator broadcasts the loaded feature vector to all the Scatter PEs. The feature vector is stored in the PEsâ€™ registers for data reuse. Then, Scatter PEs perform user-defined scatter() function, and stream the updateuğ‘¢uto its destination via the routing network. The routing network is implemented as a butterfly network(Choi
etÂ al.,2021). After the Gather PEs receive the updates, Gather PEs perform user-defined gather function and obtain the intermediate results. The intermediate results are stored on-chip. Finally, when the aggregation is done, the results stored in the on-chip memory are written back to the FPGA local memory. Since the gather phase may incur reading and writing to the same destination vertices, read-after-write (RAW) data hazard may occur.
The RAW Resolver addresses RAW data hazard by stalling.

Update Kernel: The update kernel is a systolic array based design that performs block matrix multiplication. The input buffer loads the aggregation resultsğ’‚lsuperscriptğ’‚ğ‘™\bm{a}^{l}(see Algorithm2) from the FPGA local memory.ğ’‚lsuperscriptğ’‚ğ‘™\bm{a}^{l}will then be streamed into the MAC array. Each MAC module is followed by an element-wise operatorÏƒğœ\sigma. Typically, weight of each layerğ‘¾lsuperscriptğ‘¾ğ‘™\bm{W}^{l}in GCN is small and frequently reused. Thus,ğ‘¾lsuperscriptğ‘¾ğ‘™\bm{W}^{l}are stored on-chip in the Weight-Buffer.ğ‘¾lsuperscriptğ‘¾ğ‘™\bm{W}^{l}will be broadcast to the multiply-accumulate (MAC) array during feature update. Finally, the result is stored into a result buffer before written back to the FPGA local memory.

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: Figure 6.Architecture of update kernel

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: Figure 7.Architecture of the FPGA accelerator.

Many modern FPGAs consists of multiple dies and number of interconnection wires across the dies is limited. Therefore, we implement multiple copy of the kernels that is distributed into multiple dies as shown in Figure7. Multiple dies and multiple DDR channels are connected through an all-to-all interconnection which is generated by vendor tool, such as Xilinx Vitis. The input feature matrixğ‘¿ğ‘¿\bm{X}is equally partitioned into DDR channels. To utilize the multiple computation kernels for a single mini-batch, we perform task partitioning for the mini-batch training. In the forward propagation phase of layer 1, to infer the vertices inâ„¬1={v1,v2,v3,â€¦.,v|â„¬1|}\mathcal{B}^{1}=\{{v_{1},v_{2},v_{3},....,v_{|\mathcal{B}^{1}|}}\}. The workload for inferringâ„¬1superscriptâ„¬1\mathcal{B}^{1}are equally partitioned into multiple kernels. Suppose there is444aggregate kernels and444corresponding update kernels. Aggregate kernel 1 aggregates{v1,v2,â€¦,v|â„¬1|4}subscriptğ‘£1subscriptğ‘£2â€¦subscriptğ‘£superscriptâ„¬14\{{v_{1},v_{2},...,v_{\frac{|\mathcal{B}^{1}|}{4}}}\}. Update kernel 1 updates{v1,v2,â€¦,v|â„¬1|4}subscriptğ‘£1subscriptğ‘£2â€¦subscriptğ‘£superscriptâ„¬14\{{v_{1},v_{2},...,v_{\frac{|\mathcal{B}^{1}|}{4}}}\}and write the results back to DDR. Similarly, aggregate kernel 2 aggregates{v|â„¬1|4+1,v|â„¬1|4+2,â€¦,v|â„¬1|2}subscriptğ‘£superscriptâ„¬141subscriptğ‘£superscriptâ„¬142â€¦subscriptğ‘£superscriptâ„¬12\{{{v_{\frac{|\mathcal{B}^{1}|}{4}}+1},{v_{\frac{|\mathcal{B}^{1}|}{4}}+2},...,v_{\frac{|\mathcal{B}^{1}|}{2}}}\}, and so on and so forth. The same partitioning scheme is applied to each layer.

Our framework provides a design space exploration (DSE) engine for optimizing the GNN training throughput, given the configuration of mini-batch ({|â„¬l|:0â©½lâ©½L}:superscriptâ„¬ğ‘™0ğ‘™ğ¿\{|\mathcal{B}^{l}|:0\leqslant l\leqslant L\},{|â„°l|:1â©½lâ©½L}:superscriptâ„°ğ‘™1ğ‘™ğ¿\{|\mathcal{E}^{l}|:1\leqslant l\leqslant L\}), GNN hidden dimensions{fl:0â©½lâ©½L}conditional-setsuperscriptğ‘“ğ‘™0ğ‘™ğ¿\{f^{l}:0\leqslant l\leqslant L\}, memory bandwidthÎ±ğ›¼\alpha, and hardware resources per die (DSPs, BRAMs, URAMs). To drive the optimization, we develop a performance model (Section5.1) that models the training throughput on the CPU-FPGA platform, and the resource utilization model (Section5.2) that is used to specify the resource constraints. Then, our DSE engine (Algorithm4) performs parameter sweep in the design space to identify the hardware design parameters that optimizes the training throughput.

We define the throughput of mini-batch GNN training as Number of Vertices Traversed Per Second (NVTPS):

The numerator indicates the total amount of vertices traversed in one mini-batch, and the denominatortexecutionsubscriptğ‘¡executiont_{\text{execution}}is the average execution time of one training iteration (see Algorithm2). The modeling of the average execution time is based on the our task scheduling on the CPU-FPGA platform.

We overlap sampling stage of the next batch with the execution of the current batch, so average execution timetexecutionsubscriptğ‘¡executiont_{\text{execution}}is estimated as:

wheretGNNsubscriptğ‘¡GNNt_{\text{\text{GNN}}}consists of the execution time of forward propagationtFPsubscriptğ‘¡FPt_{\text{FP}}, loss calculationtLCsubscriptğ‘¡LCt_{\text{LC}}, back propagationtBPsubscriptğ‘¡BPt_{\text{BP}}and weight updatetWUsubscriptğ‘¡WUt_{\text{WU}}.

ModelingtGNNsubscriptğ‘¡GNNt_{\text{GNN}}: Loss calculation and weight update are executed on the host processor, which have optimized implementation in the software library.
Forward propagation and backward propagation are executed on the FPGA platform, and their execution time depends on the hardware parameters and the mini-batch configuration ({|â„¬l|:0â©½lâ©½L}:superscriptâ„¬ğ‘™0ğ‘™ğ¿\{|\mathcal{B}^{l}|:0\leqslant l\leqslant L\},{|â„°l|:1â©½lâ©½L}:superscriptâ„°ğ‘™1ğ‘™ğ¿\{|\mathcal{E}^{l}|:1\leqslant l\leqslant L\}). We drive the approximate execution time of two propagation stages as:

The total propagation timetFPsubscriptğ‘¡FPt_{\text{FP}}ortBPsubscriptğ‘¡BPt_{\text{BP}}is the sum of the execution time of each layer, and the execution time of each layer is decided by the task that takes longer to complete since aggregation stage and update stage are pipelined.

The aggregation stage consists of two tasks: (1) loading vertex features or gradients, and (2) computation. Since the two tasks are pipelined,taggregatesubscriptğ‘¡aggregatet_{\text{aggregate}}can be modeled as:

We model the vertex feature loading timetloadlsuperscriptsubscriptğ‘¡loadğ‘™t_{\text{load}}^{l}asdata transferredeffective bandwidthdata transferredeffective bandwidth\frac{\text{data transferred}}{\text{effective bandwidth}}.|â„¬l|superscriptâ„¬ğ‘™|\mathcal{B}^{l}|indicates the number of vertices in each layer,flsuperscriptğ‘“ğ‘™f^{l}is the feature length, andSfeatsubscriptğ‘†featS_{\text{feat}}is the data size of each feature.Î±ğ›¼\alphais the effective bandwidth ratio. For the feature loading of the first layer of neighbor sampling method,Î±ğ›¼\alphais estimated based on the memory burst transaction lengthSfeatsubscriptğ‘†featS_{\text{feat}}(Lu
etÂ al.,2021)(for DDR4) because it incurs random memory access; for the rest of the layers,Î±ğ›¼\alphais near to111(Lu
etÂ al.,2021)since the memory accesses are sequential from DDR memory or on-chip memory (Section4.1). The value ofÎ±ğ›¼\alphais obtained from the prior work(Lu
etÂ al.,2021)which performs a profiling for the characteristics of the FPGA DDR memory. We model the compute time as (# of operations)/(# of PEsÃ—\timeskernel frequency).nğ‘›ndenotes the there arenğ‘›nScatter PEs andnğ‘›nGather PEs instantiated in the aggregation kernel.|â„°l|superscriptâ„°ğ‘™|\mathcal{E}^{l}|is the number of edges (i.e. non-zeros) in each layer. The size of|â„°l|superscriptâ„°ğ‘™|\mathcal{E}^{l}|depends on the sampling method. We show the modeling ofâ„°lsuperscriptâ„°ğ‘™\mathcal{E}^{l}for various sampling methods in Table2.

Method# of Vertices|â„¬l|superscriptâ„¬ğ‘™|\mathcal{B}^{l}|# of edges|â„°l|superscriptâ„°ğ‘™|\mathcal{E}^{l}|Neighbor|ğ’±t|Ã—Î i=l+1Lâ€‹Nâ€‹Sisuperscriptğ’±ğ‘¡superscriptsubscriptÎ ğ‘–ğ‘™1ğ¿ğ‘superscriptğ‘†ğ‘–|\mathcal{V}^{t}|\times\Pi_{i=l+1}^{L}NS^{i}|ğ’±t|Ã—Î i=lLâ€‹Nâ€‹Sisuperscriptğ’±ğ‘¡superscriptsubscriptÎ ğ‘–ğ‘™ğ¿ğ‘superscriptğ‘†ğ‘–|\mathcal{V}^{t}|\times\Pi_{i=l}^{L}NS^{i}Layer-wiseSlsuperscriptğ‘†ğ‘™{S}^{l}SlÃ—Slâˆ’1Ã—Îºâ€‹(Sl)superscriptğ‘†ğ‘™superscriptğ‘†ğ‘™1ğœ…superscriptğ‘†ğ‘™{S}^{l}\times{S}^{l-1}\times\kappa({S}^{l})Subgraphâ€¡Sâ€‹Bğ‘†ğµSBSâ€‹BÃ—Îºâ€‹(Sâ€‹B)ğ‘†ğµğœ…ğ‘†ğµSB\times\kappa(SB)

Uses node sampler in(Zeng etÂ al.,2020)as an example

For neighbor sampling, the number of edges|â„°l|superscriptâ„°ğ‘™|\mathcal{E}^{l}|in each layer is decided by the size of target vertices|ğ’±t|superscriptğ’±ğ‘¡|\mathcal{V}^{t}|and sample sizeNâ€‹Slğ‘superscriptğ‘†ğ‘™NS^{l}. For layer-wise and subgraph sampling, we formulate the number of edges in layer|â„°l|superscriptâ„°ğ‘™|\mathcal{E}^{l}|as|â„¬l|Ã—|â„¬lâˆ’1|Ã—Îºâ€‹(|â„¬l|)superscriptâ„¬ğ‘™superscriptâ„¬ğ‘™1ğœ…superscriptâ„¬ğ‘™|\mathcal{B}^{l}|\times|\mathcal{B}^{l-1}|\times\kappa(|\mathcal{B}^{l}|)where|â„¬l|Ã—|â„¬lâˆ’1|superscriptâ„¬ğ‘™superscriptâ„¬ğ‘™1|\mathcal{B}^{l}|\times|\mathcal{B}^{l-1}|corresponds to the case all the sampled vertices in layerlğ‘™landlâˆ’1ğ‘™1l-1are connected, andÎºâ€‹(|â„¬l|)ğœ…superscriptâ„¬ğ‘™\kappa(|\mathcal{B}^{l}|)is a pre-trained function that estimates the graph sparsity based on sample size|â„¬l|superscriptâ„¬ğ‘™|\mathcal{B}^{l}|.

The feature update can be modelled as

Similar totcomputesubscriptğ‘¡computet_{\text{compute}}, we model thetupdatesubscriptğ‘¡updatet_{\text{update}}as (# of operations)/(# of PEsÃ—\timeskernel frequency). The numerator is the complexity of matrix multiplication, andmğ‘šmdenotes how many parallel MACs are instantiated in the update kernel.

Modelingtsamplingsubscriptğ‘¡samplingt_{\text{sampling}}: the mini-batch sampling is performed on the host processor, which can potentially be a performance bottleneck. We exploit multi-threading to sample multiple mini-batches concurrently. In the design phase, we estimatetsamplingsubscriptğ‘¡samplingt_{\text{sampling}}under various number of threads and determine the minimum number of threads that satisfiestsampling<tGNNsubscriptğ‘¡samplingsubscriptğ‘¡GNNt_{\text{sampling}}<t_{\text{GNN}}.

We set the hardware constraints to form the solution space for our DSE Engine. Among the various hardware resources on the FPGA platform, DSPs and LUTs are used the most as we increase the parallelism of the hardware modules. Thus, we model the usage of LUTs and DSPs as our constraints:

The coefficientsÎ»iâ€‹(1â©½iâ©½2)subscriptğœ†ğ‘–1ğ‘–2\lambda_{i}~{}(1\leqslant i\leqslant 2)andÏiâ€‹(1â©½iâ©½3)subscriptğœŒğ‘–1ğ‘–3\rho_{i}~{}(1\leqslant i\leqslant 3)are constants that indicate the resource consumption for each PE. In the case of DSPs, the utilization grows linearly as we instantiate more PEs; In the case of LUTs, an additionalnâ€‹logâ¡(n)ğ‘›ğ‘›n\log(n)(see Section4.2for the definition ofnğ‘›n) term is introduced. Thenâ€‹logâ¡(n)ğ‘›ğ‘›n\log(n)LUT overhead models the routing network in the aggregation kernel shown in Figure5.NDSPsubscriptğ‘DSP{N}_{\text{DSP}}andNLUTsubscriptğ‘LUT{N}_{\text{LUT}}denote the available DSPs and LUTs on the FPGA platform.

Many modern FPGAs consists of multiple dies(Chen
etÂ al.,2019), and the available resources may vary across dies. Thus, we perform DSE for each die to explore the optimal hardware configuration. We assume that each die is connected to one DDR channel (e.g. Xilinx Alveo U250) for simplicity in Algorithm4;
The DSE engine first constructs the search space by deriving the maximum value ofnğ‘›nandmğ‘šmseparately based on Equations (10) and (11). Then, the engine performs an exhaustive search through all the possible configurations. For each configuration, the engine evaluates its throughput using Equation4, and chooses the optimal design.

We use our framework to generate GNN training implementations on a CPU-FPGA heterogeneous platform, and compare the training throughput with CPU-only platform and CPU-GPU platform. We list the information of each platform in Table3. The CPU-only and CPU-GPU baseline are implemented using PyTorch-Geometric(Fey and Lenssen,2019)111https://github.com/pyg-team/pytorch_geometric/blob/master/examples/reddit.pyand GraphSAINT(Zeng etÂ al.,2020)222https://github.com/GraphSAINT/GraphSAINT.

PlatformsCPUAMD Ryzen 3990xGPUNvidia A100FPGAXilinx Alveo U250TechnologyTSMC 7 nmTSMC 7 nmTSMC 16 nmFrequency2.90 GHz1410 MHz300 MHzPeak Performance3.7 TFLOPS19.5 TFLOPS0.6 TFLOPSOn-chip Memory256 MB L3 cache40 MB L2 Cache54 MBMemory Bandwidth107 GB/s1555 GB/s77 GB/s

Samplers, Models and Datasets:
We generate mini-batch for GNN training using two sampling algorithms: (1) GraphSAGE neighbor sampler(Hamilton
etÂ al.,2017)for neighbor sampling (NS) and (2) GraphSAINT node sampler(Zeng etÂ al.,2020)for subgraph sampling (SS). For GraphSAGE neighbor sampler, we set the size of target vertices|ğ’±t|superscriptğ’±ğ‘¡|\mathcal{V}^{t}|as 1024, neighbor sampling sizeNâ€‹Sğ‘ğ‘†NSas 25 and 10 for 1-hop neighbors and 2-hop neighbors; for GraphSAINT-node sampler, we set the sampling budgetSâ€‹Bğ‘†ğµSBas 2750. We measure the GNN training throughput of two-layer GCN model and two-layer GraphSAGE model on four medium-scale graph datasets (Flickr(Zeng etÂ al.,2020), Reddit(Hamilton
etÂ al.,2017), Yelp(Zeng etÂ al.,2020)and AmazonProducts(Zeng etÂ al.,2020)) that fit in the FPGA local DDR memory. Details of the datasets and the GNN-layer dimensions are shown in Table4.

Dataset#Nodes#Edgesf0f1â€‹f2subscriptğ‘“0subscriptğ‘“1subscriptğ‘“2f_{0}\hskip 11.38092ptf_{1}\hskip 6.544pt\hskip 0.85355ptf_{2}Flickr (FL)89250899756500â€‹256â€‹75002567500\hskip 5.69046pt256\hskip 7.11317pt7Reddit (RD)23296511606919602â€‹256â€‹4160225641\hskip 1.42271pt602\hskip 5.69046pt256\hskip 5.69046pt41Yelp (YP)7168476977410300â€‹256â€‹100300256100300\hskip 5.69046pt256\hskip 4.26773pt100AmazonProducts (AP)1598960132169734200â€‹256â€‹107200256107200\hskip 5.69046pt256\hskip 4.26773pt107

We implement the program parser, DSE engine, software and hardware generator in our framework using Python, and the accelerator templates are implemented using Xilinx HLS. The host program template is programmed in OpenCL. Users interface with our framework using our APIs programmed in Python. To serve application developers, our framework includes several commonly used GNN models that can be used off-the-shelf. For ML researchers, our APIs allow users to define new models. In Listing2, we provide some examples of user inputs to specify platforms, mini-batch samplers and GNN models via our APIs. Based on the given inputs, our framework generates the host program and synthesizable accelerator design. For example, based on the GNN model specified by the user, user decides the parameters in the host program template, and what aggregation function should be filled in the HLS template to generate the accelerator design. Based on the platform parameters, our DSE engine fills in the hardware configurations such as the unroll factor in our HLS template. In Listing3, we provide an example that shows part of generated host program and synthesizable accelerator design.

ResourcesNS-GCNNS-GraphSAGESS-GCNSS-GraphSAGELUTs50%54%44%76%DSPs70%54%70%82%URAM34%34%14%20%BRAM28%28%30%34%(m,n)(256,4)(256,4)(256,4)(256,8)

In Table5we show the resource utilization of the implementations generated by our design. The numbernğ‘›nwhich dentoes the number of Scatter PEs and Gather PEs are restricted to power of 2, and number of MACsmğ‘šmis restricted to square of power of 2 due to the design of our accelerator.

We evaluate the two optimizations of our data layout and internal representation described in Section4.1on a two-layer neighbor sampling GCN. The two optimizations are: (1) reducing memory traffic (RMT) by reusing loaded vertex features in different edges that share the same source vertex; (2) reducing random access (RRA) by vertex renaming followed by edge sorting. We first measure the throughput of the baseline implementation with no optimizations, and then incrementally apply the two optimizations. As shown in Table6, both optimizations increase the GNN training throughput and can deliver up to 57% improvement in total.

Throughput (NVTPS)FLRDYPAPBaseline10.45 M12.98 M19.71 M23.17 MRMT11.98 M16.48 M22.39 M27.22 MRMT+RRA16.38 M18.50 M24.60 M29.27 MImprovement57%43%25%26%

DataCPUCPU-GPUCPU-FPGANS-GCNFL265.5K (1Ã—\times)2.69M (10.1Ã—\times)16.38M (61.7Ã—\times)RD85.65K (1Ã—\times)7.15M (83.5Ã—\times)18.50M (216Ã—\times)YP275.6K (1Ã—\times)9.36M (34.0Ã—\times)24.61M (89.2Ã—\times)AM480.6K (1Ã—\times)13.0M (29.0Ã—\times)29.26M (60.8Ã—\times)NS-SAGEFL225.2K (1Ã—\times)2.74M (12.2Ã—\times)11.84M (52.6Ã—\times)RD78.50K (1Ã—\times)6.90M (88.0Ã—\times)13.10M (166Ã—\times)YP266.0K (1Ã—\times)9.19M (34.5Ã—\times)18.12M (68.1Ã—\times)AM479.3K (1Ã—\times)13.57M (28.3Ã—\times)21.15M (44.1Ã—\times)SS-GCNFL215.2K (1Ã—\times)768.3K (3.59Ã—\times)2.81M (13.0Ã—\times)RD118.9K (1Ã—\times)536.4K (4.51Ã—\times)2.56M (21.5Ã—\times)YP159.1K (1Ã—\times)751.0K (4.71Ã—\times)3.08M (19.4Ã—\times)AM25.55K (1Ã—\times)OoM1.47M (57.5Ã—\times)SS-SAGEFL179.9K (1Ã—\times)626.7K (3.48Ã—\times)2.71M (15.1Ã—\times)RD94.72K (1Ã—\times)505.2K (5.33Ã—\times)2.43M (25.6Ã—\times)YP126.7K (1Ã—\times)709.7K (5.60Ã—\times)2.78M (22.0Ã—\times)AM17.40K (1Ã—\times)OoM1.45M (83.3Ã—\times)Average193.4K (1Ã—\times)4.96M (25.66Ã—\times)10.77M (55.67Ã—\times)

For evaluation, we use the throughput defined in Section5as metric, i.e. number of vertices traversed per second (NVTPS).
To measure the throughput, we count the total number of vertices in each mini-batch, and measure the average execution time of one training iteration. Table7shows the throughput comparison of GNN training across the three platforms. All three implementations use single precision floating point as data type.

Comparing with the CPU-only baseline, the CPU-GPU platform achieves25.66Ã—25.66\timesthroughput on the average. This is because CPU-GPU platform provides massive data parallelism with5.27Ã—5.27\timespeak performance and14.5Ã—14.5\timesmemory bandwidth compared with the CPU-only platform (Table3).
Comparing with the CPU-only baseline and CPU-GPU baseline, the CPU-FPGA implementation generated by our framework achieves55.67Ã—55.67\timesand2.17Ã—2.17\timesthroughput on the average respectively. Though CPU-GPU platform has higher memory bandwidth and peak performance than CPU-FPGA platform, the throughput is limited by the memory access overhead during aggregation stage. While our accelerator can access the on-chip memory in one cycle (3.3 ns), CPU and GPU need to access the data in multi-level caches. Taking AMD Ryzen 3990 as an example, the L2 cache latency is 5 to 12 ns, and the L3 cache latency is around 32 ns.
Moreover, as shown in Table6, our data layout and internal representation also improves the training throughput by reducing the memory traffic and reducing random memory accesses.

We compare our results with two state-of-the-art GNN training implementations: GraphACT(Zeng and Prasanna,2020)and Rubik(Chen etÂ al.,2021b). As shown in Table8, our framework achieves up to4.45Ã—4.45\timesand3.4Ã—3.4\timesthroughput respectively. Compared with GraphACT, the achieved speedup is due to (1) the vertex features are fetched directly from the FPGA local memory, (2) the proposed aggregate kernal has higher computation parallelism compared with Feature Aggregation Module in GraphACT. Compared with ASIC design Rubik, the obtained speedup is due to (1) larger on-chip memory of FPGA that can fully store the intermediate results under the setting of the experiments, (2) our proposed data layout optimizations that reduce the external memory traffic and random memory accesses.

GraphACT(Zeng and Prasanna,2020)â€¡â€¡\ddaggerRubik(Chen etÂ al.,2021b)This workPlatformDeviceAlveo U250ASICAlveo U250Peak Perf.0.6 TFLOPS1 TFLOPS0.6 TFLOPSBandwidth77 GB/s432 GB/s77 GB/sOn-chip Mem.54 MB2 MB54 MBSS-SAGE(Throughput)RD546.8K (1Ã—\times)717.0K (1.31Ã—\times)2.43M (4.45Ã—\times)YP769.8K (1Ã—\times)N/A2.78M (3.61Ã—\times)

Scaled from U200 to U250 using the number of DSPs.

We discuss the novelty of this work compared with previous work GraphACT(Zeng and Prasanna,2020)and the applicability of proposed optimizations to other platforms (e.g., CPU, GPU).

Comparison with GraphACT. In GraphACT(Zeng and Prasanna,2020), the redundancy reduction requires that all the edges have uniform weight value. Therefore, it can not support GCN(Kipf and Welling,2017). In constrast, the proposed optimizations such as RMT and RRA do not have requirements on the weight, therefore, can support broader range of GNN models. Moreover, the Feature Aggregation Module in GraphACT has limited computation parallelism in feature-level that limits its performance for neighbor-sampling-based GNN training. In comparison, the proposed aggregate kernel adopts the scatter-gather paradigm with routing network, which can enable massive computation parallelism within feature aggregation.

Optimizations on CPU/GPU platforms. In this work, we proposed a highly optimized aggregate kernel that adopts the scatter-gather paradigm to accelerate feature aggregation. While the scatter phase is optimized by our proposed data layout optimizaiton (Section4.1), the performance of gather phase depends on the routing network of aggregate kernel (Section4.2) that efficiently routes the intermediate result from Scatter PEs to Gather PEs. On CPU/GPU platforms, the data layout optimization can potentially be adopted to optimize the scatter phase. However, the gather phase is hard to be optimized on CPU/GPU since the data communication among the computation cores is through a complex cache hierarchy. Since the scatter phase and gather phase need be optimized simultaneously, the proposed optimizations may lead to limited performance improvement on CPU/GPU platforms.

In this paper, we proposed HP-GNN, a general framework to generate high-throughput GNN training implementation on a given CPU-FPGA heterogeneous platform. Based on the high-level abstraction of GNN computation, we designed a host program template and hardware templates to support various GNN models. Our proposed data layout and internal representation improve the throughput of GNN training. The implementations generated by HP-GNN achieve55.67Ã—55.67\timesand2.17Ã—2.17\timesthroughput compared with state-of-the-art CPU-only and CPU-GPU platforms. Compared with state-of-art accelerators, our framework achieves up to4.45Ã—4.45\timesthroughput. In the future, we plan to extend our framework to multi-FPGA platforms by exploiting model parallelism.

[å›¾ç‰‡: images\image_8.png]

[å›¾ç‰‡: images\image_9.png]

