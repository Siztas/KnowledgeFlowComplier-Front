æ ‡é¢˜ï¼šOn the Computational Benefit of Multimodal Learning

Human perception inherently operates in a multimodal manner. Similarly, as machines interpret the empirical world, their learning processes ought to be multimodal. The recent, remarkable successes in empirical multimodal learning underscore the significance of understanding this paradigm. Yet, a solid theoretical foundation for multimodal learning has eluded the field for some time. While a recent study by[13]has shown the superior sample complexity of multimodal learning compared to its unimodal counterpart, another basic question remains: does multimodal learning also offer computational advantages over unimodal learning? This work initiates a study on the computational benefit of multimodal learning. We demonstrate that, under certain conditions, multimodal learning can outpace unimodal learning exponentially in terms of computation. Specifically, we present a learning task that is NP-hard for unimodal learning but is solvable in polynomial time by a multimodal algorithm. Our construction is based on a novel modification to the intersection of two half-spaces
problem.

At the heart of human perception lies multimodality. This capability enables us to perceive and interrelate different facets of the same empirical object. Itâ€™s particularly important during the infantile stage of human development, where it helps unify disparate symbols, fostering comprehensive cognition as a foundation for adulthood. The analogy of raising a child in a â€room of textâ€ alone highlights the limitations of a unimodal approach; itâ€™s bound to be counterproductive.

In the realm of machine learning, multimodality plays a role analogous to its significance in human cognition. Here, we view machine learning as the machineâ€™s process of perception. Multimodal learning entails accumulating vast amounts of training data across various modalities and subsequently deploying the trained model to handle new unimodal tasks. This learning progression mirrors the transition from infancy to adulthood in humans. Empirical studies have consistently shown that models trained using multiple modalities often surpass finely-tuned unimodal models, even when evaluated on new unimodal data.

In spite of notable empirical successes, like Gato[17]and GPT-4[15], the theoretical explanations of multimodal learning remain relatively underexplored. Thus, establishing a solid theoretical foundation becomes imperative.

A recent study by[13]set the stage for a broader understanding of the statistical advantages of multimodal learning. The research showed that multimodal learning achieves superior generalization bounds compared to unimodal learning, especially when the data exhibits both connection and heterogeneity. However, the question arose: does multimodal learning also present computational advantages?

Our work provides an affirmative answer. We show a computational separation between multimodal and unimodal learning. Specifically, we introduce a learning task, rooted in the intersection of two half-spaces problem, which poses an NP-hard challenge for any unimodal learning algorithm. Yet, this very task yields to a polynomial solution under a multimodal learning paradigm. This dichotomy demonstrates the potential exponential computational advantage of multimodal learning over its unimodal counterpart. Coupled with the statistical insights from[13], our findings further illuminate the vast potential of multimodal learning.

Theoretical Multimodal Learning: despite the empirical success of multimodal learning, a cohesive theoretical foundation was long missing in this area. Most existing theoretical findings are bound by specific assumptions and contexts. For instance, studies such as[23,1,7,19]navigate multimodal learning within a multi-view framework, operating under the assumption that individual modalities are, in isolation, adequate for predictions.[20,11]delve into algorithms pivoting on information-theoretical relationships across modalities.[18]consider the specific problem of the benefit of contrastive loss in multimodal learning with a linear
data-generating model.[9]studies the generalization ability of multimodal learning in estimating the latent space representation.

A recent work[13]proposes a broad-based theory on the statistical guarantee of multimodal learning. They prove that multimodal learning admits anOâ€‹(m)ğ‘‚ğ‘šO(\sqrt{m})improvement in generalization error over unimodal learning. This is achieved by dissecting the learning of the composition of two hypotheses, where the sum of complexities of the hypotheses is markedly smaller than that of their composition. Additionally, they pinpoint connection and heterogeneity amidst modalities as the two pivotal elements propelling these statistical advantages of multimodal learning.

Empirical Multimodal Learning: applications of multimodal learning can be traced back to the last century, aiming at combining vision and audio data to improve the performance of speech recognition[22,14]. As the field evolved, multimodal learning carved a niche in multimedia, enhancing capabilities in indexing and search functionalities[6,12].

Recently, there is a trend in applying multimodal learning in deep learning practices, including modality generation[3,8,16]and large-scale generalist models[17,15]. A consistently observed empirical phenomenon is that a multimodal model is able to outperform a finely-tuned unimodal model, even on unimodal population data.

In this section, we delineate the setup of multimodal learning and essential background on the intersection of two half-spaces problem.

In this paper, we restrict our focus to the fundamental, yet non-trivial, scenario of two modalities for a clear exposition, adopting the setup of[13]. Formally, the multimodal learning classification framework encompasses two modalities, denoted asğ’³,ğ’´âŠ‚â„nğ’³ğ’´superscriptâ„ğ‘›\mathcal{X},\mathcal{Y}\subset\mathbb{R}^{n}, and a label spaceğ’µ={Â±}ğ’µplus-or-minus\mathcal{Z}=\{\pm\}. Consequently, every data point can be represented as a tuple(x,y,z)ğ‘¥ğ‘¦ğ‘§(x,y,z).

Given a hypothesis classâ„‹â„‹\mathcal{H}and a training dataset(X,Y,Z)ğ‘‹ğ‘Œğ‘(X,Y,Z)withmğ‘šmdata points(xi,yi,zi)subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–subscriptğ‘§ğ‘–(x_{i},y_{i},z_{i}), our aim in (proper) learning from(X,Y,Z)ğ‘‹ğ‘Œğ‘(X,Y,Z)is to output a hypothesishâˆˆâ„‹â„â„‹h\in\mathcal{H}, that minimizes the empirical risk:

When each data point(x,y,z)ğ‘¥ğ‘¦ğ‘§(x,y,z)adheres to a specific data distributionDğ·Dover(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z}), the goal of (properly) PAC-learning(X,Y,Z)ğ‘‹ğ‘Œğ‘(X,Y,Z)is to output a hypothesishâˆˆâ„‹â„â„‹h\in\mathcal{H}, such that the population risk

is small with high probability. In addition, we mandate a bijective mapping betweenx,yğ‘¥ğ‘¦x,yfor any potential data point(x,y,z)ğ‘¥ğ‘¦ğ‘§(x,y,z).

For brevity, we occasionally write(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z})for short to denote the learning problem, when it is clear from the context. The unimodal learning problems(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z})and(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})can be defined in a similar way, in which the labelyğ‘¦yorxğ‘¥xis masked. In learning(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z}), we are given a hypothesis classâ„‹â„‹\mathcal{H}and a set(X,Z)ğ‘‹ğ‘(X,Z)of training data withmğ‘šmdata points(xi,zi)subscriptğ‘¥ğ‘–subscriptğ‘§ğ‘–(x_{i},z_{i}). The empirical risk and the population risk are defined as follows respectively.

In our quest to demonstrate a computational separation between multimodal and unimodal learning, we sought to architect a specific learning challenge that presents as NP-hard for unimodal learning, but for which an efficient multimodal solution exists.

A candidate of such problem is the â€™intersection of two half-spaces,â€™ formally defined below:

An instance ofğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}is a set of points inâ„nsuperscriptâ„ğ‘›\mathbb{R}^{n}each labeled either â€˜+â€™ or â€˜-â€™ and the goal is to find an intersection of two half-spaces which correctly classifies the maximum number of points, where a â€˜+â€™ point is classified correctly if it lies inside the intersection and a â€˜-â€™ point is classified correctly if it lies outside of it.

Previous work has shown that PAC-learning this intersection is inherently NP-hard, even in the realizable setting, encapsulated in the following result:

Letâ„“â„“\ellbe any fixed integer andÏµ>0italic-Ïµ0\epsilon>0be an
arbitrarily small constant. Then, given a set of labeled points
inâ„nsuperscriptâ„ğ‘›\mathbb{R}^{n}with a guarantee that there is an intersection of two half-spaces that classifies all the points correctly, there is no polynomial time algorithm to find a functionfğ‘“fof up toâ„“â„“\elllinear threshold functions that classifies12+Ïµ12italic-Ïµ\frac{1}{2}+\epsilonfraction of points correctly, unless NP = RP.

A slightly weaker version of the above result which will be of use is the following:

LetÏµ>0italic-Ïµ0\epsilon>0be an arbitrarily small constant. Then, given a set of labeled points inâ„nsuperscriptâ„ğ‘›\mathbb{R}^{n}with a guarantee that there is an intersection of two half-spaces that classifies all the points correctly, there is no polynomial time algorithm to find a functionfğ‘“fof an intersection of two half-spaces that classifies12+Ïµ12italic-Ïµ\frac{1}{2}+\epsilonfraction of points correctly, unless NP = RP.

Itâ€™s clear Proposition3is a direct consequence of Theorem2, given that the intersection of two half-spaces naturally translates toâ„“â„“\elllinear threshold functions withâ„“=2â„“2\ell=2. Through out this paper we will only consider the case of proper learning with our hypothesis class including only intersections of two half-spaces.

To demonstrate the computational benefit of multimodal learning, we present an instance in which both unimodal learning problems(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z})and(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})are NP-hard, while the multimodal learning problem(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z})can be solved efficiently. In particular, we require the existence of a bijective mappingf:ğ’³â†’ğ’´:ğ‘“â†’ğ’³ğ’´f:\mathcal{X}\to\mathcal{Y}satisfyingy=fâ€‹(x)ğ‘¦ğ‘“ğ‘¥y=f(x)for any data point(x,y,z)âˆˆ(ğ’³,ğ’´,ğ’µ)ğ‘¥ğ‘¦ğ‘§ğ’³ğ’´ğ’µ(x,y,z)\in(\mathcal{X},\mathcal{Y},\mathcal{Z}), so that the hardness result is purely computational. The task of constructing such an instance can be decomposed into three steps

We start by setting(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z})as a NP-hard problem, in this case, an instance ofğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}.

Based on(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z}), we construct a bijective mapping betweenx,yğ‘¥ğ‘¦x,y, to obtain a new NP-hard problem(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})by preserving theğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}structure.

The bijective mapping should be designed carefully, such that the multimodal problem(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z})can be solved efficiently.

Below we describe the construction of the instance and the main idea behind. A detailed proof is provided in the next section.

Step 1:We set one of the unimodal learning problem, say(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z}), as an instance ofğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}. We denote any problem ofğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}byH1âˆ©H2subscriptğ»1subscriptğ»2H_{1}\cap H_{2}with halfspacesH1,H2subscriptğ»1subscriptğ»2H_{1},H_{2}inâ„nsuperscriptâ„ğ‘›\mathbb{R}^{n}, where eachHi=(x|riâŠ¤â€‹xâ‰¤ci)subscriptğ»ğ‘–conditionalğ‘¥superscriptsubscriptğ‘Ÿğ‘–topğ‘¥subscriptğ‘ğ‘–H_{i}=(x|r_{i}^{\top}x\leq c_{i})is determined by the unit vectorrisubscriptğ‘Ÿğ‘–r_{i}andciâˆˆâ„subscriptğ‘ğ‘–â„c_{i}\in\mathbb{R}.

Step 2:A critical observation is that, anyğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}problemH1âˆ©H2subscriptğ»1subscriptğ»2H_{1}\cap H_{2}can be transformed into a newğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}problem by applying a coordinate change, under which eachxğ‘¥xis mapped to a new point with the correspondingzğ‘§zremaining the same. DenoteQâˆˆâ„nÃ—nğ‘„superscriptâ„ğ‘›ğ‘›Q\in\mathbb{R}^{n\times n}as any orthogonal matrix, we obtainH^1âˆ©H^2subscript^ğ»1subscript^ğ»2\hat{H}_{1}\cap\hat{H}_{2}whereH^i=(x|r^iâŠ¤â€‹xâ‰¤ci)subscript^ğ»ğ‘–conditionalğ‘¥superscriptsubscript^ğ‘Ÿğ‘–topğ‘¥subscriptğ‘ğ‘–\hat{H}_{i}=(x|\hat{r}_{i}^{\top}x\leq c_{i})by settingr^i=Qâ€‹risubscript^ğ‘Ÿğ‘–ğ‘„subscriptğ‘Ÿğ‘–\hat{r}_{i}=Qr_{i}. Lety=Qâ€‹xğ‘¦ğ‘„ğ‘¥y=Qx, we create a new NP-hard unimodal problem(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z}), asQğ‘„Qdefines a bijective mapping from the set of allğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}problems to itself.

Step 3:It remains unclear how the multimodal problem(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z})can be easy to learn. Our strategy is to design a specialQğ‘„Qfor eachH1âˆ©H2subscriptğ»1subscriptğ»2H_{1}\cap H_{2}, by encoding the information ofH1âˆ©H2subscriptğ»1subscriptğ»2H_{1}\cap H_{2}into the transformationQğ‘„Q. Ideally, withnğ‘›nlinearly-independentxisubscriptğ‘¥ğ‘–x_{i}, we can recover the matrixQğ‘„Qby basic linear algebra. With the exact values ofr1,r2subscriptğ‘Ÿ1subscriptğ‘Ÿ2r_{1},r_{2}in hand, we getc1,c2subscriptğ‘1subscriptğ‘2c_{1},c_{2}by listing the distances from allxğ‘¥xto the hyperplaneriâŠ¤â€‹x=0superscriptsubscriptğ‘Ÿğ‘–topğ‘¥0r_{i}^{\top}x=0inOâ€‹(mâ€‹n2)ğ‘‚ğ‘šsuperscriptğ‘›2O(mn^{2})time. The obtained classifier achieves zero loss on the training data.

However, itâ€™s challenging to directly encode the vectorsr1,r2subscriptğ‘Ÿ1subscriptğ‘Ÿ2r_{1},r_{2}into thenÃ—nğ‘›ğ‘›n\times nmatrixQğ‘„Q. There are two main obstacles. First, how to encode the information ofr1,r2subscriptğ‘Ÿ1subscriptğ‘Ÿ2r_{1},r_{2}is unclear:Qğ‘„Qis under the constraint of an orthogonal matrix, which might be violated by simply fillingr1,r2subscriptğ‘Ÿ1subscriptğ‘Ÿ2r_{1},r_{2}intoQğ‘„Q. Using more complicated techniques of encoding may bring other concerns such as the existence of a closed-form representation or whether decoding can be done efficiently. Second, the quality of such encoding is questionable: even if we find a way to encoder1,r2subscriptğ‘Ÿ1subscriptğ‘Ÿ2r_{1},r_{2}intoQğ‘„Q, we still need to make sure(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})exhausts the set of all possibleğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}instances. Otherwise although each(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})problem is anğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}instance, the set of all possible(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})problems is a merely a subset ofğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}, preventing us from directly applying the NP-hardness result.

Fortunately, we have a very simple remedy: enlarging the dimensionnğ‘›nby twice, then using the firstnğ‘›ncoordinates forğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}while the latter2â€‹n2ğ‘›2ncoordinates to encode the information ofğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}. Roughly speaking, we create2â€‹n2ğ‘›2nnull coordinates with no effect on theğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}problem, while they carry the information ofğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}which can only be retrived by knowing bothx,yğ‘¥ğ‘¦x,y. In particular, for anyğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}problemH1âˆ©H2subscriptğ»1subscriptğ»2H_{1}\cap H_{2}, we setQğ‘„Qas

The vectorsr1,r2subscriptğ‘Ÿ1subscriptğ‘Ÿ2r_{1},r_{2}are simply flattened and set as the first column of the second block. Since the norm of this column is 1,Qğ‘„Qcan be easily made feasible. The identity matrixInsubscriptğ¼ğ‘›I_{n}ensures(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})exhausts the set of all possibleğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}instances. The main result of this paper is given by the following theorem.

There exists a multimodal learning problem(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z})which is PAC-learnable in polynomial time, while both unimodal learning problems(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z}),(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})are NP-hard, even if there is a bijective mappingf:ğ’³â†’ğ’´:ğ‘“â†’ğ’³ğ’´f:\mathcal{X}\to\mathcal{Y}such thaty=fâ€‹(x),âˆ€(x,y,z)âˆ¼(ğ’³,ğ’´,ğ’µ)formulae-sequenceğ‘¦ğ‘“ğ‘¥similar-tofor-allğ‘¥ğ‘¦ğ‘§ğ’³ğ’´ğ’µy=f(x),\forall(x,y,z)\sim(\mathcal{X},\mathcal{Y},\mathcal{Z}).

Theorem4demonstrates that multimodal learning solves some learning tasks exponentially faster than unimodal learning. Such exponential separation explains the empirical superiority of multimodal learning from the perspective of computation, supplementing the statistical guatantees in[13].

Notably, the two pivotal factors leading to the statistical benefit of multimodal learning in[13], namely connection and heterogeneity, are also evident in our construction. In particular, the mappingQğ‘„Qbetweenğ’³,ğ’´ğ’³ğ’´\mathcal{X},\mathcal{Y}is bijective, meaning there exists a perfect connection between both modalities. On the other hand,ğ’³,ğ’´ğ’³ğ’´\mathcal{X},\mathcal{Y}carry different information about the problem, which is useless alone but effective when put together, indicating s strong heterogeneity.

We first introduce the necessary ingredients for the construction of the learning problem. For each pair of unit vectorsv1,v2âˆˆâ„nsubscriptğ‘£1subscriptğ‘£2superscriptâ„ğ‘›v_{1},v_{2}\in\mathbb{R}^{n}, there exist orthogonal matrices inâ„2â€‹nsuperscriptâ„2ğ‘›\mathbb{R}^{2n}with its first column to be(v12,v12)subscriptğ‘£12subscriptğ‘£12(\frac{v_{1}}{\sqrt{2}},\frac{v_{1}}{\sqrt{2}})sinceâ€–(v12,v12)â€–2=1subscriptnormsubscriptğ‘£12subscriptğ‘£1221\|(\frac{v_{1}}{\sqrt{2}},\frac{v_{1}}{\sqrt{2}})\|_{2}=1. In particular, for each pairv1,v2subscriptğ‘£1subscriptğ‘£2v_{1},v_{2}we fix one such orthogonal matrixFğ¹F, defining a functionFâ€‹(v1,v2):â„2â€‹nâ†’â„2â€‹nÃ—2â€‹n:ğ¹subscriptğ‘£1subscriptğ‘£2â†’superscriptâ„2ğ‘›superscriptâ„2ğ‘›2ğ‘›F(v_{1},v_{2}):\mathbb{R}^{2n}\to\mathbb{R}^{2n\times 2n}as below:

In addition, we define an orthogonal transformation matrixQâ€‹(v1,v2)âˆˆâ„3â€‹nÃ—3â€‹nğ‘„subscriptğ‘£1subscriptğ‘£2superscriptâ„3ğ‘›3ğ‘›Q(v_{1},v_{2})\in\mathbb{R}^{3n\times 3n}as

The matrixQâ€‹(r1,r2)ğ‘„subscriptğ‘Ÿ1subscriptğ‘Ÿ2Q(r_{1},r_{2})will serve as a fingerprint of anğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}problemH1âˆ©H2subscriptğ»1subscriptğ»2H_{1}\cap H_{2}. We also define a variant of the intersection of two half-spaces problem.

An instance ofğˆğ§ğ­ğ‡ğ’Î»subscriptğˆğ§ğ­ğ‡ğ’ğœ†\mathbf{IntHS_{\lambda}}is a set of points inâ„nsuperscriptâ„ğ‘›\mathbb{R}^{n}each labeled either â€˜+â€™ or â€˜-â€™, in which the labels only depend on the firstÎ»â€‹nğœ†ğ‘›\lambda ncoordinates whereÎ»âˆˆ(0,1)ğœ†01\lambda\in(0,1)is a constant. The goal is to find an intersection of two half-spaces which correctly classifies the maximum number of points, where a â€˜+â€™ point is classified correctly if it lies inside the intersection and a â€˜-â€™ point is classified correctly if it lies outside of it.

For every constantÎ»>0ğœ†0\lambda>0, learningğˆğ§ğ­ğ‡ğ’Î»subscriptğˆğ§ğ­ğ‡ğ’ğœ†\mathbf{IntHS_{\lambda}}is NP-hard.

We prove by reduction. Suppose for contradictionğˆğ§ğ­ğ‡ğ’Î»subscriptğˆğ§ğ­ğ‡ğ’ğœ†\mathbf{IntHS_{\lambda}}can be learnt in polynomial time, then for each instance ofğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}, we can create a new instance ofğˆğ§ğ­ğ‡ğ’Î»subscriptğˆğ§ğ­ğ‡ğ’ğœ†\mathbf{IntHS_{\lambda}}with dimensionnÎ»ğ‘›ğœ†\frac{n}{\lambda}by extension. In particular, each pointxâˆˆâ„nÎ»ğ‘¥superscriptâ„ğ‘›ğœ†x\in\mathbb{R}^{\frac{n}{\lambda}}shares the same label asx[1:n]subscriptğ‘¥delimited-[]:1ğ‘›x_{[1:n]}in the originalğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}instance. As a result, any classifier ofğˆğ§ğ­ğ‡ğ’Î»subscriptğˆğ§ğ­ğ‡ğ’ğœ†\mathbf{IntHS_{\lambda}}applies to theğˆğ§ğ­ğ‡ğ’ğˆğ§ğ­ğ‡ğ’\mathbf{IntHS}problem with the same accuracy, contradicting Proposition3.
âˆ

Now we are ready to state the learning problem(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z}):mğ‘šmdata points(xi,yi,zi)subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–subscriptğ‘§ğ‘–(x_{i},y_{i},z_{i})are given, wherexi,yiâˆˆâ„3â€‹nsubscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–superscriptâ„3ğ‘›x_{i},y_{i}\in\mathbb{R}^{3n}represent the two modalities andzi=Â±subscriptğ‘§ğ‘–plus-or-minusz_{i}=\pmis the label. Itâ€™s guaranteed that there is an intersection of two half-spaces that classifies all the points correctly, with supports of the defining unit vectors being the firstnğ‘›ncoordinates. In other words, itâ€™s a realizable instance ofğˆğ§ğ­ğ‡ğ’ğŸğŸ‘subscriptğˆğ§ğ­ğ‡ğ’13\mathbf{IntHS_{\frac{1}{3}}}.

In particular, there are unit vectorsr1,r2âˆˆâ„nsubscriptğ‘Ÿ1subscriptğ‘Ÿ2superscriptâ„ğ‘›r_{1},r_{2}\in\mathbb{R}^{n}and constantsc1,c2âˆˆâ„subscriptğ‘1subscriptğ‘2â„c_{1},c_{2}\in\mathbb{R}(unknown to the learner), such that all pairs(xi,zi)subscriptğ‘¥ğ‘–subscriptğ‘§ğ‘–(x_{i},z_{i})can be perfectly classified byH^1âˆ©H^2subscript^ğ»1subscript^ğ»2\hat{H}_{1}\cap\hat{H}_{2}, whereH^i=(x|r^iâŠ¤â€‹xâ‰¤ci)subscript^ğ»ğ‘–conditionalğ‘¥superscriptsubscript^ğ‘Ÿğ‘–topğ‘¥subscriptğ‘ğ‘–\hat{H}_{i}=(x|\hat{r}_{i}^{\top}x\leq c_{i})andr^i=(ri,ğŸ2â€‹n)subscript^ğ‘Ÿğ‘–subscriptğ‘Ÿğ‘–subscript02ğ‘›\hat{r}_{i}=(r_{i},\mathbf{0}_{2n}). Meanwhile,yi=Qâ€‹(r1,r2)â€‹xisubscriptğ‘¦ğ‘–ğ‘„subscriptğ‘Ÿ1subscriptğ‘Ÿ2subscriptğ‘¥ğ‘–y_{i}=Q(r_{1},r_{2})x_{i}holds for all data points, and all pairs(yi,zi)subscriptğ‘¦ğ‘–subscriptğ‘§ğ‘–(y_{i},z_{i})can be perfectly classified byH~1âˆ©H~2subscript~ğ»1subscript~ğ»2\tilde{H}_{1}\cap\tilde{H}_{2}, whereH~i=(x|r~iâŠ¤â€‹xâ‰¤ci)subscript~ğ»ğ‘–conditionalğ‘¥superscriptsubscript~ğ‘Ÿğ‘–topğ‘¥subscriptğ‘ğ‘–\tilde{H}_{i}=(x|\tilde{r}_{i}^{\top}x\leq c_{i})andr~i=Qâ€‹(r1,r2)â€‹(ri,ğŸ2â€‹n)subscript~ğ‘Ÿğ‘–ğ‘„subscriptğ‘Ÿ1subscriptğ‘Ÿ2subscriptğ‘Ÿğ‘–subscript02ğ‘›\tilde{r}_{i}=Q(r_{1},r_{2})(r_{i},\mathbf{0}_{2n}).

Define the hypothesis setğ’®ğ’®\mathcal{S}as

which is exactly the set of all intersection of two half-spaces. We have the following results.

Properly learning(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z})withğ’®ğ’®\mathcal{S}is NP-hard.

It is a direct consequence of Lemma6, noticing that(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z})is anğˆğ§ğ­ğ‡ğ’ğŸğŸ‘subscriptğˆğ§ğ­ğ‡ğ’13\mathbf{IntHS_{\frac{1}{3}}}instance.
âˆ

Properly learning(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})withğ’®ğ’®\mathcal{S}is NP-hard.

Although(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})is also anğˆğ§ğ­ğ‡ğ’ğŸğŸ‘subscriptğˆğ§ğ­ğ‡ğ’13\mathbf{IntHS_{\frac{1}{3}}}instance, we still need to verify that(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})exhausts all possibleğˆğ§ğ­ğ‡ğ’ğŸğŸ‘subscriptğˆğ§ğ­ğ‡ğ’13\mathbf{IntHS_{\frac{1}{3}}}instances (otherwise we canâ€™t apply Lemma6, for example when all(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})obey the sameğˆğ§ğ­ğ‡ğ’ğŸğŸ‘subscriptğˆğ§ğ­ğ‡ğ’13\mathbf{IntHS_{\frac{1}{3}}}instance). Notice thatQğ‘„Qinduces a mappingH1âˆ©H2â†’H1âˆ©H2â†’subscriptğ»1subscriptğ»2subscriptğ»1subscriptğ»2H_{1}\cap H_{2}\to H_{1}\cap H_{2}, and itâ€™s equivalent to proving it is a surjective mapping. For anyğˆğ§ğ­ğ‡ğ’ğŸğŸ‘subscriptğˆğ§ğ­ğ‡ğ’13\mathbf{IntHS_{\frac{1}{3}}}instanceH^1âˆ©H^2subscript^ğ»1subscript^ğ»2\hat{H}_{1}\cap\hat{H}_{2}whereH^i=(x|r^iâŠ¤â€‹xâ‰¤ci)subscript^ğ»ğ‘–conditionalğ‘¥superscriptsubscript^ğ‘Ÿğ‘–topğ‘¥subscriptğ‘ğ‘–\hat{H}_{i}=(x|\hat{r}_{i}^{\top}x\leq c_{i})andr^i=(ri,ğŸ2â€‹n)subscript^ğ‘Ÿğ‘–subscriptğ‘Ÿğ‘–subscript02ğ‘›\hat{r}_{i}=(r_{i},\mathbf{0}_{2n}), becauser^isubscript^ğ‘Ÿğ‘–\hat{r}_{i}also has support in the firstnğ‘›ncoordinates, we have thatr^i=Qâ€‹(r1,r2)â€‹risubscript^ğ‘Ÿğ‘–ğ‘„subscriptğ‘Ÿ1subscriptğ‘Ÿ2subscriptğ‘Ÿğ‘–\hat{r}_{i}=Q(r_{1},r_{2})r_{i}withri=r^isubscriptğ‘Ÿğ‘–subscript^ğ‘Ÿğ‘–r_{i}=\hat{r}_{i}, proving the mapping is surjective.
âˆ

Assumemâ‰¥3â€‹nğ‘š3ğ‘›m\geq 3n,(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z})is properly learnable withğ’®ğ’®\mathcal{S}(applied toxğ‘¥xonly) inOâ€‹(mâ€‹n2)ğ‘‚ğ‘šsuperscriptğ‘›2O(mn^{2})time, when there exist3â€‹n3ğ‘›3ndata points with linearly-independentxisubscriptğ‘¥ğ‘–x_{i}.

Consider the simple algorithm1which consists three steps:

find a setSğ‘†Sof linearly-independentxisubscriptğ‘¥ğ‘–x_{i}(line 2-6).

findQğ‘„Qby solving a linear system ofSğ‘†S(line 7-8).

rankxisubscriptğ‘¥ğ‘–x_{i}along the directions ofr1,r2subscriptğ‘Ÿ1subscriptğ‘Ÿ2r_{1},r_{2}to getc1,c2subscriptğ‘1subscriptğ‘2c_{1},c_{2}(line 9-10).

Step 1 runs inOâ€‹(mâ€‹n2)ğ‘‚ğ‘šsuperscriptğ‘›2O(mn^{2})time, since testing orthogonality between two points runs inOâ€‹(n)ğ‘‚ğ‘›O(n)time and|S|=Oâ€‹(n)ğ‘†ğ‘‚ğ‘›|S|=O(n). Step 2 runs inOâ€‹(n3)ğ‘‚superscriptğ‘›3O(n^{3})time which is the complexity of solving a system of linear equations. Step 3 runs inOâ€‹(mâ€‹n)ğ‘‚ğ‘šğ‘›O(mn)time. Under our assumptionmâ‰¥3â€‹nğ‘š3ğ‘›m\geq 3n, the total running time isOâ€‹(mâ€‹n2+n3+mâ€‹n)=Oâ€‹(mâ€‹n2)ğ‘‚ğ‘šsuperscriptğ‘›2superscriptğ‘›3ğ‘šğ‘›ğ‘‚ğ‘šsuperscriptğ‘›2O(mn^{2}+n^{3}+mn)=O(mn^{2}).

We still need to verify the found classifierhâ€‹(x)â„ğ‘¥h(x):

does classify all data points correctly. By the construction ofQğ‘„Q, we know there is a classifierhâˆ—â€‹(x)superscriptâ„ğ‘¥h^{*}(x)which classifies all data points correctly, which shares the samerisubscriptğ‘Ÿğ‘–r_{i}withhâ€‹(x)â„ğ‘¥h(x):

By the choice ofc1,c2subscriptğ‘1subscriptğ‘2c_{1},c_{2}, we have thatc1â‰¤c1âˆ—,c2â‰¤c2âˆ—formulae-sequencesubscriptğ‘1subscriptsuperscriptğ‘1subscriptğ‘2subscriptsuperscriptğ‘2c_{1}\leq c^{*}_{1},c_{2}\leq c^{*}_{2}. Denoteh+={xâˆˆâ„3â€‹n,hâ€‹(x)=+}subscriptâ„formulae-sequenceğ‘¥superscriptâ„3ğ‘›â„ğ‘¥h_{+}=\{x\in\mathbb{R}^{3n},h(x)=+\}, we have that

by the facth+âŠ‚h+âˆ—subscriptâ„subscriptsuperscriptâ„h_{+}\subset h^{*}_{+}. Meanwhile, by the construction ofhâ€‹(x)â„ğ‘¥h(x), we have thatX+âŠ‚h+subscriptğ‘‹subscriptâ„X_{+}\subset h_{+}, and further

As a result,X+=h+âˆ©Xsubscriptğ‘‹subscriptâ„ğ‘‹X_{+}=h_{+}\cap Xwhich meanshâ€‹(x)â„ğ‘¥h(x)does classify all data points correctly.
âˆ

Lemma9concerns only the learnability on the training data, to extend this result to PAC-learnability we introduce the following definition.

A data distributionDğ·Don(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z})is called non-degenerate, if

Most distributions whose support has non-zero measure are non-degenerate, including common uniform and Gaussian distributions. We have the following result for PAC-learnability.

Assumemğ‘šmdata points are sampled from a non-degenerate distributionDğ·Dandmâ‰¥3â€‹nğ‘š3ğ‘›m\geq 3n,(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z})is properly PAC-learnable withğ’®ğ’®\mathcal{S}(applied toxğ‘¥xonly) inOâ€‹(mâ€‹n2)ğ‘‚ğ‘šsuperscriptğ‘›2O(mn^{2})time. In particular, with probability at least1âˆ’Î´1ğ›¿1-\delta, the generalization errorÏµitalic-Ïµ\epsilonof algorithm1is upper bounded by

By the assumption thatDğ·Dis non-degenerate, we have that with probability 1, there exist3â€‹n3ğ‘›3ndata points with linearly-independentxisubscriptğ‘¥ğ‘–x_{i}. By the conclusion of Lemma9, the learnt classifier achieves zero loss on training data.

From classic statistical learning theory, the generalization error of such classifier can be characterized by the VC-dimension of the hypothesis class.

With probability at least1âˆ’Î´1ğ›¿1-\delta, for everyhâ„hin the hypothesis classâ„‹â„‹\mathcal{H}, ifhâ„his consistent withmğ‘šmtraining samples, the generalization errorÏµitalic-Ïµ\epsilonofhâ„his upper bounded by

wheredğ‘‘ddenotes the VC-dimension ofâ„‹â„‹\mathcal{H}.

We only need to determine the VC-dimension of the class of intersection of two half-spaces inâ„3â€‹nsuperscriptâ„3ğ‘›\mathbb{R}^{3n}. Itâ€™s well known the VC-dimension of a single half-space isOâ€‹(n)ğ‘‚ğ‘›O(n).[2]shows that thekğ‘˜k-fold intersection of any VC-class has VC-dimension bounded byOâ€‹(dâ€‹kâ€‹logâ¡k)ğ‘‚ğ‘‘ğ‘˜ğ‘˜O(dk\log k). Puttingd=nğ‘‘ğ‘›d=nandk=2ğ‘˜2k=2concludes the proof.
âˆ

As an extension of our result in proper learning, we consider the problem whether multimodality still possesses such exponential computational benefit when the learner is allowed to output arbitrary hypothesis beyond the hypothesis setâ„‹â„‹\mathcal{H}, i.e. the improper learning setting.

The general problem of learning intersections of halfspaces is known to be hard even in the improper learning setting, defined as below.

An instance ofğˆğ§ğ­ğ‡ğ’â€‹(ğ)ğˆğ§ğ­ğ‡ğ’ğ\mathbf{IntHS(N)}is a set of points inâ„nsuperscriptâ„ğ‘›\mathbb{R}^{n}each labeled either â€˜+â€™ or â€˜-â€™ and the goal is to find an intersection ofNğ‘Nnumber of half-spaces which correctly classifies the maximum number of points, where a â€˜+â€™ point is classified correctly if it lies inside the intersection and a â€˜-â€™ point is classified correctly if it lies outside of it.

We will rely on the following hardness of improper learning intersections of halfspaces.

[[4,5]]
Iflimnâ†’âˆqâ€‹(n)=âˆsubscriptâ†’ğ‘›ğ‘ğ‘›\lim_{n\to\infty}q(n)=\inftyis a super-constant, there is no efficient algorithm that improperly learnsqâ€‹(n)ğ‘ğ‘›q(n)numbers of intersections of halfspaces inRnsuperscriptğ‘…ğ‘›R^{n}.

Using a similar analysis to Theorem4, we obtain the following separation result.

There exists a multimodal learning problem(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z})which is PAC-learnable in polynomial time, while both unimodal learning problems(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z}),(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})are NP-hard in the improper learning setting, even if there is a bijective mappingf:ğ’³â†’ğ’´:ğ‘“â†’ğ’³ğ’´f:\mathcal{X}\to\mathcal{Y}such thaty=fâ€‹(x),âˆ€(x,y,z)âˆ¼(ğ’³,ğ’´,ğ’µ)formulae-sequenceğ‘¦ğ‘“ğ‘¥similar-tofor-allğ‘¥ğ‘¦ğ‘§ğ’³ğ’´ğ’µy=f(x),\forall(x,y,z)\sim(\mathcal{X},\mathcal{Y},\mathcal{Z}).

The proof is identical to that of Theorem4except for two minor places:

We need a strengthened version of Lemma6withÎ»ğœ†\lambdabeing1/polyâ€‹(n)1polyğ‘›1/\textbf{poly}(n).

The hard instance construction and the algorithm of multimodal learning is slightly modified to accommodate the new Lemma.

We begin with the strengthened version of Lemma6. The definition ofğˆğ§ğ­ğ‡ğ’â€‹(N)Î»ğˆğ§ğ­ğ‡ğ’subscriptğ‘ğœ†\mathbf{IntHS}(N)_{\lambda}follows directly from Definition5, and we wonâ€™t repeat here.

Given any super-constantqâ€‹(n)ğ‘ğ‘›q(n). For all constantsCâ‰¥1,c>0formulae-sequenceğ¶1ğ‘0C\geq 1,c>0, improperly learningğˆğ§ğ­ğ‡ğ’â€‹(qâ€‹(n))1Câ€‹ncğˆğ§ğ­ğ‡ğ’subscriptğ‘ğ‘›1ğ¶superscriptğ‘›ğ‘\mathbf{IntHS}(q(n))_{\frac{1}{Cn^{c}}}is NP-hard.

We prove by reduction. Suppose for contradictionğˆğ§ğ­ğ‡ğ’â€‹(qâ€‹(n))1Câ€‹ncğˆğ§ğ­ğ‡ğ’subscriptğ‘ğ‘›1ğ¶superscriptğ‘›ğ‘\mathbf{IntHS}(q(n))_{\frac{1}{Cn^{c}}}can be learnt in polynomial time, then for each instance ofğˆğ§ğ­ğ‡ğ’â€‹(qâ€‹(n))ğˆğ§ğ­ğ‡ğ’ğ‘ğ‘›\mathbf{IntHS}(q(n)), we create a new instance ofğˆğ§ğ­ğ‡ğ’â€‹(qâ€²â€‹(Câ€‹nc+1))1Câ€‹ncğˆğ§ğ­ğ‡ğ’subscriptsuperscriptğ‘â€²ğ¶superscriptğ‘›ğ‘11ğ¶superscriptğ‘›ğ‘\mathbf{IntHS}(q^{\prime}(Cn^{c+1}))_{\frac{1}{Cn^{c}}}with dimensionCâ€‹nc+1ğ¶superscriptğ‘›ğ‘1Cn^{c+1}by extension, whereqâ€²â€‹(Câ€‹nc+1)=qâ€‹(n)superscriptğ‘â€²ğ¶superscriptğ‘›ğ‘1ğ‘ğ‘›q^{\prime}(Cn^{c+1})=q(n)is still a super-constant. In particular, each pointxâˆˆâ„Câ€‹nc+1ğ‘¥superscriptâ„ğ¶superscriptğ‘›ğ‘1x\in\mathbb{R}^{Cn^{c+1}}shares the same label asx[1:n]subscriptğ‘¥delimited-[]:1ğ‘›x_{[1:n]}in the originalğˆğ§ğ­ğ‡ğ’â€‹(qâ€‹(n))ğˆğ§ğ­ğ‡ğ’ğ‘ğ‘›\mathbf{IntHS}(q(n))instance. Since and polynomial ofCâ€‹ncğ¶superscriptğ‘›ğ‘Cn^{c}is also a polynomial ofnğ‘›n, we conclude that any classifier ofğˆğ§ğ­ğ‡ğ’â€‹(qâ€‹(n))1Câ€‹ncğˆğ§ğ­ğ‡ğ’subscriptğ‘ğ‘›1ğ¶superscriptğ‘›ğ‘\mathbf{IntHS}(q(n))_{\frac{1}{Cn^{c}}}applies to theğˆğ§ğ­ğ‡ğ’â€‹(qâ€‹(n))ğˆğ§ğ­ğ‡ğ’ğ‘ğ‘›\mathbf{IntHS}(q(n))problem with the same accuracy, contradicting Theorem14.
âˆ

Specifically, we will only use Lemma16withC=1,c=1/2formulae-sequenceğ¶1ğ‘12C=1,c=1/2. Now we are ready to state the learning problem(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z}):mğ‘šmdata points(xi,yi,zi)subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–subscriptğ‘§ğ‘–(x_{i},y_{i},z_{i})are given, wherexi,yiâˆˆâ„nsubscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–superscriptâ„ğ‘›x_{i},y_{i}\in\mathbb{R}^{n}represent the two modalities andzi=Â±subscriptğ‘§ğ‘–plus-or-minusz_{i}=\pmis the label. Itâ€™s guaranteed that there is an intersection ofnâˆ’1ğ‘›1\sqrt{n}-1number of half-spacesH1,H2,â‹¯,Hnâˆ’1subscriptğ»1subscriptğ»2â‹¯subscriptğ»ğ‘›1H_{1},H_{2},\cdots,H_{\sqrt{n}-1}that classifies all the points correctly, with supports of the defining unit vectors being the firstnğ‘›\sqrt{n}coordinates. In other words, itâ€™s a realizable instance ofğˆğ§ğ­ğ‡ğ’â€‹(nâˆ’1)1/nğˆğ§ğ­ğ‡ğ’subscriptğ‘›11ğ‘›\mathbf{IntHS}(\sqrt{n}-1)_{1/\sqrt{n}}(withqâ€‹(x)=xâˆ’1ğ‘ğ‘¥ğ‘¥1q(x)=x-1).

In particular, there are unit vectorsr1,r2,â‹¯,rnâˆ’1âˆˆâ„nsubscriptğ‘Ÿ1subscriptğ‘Ÿ2â‹¯subscriptğ‘Ÿğ‘›1superscriptâ„ğ‘›r_{1},r_{2},\cdots,r_{\sqrt{n}-1}\in\mathbb{R}^{\sqrt{n}}and constantsc1,c2,â‹¯,cnâˆ’1âˆˆâ„subscriptğ‘1subscriptğ‘2â‹¯subscriptğ‘ğ‘›1â„c_{1},c_{2},\cdots,c_{\sqrt{n}-1}\in\mathbb{R}(unknown to the learner), such that all pairs(xi,zi)subscriptğ‘¥ğ‘–subscriptğ‘§ğ‘–(x_{i},z_{i})can be perfectly classified byâˆ©iH^isubscriptğ‘–subscript^ğ»ğ‘–\cap_{i}\hat{H}_{i}, whereH^i=(x|r^iâŠ¤â€‹xâ‰¤ci)subscript^ğ»ğ‘–conditionalğ‘¥superscriptsubscript^ğ‘Ÿğ‘–topğ‘¥subscriptğ‘ğ‘–\hat{H}_{i}=(x|\hat{r}_{i}^{\top}x\leq c_{i})andr^i=(ri,ğŸnâˆ’n)subscript^ğ‘Ÿğ‘–subscriptğ‘Ÿğ‘–subscript0ğ‘›ğ‘›\hat{r}_{i}=(r_{i},\mathbf{0}_{n-\sqrt{n}}). Similarly, we can define theQğ‘„Qmatrix as

where the functionFâ€‹(v1,v2,â‹¯,vnâˆ’1):â„nâˆ’nâ†’â„(nâˆ’n)Ã—(nâˆ’n):ğ¹subscriptğ‘£1subscriptğ‘£2â‹¯subscriptğ‘£ğ‘›1â†’superscriptâ„ğ‘›ğ‘›superscriptâ„ğ‘›ğ‘›ğ‘›ğ‘›F(v_{1},v_{2},\cdots,v_{\sqrt{n}-1}):\mathbb{R}^{n-\sqrt{n}}\to\mathbb{R}^{(n-\sqrt{n})\times(n-\sqrt{n})}is chosen as below:

Meanwhile,yi=Qâ€‹(v1,v2,â‹¯,vnâˆ’1)â€‹xisubscriptğ‘¦ğ‘–ğ‘„subscriptğ‘£1subscriptğ‘£2â‹¯subscriptğ‘£ğ‘›1subscriptğ‘¥ğ‘–y_{i}=Q(v_{1},v_{2},\cdots,v_{\sqrt{n}-1})x_{i}holds for all data points, and all pairs(yi,zi)subscriptğ‘¦ğ‘–subscriptğ‘§ğ‘–(y_{i},z_{i})can be perfectly classified byâˆ©iH~isubscriptğ‘–subscript~ğ»ğ‘–\cap_{i}\tilde{H}_{i}, whereH~i=(x|r~iâŠ¤â€‹xâ‰¤ci)subscript~ğ»ğ‘–conditionalğ‘¥superscriptsubscript~ğ‘Ÿğ‘–topğ‘¥subscriptğ‘ğ‘–\tilde{H}_{i}=(x|\tilde{r}_{i}^{\top}x\leq c_{i})andr~i=Qâ€‹(v1,v2,â‹¯,vnâˆ’1)â€‹(ri,ğŸnâˆ’n)subscript~ğ‘Ÿğ‘–ğ‘„subscriptğ‘£1subscriptğ‘£2â‹¯subscriptğ‘£ğ‘›1subscriptğ‘Ÿğ‘–subscript0ğ‘›ğ‘›\tilde{r}_{i}=Q(v_{1},v_{2},\cdots,v_{\sqrt{n}-1})(r_{i},\mathbf{0}_{n-\sqrt{n}}).

Via the same argument as Theorem4, according to Lemma16, both improperly learning(ğ’³,ğ’µ)ğ’³ğ’µ(\mathcal{X},\mathcal{Z})and improperly learning(ğ’´,ğ’µ)ğ’´ğ’µ(\mathcal{Y},\mathcal{Z})are hard.

We only need to show(ğ’³,ğ’´,ğ’µ)ğ’³ğ’´ğ’µ(\mathcal{X},\mathcal{Y},\mathcal{Z})can be learnt efficiently. The same algorithm will be applied to decode all therisubscriptğ‘Ÿğ‘–r_{i}andcisubscriptğ‘ğ‘–c_{i}is set asmaxxâˆˆX+â¡riâŠ¤â€‹xsubscriptğ‘¥subscriptğ‘‹superscriptsubscriptğ‘Ÿğ‘–topğ‘¥\max_{x\in X_{+}}r_{i}^{\top}x. The classifier we use is still

By the construction ofQğ‘„Q, we know there is a classifierhâˆ—â€‹(x)superscriptâ„ğ‘¥h^{*}(x)which classifies all data points correctly, which shares the samerisubscriptğ‘Ÿğ‘–r_{i}withhâ€‹(x)â„ğ‘¥h(x):

By the choice ofcisubscriptğ‘ğ‘–c_{i}, we have thatciâ‰¤ciâˆ—,âˆ€isubscriptğ‘ğ‘–subscriptsuperscriptğ‘ğ‘–for-allğ‘–c_{i}\leq c^{*}_{i},\forall i. Denoteh+={xâˆˆâ„n,hâ€‹(x)=+}subscriptâ„formulae-sequenceğ‘¥superscriptâ„ğ‘›â„ğ‘¥h_{+}=\{x\in\mathbb{R}^{n},h(x)=+\}, we have that

by the facth+âŠ‚h+âˆ—subscriptâ„subscriptsuperscriptâ„h_{+}\subset h^{*}_{+}. Meanwhile, by the construction ofhâ€‹(x)â„ğ‘¥h(x), we have thatX+âŠ‚h+subscriptğ‘‹subscriptâ„X_{+}\subset h_{+}, and further

As a result,X+=h+âˆ©Xsubscriptğ‘‹subscriptâ„ğ‘‹X_{+}=h_{+}\cap Xwhich meanshâ€‹(x)â„ğ‘¥h(x)does classify all data points correctly. The rest of the proof on PAC learning easily follows from Theorem4and we omit it here.

âˆ

In this paper, we take a preliminary step towards unraveling the computational benefit of multimodal learning. We demonstrate an exponential separation in computation between multimodal and unimodal learning by constructing a variant of the intersection of two half-spaces
problem, which is NP-hard for any unimodal algorithm but can be efficiently solved by a multimodal algorithm. Complementing the statistical merits of multimodal learning as shown in[13], our result provides a more comprehensive theoretical understanding of the power of multimodal learning.

The main limitation of this work, in our opinion, is on the contrived argument that multimodal learning is tractable: the efficient learning scheme provided in this work only succeeds on a narrow, intricately designed class of problem instances. These results alone are not enough to suggest that computational benefits of multimodal learning are a more general phenomenon.

We conclude with two questions as future directions to improve the preliminary results presented in this work.

Can we show such separation in computation for more natural learning problems? Ideally, a good efficient learning algorithm for the multimodal setting should have less dependence on the problem structure, such as ERM.

Can we obtain a general sufficient condition for the computational benefit of multimodal learning? Even a polynomial improvement is interesting.

[å›¾ç‰‡: images\image_1.png]

[å›¾ç‰‡: images\image_2.png]

