Ê†áÈ¢òÔºöA Sequential Optimal Learning Approachto Automated Prompt Engineering in Large Language Models

Designing effective prompts is essential to guiding large language models (LLMs) toward desired responses. Automated prompt engineering aims to reduce reliance on manual effort by streamlining the design, refinement, and optimization of natural language prompts. This paper proposes an optimal learning framework for automated prompt engineering, designed to sequentially identify effective prompt features while efficiently allocating a limited evaluation budget. We introduce a feature-based method to express prompts, which significantly broadens the search space. Bayesian regression is employed to utilize correlations among similar prompts, accelerating the learning process. To efficiently explore the large space of prompt features for a high quality prompt, we adopt the forward-looking Knowledge-Gradient (KG) policy for sequential optimal learning. The KG policy is computed efficiently by solving mixed-integer second-order cone optimization problems, making it scalable and capable of accommodating prompts characterized only through constraints. We demonstrate that our method significantly outperforms a set of benchmark strategies assessed on instruction induction tasks. The results highlight the advantages of using the KG policy for prompt learning given a limited evaluation budget. Our framework provides a solution to deploying automated prompt engineering in a wider range applications where prompt evaluation is costly.

Key Words: automated prompt engineering, optimal learning, Knowledge-Gradient, Bayesian regression, feature-based prompts

Large Language Models (LLMs) demonstrate exceptional capabilities in following instructions, making them a powerful tool to various downstream tasks[31,20,2,30]. A well-designed prompt steers an LLM to generate desired responses, enabling effective adaptation to downstream applications without incurring the high cost of fine-tuning the model weights. Nevertheless, creating effective prompts can be challenging due to the sensitivity of LLM outputs to prompt variations[32,21,38]. In addition, manually identifying ideal prompts is often time-consuming and lacks systematic guidance. Automated approaches to designing, optimizing, and refining LLM prompts mitigate this challenge by minimizing the need for manual intervention.

Recent efforts in automated prompt engineering primarily have focused on iterative evaluation and refinement in order to converge to ideal prompts[12,39,28,27,35]. The methods by these studies often assume the availability of numerous iterations. However, in many real-world scenarios, opportunities to evaluate prompts are limited, as each prompt evaluation is costly or time-consuming. For example, in medical research, each prompt evaluation could involve extensive time and resources from medical professionals, making it impractical to test a large number of variations before selecting the final one.

Moreover, many existing approaches search over a set of precrafted candidate prompts for ideal prompts[33,39,22]These methods require identifying and enumerating a set of prompts, which restricts the scalability as the candidate set expands. Furthermore, it fails to utilize the correlation among similar prompt to expedite the learning.

To fully unlock the potential of LLMs across diverse scenarios, it is crucial to develop an automated prompting framework that is capable of capturing dependencies among prompts and efficiently identifying high-performing prompts within few evaluations. This paper presents a principled forward-looking iterative process for automated prompt engineering through the optimal design of a sequence of prompts.

We propose an interpretable feature-based approach to prompt representation. Various categorical or numerical features can be considered to characterize detailed aspects of a prompt, such as the selection and ordering of demonstrative examples. Previous works identify factors within a prompt that influence the LLM outputs but often treat these aspects in isolation. We allow for capturing various interactions among prompt attributes and can accommodate potential constraints on the features. This feature-based prompt representation enables the inclusion and exploration of a vast and diverse set of prompts, which, unlike previous approaches, do not require manual prompt provision. In addition, in contrast to prompt descriptions based on embedding vectors, our representation is inherently interpretable.

We adopt a Bayesian approach to refine beliefs about the influence of prompt features on the LLM response. This approach supports the integration of prior knowledge and user opinions as well as enabling to capture feature correlations. To operationalize this, we define a probabilistic model to link prompt features to a response quality of interest. In this paper, we demonstrate our approach using LLM response accuracy as the primary quality metric.

Next, we formalize the iterative process of automated prompt engineering in the presence of limitations on the number of prompt evaluations as a sequential decision-making problem. Given the limited opportunities for prompt evaluation, this problem falls into the category of finite-horizon discrete-time Markov decision processes; see[29]. An optimal learning policy sequentially selects a feasible prompt representation for evaluation, aiming to maximize the expected outcome of the final prompt. Due to the potentially large prompt feature space, the curse of dimensionality[25]hinders the exact computation of the optimal prompt selection. We adopt an approximate policy for the optimal learning problems, known as the expected improvement policy in[4,5]or Knowledge-Gradient (KG) policy in[9,26]. This is a forward-looking policy that maximizes the expected improvement in the value of information in each learning phase. The KG policy often excels in practical scenarios
with limited evaluation budgets, frequently outperforming common static data acquisition strategies and dynamic test-and-learn policies. For the consistency of the KG policy, see[10]for correlated features and[14]for constrained search space.

The large space of prompt candidates, defined by constrained features, makes it impractical to enumerate all feasible alternatives for determining KG decisions. To address this challenge, we leverage recent advancements in scalable optimal learning and KG computation. In contrast to earlier results to compute KG decisions[9]based on enumeration of all feasible alternatives, recent computational methods[24,14,6]build on optimal quantization of the response probability followed by mixed-integer conic optimization reformulations to leverage efficient optimization solution methods. For optimal learning problems with larger feature spaces, an iterative process involving solving mixed-integer linear optimization problems is employed to achieve even greater computational efficiency and scalability.

Our framework allows for different prompt representations and selection policies, hence encompassing various existing methods for automated prompt engineering. For example, when a small, finite set of precrafted prompt templates is provided and a point-wise utility model is used, our setting simplifies to the setup in[22,39,33].

We assess the performance of sequential prompt learning with adaptive prompt selection policies on a dataset of instruction induction tasks[15]. This benchmark dataset contains 24 instances of instruction induction, designed for LLMs to deduce implicit tasks or instructions from language prompts including answers or contextual information. For the instruction induction tasks, we first propose a feature-based prompt template and use the accuracy of responses collected from GPT-3.5 on the validation data as the primary performance metric. For the prompt selection, we evaluate the KG policy, the adaptive myopic policy, the increasingly popular Thompson sampling policy, and a number of other automated prompt engineering methods such as EvoPrompt[12]using an evolutionary algorithm to refine prompts and TRIPLE[33]using a multi-armed bandit approach to select prompts.

Our analyses show the effectiveness of our approach particularly with the KG prompt selection policy, which is capable to converge to high-quality prompts within 30 or fewer prompt evaluations. These prompts significantly outperform those generated by the benchmarks on the test data using the same number of LLM interactions. Further analysis reveals that the KG policy is particularly favorable for challenging tasks with high uncertainty in LLM responses and significant sensitivity to prompts, achieving a substantial margin over baseline policies. Our findings highlight the advantages of using the KG policy in prompt engineering to selectively evaluate prompts, expanding the potential for deploying automated prompt engineering in applications with large prompt evaluation costs.

Our contributions can be summarized as follows.

We introduce a sequential optimal learning framework for automated prompt engineering to guide through the process of designing a sequence of prompts that effectively elicit accurate responses from an LLM. The approach is particularly effective for applications where prompt evaluation is resource-intensive.

We propose a feature-based approach to represent language prompts, which greatly expands the prompt search space. A link function maps the features to LLM response accuracy through Bayesian model parameters to leverage correlations among prompts with shared characteristics. Our method enables simultaneous optimization of multiple features to generate improved prompts.

We leverage the KG policy within our sequential prompt learning to efficiently identify high-performing prompts in large prompt spaces. The KG policy outperforms various benchmark policies, especially for challenging tasks with high uncertainty in LLM response.

The remainder of the paper is organized as follows: Section2reviews the related literature. Section3discusses the generic iterative process for automated prompt learning, formalizing the problem as a sequential decision making problem. Section4discusses the forward-looking KG policy for the prompt selection in iterative automated prompting. Illustrative examples and computational results are provided in Sections5and6. Finally, conclusions and potential extensions are discussed in Section7.

[39]present a two-phase pipeline of instruction generation and selection. This method generates a set of candidate instructions, which are evaluated and filtered based on their performance on the downstream tasks until the best one from the candidate set is found.[22]uses an optimal control paradigm to systematize the process of iteratively updating and selecting from a set of candidate prompts. However, these approaches are limited to search spaces represented by individual prompt candidates. Our proposed framework encompasses different prompt representations and exploration policies. Our feature-based approach to represent prompts captures their dependencies and enables a diverse search space.[33]follows the two-phase pipeline with a focus on prompt selection subject to a fixed evaluation budget. They formulate the selection as a multi-armed bandit (MAB) problem and utilize the continuously reject method to select from the candidate set. However, their solution is not scalable with the size of the prompt candidate set. In contrast, our framework with the KG prompt selection policy can accommodate larger spaces of prompts.

An approach to automated prompting focuses on generating refined prompts by iteratively editing base prompts.[27]propose GrIPS, which iteratively applies text-based edit operations, such as word substitutions and deletions, to a base prompt. The best candidate is then selected as the new base prompt. Alternatively,[12]apply evolutionary algorithms and generate new candidate prompts by performing mutation and crossover operations using an LLM, retaining high-quality prompts for the next generation.[8]also use an LLM to perform mutation operations on a population of prompts but employ a self-referential way of improving both the prompts and the mutation operations.[16]propose an iterative prompt refinement scheme specially crafted for relevance ranking in information retrieval. While these attempts show the potential of edit-based approaches for generating high-quality prompts, they mainly rely on local search by modifying existing prompts. Our method, however, explores the prompt space in a forward-looking principled manner using Bayesian optimal learning, and utilizes knowledge from prior observations to inform future selections of prompts to evaluate.

Recent works[3,19]experiment using Bayesian Optimization to search in the embedding space of prompts, but white-box LLMs are required to facilitate the optimization steps. Our approach also builds on Bayesian learning, but we directly search in the space of discrete prompts and eliminate the need for white-box LLMs. The concept of prompt learning is also used in[23], which trains a reinforcement learning model to select tokens as actions to form prompts. The training stage requires sufficient evaluation budget, while our framework is capable of learning from only a limited number of prompt evaluation.

Gradient-based algorithms have been used to solve the problem of optimizing the prompt performance over the prompt space.[35,34]model prompts as sequences of trigger tokens, and compute the gradients of the log-likelihood of the language model generating the target outputs with respect to the embeddings of candidate tokens to guide the search for optimal tokens.[36]compute the gradients of a similarity metric between the generated and target outputs with respect to the prompt embeddings to guide prompt optimization, and project the optimized embeddings to discrete prompts using nearest neighbors. These methods require access to the internal parameters of the LLM, making them incompatible with black-box LLMs. Moreover, these methods require computationally intensive gradient computations. In contrast, our approach generates human-readable prompts without accessing internal parameters.

We introduce a sequential optimal prompt learning framework, calledSOPL, for automated prompt engineering that is compatible with black-box LLMs and generates human-readable prompts, while addressing the challenge of limited number of iterations for prompt evaluation. Our framework, depicted in Figure1, follows the iterative process outlined in Algorithm1. The components of the framework are explained in the subsequent subsections.

[ÂõæÁâá: images\image_1.png]
ÂõæÁâáËØ¥Êòé: Figure 1:SOPL: Sequential optimal prompt learning for automated prompt engineering

RequireMaximum iterationNN, score functionEval:ùí≥‚Üí(0,1)\mathrm{Eval}:\mathcal{X}\to(0,1), prompt representation selection policyœÄ:ùíÆ‚Üíùí≥\pi:\mathcal{S}\to\mathcal{X}, Bayesian update functionUpdate:ùíÆ√ó‚Ñù‚ÜíùíÆ\operatorname{Update}:\mathcal{S}\times\mathbb{R}\to\mathcal{S}that implements (4)-(7).

To identify high-quality prompts, it is essential to explore a diverse set of prompts tailored to the specific downstream task. We adopt a feature-based representation for prompts, expressing a language meta prompt through various features that capture its content and structure. Prompt features are denoted by the vectorxxthat specifies a textual prompt. Examples of such features include the structural template, tone, role, context, demonstrations, specificity, complexity, embeddings, task type, question framing, constraints, and temporal references. These features are generally either manually engineered by the user or derived from established prompt templates in the literature corresponding to the task. Refer to Section 5.1 for the specific features and categories utilized in the instruction induction task for our experiments, developed based on the template proposed in[15].

Previous studies have primarily examined each feature in isolation, whereas we integrate these features in the prompt representation to leverage the potential synergies that emerge from their combination. By enriching the meta prompt with multiple features known to influence LLM responses, we can expand the search space.

In general, the feature space may contain variables of different types: continuous, categorical, and ordinal. In addition, various requirements may be imposed on the features either by definition or by the user‚Äôs preferences to account for mutually exclusive features, conditional features, combined effects of multiple features, disjunctive features, and multiple-choice decisions. The set of feasible feature combinations forms a diverse and potentially large search space, denoted byùí≥\mathcal{X}. Our framework does not require explicitly identifying and enumerating all feasible prompts. Instead, the feasible prompt spaceùí≥\mathcal{X}, which encompasses the feasible values of the prompt features, is specified solely by linear inequality or equality constraints.

Our framework encompasses existing approaches as special cases. For example, the methods[39,33,22]that follow the pipeline of generating and selecting from a set of candidates can be thought of as using the candidate set asùí≥\mathcal{X}.

The prompt evaluation phase involves querying a black-box LLM with the prompt constructed fromxxand collecting the LLM‚Äôs response. When evaluating the prompt on the downstream task, we measure the quality of the observed LLM response to the prompt associated withxxusing a numeric scoreuxu_{x}, which is computed from the score functionEval\operatorname{Eval}defined onùí≥\mathcal{X}; see Algorithm 2 for details onEval‚Äã(x)\operatorname{Eval(x)}. Given labeled data, the score can be computed by comparing the LLM responses to the ground truth labels and calculating the percentage of accurate response. For downstream tasks that require human evaluation, the score is based on human feedback.

For common metrics such as accuracy, F1-score, and point-wise mutual information[1], the score lies in the interval between 0 and 1. We assume that

whereŒò‚àºùí©‚Äã(ŒºŒò,Œ£Œò/œÅ)\Theta\sim\mathcal{N}(\mu_{\Theta},\Sigma_{\Theta}/\rho)is theDD-dimensional model parameter andœµ‚àºùí©‚Äã(0,1/œÅ)\epsilon\sim\mathcal{N}(0,1/\rho)with variance1/œÅ1/\rhois the measurement noise. The quantityŒ∑x\eta_{x}represents the utility ofxx. The meanŒºŒò\mu_{\Theta}and the precisionœÅ\rhoare the unknown parameters to estimate. For general score functions that return values beyond the interval between 0 and 1, alternative link functions can be employed in (1) in place of the logit function.

An approach to addressing uncertainty in the effectiveness of prompt features is Bayesian learning. The Bayesian approach to inference can account for multiple levels of randomness and correlation by using prior distributions for model parameters. Additionally, existing knowledge and user input can be incorporated into these prior probability distributions. We adopt the Bayesian framework, letting a multivariate normal prior for the coefficients,ŒºŒò\mu_{\Theta}, and a Gamma prior for the precision,œÅ\rho, i.e.,

The belief represented byS=(Œ∏,œÉ,a,b)S=(\theta,\sigma,a,b)is referred to as theknowledge state. The knowledge state encodes prior observations of prompt performance and serves as the basis to inform future decisions. The multivariate distribution (2) captures dependencies among unknown parameters, implying that learning about one prompt can provide insights into the effectiveness of several other prompts, thereby enhancing the learning speed.

We iteratively update the knowledge state based on observed responses to queried prompts. At iterationnn, the knowledge state is denoted bySn=(Œ∏n,Œ£n,an,bn)S_{n}=(\theta_{n},\Sigma_{n},a_{n},b_{n}), and the selected prompt representation is denoted byxnx_{n}. After thenn-th iteration of prompt evaluation, we observe the scoreun:=uxnu_{n}:=u_{x_{n}}and update the knowledge state as follows.

Equations¬†(4)-(7) collectively define the mappingUpdate‚Å°(S,logit‚Å°(x))\operatorname{Update}(S,\operatorname{logit}(x))appeared in line 11 of Algorithm1.

For each iteration, a prompt representationxnx_{n}is selected according to a policyœÄ\pi. The goal is to find a prompt that maximizes the score on the downstream task afterNNiterations of prompt evaluation.

Our framework allows for different prompt representation selection policies to explore the feasible prompt spaceùí≥\mathcal{X}and update the knowledge state. Heuristic policies, such as adaptive myopic (Greedy) and Thompson sampling (TS), can be adopted. The Greedy policy selects the best prompt representatioxxbased on the current knowledge state by

The TS policy samples from the posteriors of the parameters by

and then selects the prompt representationxxby

The heuristic policies such as Greedy and TS policies are adaptive, but they are not forward-looking, in the sense that they do not explicitly take into account the effect of selected prompts on the subsequent prompt selections and the overall learning process.

The prompt representation selection policyœÄ\piis a key building block influencing the performance of the SOPL framework. When the number of iterations is limited toNN, the process of sequential optimal prompt learning can be formulated as a finite-horizon Markov decision process, where the action spaceùí≥\mathcal{X}consists of prompt representations, and the state spaceùíÆ\mathcal{S}consists of knowledge states. In the next section, we discuss an approximate policy for the optimal prompt representation selection policy.

We consider a forward-looking optimal learning policy designed to maximize the expected improvement in an approximated value of information during each iteration. This approach, known as the Knowledge-Gradient (KG) policy, offers an approximate solution to the MDP for prompt selection. For additional discussion and analysis, refer to[13,5,11,26]. The value of information is measured by the expected single-period improvement, i.e., the difference between the values of the knowledge statesSn+1S_{n+1}andSnS_{n}if the prompt representationxn+1=xx_{n+1}=xis selected. Hence, at iterationnn, the following KG quantity is maximized:

whereVN‚Äã(S)V_{N}(S)is the value of the optimal policy at iterationNNfor any knowledge stateSS, i.e.,VN‚Äã(S)=maxœÄ‚ààŒ†‚Å°VNœÄ‚Äã(S)=maxx‚ààùí≥‚Å°ùîº‚Äã[Œ∑x|S]V_{N}(S)=\max_{\pi\in\Pi}V_{N}^{\pi}(S)=\max_{x\in\mathcal{X}}\mathbb{E}[\eta_{x}|S], whereŒ∑x\eta_{x}is as in equation (1). Recall thatSn+1S_{n+1}is the transition from stateSnS_{n}induced by the updating procedure in equations (4)-(7). The quantityŒΩxn\nu_{x}^{n}is the marginal value of one more prompt representationxxbeing queried. Its value is always nonnegative.

The decision of the KG policy selects the prompt representation that maximizes the KG quantity in equation (11):

For discussion on the related concepts of asymptotic optimality and statistical consistency of the KG policy, the reader is referred to[11,10]whenùí≥\mathcal{X}is specified in the enumerative form, and see[14]whenùí≥\mathcal{X}is represented in a constraint-based form.

For anyx‚ààùí≥x\in\mathcal{X}, the KG quanitity corresponding to model (1) is given by:

whereT2‚ÄãanT_{2a_{n}}follows a student‚Äôs t distribution with2‚Äãan2a_{n}degrees of freedom, and

The expectation in equation (13) is with respect to the one-dimensional random variableT2‚ÄãanT_{2a_{n}}.

The first term in the KG quantity in equation (13) can be approximated by

wheret1,‚Ä¶,tJ‚àà‚Ñùt_{1},...,t_{J}\in\mathbb{R}is the sequence of points that minimizes the quadratic quantization error of the Voronoi quantizer forT2‚ÄãanT_{2a_{n}},t0=‚àí‚àût_{0}=-\infty, andtJ+1=‚àût_{J+1}=\infty. The weights are defined aswj=FT2‚Äãan‚Äã(tj+tj+12)‚àíFT2‚Äãan‚Äã(tj‚àí1+tj2)w_{j}=F_{T_{2a_{n}}}\left(\frac{t_{j}+t_{j+1}}{2}\right)-F_{T_{2a_{n}}}\left(\frac{t_{j-1}+t_{j}}{2}\right)forj=1,‚ãØ,Jj=1,\cdots,J. Here,FT2‚ÄãanF_{T_{2a_{n}}}is the cumulative distribution function ofT2‚ÄãanT_{2a_{n}}.
Therefore, the selected prompt representation based on the KG policy at stateSnS_{n}is computed by solving the following mixed-integer optimization problem:

Here,mmis the dimensionality of the prompt representation features, andPn:=anbn‚Äã(A‚ä§‚ÄãAh‚ä§‚Äãh+Œ£n+Œ£Œò)P_{n}:=\frac{a_{n}}{b_{n}}(\frac{A^{\top}A}{h^{\top}h}+\Sigma_{n}+\Sigma_{\Theta}), whereAAandhhform the equality constraints of the feasible setùí≥={x|A‚Äãx=h,B‚Äãx‚â§g}\mathcal{X}=\{x|Ax=h,Bx\leq g\}. In this problem,ùí≥+\mathcal{X}^{+}, consisting of elements(x,œÑ)‚àà‚Ñùm(x,\tau)\in\mathbb{R}^{m}, represents the homogenized version of the setùí≥\mathcal{X}. In the last constraint,MMdenotes a large constant.
For further details and a computationally efficient iterative algorithm only involving solving mixed-integer programming problems to solve this problem, see Propositions 6 and 7 in[24].

We demonstrate the performance of the proposed SOPL framework on the instruction induction tasks[15]. The dataset consists of 24 individual tasks, covering various aspects of text comprehension. Each data point comprises a pair of input and output. For example, for the tasklarger_animal, one data point consists of an input ‚Äúcougar, flea‚Äù and an output ‚Äúcougar‚Äù. The objective is to find an instruction such that when the LLM is queried with the instruction and an input, its response matches the correct output. A possible instruction for this task can be ‚Äúchoose the larger animal‚Äù. Similar to[39], we generate possible instructions by prompting an LLM using a meta prompt, which consists of demonstrative examples of input-output pairs and asks for a possible instruction. For each task, we partition the dataset into three sets: a demonstration dataset, a validation dataset, and a held-out test dataset.

We focus on five aspects of prompts that have been shown to impact LLM responses.[39]note that the template of the meta prompt impacts the effectiveness of the induced prompts;[21,38]find that both the selection and the order of demonstration examples influence generated texts;[37,17]show that specifying different roles elicits diverse text generations from LLMs;[7,39]discover that prompts paraphrased by LLMs yield improved performance on downstream tasks;[18]observe that LLMs respond differently to different descriptions of tones.

We create a set of choices for each feature, summarized in Table1. A feature vectorxxspecifies one choice for each feature. By applying one-hot encoding to represent each categorical feature, the prompt representation featurexxbecomes a binary vector. All combinations of the prompt features, subject to the constraint that exactly one choice is selected for each feature, form the search spaceùí≥\mathcal{X}.

[ÂõæÁâá: images\image_2.png]
ÂõæÁâáËØ¥Êòé: (a)Meta Prompt Template 1

[ÂõæÁâá: images\image_3.png]
ÂõæÁâáËØ¥Êòé: (a)Meta Prompt Template 1

[ÂõæÁâá: images\image_4.png]
ÂõæÁâáËØ¥Êòé: (a)Meta Prompt Template 1

[ÂõæÁâá: images\image_5.png]
ÂõæÁâáËØ¥Êòé: (a)Meta Prompt Template 1

[ÂõæÁâá: images\image_6.png]
ÂõæÁâáËØ¥Êòé: Figure 3:Paraphrasing Template

[ÂõæÁâá: images\image_7.png]
ÂõæÁâáËØ¥Êòé: Figure 4:Evaluation Template

We evaluate the selected feature vectorxxon the validation data and obtain the validation scoreuxu_{x}by Algorithm2. We first convert the feature vectorxxto a textual instruction in line 1 and 2. For example, if the feature vector specifies meta prompt template 1, 5 demonstrative examples, roles of I and friend, no paraphrasing, and description of clear, then we create a meta prompt byMetaPrompt‚Å°(x)\operatorname{MetaPrompt}(x), which inserts the 5 demonstrative examples in meta prompt template 1 in Figure2, and replaces [ROLE1] by ‚ÄúI‚Äù, [ROLE2] by ‚Äúfriend‚Äù, and [DESCRIPTION] by ‚Äúclear‚Äù. We query an LLM with the meta prompt to generate an instructionIxI_{x}that reflects the selected features. For each inputpip_{i}in the validation data, we create an evaluation prompt byEvalPrompt‚Å°(Ix,pi)\operatorname{EvalPrompt}(I_{x},p_{i}), which combines the instruction and the input using the template in Figure4. The prompt is then used to query the LLM. A task-specific metric, such as exact match or F1-score defined in[15], is used to compute a score by comparing the LLM response with the correct outputqiq_{i}. The average score across all validation examples is used as the scoreuxu_{x}.

RequireAnLLM\operatorname{LLM}, validation data{(pi,qi)}i=1V\{(p_{i},q_{i})\}_{i=1}^{V}, meta prompt construction functionMetaPrompt\operatorname{MetaPrompt}, evaluation prompt construction functionEvalPrompt\operatorname{EvalPrompt}, metric functionMetric\operatorname{Metric}to evaluate LLM response.

We compare our method with two benchmarks EvoPrompt[12]and TRIPLE[33]. Both methods provide solutions compatible with black-box access to LLMs and generate human-readable prompts. EvoPrompt uses the differential evolution algorithm to iteratively refine a population of prompts. TRIPLE employs the continuously reject algorithm to identify an effective prompt from a candidate pool under a fixed budget. In addition, we use Greedy and TS presented in (8) and (10) as the baseline policies.

We record the instruction with the highest validation score during the process. When the maximum number of prompt evaluation is reached, the instruction with the highest validation score is used as the final instruction. The test score is obtained by evaluating the final instruction on the held-out test data, using a similar procedure in Algorithm2. We report the average test score across 20 replications with different random seeds.

The held-out test dataset for each task consists of 100 examples unless specified otherwise in[15], and is kept the same for all replications. For each replication, we randomly select 10 examples from the rest of the data as the demonstration dataset, and then randomly select 100 examples or all remaining examples if fewer are available as the validation dataset.

We use OpenAI GPT-3.5 as the LLM for both generating and evaluating instructions. We allowN=30N=30opportunities to evaluate on the entire validation dataset. We set the population size to be1010for EvoPrompt as in[12], and set the size of the candidate pool to be3030for TRIPLE as in[33]. We ensure that the same number of API calls to the LLM is used for evaluating on the validation data across all methods.

We focus on the 13 challenging tasks where the validation score are below 80% with relatively large variance using the default meta prompt template in[15]. Table2presents the average test performance across 13 tasks for SOPL and the benchmark approaches. For each task, we illustrate the mean and standard deviation of the test score across 20 replications in Figure5. The results indicate that SOPL-KG outperforms the benchmarks. It exhibits the highest average test score of 0.6281 among all methods, with a 6.47% improvement in average test score and a 17.92% average improvement per task relative to EvoPrompt. For each task, we rank the five methods from the highest to the lowest test score, and calculate the average ranking across 13 tasks. The SOPL-KG achieves the highest average ranking of 1.85, and the SOPL-TS has the second best ranking of 2.69, outperforming both EvoPrompt and TRIPLE. We compute the standard deviation of the test scores across 20 replications for each task, and report the average across 13 tasks in Table2. The SOPL-KG exhibits the lowest standard deviation, demonstrating its robustness to variations in random seeds.

The findings highlight the effectiveness of our proposed framework with feature-based prompt representations and the KG prompt selection policy in comparison to other benchmarks. Although the space of all feature combinations is prohibitively large for exhaustive exploration, the SOPL-KG is capable of discovering high-quality prompts within given prompt evaluations.

[ÂõæÁâá: images\image_8.png]
ÂõæÁâáËØ¥Êòé: Figure 5:Test performance on 13 tasks for different methods. The height of each bar represents the average test score and the error bar represents the standard deviation across 20 replications with different random seeds

We consider more challenging scenarios with fewer iterations ofN=20N=20andN=10N=10. Table3presents the average test performance across 13 tasks. The SOPL-KG outperforms all other methods within fewer iterations, with a slightly lower average test score of0.61740.6174whenN=20N=20compared toN=30N=30. The SOPL-KG also has the lowest standard deviation whenN=20N=20, while the SOPL-TS is slightly more robust to variations in random seeds whenN=10N=10.

We implement an early stopping mechanism in our framework to further reduce the cost of prompt evaluation. We terminate the process early if the best validation score does not improve forœÑ\tauconsecutive steps, or when the maximum numberNNof steps is reached. We conduct experiments withœÑ=5\tau=5andœÑ=10\tau=10for 20 replications and report the results in Table4. Within 17 realized iterations, all three prompt selection policies yield close to but slightly worse test score compared toN=30N=30. TS has a small advantage in the average number of steps used, but KG dominates the test score relative to baseline policies. It shows that the KG policy has the potential of reducing the number of evaluations required without significantly compromising performance.

As Figure5illustrates, the advantage of the KG policy is more evident for some tasks while more subtle for others. For each task, we randomly sample 100 feature vectors and evaluate their validation scores. We compute the meanŒº100\mu_{100}and the standard deviationœÉ100\sigma_{100}of the 100 scores, and calculate the coefficient of variation byC‚ÄãV100:=œÉ100/Œº100CV_{100}:=\sigma_{100}/\mu_{100}. This metric aims to represent the uncertainty in the LLM response performance as the prompt feature values vary. Figure6depicts the relationship between the relative improvement of the KG prompt representation selection policy over the TS and Greedy policies in the average test score, and the coefficient of variationC‚ÄãV100CV_{100}for different tasks. The correlation coefficient betweenC‚ÄãV100CV_{100}and the relative improvement of KG over TS and Greedy across 13 tasks areœÅ1=0.8901\rho_{1}=0.8901andœÅ2=0.7789\rho_{2}=0.7789. The positive correlations suggest that for tasks where the LLM response is highly sensitive to prompt features, the KG policy can prominently outperform TS and Greedy in the SOPL framework.

We also observed that for easier tasks when the LLM response is relatively insensitive to the prompts and the performance function is relatively flat with respect to prompt features, full exploitation policies such as Greedy are adequate to identify an effective prompt. In particular, we observe that Greedy outperforms KG by a very small margin for two easier tasks,antonymsandinformal_to_formal. However, for challenging tasks with high uncertainty, KG consistently yields over a 10% improvement relative to both baselines when other parts of the framework remain the same. Our analysis reveals the importance of the choice of the policy depending on the problem context.

[ÂõæÁâá: images\image_9.png]
ÂõæÁâáËØ¥Êòé: Figure 6:Upper plot: Improvement over SOPL-TS versus the coefficient of variation. Lower plot: Improvement over SOPL-Greedy versus the coefficient of variation

We assess the effectiveness of enriching the prompt representation by multiple features on the two tasks with the largest improvement of SOPL-KG compared to the second best performance, i.e.,orthography_starts_withandrhymes. We use the default meta prompt from[15], which only includes the feature for the required demonstration examples, and use SOPL-KG to select from 20 predefined configurations of demonstration examples. We allowN=30N=30evaluations and repeat the experiments for 20 replications. Figure7shows that the performance deteriorates as the features that enhance the meta prompt are excluded. While searching in a single dimension is easier than optimizing multiple features simultaneously, it results in finding suboptimal prompts. The result suggests that enriching the meta prompt with different features effectively expands the search space and leads to improved performance of the generated prompts.

[ÂõæÁâá: images\image_10.png]
ÂõæÁâáËØ¥Êòé: Figure 7:Comparison of the average test score between our proposed method using all five features and the method using only one feature for the demonstrative examples

This paper introduces SOPL, a sequential optimal prompt learning framework for automated prompt engineering focused on efficient prompt learning in practical scenarios where exhaustive evaluation is costly or impossible. Specifically, we develop a feature-based approach to model prompts, enabling a constraint-based and expansive prompt search space. The forward-looking KG policy with correlated beliefs facilitates efficient and scalable prompt learning. We demonstrate that our proposed method achieves superior performance on instruction induction tasks with only 30 or fewer opportunities of prompt evaluation. We find that the KG policy yields substantial performance gains compared to baseline policies, especially for challenging tasks with high uncertainty. Moreover, our framework allows for future investigation of continuous representations of prompts by embedding vectors. Our work shows a promising direction of leveraging optimal learning methods for efficient prompt learning, paving the way for future research on scalable prompt engineering.

This research was supported in part through the computational resources and staff contributions provided for the Quest high performance computing facility at Northwestern University which is jointly supported by the Office of the Provost, the Office for Research, and Northwestern University Information Technology.

[ÂõæÁâá: images\image_11.png]

[ÂõæÁâá: images\image_12.png]

