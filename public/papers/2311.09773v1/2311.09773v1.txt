Ê†áÈ¢òÔºöTo be or not to be? An exploration of continuously controllable prompt engineering‚Ä†‚Ä†thanks:

As the use of large language models becomes more widespread, techniques like parameter-efficient fine-tuning and other methods for controlled generation are gaining traction for customizing models and managing their outputs. However, the challenge of precisely controlling how prompts influence these models is an area ripe for further investigation. In response, we introduce ControlPE (Continuously Controllable Prompt Engineering). ControlPE enables finer adjustments to prompt effects, complementing existing prompt engineering, and effectively controls continuous targets. This approach harnesses the power of LoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting, enabling fine-tuned adjustments to the impact of prompts. Our methodology involves generating specialized datasets for prompt distillation, incorporating these prompts into the LoRA model, and carefully adjusting LoRA‚Äôs merging weight to regulate the influence of prompts. This provides a dynamic and adaptable tool for prompt control. Through our experiments, we have validated the practicality and efficacy of ControlPE. It proves to be a promising solution for control a variety of prompts, ranging from generating short responses prompts,refusal prompts to chain-of-thought prompts.111Work in progress

Recently, large language models (LLMs) have achieved great success in both industrial and academia applications, such as Med-PaLM[1], ChatLaw[2]and LLMs for education[3].
They not only unify various tasks in the form of text generation, but also eliminate the need of task-specific training via a new paradigm "pre-train, prompt then predict". A key challenge lies in how to design suitable prompts. Evidence shows that a subtle modification of prompt may lead to significant performance change[4].
Among almost unlimited number of natural language prompts, it is difficult to identify the optimal ones. Therefore, prompt engineering has received much attention to ease the discovery.

Although many existing prompt optimization techniques have been proposed to automate the prompt engineering procedure, few work focuses on further controlling or combining these discrete prompts in a continuous space. APE(Automatic Prompt Engineer)[5], a novel approach for automatically generating and selecting effective prompts to guide LLMs). It treats the creation of prompts as a form of program synthesis, where an LLM generates a pool of potential prompts. PromptAgent[6]treats prompt optimization as a strategic planning problem. This means it systematically navigates through the space of potential prompts, aiming to find the most effective ones. However, these automatic prompt engineering methods struggle to find an intermediary state between adding and not adding the already retrieved prompts, making it challenging to identify a balanced approach.

In this paper, we propose a novel continuously controllable prompt engineering method, ControlPE. It can not only further adjust the effects of a prompt at a finer grained level, complementary to existing prompt engineering works, but also is particularly helpful in controlling continuous targets. The previous automated prompt engineering tasks still revolve around searching for the most suitable prompt within discrete natural language prompts. The use of discrete prompts in prompt engineering presents significant limitations. For instance, incorporating a prompt like "keep the answer short and concise" can lead to a 55% reduction in the model‚Äôs average response length. Such a reduction, though significant, can result in the loss of information in responses. Therefore, if we aim to further shorten the model‚Äôs average response length to 20% or 30%, achieving this solely through discrete prompt searching proves to be quite challenging.

To achieve ControlPE, we present a three-step methodology: first, we generate targeted prompt distillation[7,8]datasets, then train LoRA[9]models on these datasets in purpose of distilling the prompt into the LoRA parameters, and finally, we tune LoRA‚Äôs merging weight[10,11]to achieve prompt weighting. Our experiments reveal the utility of this approach in optimizing model responses, be it for providing short and concise answers or handling refusal prompts. This research offers a flexible and powerful tool for tailoring model behavior, paving the way for more nuanced and context-aware responses in natural language processing.

Our contribution can be summarized as follows:

We identify and highlight the needs of continous variable in prompt optimization, complementary to existing prompt engineering techniques.

As we know first work to propose a methodology that enables continuous prompt engineering. This method allows for finer control over prompt effects, introducing a new dimension of flexibility in LLMs.

Implement diverse scenarios such as controlling the response length, providing refusal answers in Document-Based Question Answering (DocQA) tasks, and utilizing chain-of-thought reasoning in mathematical problem-solving tasks.

[ÂõæÁâá: images\image_1.png]
ÂõæÁâáËØ¥Êòé: Figure 1:Effect demonstration picture of the ControlPE applied to "keep the answer short and concise"

[ÂõæÁâá: images\image_2.png]
ÂõæÁâáËØ¥Êòé: Figure 2:Effect demonstration picture of the ControlPE applied to "If there are no references in the known information, respond with "No relevant information available," and avoid fabricating facts"

Prompt engineering is a key method for interacting with generalist models like LLMs, offering a user-friendly way for people to use these technologies. This approach has become popular for various NLP tasks, as highlighted in studies by[12,13,14]. However, designing effective prompts for LLMs is a nuanced task. It can be done either manually, as seen in[15], or through automated systems, as in the works of[16,17]. This is because LLMs don‚Äôt interpret prompts as humans do, a challenge noted by[18,19].

Many successful approaches to prompt tuning involve gradient-based optimization in a continuous space, as discussed in studies by[4,20,21]. However, this method becomes less feasible at larger scales due to the high computational cost and the shift towards using API-accessible models that don‚Äôt provide gradient information.

For instance, a prompt such as "keep the answer short and concise" can effectively reduce the length of a model‚Äôs responses by an average of 55%. This becomes particularly significant in document-based question-answering (QA) tasks, where it is crucial to manage the model‚Äôs tendency to produce information not present in the source material. By incorporating prompts like "If there are no references in the known information, respond with ‚ÄôNo relevant information available,‚Äô and avoid fabricating facts," the model can be guided to refuse to answer when the context does not contain information pertinent to the question. Similarly, prompts designed to encourage step-by-step reasoning[22], such as "Let‚Äôs think step by step," have shown to enhance the model‚Äôs performance on mathematical datasets. The effectiveness of prompt engineering underscores its value in refining the utility and accuracy of language model interactions.

Conventional decoding algorithms are limited in that they cannot integrate constraints during the generation process, making it challenging to ensure that the output meets specific requirements crucial for some applications. In response, research has branched into two main areas focusing on controlled text generation: constrained search algorithms and score-based sampling methods.

Constrained search algorithms work by imposing strict lexical constraints on the outputs, altering the search space to adhere to these constraints. An example is the Constrained Beam Search (CBS) algorithm by[23], which uses a finite-state automaton to track constraint satisfaction. However, CBS has a significant drawback in its complexity, as it requires maintaining a finite-state machine with2Csuperscript2ùê∂2^{C}states (C being the number of constraints), leading to increased time complexity. To address this,[24]and[25]introduced Grid Beam Search (GBS) and Dynamic Beam Allocation (DBA), respectively. Furthermore,[26]developed NeuroLogic decoding, a constrained search algorithm for satisfying complex logic-based lexical constraints in conjunctive normal form, later enhanced by NeuroLogic A* decoding[27]with lookahead heuristics. While these algorithms improve constraint satisfaction rates, they often result in slower generation speeds and lower quality text due to aggressive pruning of the output distribution space, leading to suboptimal results. Additionally, most of these algorithms are limited to lexical constraints.

On the other hand, score-based sampling methods incorporate constraints by transforming them into differentiable score functions. Soft constraints, like sentiment control, can be implemented using the cross-entropy loss of classifiers, while hard constraints, such as lexical constraints, can be modeled by a differentiable n-gram matching function[28]. These methods are more versatile than constrained search algorithms, handling a wider variety of constraints and their combinations. However, they lack guarantees for constraint satisfaction and often compromise generation quality due to alterations in the output distribution[29]. Additionally, these methods are slower due to the need for multiple score-matching steps and require careful tuning of the weights between different constraints and task-specific losses to balance output quality and constraint satisfaction.

Beyond these two methods,[30]proposed a specialized training approach that includes lexical constraints in the input during model training. Similarly,[31]is pre-trained with structures that naturally co-occur with raw texts, allowing it to handle constraints related to style and domain.

[ÂõæÁâá: images\image_3.png]
ÂõæÁâáËØ¥Êòé: Figure 3:Three steps to achieve prompt weighting via LoRA

This section outlines the method used to achieve prompt weighting via LoRA in three steps. As illustrated in Figure 3, the first step involves utilizing instruction data augmented with a target prompt for model inference, which yields labels for constructing a distillation dataset. In the second step, we train the LoRA model using instruction data without adding the target prompt, thereby distilling the influence of the prompt into the LoRA model. In the third step, we fine-tune the influence of the prompt by adjusting the weights of the LoRA model.

LoRA[9]stands out as a parameter-efficient fine-tuning[32]methods large language models, particularly in scenarios with limited computational resources. This approach has gained traction, as evidenced by its inclusion in state-of-the-art models like LLaMA[33,34], and it‚Äôs highlighted for its efficiency in recent literature. LoRA functions by adjusting the output of transformer weight matrices. Typically, these matrices convert an inputxùë•xfrom one dimension to anotherh‚Ñéh, but with LoRA, this conversion is tweaked by adding the product of two projection matrices,BùêµBandAùê¥A, withxùë•x. These matrices are much smaller, which reduces complexity sincerùëür, the rank, is much smaller than the dimensionsdùëëdandkùëòk. This modification is primarily applied to the query and value projection matrices within the attention mechanism of transformers. When initiating LoRA tuning, the matrixAùê¥Ais filled with random Gaussian values, whileBùêµBstarts with zeros. This setup ensures that the original pre-trained model‚Äôs behavior is preserved at the start. The combination ofAùê¥AandBùêµBcomprises the LoRA module, which can be integrated with other distinctively trained LoRA modules for enhanced performance.

In the initial phase, we will embark on training a LoRA that is functionally equivalent to the target prompt. In essence, our objective is to distill the prompt‚Äôs influence on the model into LoRA. Aiming to achieve this effect, we begin by constructing a target prompt distillation dataset. This dataset serves as the foundation for training LoRA models to represent specific prompts.

The process involves using input prompts that incorporate the target prompt, performing inference, and constructing a new datasetDd‚Äãi‚Äãs‚Äãt‚Äãi‚Äãl‚Äãlsubscriptùê∑ùëëùëñùë†ùë°ùëñùëôùëôD_{distill}with input dataxùë•xinherited from the original dataset and labelsyt‚Äãa‚Äãr‚Äãg‚Äãe‚Äãtsubscriptùë¶ùë°ùëéùëüùëîùëíùë°y_{target}are generated by the large language modelŒ∏ùúÉ\thetausing the original prompt templatePo‚Äãr‚Äãi‚Äãg‚Äãi‚Äãn‚Äãa‚ÄãlsubscriptùëÉùëúùëüùëñùëîùëñùëõùëéùëôP_{original}, target promptPt‚Äãa‚Äãr‚Äãg‚Äãe‚ÄãtsubscriptùëÉùë°ùëéùëüùëîùëíùë°P_{target}injected with input dataxùë•xas input.

The next step involves training LoRA models to distill the target prompts. During this training phase, we remove the target prompts from the input prompts.

We demonstrate the process of applying LoRA to the model‚Äôs parametersŒ∏ùúÉ\theta. This results in the creation of a modified model, denoted asŒ∏L‚Äão‚ÄãR‚ÄãAsubscriptùúÉùêøùëúùëÖùê¥\theta_{LoRA}, which incorporates the influence of LoRA.LL‚Äão‚ÄãR‚ÄãAsubscriptùêøùêøùëúùëÖùê¥L_{LoRA}denoted as the loss function of LoRA, that assesses the disparity between the model‚Äôs predictions and the target labels generated in the last step. It operates on a datasetDd‚Äãi‚Äãs‚Äãt‚Äãi‚Äãl‚Äãlsubscriptùê∑ùëëùëñùë†ùë°ùëñùëôùëôD_{distill}, where each data point consists of inputxùë•xand its corresponding target labelyt‚Äãa‚Äãr‚Äãg‚Äãe‚Äãtsubscriptùë¶ùë°ùëéùëüùëîùëíùë°y_{target}. TheC‚Äãr‚Äão‚Äãs‚Äãs‚ÄãE‚Äãn‚Äãt‚Äãr‚Äão‚Äãp‚Äãyùê∂ùëüùëúùë†ùë†ùê∏ùëõùë°ùëüùëúùëùùë¶CrossEntropyfunction quantifies the dissimilarity between the model‚Äôs prediction and the target label for each data point. The objective is to minimize this loss, guiding the model to make LoRA approximate the impact of the target prompts. In essence, LoRA is fitted to the target prompts in this step.

In the final step, we introduce prompt weighting by tuning LoRA‚Äôs merging[10,11]weight. Having already obtained LoRA models that approximate the target prompts, we fine-tune LoRA‚Äôs parameter weights to control its actual influence on the model. As a result, the prompt‚Äôs impact can be adjusted by dynamically modifying the LoRA parameter weights.

This approach allows for precise control over prompt influence, and its effectiveness can be empirically verified by demonstrating the validity of equations (6) as presented in the method.

In this section, we present the experiments conducted to evaluate the effectiveness of our proposed method for weighting prompts using LoRA in the field of Natural Language Processing.

Our foundational model is LLaMA2-7B-chat[34], and we utilize the peft[32]library for training all LoRA adapters. These adapters are integrated into every q_proj and v_proj module within the transformer architecture. We‚Äôve configured the LoRA rank at 16. For our training process, we employ a batch size of 128 and a learning rate of 3e-4. The entire distillation training is conducted over a span of 3 epochs.

[ÂõæÁâá: images\image_4.png]
ÂõæÁâáËØ¥Êòé: (a)The response length of a language model varies with adjustments to the LoRA weights

[ÂõæÁâá: images\image_5.png]
ÂõæÁâáËØ¥Êòé: (a)The response length of a language model varies with adjustments to the LoRA weights

Firstly, we demonstrated the effectiveness of ControlPE in controlling the response length of a model.
Our test dataset comprised instructions numbered 10001 to 10100 from the 52k alpaca-data[35]. When employing the standard alpaca response prompt template, the total response length for these 100 instructions was 14,766 tokens. However, after integrating the prompt "Keep the answer short and concise" into the standard template, the total response length for the same set of instructions was reduced to 7,096 tokens. We then used this modified prompt template with the added "Keep the answer short and concise" on the first 10,000 instructions from the 52k alpaca-data for inference with the original model. The inference results were used as labels for a distillation dataset, and the standard prompt template without the "Keep the answer short and concise" was used as input for distillation training. Through this process, we aimed to distill the influence of the target prompt into LoRA parameters. The LoRA model, post-distillation, produced a total response length of 6,640 tokens on the 100 test instructions.

Subsequently, as shown in Figure 4(a) we adjusted the weights in the LoRA model and observed its performance on the 100 test instructions. We found that adjusting the weights of the LoRA matrix can effectively control the model‚Äôs response length. Applying weights only to the sample down LoRA matrix allows for a linear adjustment of the model‚Äôs response length. Applying weights to both matrices results in a non-linear relationship between the response length and the weights. Therefore, we recommend applying weights to only a single LoRA matrix for all experiments.

[ÂõæÁâá: images\image_6.png]
ÂõæÁâáËØ¥Êòé: Figure 5:Effect demonstration picture of the ControlPE applied to "If there are no references in the known information, respond with "No relevant information available," and avoid fabricating facts"

In this section, we will demonstrate how the ControlPE performs in refusing to answer questions in the DocQA[36]task. The DocQA task involves a language model answering questions based on a provided document. However, in real-world scenarios, there are instances where the document cannot answer the question, necessitating a prompt that guides the model to refuse to answer - "If there are no references in the known information, respond with ‚ÄôNo relevant information available,‚Äô and avoid fabricating facts." We utilized the wikipedia-trivia[37]dataset, specifically entries 10001 to 10200, where 80% of the documents were positive (meaning the given context was sufficient to answer the question, and a direct answer was expected rather than a refusal) and 20% were negative (meaning the context was insufficient, and the model should respond with "No relevant information available"). Before adding the refusal prompt, the refusal rate was 0%. After adding the refusal prompt, the refusal rate increased to 27%, as indicated by the red crosses in the graph. Within this 27% refusal rate, the precision (the accuracy of refused answers among questions that should be refused) was 31%, and the recall (the rate of questions that should be refused and were indeed refused) was 53%.

We used the first 10,000 entries of the wikipedia-trivia dataset with the refusal prompt added to train the LoRA model, treating the inference results as labels. This method distilled the refusal capability of the prompt into LoRA. The refusal rate of the distilled model on the test set was 29%. As shown in the graph, by adjusting the weight, the refusal rate of the model linearly increased with weights between 0.6 to 1.0. In the 0.1-0.5 weight range, the model did not directly refuse to answer but reduced hallucinations and indirectly refused many questions (we consider a direct refusal only when the model explicitly states "No relevant information available").

Furthermore, by adjusting the weight between 0.6 and 1.0, we tested the model‚Äôs recall and precision in answering. We observed that as the refusal capability strengthened, precision weakened while recall increased. Therefore, we can find an appropriate trade-off between recall and precision through parameter tuning.

[ÂõæÁâá: images\image_7.png]
ÂõæÁâáËØ¥Êòé: Figure 6:Effect demonstration picture of the ControlPE applied to "Let‚Äôs think step by step"

In this experiment, we directly utilized OpenAI‚Äôs GSM8K[38]dataset for ControlPE in Chain of Thought (CoT)[22]analysis. We observed that without the prompt "Let‚Äôs think step by step," the accuracy of Llama2-7b-chat on the GSM8K dataset was 12.6%. However, adding the prompt "Let‚Äôs think step by step" increased the model‚Äôs accuracy to 14.6%. We then constructed a CoT distillation dataset on the first 10k instructions from the 52k alpaca dataset: we used inferences with the added prompt "Let‚Äôs think step by step" as labels and instructions without this prompt as inputs. The "Let‚Äôs think step by step" was distilled into the LoRA parameters. The distilled model exhibited a performance of 14.9% on GSM8K.

Subsequently, we adjusted the weights of LoRA and observed its performance on the GSM8K test set. Surprisingly, the best performance on the GSM8K dataset was not achieved by fully applying the CoT influence on the model. Instead, according to figure 6, a slightly weaker influence, such as 80%, led to the optimal performance of Llama2-7B-chat on GSM8K.

[ÂõæÁâá: images\image_8.png]
ÂõæÁâáËØ¥Êòé: (a)The response length of a language model varies with adjustments to the 2 LoRAs weights

[ÂõæÁâá: images\image_9.png]
ÂõæÁâáËØ¥Êòé: (a)The response length of a language model varies with adjustments to the 2 LoRAs weights

In the fusion experiment, we will combine the LoRA model previously trained on alpaca-data, which was distilled with the directive "Keep the answer short and concise," with the LoRA model distilled on wikipedia-trivia with the guideline "If there are no references in the known information, respond with ‚ÄôNo relevant information available,‚Äô and avoid fabricating facts." Note that in this experiment, we will not train a new model but will directly use the models distilled from the two previous experiments. The test dataset used is the wikipedia-trivia DocQA data from entries 10001 to 10200, with 80% of the documents being positive and 20% negative.

As illustrated in the Figure 7(a) and Figure 7(b), although the LoRA model distilled with "Keep the answer short and concise" was not trained on wikipedia-trivia, it still maintains the ability to linearly regulate the response length. During the fusion, while they influence each other, they still retain the capability to adjust their influence on the prompt itself.

Our research presents an innovative approach to prompt weighting in natural language processing through the application of Low-Rank Adaptation (LoRA). This study is motivated by the growing importance of prompt engineering in controlling large language models, where precise control of prompt influence is crucial. By leveraging the efficiency of LoRA, our method introduces a sophisticated, yet accessible means of tuning the influence of specific prompts on model behavior.

We explored three key areas in our experiments: response length control, refusal to answer, and chain of thought reasoning. The results show that our approach can effectively modulate the influence of prompts in each scenario. For instance, in controlling response length, our method demonstrated the ability to adjust model outputs linearly, offering a fine-grained control over verbosity. In refusal-to-answer scenarios, we successfully increased the refusal rate while balancing precision and recall, addressing the challenge of information fabrication. Furthermore, in chain of thought reasoning, our method improved accuracy by selectively applying prompt influence, underscoring the importance of nuanced prompt control.

Additionally, our experiments on multiple ControlPEs fusion showcased the potential of our method in managing multiple prompts simultaneously, maintaining the ability to independently adjust each prompt‚Äôs influence.

In conclusion, this research contributes significantly to the field of prompt engineering in NLP. The ability to weight prompts using LoRA not only enhances the control over large language models but also opens up new avenues for research and practical applications. Our method stands as a testament to the evolving landscape of NLP, where user control and model flexibility are increasingly paramount.

TODO

[ÂõæÁâá: images\image_10.png]

[ÂõæÁâá: images\image_11.png]

