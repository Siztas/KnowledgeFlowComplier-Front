æ ‡é¢˜ï¼šOn the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems

Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited. This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.

In recent years, notable advancements have materialized in the realm of recommendation techniques, transcending the scope of traditional approaches (such as collaborative filtering, content-based recommendation, and matrix factorization(Lu etÂ al.,2015)). This evolution has led to the emergence of deep learning-based methods in the field of recommender systems (RS). The appeal of deep learning stems from its ability to comprehend intricate non-linear relationships between users and items, making it adept at accommodating diverse data sources like images and text.
The adoption of deep learning in RS has proven beneficial in tackling multifaceted challenges. Its strength lies in addressing intricate tasks and managing complex data structures(Zhang etÂ al.,2019a).

Traditional recommendation systems (RS) have limitations in capturing interest dynamics, a challenge that emphasizes the distinction between usersâ€™ long-term and short-term interests(Chen etÂ al.,2020; Zhang etÂ al.,2019a). Specifically, while these traditional methods are adept at recognizing and modeling long-term interests based on historical data and patterns, they often fall short in accounting for the rapidly changing and more nuanced short-term interests. This gap in responsiveness to short-term shifts can lead to recommendations that are out-of-sync with a userâ€™s current preferences or situational needs.
In contrast, deep reinforcement learning (RL) aims to train an agent with the capacity to learn from interaction trajectories provided by the environment, achieved through the integration of deep learning and RL techniques as expounded in(Chen etÂ al.,2023c). Notably, this approach empowers the agent to proactively glean insights from real-time user feedback, thereby enabling the discernment of evolving user preferences within the dynamic context of reinforcement learning.

RL provides a structured mathematical framework for acquiring learning-based control strategies. By employing RL, we can systematically attain highly effective behavioral policies, which encapsulate action strategies. These policies are engineered to optimize predefined objectives referred to as reward functions. In essence, the reward function serves as a directive, guiding the RL algorithm towards desired actions, while the algorithm itself devises the means to enact these actions. Throughout its history, the field of RL has been a subject of intensive research. More recently, the integration of robust tools like deep neural networks into RL methodologies has yielded substantial advancements. These neural networks act as versatile approximators, empowering RL techniques to exhibit exceptional performance across a diverse array of problem domains.

Nevertheless, a pertinent challenge to the widespread implementation of RL techniques emerges. RL methods fundamentally follow an incremental learning approach, wherein they gather knowledge by iteratively engaging with their environment. Subsequent refinements are informed by previous experiences. While this iterative learning approach is effective in numerous scenarios, its practicality is not universal. Consider cases such as real-world robotics, educational software pedagogy, or healthcare interventions; these situations entail potential risks and resource expenses that cannot be disregarded.
Moreover, even within scenarios conducive to online learning, such as in the context of RS, a preference for historical data often arises. This preference is particularly pronounced in intricate domains where sound decision-making hinges upon substantial data inputs. The rationale is that leveraging previously amassed data enables informed decisions without necessitating continuous real-world experimentation.

The success of machine learning methods in solving real-world problems in the past decade is largely thanks to new ways of learning from large amounts of data. These methods get better as theyâ€™re trained with more data. However, applying this approach to online Reinforcement Learning (RL) doesnâ€™t fit well. While this wasnâ€™t a big problem when RL methods were simpler and used small datasets for easy problems, adding complex neural networks to RL makes us wonder if we can use the same data-driven approach for RL goals. This would mean creating a system where RL learns from existing data without needing more data collected in real-time(Levine etÂ al.,2020).

However, this idea of using existing data for RL brings its own challenges. As we discuss in this article, many common RL methods can learn from data collected differently from how the policy behaves. But these methods often struggle when trying to learn effectively from a whole set of data collected in advance, without more data being collected as the policy improves. Making things more complicated with high-dimensional neural networks can make this problem worse. A big issue with using pre-existing data for RL is that the dataâ€™s distribution may not match real-world conditions(Levine etÂ al.,2020). Still, the potential of a fully offline RL system is exciting. Just like how machine learning can turn data into useful tools like image recognition or speech understanding, an offline RL system, using strong function approximators, might turn data into smart decision-makers. This could mean that people with lots of data could make policies that help them make better choices for what they want to achieve(Mazoure etÂ al.,2022).

RS and advertising are particularly well-suited areas for applying offline RL. This is because collecting data is straightforward and efficient, often done by recording user actions. Moreover, the existing RS literature provides sufficient datasets which can be used for training offline RL. However, these domains are also critical in terms of safety. Making a very poor decision could lead to significant financial losses. Therefore, traditional online exploration methods are not practical here. This is why offline RL methods have a history of being used in these fields.

One technique commonly employed is called off-policy evaluation. This approach is useful for running A/B tests and estimating the effectiveness of advertising and RS methods without needing to interact with the environment further.

In the case of RS, things are a bit different compared to other applications. RS policy evaluation is often set up as a contextual bandit problem. Here, â€statesâ€ might represent a userâ€™s past behavior, and â€actionsâ€ are the recommendations made to them. This simplification avoids the complexity of sequential decision making, which is useful. However, it can lead to inaccuracies if actions are connected over time, like in robotics or healthcare scenarios.

Using offline RL for RS has practical applications such as optimizing recommendations presented together on a page, improving entire web pages, and estimating website visits with the help of doubly robust estimation. Another use is A/B testing to fine-tune click rates for optimization. Researchers have also used offline data to learn policies. This includes efforts like improving click-through rates for newspaper articles, ranking advertisements on search pages, and tailoring ad recommendations for digital marketing.

In this survey, our main focus will be on offline RL in RS (offline RL4RS). We aim to provide a comprehensive overview of existing works, along with discussing open challenges and future directions.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1.The overall structure of this survey including the section index.

Two existing surveys have centered on the topic of RL in RS(Chen etÂ al.,2023c; Afsar etÂ al.,2022). WhileAfsar etÂ al.(2022)provides an overview of RL in RS, it does not comprehensively explore the expanding realm of deep RL. In contrast,(Chen etÂ al.,2023c)delves more deeply into the analysis and discussion of RL in RS, but predominantly focuses on online RL and its RS applications. Itâ€™s noteworthy that(Chen etÂ al.,2023c)identifies offline RL in RS as a potential future direction but does not offer an all-encompassing review of this area. The limited coverage of offline RL in RS can be attributed to its emergence around the same time as these two surveys. Furthermore, due to the recent establishment of the offline RL concept, certain works examined in these two existing surveys are classified as special cases of policy-based methods. Differently, this survey endeavors to refine these categorizations by reclassifying prior works into the domain of offline RL in RS. Additionally, we extend the literature to encompass the most recent developments in offline RL for RS, thereby augmenting our understanding of recent progress in this field.

This survey is structured into four distinct sections. Firstly, we offer an introduction to RL basics, providing readers with a foundational understanding of various RL algorithms, including Q-Learning, Policy-based Methods, Actor-Critic Methods, and Model-based RL. Subsequently, we delve into the concept of offline RL and present a problem formulation that explores how to integrate recommender systems (RS) into the offline RL framework.
Continuing, we conduct a comprehensive review of existing works from two main perspectives: off-policy evaluation using logged data and the realm of offline RL in RS. This examination highlights current research trends and insights. Following the review, we outline the open challenges and promising opportunities that warrant in-depth exploration.
Finally, building upon the identified challenges and opportunities, we propose potential future directions that could serve as solutions to these challenges. This forward-looking section aims to guide future research endeavors in the field, by suggesting pathways to address the outstanding issues and capitalize on the untapped opportunities.

In this section, we delve into fundamental concepts essential to understanding the field of RL. We initiate with RL preliminaries, encompassing Markov Decision Processes, On-Policy and Off-Policy Learning, and Typical RL algorithms. In doing so, we establish the foundational understanding by clarifying key principles and terminologies employed throughout this survey. Subsequently, we shift our focus toward the concept of offline RL and how it can be used to formulate RS. For the sake of clarity, we have summarized the common notations used in this survey inTable1.

In this section, we shall expound upon fundamental concepts within the realm of RL, adhering closely to established standard definitions as outlined in(Sutton and Barto,2018). RL deals with the challenge of learning how to control dynamic systems in a broad context. RL4RS are typically described by fully observed Markov decision processes (MDP) or partially observed ones known as Partially Observable Markov Decision Processes (POMDP). Moreover, we will also provide

The Markov decision process is formalized as the tupleâ„³={ğ’®,ğ’œ,ğ’«,â„›,Î³}â„³ğ’®ğ’œğ’«â„›ğ›¾\mathcal{M}=\{\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma\}. Within this structure, each component serves a distinct purpose:ğ’®ğ’®\mathcal{S}encompasses the set of statessâˆˆğ’®ğ‘ ğ’®s\in\mathcal{S}, capable of adopting discrete or continuous values, potentially even multi-dimensional vectors.ğ’œğ’œ\mathcal{A}characterizes the set of actionsaâˆˆğ’œğ‘ğ’œa\in\mathcal{A}, which may be discrete or continuous in nature.ğ’«ğ’«\mathcal{P}defines a conditional probability distribution,ğ’«â€‹(st+1|st,at)ğ’«conditionalsubscriptğ‘ ğ‘¡1subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡\mathcal{P}(s_{t+1}|s_{t},a_{t}), delineating the progression of the systemâ€™s dynamics over time.â„›:ğ’®Ã—ğ’œâ†’â„:â„›â†’ğ’®ğ’œâ„\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}serves as the reward function, linking states and actions to real-valued rewards.Î³âˆˆ[0,1]ğ›¾01\gamma\in[0,1]assumes the role of a scalar discount factor, influencing the extent to which future rewards are taken into consideration.

Throughout most of this article, we will primarily employ fully-observed formalism. However, we also include the definition of the partially observed Markov decision process (POMDP) to ensure comprehensiveness. The MDP definition can be extended to the partially observed setting in the following manner:

The partially observed Markov decision process is defined as a tupleâ„³={ğ’®,ğ’œ,ğ’ª,ğ’«,â„›,Î³}â„³ğ’®ğ’œğ’ªğ’«â„›ğ›¾\mathcal{M}=\{\mathcal{S},\mathcal{A},\mathcal{O},\mathcal{P},\mathcal{R},\gamma\}, whereğ’®,ğ’œ,ğ’«,â„›,Î³ğ’®ğ’œğ’«â„›ğ›¾\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gammaare defined as before,ğ’ªğ’ª\mathcal{O}is a set of observations, where each observation is given byoâˆˆğ’ªğ‘œğ’ªo\in\mathcal{O}.

The ultimate objective within a RL problem is to acquire a policy, denoted asÏ€ğœ‹\pi, which establishes a probability distribution over actions conditioned upon states,Ï€â€‹(at|st)ğœ‹conditionalsubscriptğ‘ğ‘¡subscriptğ‘ ğ‘¡\pi(a_{t}|s_{t}), or alternatively conditioned upon observations within the context of partially observed scenarios,Ï€â€‹(at|ot)ğœ‹conditionalsubscriptğ‘ğ‘¡subscriptğ‘œğ‘¡\pi(a_{t}|o_{t}). From these definitions, we can derive the trajectory distribution. A trajectory in this context refers to a sequence encompassing both states and actions, spanning a length ofTğ‘‡T, represented asÏ„={s0,a0,â‹¯,sT,aT}ğœsubscriptğ‘ 0subscriptğ‘0â‹¯subscriptğ‘ ğ‘‡subscriptğ‘ğ‘‡\tau=\{s_{0},a_{0},\cdots,s_{T},a_{T}\}. It is noteworthy that the parameterTğ‘‡Tcan be an infinite value, implying the consideration of scenarios with an indefinite time horizon, as seen in infinite horizon MDP(Sutton and Barto,2018).

The trajectory distributionpÏ€subscriptğ‘ğœ‹p_{\pi}for a given MDP tupleâ„³â„³\mathcal{M}and policyÏ€ğœ‹\piis given by

whered0â€‹(s0)subscriptğ‘‘0subscriptğ‘ 0d_{0}(s_{0})represents the initial state distribution.
This definition can easily be extended into the partially observed setting by including the observationsotsubscriptğ‘œğ‘¡o_{t}. The RL objectiveJâ€‹(Ï€)ğ½ğœ‹J(\pi), can then be written as an expectation under this trajectory distribution:

While the process of interaction unfolds, gathering additional episodes enhances the precision of the function estimates. Nevertheless, a challenge arises. If the policy improvement algorithm consistently adjusts the policy in a greedy mannerâ€”prioritizing actions with immediate rewardsâ€”then actions and states lying outside this advantageous route might not be adequately sampled. Consequently, superior rewards that could exist in these unexplored areas remain concealed from the learning process. Fundamentally, we confront a decision between opting for the optimal choice based on existing data or delving deeper into exploration to collect more data. This predicament is commonly recognized as the Exploration vs.Â Exploitation Dilemma.

What we need is a middle ground between these two extremes. Pure exploration would require a significant amount of time to collect the necessary information, while pure exploitation could trap the agent in a local reward maximum. To address this, there are two approaches that ensure all actions are adequately sampled, known ason-policyandoff-policymethods.

On-policymethods resolve the exploration vs.Â exploitation dilemma by incorporating randomness through a soft policy. This means that non-greedy actions are chosen with some probability. These policies are referred to asÏµitalic-Ïµ\epsilon-greedy policies because they select random actions with a probability ofÏµitalic-Ïµ\epsilonand follow the optimal action with a probability of 1-Ïµitalic-Ïµ\epsilon.

Since the probability of randomly selecting an action from the action space isÏµitalic-Ïµ\epsilon, the probability of choosing any specific non-optimal action isÏµ/|ğ’œâ€‹(s)|italic-Ïµğ’œğ‘ \epsilon/|\mathcal{A}(s)|. On the other hand, the probability of following the optimal action will always be slightly higher due to the 1 -Ïµitalic-Ïµ\epsilonprobability of selecting it outright and theÏµ/|ğ’œâ€‹(s)|italic-Ïµğ’œğ‘ \epsilon/|\mathcal{A}(s)|probability of choosing it through sampling the action space:

Off-policymethods offer a different solution to the exploration vs.Â exploitation problem. While on-policy algorithms attempt to improve the sameÏµitalic-Ïµ\epsilon-greedy policy used for exploration, off-policy approaches utilize two distinct policies: a behavior policy and a target policy. The behavioral policy (denoted asÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}) is employed for exploration and episode generation, while the target or goal policy (denoted asÏ€ğœ‹\pi) is used for function estimation and improvement.

The efficacy of this approach lies in the capacity of the target policyÏ€ğœ‹\pito attain a balanced perspective of the environment, enabling it to assimilate insights from the behavioral policybğ‘b, while concurrently capturing advantageous actions and seeking further improvements. Nevertheless, it is imperative to acknowledge that in off-policy learning, a distributional discrepancy arises between the target policy estimation and the sampled policy. Consequently, a widely employed technique known as importance sampling is applied to address this disparity(Li,2017).

Letâ€™s briefly outline various types of RL algorithms and present their definitions. At a high level, all standard RL algorithms follow a common learning loop: the agent engages with the MDPâ„³â„³\mathcal{M}using a behavior policyÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}. This behavior policy, which could or could not align withÏ€â€‹(a|s)ğœ‹conditionalğ‘ğ‘ \pi(a|s), leads the agent to observe the current statestsubscriptğ‘ ğ‘¡s_{t}, choose an actionatsubscriptğ‘ğ‘¡a_{t}, and then witness the subsequent statest+1subscriptğ‘ ğ‘¡1s_{t+1}and the reward valuert=â„›â€‹(st,at)subscriptğ‘Ÿğ‘¡â„›subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡r_{t}=\mathcal{R}(s_{t},a_{t}). This sequence can repeat over multiple steps, allowing the agent to gather transitions{st,at,st+1,rt}subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ‘ ğ‘¡1subscriptğ‘Ÿğ‘¡\{s_{t},a_{t},s_{t+1},r_{t}\}. These observed transitions are then used by the agent to adjust its policy, and this update process might incorporate earlier observed transitions as well. Weâ€™ll denote the set of available transitions for policy updating asğ’Ÿ={(st,at,st+1,rt)}ğ’Ÿsubscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ‘ ğ‘¡1subscriptğ‘Ÿğ‘¡\mathcal{D}=\{(s_{t},a_{t},s_{t+1},r_{t})\}. This set could encompass all the transitions seen thus far or a subset thereof.

Q-learning(Watkins and Dayan,1992)is an off-policy value-based learning scheme aimed at finding a target policy that selects the best action:

Here,Quâ€‹(s,a)subscriptğ‘„ğ‘¢ğ‘ ğ‘Q_{u}(s,a)represents the Q-value and applies to a discrete action space. For deterministic policies, the Q-value can be computed as:

Deep Q learning (DQN)(Mnih etÂ al.,2015)employs deep learning to approximate a nonlinear Q function parameterized byÎ¸qsubscriptğœƒğ‘\theta_{q}:QÎ¸qâ€‹(s,a)subscriptğ‘„subscriptğœƒğ‘ğ‘ ğ‘Q_{\theta_{q}}(s,a). DQN involves a networkQÎ¸qsubscriptğ‘„subscriptğœƒğ‘Q_{\theta_{q}}thatâ€™s updated asynchronously by minimizing the Mean Squared Error (MSE) as defined by:

In this equation,Ï„ğœ\tausignifies a sampled trajectory including(s,a,sâ€²,râ€‹(s,a))ğ‘ ğ‘superscriptğ‘ â€²ğ‘Ÿğ‘ ğ‘(s,a,s^{\prime},r(s,a)). Notably,stâ€²subscriptsuperscriptğ‘ â€²ğ‘¡s^{\prime}_{t}andatâ€²subscriptsuperscriptğ‘â€²ğ‘¡a^{\prime}_{t}originate from the behavior policyÏ€bsubscriptğœ‹ğ‘\pi_{b}, whiles,ağ‘ ğ‘s,acome from the target policyÏ€ğœ‹\pi.

Furthermore, the concept of value functions plays a role. These assess states and actions. The value functionVÏ€â€‹(s)subscriptğ‘‰ğœ‹ğ‘ V_{\pi}(s)evaluates states, andQÏ€â€‹(st,at)subscriptğ‘„ğœ‹subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡Q_{\pi}(s_{t},a_{t})evaluates actions. The relationship between them is given by:

The value function gets updated via the Temporal Difference (TD) method:

whereÎ±ğ›¼\alpharepresents the learning rate.

Policy gradient(Williams,1992)is a technique used in reinforcement learning that tackles scenarios where actions are high-dimensional or continuousâ€”something not easily managed by Q-learning. Unlike Q-learning, which focuses on finding optimal actions, policy gradient aims to find optimal parametersÎ¸ğœƒ\thetafor a policyÏ€Î¸subscriptğœ‹ğœƒ\pi_{\theta}in order to maximize the total reward.

The central goal of policy gradient is to maximize the expected return, or accumulated reward, starting from the initial state. This is captured by the equation:

Here,Ï€Î¸â€‹(Ï„)subscriptğœ‹ğœƒğœ\pi_{\theta}(\tau)signifies the probability of observing trajectoryÏ„ğœ\tau. The technique learns the optimal parameterÎ¸ğœƒ\thetaby computing the gradientâˆ‡Î¸Jâ€‹(Ï€Î¸)subscriptâˆ‡ğœƒğ½subscriptğœ‹ğœƒ\nabla_{\theta}J(\pi_{\theta})as follows:

In the above equation,dÏ€Î¸subscriptğ‘‘subscriptğœ‹ğœƒd_{\pi_{\theta}}is the distribution of trajectories generated by policyÏ€Î¸subscriptğœ‹ğœƒ\pi_{\theta}.

The derivation involves the substitution:

Here,pâ€‹(â‹…)ğ‘â‹…p(\cdot)is independent of the policy parameterÎ¸ğœƒ\theta, and for simplicity, itâ€™s not explicitly included in the derivation.

Prior policy gradient algorithms, like REINFORCE, have often used Monte-Carlo sampling to estimateÏ„ğœ\taufromdÏ€Î¸subscriptğ‘‘subscriptğœ‹ğœƒd_{\pi_{\theta}}.

Actor-critic networksbring together the strengths of both Q-learning and policy gradient techniques. They can function as either on-policy methods(Konda and Tsitsiklis,2000)or off-policy methods(Degris etÂ al.,2012). An actor-critic network is composed of two key components:

The actor: This component optimizes the policyÏ€Î¸subscriptğœ‹ğœƒ\pi_{\theta}based on the guidance provided byâˆ‡Î¸Jâ€‹(Ï€Î¸)subscriptâˆ‡ğœƒğ½subscriptğœ‹ğœƒ\nabla_{\theta}J(\pi_{\theta}).

The critic: The critic evaluates the learned policyÏ€Î¸subscriptğœ‹ğœƒ\pi_{\theta}using the Q-value functionQÎ¸qâ€‹(s,a)subscriptğ‘„subscriptğœƒğ‘ğ‘ ğ‘Q_{\theta_{q}}(s,a).

The overall gradient expression for an actor-critic network is as follows:

In the case of off-policy learning, the value function forÏ€Î¸â€‹(a|s)subscriptğœ‹ğœƒconditionalğ‘ğ‘ \pi_{\theta}(a|s)can be further defined using deterministic policy gradient (DPG):

Itâ€™s worth noting that while traditional policy gradient calculations involve integrating over both the state spaceğ’®ğ’®\mathcal{S}and the action spaceğ’œğ’œ\mathcal{A}, DPG only requires integrating over the state spaceğ’®ğ’®\mathcal{S}. In DPG, given a statesâˆˆğ’®ğ‘ ğ’®s\in\mathcal{S}, there corresponds only one actionaâˆˆğ’œğ‘ğ’œa\in\mathcal{A}such thatÎ¼Î¸â€‹(s)=asubscriptğœ‡ğœƒğ‘ ğ‘\mu_{\theta}(s)=a.

Model-based RLis a broad term encompassing methods that rely on explicit estimates of the transition or dynamics functionğ’«ğ’«\mathcal{P}. The distinguishing feature of model-based RL is that it assumes the dynamics modelğ’«ğ’«\mathcal{P}is known and can be learned. This is in contrast to other forms of RL where such a dynamics model is neither known nor learnable.

The offline RL problem can be defined as a data-driven formulation of the RL problem(Levine etÂ al.,2020). The ultimate objective remains centered on optimizing the goal presented inEquation2. Notably, the agentâ€™s capacity to engage with the environment and amass supplementary transitions using the behavior policy is nullified. Instead, the learning algorithm receives a fixed collection of transitions denoted asğ’Ÿ={sti,ati,st+1i,rti}ğ’Ÿsuperscriptsubscriptğ‘ ğ‘¡ğ‘–superscriptsubscriptğ‘ğ‘¡ğ‘–superscriptsubscriptğ‘ ğ‘¡1ğ‘–superscriptsubscriptğ‘Ÿğ‘¡ğ‘–\mathcal{D}=\{s_{t}^{i},a_{t}^{i},s_{t+1}^{i},r_{t}^{i}\}, and its task is to acquire the most optimal policy using this provided dataset. This approach aligns more closely with the supervised learning paradigm, and we can viewğ’Ÿğ’Ÿ\mathcal{D}as the training dataset for the policy.

Fundamentally, offline RL necessitates that the learning algorithm comprehends the underlying dynamics of the MDPâ„³â„³\mathcal{M}solely from a fixed dataset. Subsequently, it must create a policyÏ€â€‹(a|s)ğœ‹conditionalğ‘ğ‘ \pi(a|s)that achieves the highest cumulative reward during the interaction with the MDP. We will denote the distribution over states and actions inğ’Ÿğ’Ÿ\mathcal{D}asÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}(also referred to as the behavior policy). Here, we assume that state-action pairs(s,a)âˆˆğ’Ÿğ‘ ğ‘ğ’Ÿ(s,a)\in\mathcal{D}are drawn fromsâˆ¼dÏ€Î²â€‹(s)similar-toğ‘ superscriptğ‘‘subscriptğœ‹ğ›½ğ‘ s\sim d^{\pi_{\beta}}(s), and actions are sampled according to the behavior policy, i.e.,aâˆ¼Ï€Î²â€‹(a|s)similar-toğ‘subscriptğœ‹ğ›½conditionalğ‘ğ‘ a\sim\pi_{\beta}(a|s).

This problem formulation has been expressed using a range of terminologies. Within the field of RS, the term that frequently induces confusion is â€œoff-policy RLâ€. This phrase is commonly employed as a broad label encompassing all RL algorithms that can leverage datasets of transitionsğ’Ÿğ’Ÿ\mathcal{D}, wherein the actions in each transition were acquired using policies distinct from the current policyÏ€â€‹(a|s)ğœ‹conditionalğ‘ğ‘ \pi(a|s). However, itâ€™s important to note that the term â€œoff-policyâ€ typically signifies an RL algorithm where the behavior policyÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}differs from the target policyÏ€ğœ‹\pi, as discussed earlier. This distinction can sometimes cause confusion. Hence, the terms â€œfully off-policy RLâ€ or â€œoffline RLâ€ are recently introduced to indicate situations where no additional online data collection takes place. We have presented various illustrations of distinct RL approaches to emphasize the disparities between them inFigure2.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2.Illustration of classic on-policy RL (a), classic off-policy RL (b), and offline RL (c). Where (a) and (b) can also be recognized as online RL.

The challenge of offline RL can be tackled through algorithms belonging to any of the four main categories in RL: Q-learning, policy gradient, actor-critic, and model-based RL. In principle, any off-policy RL algorithm could function as an offline RL approach when the online interaction process is excluded. For instance, a straightforward offline RL technique can be created by employing Q-learningwithout requiring supplementary online exploration. This method utilizes the datasetğ’Ÿğ’Ÿ\mathcal{D}to pre-fill the data buffer.

In this section, we establish a problem formulation for Offline RL4RS. We begin with a standard MDP framework, commonly used in RS. The setup involves a set of users denoted asğ’°=u,u1,u2,u3,â€¦ğ’°ğ‘¢subscriptğ‘¢1subscriptğ‘¢2subscriptğ‘¢3â€¦\mathcal{U}={u,u_{1},u_{2},u_{3},...}and a set of items denoted asâ„=i,i1,i2,i3,â€¦â„ğ‘–subscriptğ‘–1subscriptğ‘–2subscriptğ‘–3â€¦\mathcal{I}={i,i_{1},i_{2},i_{3},...}. The process begins with the system recommending itemiğ‘–ito useruğ‘¢uand receiving feedbackfiusuperscriptsubscriptğ‘“ğ‘–ğ‘¢f_{i}^{u}. This feedback is then utilized to enhance future recommendations, leading to the identification of an optimal policyÏ€âˆ—superscriptğœ‹\pi^{*}that guides the selection of items to recommend in order to achieve positive feedback.

The MDP framework treats the user as the environment while the system acts as the agent. The fundamental components within the MDP context, especially in Deep Reinforcement Learning (DRL)-based RS, include:

Stateğ’®ğ’®\mathcal{S}: At a given timetğ‘¡t, the statestâˆˆğ’®subscriptğ‘ ğ‘¡ğ’®s_{t}\in\mathcal{S}is defined by a combination of the userâ€™s characteristics and the recentlğ‘™litems that the user has shown interest in prior to timetğ‘¡t. This may also include demographic information if relevant.

Actionğ’œğ’œ\mathcal{A}: The actionatâˆˆğ’œsubscriptğ‘ğ‘¡ğ’œa_{t}\in\mathcal{A}represents the agentâ€™s prediction of the userâ€™s evolving preferences at timetğ‘¡t. Here,ğ’œğ’œ\mathcal{A}encompasses the entire set of potential candidate items, which could be vast, potentially numbering in the millions.

Transition Probabilityğ’«ğ’«\mathcal{P}: The transition probabilitypâ€‹(st+1|st,at)ğ‘conditionalsubscriptğ‘ ğ‘¡1subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡p(s_{t+1}|s_{t},a_{t})quantifies the likelihood of transitioning from statestsubscriptğ‘ ğ‘¡s_{t}tost+1subscriptğ‘ ğ‘¡1s_{t+1}when the agent performs actionatsubscriptğ‘ğ‘¡a_{t}. In the context of a recommender system, this probability corresponds to the likelihood of user behavior changes.

Reward functionâ„›â„›\mathcal{R}: After the agent selects an appropriate actionatsubscriptğ‘ğ‘¡a_{t}based on the current statestsubscriptğ‘ ğ‘¡s_{t}at timetğ‘¡t, the user receives the item recommended by the agent. The feedback from the user regarding the recommended item contributes to the rewardrt=â„›â€‹(st,at)subscriptğ‘Ÿğ‘¡â„›subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡r_{t}=\mathcal{R}(s_{t},a_{t}). This reward reflects the userâ€™s response and guides the enhancement of the learned policyÏ€ğœ‹\piby the recommendation agent.

Discount FactorÎ³ğ›¾\gamma: The discount factorÎ³âˆˆ[0,1]ğ›¾01\gamma\in[0,1]is employed to balance the consideration of future and immediate rewards. A value ofÎ³=0ğ›¾0\gamma=0indicates the agent prioritizes immediate rewards, while a non-zero value implies a blend of both immediate and future rewards.

Offline Datasetğ’Ÿğ’Ÿ\mathcal{D}: The offline datasetğ’Ÿğ’Ÿ\mathcal{D}is amassed by an unknown behavior policyÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}. This dataset serves as the historical records of user interactions and is utilized to improve the recommendation policy.

This MDP-based framework lays the groundwork for Offline RL4RS, where the aim is to devise effective recommendation policies using historical interaction data, even when the data is collected under an unknown or different behavior policy. If a POMDP is used, we just need to add the observationğ’ªğ’ª\mathcal{O}which is the partial information from users andlğ‘™litems in which the user was interested before timetğ‘¡t.

Given an offline datasetğ’Ÿğ’Ÿ\mathcal{D}, which contains the trajectories when useruâˆˆğ’°ğ‘¢ğ’°u\in\mathcal{U}interacts with the system for a certain period with an unknown behavior policyÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}, the RL agent aims to learn a policyÏ€ğœ‹\pifrom the offline datasetğ’Ÿğ’Ÿ\mathcal{D}. After that, the trained policyÏ€ğœ‹\piwill be deployed/tested on a production or evaluation environment with a similar scenario with the collected datasetğ’Ÿğ’Ÿ\mathcal{D}.

In this section, we survey offline RL-based RS. Generally speaking, it can be divided into two categories: off-policy with logged data (i.e., â€œfullâ€ off-policy) and offline RL. These two concepts are generally the same except for some specific settings in off-policy methods such as assuming bandit conditions. Due to the recent introduction of offline RL, we have opted to distinguish and separate these for clarity and to prevent potential confusion.

The typical method in this domain is known as off-policy evaluation.
Off-policy evaluation methods are rooted in the direct estimation of policy returns. These methods often utilize a technique known as importance sampling, which involves estimating the return of a given policy or approximating the corresponding policy gradient. A straightforward application of importance sampling involves using trajectories sampled fromÏ€Î²â€‹(Ï„)subscriptğœ‹ğ›½ğœ\pi_{\beta}(\tau)to derive an unbiased estimator ofJâ€‹(Ï€)ğ½ğœ‹J(\pi):

However, this estimator often exhibits high variance, particularly ifTğ‘‡T(the time horizon) is large, due to the product of importance weights. To address this, a weighted importance sampling estimator can be used. This involves dividing the weights byâˆ‘i=1nwTisuperscriptsubscriptğ‘–1ğ‘›superscriptsubscriptğ‘¤ğ‘‡ğ‘–\sum_{i=1}^{n}w_{T}^{i}to normalize them, resulting in a biased estimator with significantly lower variance, while still maintaining strong consistency.

When considering the estimation of Q-values for each state-action pair(st,at)subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡(s_{t},a_{t}), denoted asQ^Ï€â€‹(st,at)superscript^ğ‘„ğœ‹subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡\hat{Q}^{\pi}(s_{t},a_{t}), an approximate model comes into play. This model could be derived from estimating the transition probabilityğ’«â€‹(st+1|st,at)ğ’«conditionalsubscriptğ‘ ğ‘¡1subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡\mathcal{P}(s_{t+1}|s_{t},a_{t})of the Markov Decision Process (MDP) and subsequently solving for the corresponding Q-function. Alternatively, other methods for approximating Q-values could be employed.

The integration of these approximated Q-values as control variates within the importance sampled estimator leads to an enhanced approach:

This method, referred to as a doubly robust estimator(Jiang and Li,2016), exhibits unbiasedness either whenÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}is known or when the model is accurate. In essence, it leverages both the unbiasedness of the importance sampling method and the approximated Q-values to produce an estimator with lower variance and strong consistency.

The recent advancements in off-policy using logged data method can be broadly categorized into three distinct domains: estimator improvement (focus on the discrepancy between the offline data and online data), algorithmic improvement (focus on the recommendation algorithm itself), and miscellaneous application domains. We have compiled a summary of these works inFigure3.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3.Off-Policy Evaluation Works Classifications

Estimator ImprovementHoiles and Schaar (2016)focus on the problem of student course scheduling and curriculum design. It proposes an algorithm for personalized course recommendation and curriculum design based on logged student data. The algorithm uses a regression estimator for contextual multi-armed bandits and provides guarantees on their predictive performance. The paper also addresses the issue of missing data and provides guidelines for including expert domain knowledge in the recommendations. The algorithms can be used to identify curriculum gaps and provide recommendations for course schedules. The paper also discusses off-policy evaluation techniques and the use of the regression estimator for estimating the expected reward of a new policy. One drawback is that the proposed approach assumes a fixed set of courses and does not consider the dynamic nature of course offerings and student preferences.

Swaminathan etÂ al.(2017)address the problem of off-policy evaluation and optimization in combinatorial contextual bandits. The motivation behind this research is the need to estimate the reward of a new target policy using data collected by a different logging policy. The authors propose a pseudoinverse (PI) estimator that makes a linearity assumption about the evaluated metric, allowing for more efficient estimation compared to importance sampling. The PI estimator requires exponentially fewer samples to achieve a given error bound and can be used for off-policy optimization as well.
The methodology involves using the PI estimator to impute action-level rewards for each context, enabling direct optimization of whole-page metrics through pointwise learning to rank algorithms. The authors demonstrate the effectiveness of their approach on real-world search ranking datasets, showing that the PI estimator outperforms prior baselines in terms of off-policy evaluation of whole-page metrics.
This method has several limitations. One drawback of this method is that it relies on the linearity assumption, which may not always hold in practice. Moreover, there is a bias-variance tradeoff between the weighted pseudoinverse (wPI) method and the direct method, with wPI showing bias for the Expected Reciprocal Rank metric. The wPI method also deteriorates for larger slate spaces and is sensitive to linearity assumptions. These drawbacks highlight areas where further refinement and research are needed to enhance the robustness and effectiveness of the approach.

Jeunen and Goethals (2021,2023)focus on improving the recommendation performance of policies that rely on value-based models (i.e., Q-learning) of expected reward. The authors propose a pessimistic reward modeling framework that incorporates Bayesian uncertainty estimates to express skepticism about the reward model. This allows for the generation of conservative decision rules based on lower-confidence-bound estimates, rather than the usual maximum likelihood or maximum PI estimates. The approach is agnostic to the logging policy and does not require propensity scores, making it more flexible and avoiding the limitations of inverse propensity score weighting.
The methodology involves training reward models using a range of datasets generated under different environmental conditions. The authors compare the performance of policies that act based on reward models using maximum likelihood or maximum PI estimates, with policies that use lower confidence bounds based on tuned parameters. The evaluation is done through simulated A/B tests, with the resulting click-through-rate (CTR) estimates compared to the logging policy and an unattainable skyline policy. The experiments show that the pessimistic decision-making approach consistently decreases post-decision disappointment and can significantly increase the policyâ€™s attained CTR.
One drawback of this approach is that it relies on the assumption that the reward estimates are conditionally unbiased, which may not always hold in practice. The authors acknowledge that underfitting and model misspecification can make this assumption unrealistic. Additionally, the approach requires tuning the hyperparameter alpha, which determines the lower confidence bound, and finding the optimal value may not always be straightforward.

Narita etÂ al.(2021)proposes a new off-policy evaluation method for RL4RS. The motivation behind this work is to address the limitations of existing estimators, such as inverse propensity weighting and doubly robust estimators, which suffer from bias and overfitting issues. The authors introduce a new estimator that combines the doubly robust estimator with double/debiased machine learning techniques. The key features of this estimator are its robustness to bias in behavior policy and state-action value function estimates, as well as the use of a sample-splitting procedure called cross-fitting to remove overfitting bias. However, the experiments are limited to specific domains, such as the CartPole-v0 environment and online ads, and it is unclear how the estimator would perform in other tasks in RS.

Jagerman etÂ al.(2019)address the problem of off-policy evaluation in non-stationary environments, where user preferences change over time. Existing off-policy evaluation techniques fail to work in such environments. It proposes several off-policy estimators that operate well in non-stationary environments. These estimators rely more on recent bandit feedback and accurately capture changes in user preferences. They provide a rigorous analysis of the proposed estimatorsâ€™ bias and show that the bias does not grow over time, unlike the standard Inverse Propensity Scoring (IPS) estimator. They also create adaptive variants of the estimators that change their parameters in real-time to improve estimation performance. Extensive empirical evaluation on recommendation datasets shows that the proposed estimators significantly outperform the regular IPS estimator and provide a more accurate estimation of a policyâ€™s true performance. One drawback of the work is the trade-off between bias and variance. While the estimators avoid a bias term that grows with time, they introduce variance that scales with the window size or decay factor. Choosing a smaller window size or larger decay factor reduces bias but increases variance, and vice versa. Finding the optimal balance between bias and variance is a challenge.

Algorithmic ImprovementWang etÂ al.(2020b)address the problem of designing a stable off-policy RL method for RS.
Moreover, the exploration error is also highlighted, which arises from the mismatch between the recommendation policy and the distribution of customersâ€™ feedback in the training data.
This exploration error can lead to unstable training processes and potentially diverging results. To mitigate this problem, the authors propose an off-policy logged data method called Generator Constrained deep Q-learning (GCQ). GCQ combines a neural generator that simulates customersâ€™ possible feedback with a Q-network that selects the highest valued action to form the recommendation policy. The authors also design the generatorâ€™s architecture based on Huffman Trees to reduce decision time. One drawback of this work is the limited capability to handle long sequences of user behavior.

Chen etÂ al.(2019)address the problem of data biases that arise when applying policy gradient methods in a recommendation system. The primary goal is to address the distribution mismatch from the behavior policyÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}and the learned policyÏ€ğœ‹\pi. As a result, an off-policy-corrected gradient estimator is introduced to reduce the variance of each gradient term while still correcting for the bias of a non-corrected policy gradient. A recurrent neural network (RNN) is adopted to model the user state at each time step. To estimate the behavior policyÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}, which is a mixture of the policies of multiple agents in the system, the authors use a context-dependent neural estimator which is a contextual bandit based method.
One drawback of the proposed method is the variance of the estimator, which can be large when there are very low or high values of the importance weights. To reduce this variance, the authors take a first-order approximation and ignore the state visitation differences under the two policies. This results in a slightly biased estimator with a lower variance.
Another drawback is the difficulty in estimating the behavior policyÏ€bâ€‹eâ€‹tâ€‹asubscriptğœ‹ğ‘ğ‘’ğ‘¡ğ‘\pi_{b}eta, especially when there are multiple agents in the system and the collected trajectories are generated by a mixture of deterministic policies and stochastic policies.

Jeunen etÂ al.(2020)propose a new approach called the Dual Bandit, which combines value-based and policy-based methods to improve performance in recommendation settings. It highlights that existing offline evaluation results are often even contradictory over different runs and datasets, or extremely hard to reproduce in a robust manner. Hence, they introduce simulation environments as an alternative and reproducible evaluation approach.

OthersSakhi etÂ al.(2020)introduce a probabilistic model known as BLOB (Bayesian Latent Organic Bandit) designed for bandit-based RS. BLOB aims to enhance recommendation quality by combining organic user behavior (items viewed without intervention) with bandit signals (recommendations and their outcomes). Traditional recommendation algorithms often focus on either organic-based or bandit-based approaches, but the authors recognize the potential to enhance recommendation quality by integrating both aspects. The goal is to create a model that leverages the relationship between organic and bandit behaviors to provide more accurate and personalized recommendations. The proposed model uses a matrix variate prior distribution to relate these two types of behaviors, and variational autoencoders are employed for training. However, the proposed model requires a two-state training process which needs to train the model for organic behavior and bandit signals separately instead of training simultaneously.

Xiao and Wang (2022)present a value ranking algorithm that combines RL and ranking metrics to improve the effectiveness of ranking algorithms. The proposed method uses the concept of extrapolation and regularization to address the challenges of partial and sparse rewards. Extrapolation is used to estimate rewards from logged feedback, while regularization is used to incorporate ranking signals into the RL policy. The authors propose a sequential Expectation-Maximization (EM) framework that alternates between the E-step, which estimates rewards and ranking signals, and the M-step, which optimizes the RL policy. They show that this framework can effectively learn from rewards and ranking signals. This proposed algorithmâ€™s drawback lies in the bandit setting, as it doesnâ€™t account for future rewards. Additionally, in the full RL setting, it might suffer from the curse of dimensionality.

Hong etÂ al.(2023)address the complex issue of multi-task off-policy learning from bandit feedback, a challenge that has significant implications for various applications, including RS. It is motivated to develop a solution that can efficiently handle multiple tasks simultaneously, leveraging the relationships between tasks to enhance performance. It proposes a hierarchical off-policy optimization algorithm (HierOPO) to tackle this problem. The problem is formulated as a contextual off-policy optimization within a hierarchical graphical model, specifically focusing on linear Gaussian models. The authors provide an efficient implementation and analysis, proving per-task bounds on the sub-optimality of the learned policies. They demonstrate that using the hierarchy improves performance compared to solving each task independently. The algorithm is evaluated on synthetic problems and applied to a multi-user recommendation system. However, the proposed method is a model-based off-policy approach, the model-based approaches
tend to be biased, due to using a potentially misspecified model.

In this section, we will provide reviews of existing offline RL4RS methods. Different from off-policy evaluation, offline RL4RS does not limit the setting to bandit-based methods. Moreover, in this part, we have included the off-policy learning based methods as offline RL.
However, the existing works in this field lack organization, with no apparent interconnection among the various works that often emphasize different aspects. Currently, we lack a systematic approach to review these works, resorting to a sequential examination of each one individually.

Ma etÂ al.(2020)discuss off-policy learning in two-stage RS. The proposed method consists of a candidate generation model in the first stage and a ranking model in the second stage. The authors propose a two-stage off-policy policy gradient method that takes into account the ranking model when training the candidate generation model. The proposed method employs IPS to correct the bias and design variance reduction tricks to reduce the variance. However, the proposed method does not provide a comprehensive experiment about how the ranking model and the candidate generation model affect the final performance.

Chen etÂ al.(2022a)focus on scaling an off-policy actor-critic algorithm for industrial recommendation systems. The motivation behind their research is to address the challenges of offline evaluation and learning in RS, where only partial feedback is available. The authors propose an approach that combines off-policy learning with importance weighting to estimate the value of state-action pairs under the target policy. They use a critic network to estimate the value function and update the policy network accordingly. The methodology involves minimizing the temporal difference loss and using a Huber loss to handle outliers. The authors also investigate the impact of different estimation methods for the target value function. However, the proposed methods have several limitations. One drawback is the potential bias introduced by using the cumulative future return on the behavior trajectory while ignoring the importance weighting on future trajectories. Another drawback is the conservative nature of the learned policy when using sampling from the learned policy. The softmax policy parameterization used in the approach leads to a more myopic policy, recommending more popular and longer content and less novel content.

Gao etÂ al.(2023)centre around the problem of the Matthew effect in offline RL based RS. The Matthew effect(Merton,1988)describes a phenomenon where popular items or categories are recommended more frequently, leading to the neglect of less popular ones. This bias towards popular items can reduce the diversity in recommendations and decrease user satisfaction. To address the Matthew effect, the authors propose a Debiased model-based Offline RL (DORL) method. DORL introduces a penalty term to the RL algorithm, encouraging exploration and diversity in recommendations. By adding this penalty, the method aims to reduce the bias towards popular items and promote a more varied selection.

Wang etÂ al.(2023)address the challenges inherent in designing reward functions and handling large-scale datasets within RL4RS. Traditional RL4RS approaches may fall short in accurately estimating rewards only based on limited observations. To address this problem, a Causal Decision Transformer for RS (CDT4Rec) is proposed, a novel model that integrates offline RL and transformer architecture. CDT4Rec employs a causal mechanism to estimate rewards based on user behavior, allowing for a more accurate understanding of user preferences. The transformer architecture is used to process large datasets and capture dependencies, enabling the model to handle complex data structures.

Yuan etÂ al.(2022)is motivated by the challenges associated with optimizing mobile notification systems. Traditional response-prediction models often struggle to accurately attribute the impact to a single notification, leading to inefficiencies in managing and delivering notifications. Recognizing this limitation, the authors aim to explore the application of RL to enhance the decision-making process for sequential notifications, seeking to provide a more effective and targeted approach to mobile notification systems. Hence, an offline RL framework specifically designed for sequential notification decisions is proposed. They introduce a state-marginalized importance sampling policy evaluation approach, which is a novel method to assess the effectiveness of different notification strategies. Through simulations, the authors demonstrate the performance of the approach, and they also present a real-world application of the framework, detailing the practical considerations and results.

Wang etÂ al.(2020a)are motivated by the challenge of adapting to new users in recommendation systems, particularly when there are limited interactions to understand user preferences. This situation, often referred to as the â€œcold-startâ€ problem, can hinder the ability to provide personalized recommendations that align with long-term user interests. The proposed approach introduces a user context variable to represent user preferences, employing a meta-level model-based RL method for rapid user adaptation. The user model and recommendation agent interact alternately, with the interaction relationship modeled from an information-theoretic perspective.

Zhang etÂ al.(2022b)discuss the problem of interactive recommendation with natural-language feedback and proposes an offline RL framework to address the challenges of collecting experience through user interaction. The authors develop a behavior-agnostic off-policy correction framework that leverages the conservative Q-function for off-policy evaluation. This allows for learning effective policies from fixed datasets without further interactions.

Xiao and Wang (2021)propose a general offline RL framework for the interactive recommendation. The proposed method introduces different techniques such as support constraints, supervised regularization, policy constraints, dual constraints, and reward extrapolation. These methods aim to minimize the mismatch between the recommendation policy and logging policy and to balance the supervised signal and task reward.

Offline RL4RS is an emerging domain that introduces multiple challenges demanding comprehensive exploration. In this section, we aim to outline the open challenges in offline RL4RS. Given that RS fall under the application scope of offline RL, several shared challenges naturally arise. We will begin by addressing some common challenges before delving into the specific challenges unique to RS when utilizing offline RL techniques.

One of the most prominent challenges in offline Reinforcement Learning (RL) lies in the fact that the learning process hinges solely on the provided static datasetğ’Ÿğ’Ÿ\mathcal{D}. This limitation results in a significant obstacle to enhancing exploration, as exploration falls outside the algorithmâ€™s purview. Consequently, if the datasetğ’Ÿğ’Ÿ\mathcal{D}lacks transitions that demonstrate regions of the state space yielding high rewards, the algorithm may be fundamentally incapable of uncovering these rewarding regions.
In contrast to control tasks, which are common in offline RL applications and often face challenges in gathering comprehensive data to facilitate effective learning from high-reward scenarios, the landscape changes when it comes to RS. In this domain, a plethora of offline datasets, such as those from MovieLens, GoodReads, and Amazon, are publicly available. These datasets stem from real-world interactions and adeptly capture usersâ€™ preferences.

However, RS diverge from traditional offline RL application domains due to their distinct characteristics. To illustrate, letâ€™s consider implicit feedback, particularly review data. This kind of data poses a challenge when attempting to embed it within the state space due to its reliance on text. Although techniques like word2vec(Mikolov etÂ al.,2013)exist to transform textual data into vectors that might potentially be integrated into the state space, the question of how to effectively guide the agent in utilizing such data in RS remains unexplored.

Another intriguing aspect is the presence of graph data, extensively used in RS to represent social connections, item relationships, and more. The prevalent form of representation is a knowledge graph, which can be transformed into embeddings through the application of Graph Neural Networks (GNN)(Wu etÂ al.,2022). Nonetheless, it faces a similar challenge as textual data: how to empower the agent to effectively utilize this information. There are some works investigating graph RL which may be able to provide some directions to offline RL4RS(Xiong etÂ al.,2017; Jiang etÂ al.,2018; Madjiheurem and Toni,2019).

However, a challenge surfaces due to whatâ€™s known as the â€œdata sparsity problemâ€. This means that despite having ample data, thereâ€™s no assurance that the collected user interactions or behaviors cover all the situations where users have expressed positive feedback, like giving high ratings. In other words, there might be important scenarios where users found something valuable, but the data doesnâ€™t reflect those instances well(Chen etÂ al.,2022c).

On the other hand, there is s another widely recognized hurdle in RS that also applies to Offline RL4RS: the cold-start problem. Unlike data sparsity, cold-start challenges emerge when the agent aims to provide recommendations to a new user. This issue arises due to the absence of adequate historical data or interactions, which in turn hampers the understanding of preferences and traits related to these new users or items. While addressing the cold-start problem is an ongoing research avenue in conventional RS tasks, it hasnâ€™t received sufficient attention in the context of RL4RS. Considering the interactive procedure of the RL4RS, new users have limited contextual information that they can use to formulate the state representation; this contributes to the difficulty of making recommendations. This predicament continues to remain an unsolved puzzle within the realm of offline RL4RS.

A challenge of significant intricacy within the context of offline RL pertains to the effective formulation and addressing of counterfactual queriesâ€”a task that might not be readily apparent but is of great importance.

Counterfactual queries, in essence, are defined as hypothetical â€œwhat ifâ€ scenarios. These queries involve creating educated guesses about potential outcomes if the agent were to undertake actions different from those observed in the data. It is the core behind offline RL, as our objective is to learn a policy that can perform better than the behavior recorded in the datasetğ’Ÿğ’Ÿ\mathcal{D}. Hence, the agent must execute an action that is different from the learned policy. This situation, unfortunately, places a substantial strain on the capabilities of several prevailing deep-learning methods. Existing methods have been methodically fashioned under the assumption that the data is independence and identical distribution (i.i.d.). In traditional supervised learning based RS, the goal is to train a model to achieve superior performance, such as higher accuracy, recall or precision. The evaluation dataset follows the same distribution as the training dataset.
Hence, in offline RL4RS, the key point is to learn a policy that can recommend different items (ideally with better feedback) from the behavior recorded in the datasetğ’Ÿğ’Ÿ\mathcal{D}.

The challenge behind counterfactual queries is that of distribution shift. The policy is trained under one distribution, but it will be evaluated on a different distribution. Given that such a problem is not widely discussed in the RS literature, we will provide some algorithmic insights from the offline RL perspective to help address this in offline RL4RS.
Distribution shift issues can be addressed in several ways, with the simplest one being to constrain something about the learning process such that the distribution shift is bounded. For example, we can constrain how much the learned policyÏ€â€‹(a|s)ğœ‹conditionalğ‘ğ‘ \pi(a|s)differs from behavior policyÏ€Î²â€‹(a|s)subscriptğœ‹ğ›½conditionalğ‘ğ‘ \pi_{\beta}(a|s)by using some techniques like Trust Region Policy Optimization (TRPO)(Schulman etÂ al.,2015).

However, if there is a significant disparity between the distribution of the training dataset and that of the evaluation environment, it might lead to the emergence of out-of-distribution (OOD) behavior. Several recent studies have delved into OOD recommendation(Wang etÂ al.,2022; He etÂ al.,2022), taking into account shifts in user features. These efforts can be categorized into two main groups: OOD generalization(He etÂ al.,2022)and OOD adaptation(Wang etÂ al.,2022).

The underlying notion here is to acquire a causal representation of usersâ€™ preferences by leveraging their most recent behaviors. This representation is then utilized within a causal graph framework to comprehend how shifts in features could impact usersâ€™ preferences. Furthermore, the current methodologies primarily target sequential recommendation systems, which share certain properties with MDPs, rendering them relevant to offline RL4RS.

However, this domain is still in its exploratory phase, and it has not garnered substantial attention. As a result, this presents an open challenge with significant potential for further exploration.

Another prevalent issue within offline RL4RS pertains to the bias inherited from RS, a topic that has recently gained increasing research attention. This bias stems from the nature of offline data, with recent studies(Chen etÂ al.,2023a)revealing that user behavior data are not experimental but rather observational, introducing bias-related challenges.

The prevalence of bias can be attributed to two primary factors. Firstly, the inherent character of user behavior data is observational rather than experimental. In simpler terms, the data fed into RS are susceptible to selection bias. For instance, in a video recommendation system, users tend to engage with, rate, and comment on movies that align with their personal interests. Secondly, a discrepancy in distribution exists, signifying that the distributions of users and items within the recommender system are uneven. This imbalance can lead to a â€œpopularity biasâ€, where popular items receive disproportionately frequent recommendations compared to others. Nonetheless, disregarding products within the â€long tailâ€ of less popular items can have adverse effects on businesses, given that these items are equally essential, albeit less likely to be discovered by chance.

As mentioned earlier, a substantial portion of existing offline off-policy with logged data methods primarily focus on off-policy evaluation. This approach employs importance sampling to tackle the bias issue. However, the importance sampling gives rise to another hurdleâ€”high variance. While importance sampling already contends with high variance, this issue is further exacerbated in the context of sequential scenarios. In this setting, the importance weights at consecutive time steps are multiplied together (as depicted in Equation14), leading to an exponential amplification of variance.

Approximate and marginalized importance sampling methods mitigate this concern to some extent by circumventing the multiplication of importance weights across multiple time steps. Yet, the fundamental challenge persists: when the behavior policyÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}substantially diverges from the current learned policyÏ€Î¸subscriptğœ‹ğœƒ\pi_{\theta}, the importance weights degenerate. Consequently, any estimations of the return or gradient encounter excessive variance, particularly in scenarios characterized by high-dimensional state and action spaces or extended time horizons (as seen in problems like recommendation systems).

For this reason, importance-sampled estimators are most effective when the policyâ€™s deviation from the behavior policy remains within a reasonable limit. In the general off-policy setting, this condition generally holds true, as new trajectories are frequently amassed and integrated into the dataset using the latest policy. However, in the offline context, this is not typically the case. Consequently, the extent of enhancement achievable through importance sampling is confined by several factors: (i) the relative suboptimality of the behavior policy; (ii) the dimensionality of the state and action space; (iii) the effective task horizon.

Hence, the tradeoff between bias and variance in offline RL4RS presents an intriguing potential avenue for advancement.

While deep learning-based models can significantly enhance the performance of RS, they often lack interpretability. Consequently, the task of rendering recommender outputs understandable becomes vital, all while maintaining high-quality recommendations. Elevating explainability in RS carries benefits beyond aiding end-users in comprehending suggested items. It empowers system designers to scrutinize the inner workings of RS(Zhang etÂ al.,2020). Additionally, the realm of explainability in RL (RL) has been garnering attention(Heuillet etÂ al.,2021), although the current focus primarily revolves around visualizing learned representations. What remains is an explanation of how the learned policy translates into actionable decisions. In the transition to RL4RS, the emphasis on explainability will shift towards elucidating how the agent justifies its recommended items. Hence, explainability becomes a relatively easy task compared with interpreting the learning process or decision process.

Attention models have emerged as powerful tools that not only bolster predictive performance but also enhance explainability(Zhang etÂ al.,2019a). For instance,Wang etÂ al.(2018)introduce an RL framework coupled with an attention model for explainable recommendations. This approach ensures model-agnostic by segregating the recommendation model from the explanation generator. Agents instantiated through attention-based neural networks facilitate the generation of sentence-level explanations. This approach could prove promising given the close connection between offline RL4RS and online RL4RS.

Moreover, with access to offline datasets in offline RL4RS, more solutions become feasible. Knowledge graphs, for instance, contain abundant user and item information, enabling the creation of more personalized, intuitive explanations for recommendation systems(Zhang etÂ al.,2020). However, the processing of graph data presents challenges. One potential strategy involves embedding a pre-learned knowledge graph from the offline dataset into the environment. The final objective then shifts from recommending items to navigating the knowledge graph. As an example,Zhao etÂ al.(2020)extract informative path demonstrations with minimal labeling effort. Then an adversarial actor-critic model for demonstration-guided pathfinding is proposed. This approach enhances recommendation accuracy and explainability through RL and knowledge graph reasoning and can be further expanded by integrating offline RL features.

In offline RL4RS, several key areas emerge as promising avenues. Cross-domain recommendation systems offer the potential in transferring insights between diverse domains, enhancing recommendation effectiveness. The integration of large language models holds the prospect of enriching contextual understanding and refining user-item interactions. Incorporating causality into offline RL4RS can deepen comprehension of user behaviors, leading to more accurate and interpretable recommendations. The exploration of self-supervised learning and graph-based techniques presents innovative possibilities for capturing intricate user-item relationships. Moreover, addressing uncertainty and fortifying the robustness of RL4RS against noise and adversarial inputs stand out as essential directions for ensuring dependable and consistent recommendation outcomes.

Cross-domain recommendation refers to the task of providing recommendations to users by leveraging data and knowledge from multiple distinct domains. Cross-domain recommendation systems can be particularly useful in scenarios where user data is sparse within a single domain but might be enriched when multiple domains are combined. Additionally, they enable more comprehensive and diverse recommendations by tapping into different aspects of usersâ€™ interests. From this viewpoint, we may be able to treat offline RL4RS as a type of cross-domain recommendation in certain situations. For example, when the evaluation environment is significantly different from the offline datasetğ’Ÿğ’Ÿ\mathcal{D}, we may treat the evaluation platform as a new domain and we would like to transfer those learned knowledge fromğ’Ÿğ’Ÿ\mathcal{D}into such a platform.

The challenge in cross-domain recommendation lies in effectively transferring knowledge and patterns across domains while accounting for variations in user behaviors and item characteristics. Techniques such as domain adaptation, transfer learning, and hybrid models are often employed to bridge the gaps between different domains and optimize recommendation performance. Moreover, recent work in cross-domain offline RL would be beneficial.Liu etÂ al.(2023)present BOSA (Beyond OOD State Actions), a method for cross-domain offline RL (RL). BOSA tackles the challenges of out-of-distribution (OOD) state actions and data inefficiency by incorporating additional source-domain data. The authors propose specific objectives to address OOD transition dynamics and demonstrate that BOSA improves data efficiency and outperforms existing methods. The method is also applicable to model-based RL and data augmentation techniques. However, in offline RL4RS, this problem is still open for investigation as the techniques mentioned have not yet been explored in offline RL4RS.

Implicit feedback serves as a commonly employed feedback mechanism for learning recommendation policies in RS. Implicit feedback encompasses user actions like clicks, views, purchases, time spent, and dwell time during interactions with platforms or systems, signifying user preferences and interests. Although not as explicit as ratings or reviews, these behaviors offer valuable insights. In the context of RL4RS, the reward mechanism evaluates recommended items. Typically, this involves binary rewards based on click behavior, with some efforts, likeZheng etÂ al.(2018), incorporating dwell times for a more comprehensive reward signal. However, accommodating multiple implicit feedback sources concurrently in RL4RS poses challenges due to limited relevant datasets or simulations. Additionally, harnessing review comments, a common type of implicit feedback in RS, within RL4RS remains a subject of exploration.Zhang etÂ al.(2019b)propose a text encoder solution, albeit relying on a manually gathered generator to produce review texts, which primarily validate feature learning rather than directly influencing the final reward. Transitioning this approach to offline RL4RS presents difficulties. Firstly, integrating review comments into the reward function requires careful study. Secondly, textual data introduces high-dimensional state representations, potentially necessitating novel algorithms tailored to this scenario.

Recently, Large Language Models (LLMs) have received increasing research interest in RS. LLM demonstrates a superior capability in handling textual data from multiple tasks such as natural language understanding, contextual understanding and sentiment analysis(Zhao etÂ al.,2023). Existing RS works provides some insights about how LLMs can be adopted in RS such as prompt engineering to instruct the LLM to make recommendations(Zhang etÂ al.,2023), utilizing the Generative Pre-trained Transformer (GPT) as the backbone to process features(Sun etÂ al.,2019)etc.

Moreover, some attempts have been undertaken about how LLM can be used in RL.Du etÂ al.(2023)introduce a method called ELLM (Exploring with Large Language Models) that aims to enhance pretraining in RL by using LLM. ELLM works by prompting an LLM with a description of the agentâ€™s current state and then rewarding the agent for achieving goals suggested by the LLM. This method biases exploration towards behaviors that are meaningful and potentially useful from a human perspective, without needing human intervention. Meanwhile,Carta etÂ al.(2023)explore the use of LLM in interactive environments through an approach called GLAM (Grounding Language Models). This method aligns the knowledge of LLMs with the environment, focusing on aspects like sample efficiency, generalization to new tasks, and the impact of online RL.

In the previous section, we mentioned that offline RL can be formulated as answering counterfactual queries. It is an intuitive choice to integrate the causality into offline RL from this perspective. Moreover, causality is widely used in RS and receiving increasing interest in offline RL. We believe it would be a promising topic in offline RL4RS.

In the work byZhu etÂ al.(2022), an exploration is undertaken regarding the integration of causal world models into the domain of model-based offline RL. The theoretical underpinning of their study accentuates the superiority of causal world models over ordinary world models in the context of offline RL. This advantage is attributed to the incorporation of causal structure within the generalization error bound. The authors introduce an operational algorithm termed FOCUS (Offline Model-based RL with Causal Structure) to exemplify the potential value derived from comprehending and effectively utilizing causal structure in the domain of offline RL.

Additionally,Liao etÂ al.(2021)introduce the notion of instrumental variable value iteration for causal offline RL. The presentation of their work introduces IV-aided Value Iteration (IVVI), an algorithm designed with efficiency in mind, aimed at extracting optimal policies from observational data in the presence of unobserved variables. The utilization of instrumental variables (IVs) forms the foundation, with the authors devising a framework named Confounded Markov Decision Process with Instrumental Variables (CMDP-IV) to contextualize the problem. Notably, the IVVI algorithm, established upon a primal-dual reformulation of a conditional moment restriction, emerges as the first demonstrably efficient solution for instrument-aided offline RL.

One of the most common applications of integrating causality into the RL4RS is counterfactual augmentation.Chen etÂ al.(2022b,2023b)develop a data augmentation technique that employs counterfactual reasoning to produce more informative interaction trajectories for RL4RS.Wang etÂ al.(2022)introduces the Causal Decision Transformer for RS (CDT4Rec), a model that merges offline RL with the transformer architecture. CDT4Rec is designed to tackle the challenges of crafting reward functions and leveraging large-scale datasets in RS. It employs a causal mechanism to deduce rewards from user behavior and uses the transformer architecture to handle vast datasets and identify dependencies.

Drawing inspiration from the works mentioned above, exploring causality in offline RL4RS emerges as a promising avenue for future research. Particularly, as causal offline RL4RS advances, its primary emphasis on counterfactual augmentation highlights an exciting direction. However, it is important to recognize the need for additional endeavors in different domains, including but not limited to distribution shifts and the presence of biases.

The vulnerability of deep learning-based methods is evident through adversarial samples, underscoring the pressing concern of robustness in both RS and RL. Particularly, the exploration of adversarial attacks and defense strategies within the domain of RS has garnered significant attention in recent times, as emphasized by the comprehensive survey conducted by(Deldjoo etÂ al.,2021). This attention is fueled by the critical importance of security within the realm of RS operations.

Furthermore, the vulnerability of RL policies to adversarial perturbations in agentsâ€™ observations has been established by(Lin etÂ al.,2017). In the context of RL4RS,Cao etÂ al.(2020)introduce an adversarial attack detection approach. This method leverages the utilization of a Gated Recurrent Unit (GRU) to encode the action space into a lower-dimensional representation, alongside the design of decoders to identify potential attacks. However, itâ€™s important to note that this method exclusively addresses attacks rooted in the Fast Gradient Sign Method (FGSM) and strategically-timed maneuvers. As a result, its ability to detect other forms of attacks is limited.

Within the arena of offline RL, recent advancements provide a promising direction.Panaganti etÂ al.(2022)address the challenge of robust offline RL, centering on the learning of policies that can withstand uncertainties in model parameters. The authors introduce the Robust Fitted Q-Iteration (RFQI) algorithm, which relies solely on offline data to determine the optimal robust policy. This algorithm adeptly tackles concerns such as offline data collection, model optimization, and unbiased estimation. Additionally,Zhang etÂ al.(2022a)concentrate on a scenario involving a batch dataset of state-action-reward-next state tuples, susceptible to potential corruption by adversaries. Their objective is to extract a near-optimal policy from this compromised dataset.

The recent advancements in RL4RS pave the way for efficiently capturing usersâ€™ dynamic interests. However, the nature of online interactions necessitates costly trajectory collection procedures, posing a significant hurdle for researchers interested in this field.
In this survey, our goal is to provide a comprehensive overview of offline RL4RS, a novel paradigm that eliminates the need for an expensive data collection process. Alongside reviewing recent works, we also offer insights into potential future opportunities. Specifically, weâ€™ve compiled and analyzed recent progress in offline RL4RS, organized into two distinct categories: off-policy learning utilizing logged data and offline RL4RS techniques.
Furthermore, we address several prevailing challenges in this domain: offline data quality, distribution shift, bias and variance, and explainability. Additionally, we present potential avenues for future exploration in this rapidly evolving field, such as cross-domain recommendation, LLMs, causality, and robustness.
Being an emerging topic, offline RL4RS introduces fresh possibilities for integrating pre-existing offline datasets into the realm of RL4RS. This survey can also be perceived as a visionary paper, offering potential benefits to researchers who are newcomers to this field.

[å›¾ç‰‡: images\image_4.png]

[å›¾ç‰‡: images\image_5.png]

