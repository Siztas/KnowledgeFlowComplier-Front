æ ‡é¢˜ï¼šRecent Advances on Federated Learning: A Systematic Survey

Federated learning has emerged as an effective paradigm to achieve privacy-preserving collaborative learning among different parties. Compared to traditional centralized learning that requires collecting data from each party, in federated learning, only the locally trained models or computed gradients are exchanged, without exposing any data information. As a result, it is able to protect privacy to some extent. In recent years, federated learning has become more and more prevalent and there have been many surveys for summarizing related methods in this hot research topic. However, most of them focus on a specific perspective or lack the latest research progress. In this paper, we provide a systematic survey on federated learning, aiming to review the recent advanced federated methods and applications from different aspects. Specifically, this paper includes four major contributions. First, we present a new taxonomy of federated learning in terms of the pipeline and challenges in federated scenarios. Second, we summarize federated learning methods into several categories and briefly introduce the state-of-the-art methods under these categories. Third, we overview some prevalent federated learning frameworks and introduce their features. Finally, some potential deficiencies of current methods and several future directions are discussed.

Over the past few years, deep neural networks (DNNs) have received a lot of attention due to their remarkable performance on various tasks such as Computer Vision (CV)(Krizhevsky
etÂ al.,2012; Simonyan and
Zisserman,2014; Liu
etÂ al.,2019b,2021a), Natural Language Processing (NLP)(Devlin
etÂ al.,2018; Vaswani etÂ al.,2017; Xu
etÂ al.,2021), Recommendation Systems (RS)(Chen
etÂ al.,2022,2020b,2020c)and Data Mining (DM)(Liu
etÂ al.,2020; Li
etÂ al.,2021c; Shao
etÂ al.,2014). However, the superiority of DNNs depends on the support of big data, which is hard to access in a certain party considering the limitation of the storage space and the difficulty of data collection. Gathering data from different parties to a central server for training is a direct solution to the issue. Nevertheless, data in each party may be sensitive or include some user privacy information. For example, medical images in a hospital are prohibited from outsourcing due to their privacy property. Besides, policies such as General
Data Protection Regulation (GDPR)(Albrecht,2016)also highlight the importance of protecting privacy when sharing information among different organizations. Thus, how to aggregate the data knowledge from different parties while ensuring privacy is an important and practical problem in real-world scenarios.

Federated learning (FL)(McMahan etÂ al.,2017), which enables multiple parties to collaboratively train a DNN with the help of a central server, can be regarded as an effective solution to the aforementioned problem. Different from the traditional centralized learning that needs to collect data from each party, in FL, data do not need to upload for a joint training. Instead, the local trained models are exchanged with a central server, which are used to aggregate the knowledge from all of the uploaded models and then distribute the global model to each party. As a result, each party is able to benefit from other parties, improving the model accuracy. In recent years, there have been
many applications based on FL in practice, such as loan status prediction, health situation assessment, and next-word prediction(Hard etÂ al.,2018; Yang etÂ al.,2018; Yang
etÂ al.,2019).

We take Fig.1as an example to illustrate a typical FL pipeline. First, each hospital (party) trains the local model distributed from a central cloud. The training process is usually implemented based on SGD with local data and then generates corresponding local updates. Second, the local updates rather than local data are transferred to the cloud, where the updates are sampled in terms of some heuristic rules to ensure the overhead and some aggregation algorithms (e.g., FedAvg(McMahan etÂ al.,2017)) are conducted to achieve effective knowledge integration. In this way, the cloud can get an improved new global model and distributes it to each hospital for further tuning. These steps may repeat several times until the healthcare service can be satisfied (e.g., the accuracy of the learned model is acceptable for practical deployment).

There have been other surveys on FL over the past few years. For instance, Liet al.(Li
etÂ al.,2021b)summarized related FL methods from the system perspective, where the authors provided the definition of federated learning systems and analyzed the system
components. Limet al.(Lim etÂ al.,2020)focused on the FL application in mobile edge networks. Lyuet al.(Lyu etÂ al.,2020b)paid more attention to the security and privacy issues existed in current FL schemes. However, these surveys only review a specific aspect of federated learning, failing to give readers a comprehensive understanding on FL. Towards the general FL overviews, most of them are out of date and cannot catch the latest trend in FL research. For example, Yanget al.(Yang
etÂ al.,2019)divided FL methods into three categories (i.e., horizontal federated learning, vertical federated learning and federated transfer learning) and described their features respectively. Kairouzet al.(Kairouz
etÂ al.,2021b)gave a comprehensive introduction of federated learning theory and application. Notice that both of the surveys mainly cited papers published before 2020, which is impossible to track the latest research progress on FL considering the rapid development in this field. As shown in Fig.2, we can clearly see that the number of accepted FL papers in top-tier conferences increases dramatically after 2020, which calls for a timely survey to summarize the advances in the FL community. Besides, the rapid update of FL frameworks also requires us to highlight their latest features.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1.An example of the FL pipeline(Li
etÂ al.,2020b).

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2.The number of pulished FL papers in top-tie conference from 2019-2022.

In this paper, we attempt to provide a systematic survey on federated learning, targeting at reviewing the recent advanced federated methods and applications from different aspects. Specifically, the key contributions of this survey are as follows: (1) we present a new taxonomy based on the federated learning pipeline and challenges, which includes four typical aspects:aggregation optimization, heterogeneity, privacy protection, fairness. We will give detailed explanation in the following sections. (2) we summarize different federated learning methods into the proposed categories and briefly describe the state-of-the-art methods under these categories. (3) we overview the latest federated learning frameworks and introduce their features. (4) we discuss some potential deficiencies of current methods and several future directions.

The remainder of this survey is structured as follows. In Section2, we first introduce preliminaries of federated learning. In Section3, we propose the taxonomy of federated learning according to different aspects, in which various federated learning approaches are discussed and categorized. Then, in Section4, we introduce some prevalent frameworks to show the practical deployment of federated learning. Finally, Section5and Section6discuss the future work and concludes this paper.

In this section, we first introduce some notations and symbols used in this survey to formally define federated learning. In general, there are two ends participated in the round of federated learning:client endandserver end. The client end holds a series of local private datağ’Ÿ={ğ’Ÿ1,ğ’Ÿ2,â€¦,ğ’ŸN}ğ’Ÿsubscriptğ’Ÿ1subscriptğ’Ÿ2â€¦subscriptğ’Ÿğ‘\mathcal{D}=\{\mathcal{D}_{1},\mathcal{D}_{2},...,\mathcal{D}_{N}\}, which are then used to train the model in each client and generate local modelsâ„³={M1,M2,â€¦,MN}â„³subscriptğ‘€1subscriptğ‘€2â€¦subscriptğ‘€ğ‘\mathcal{M}=\{M_{1},M_{2},...,M_{N}\}. HereNğ‘Ndenotes the number of clients. After the local training process, the local modelsâ„³â„³\mathcal{M}, rather than the datağ’Ÿğ’Ÿ\mathcal{D}, are uploaded to the server end, where aggregation algorithms are implemented to obtain a global modelMgâ€‹lâ€‹oâ€‹bâ€‹aâ€‹lsubscriptğ‘€ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™M_{global}. The process can be defined as

whereAâ€‹Gâ€‹Gğ´ğºğºAGGrepresents the aggregation algorithms. In this way, we finish one round of federated learning and distribute the global model to each client side for further local training. The concrete number of round is usually determined by the model performance (i.e., we stop the process until the model can achieve desirable accuracy). In addition, to provide a more rigorous privacy protection, each client may enforce some encryption techniques to the models before uploading them. Differential privacy (DP)(Dwork,2008)and homomorphic encryption (HE)(Gentry,2009)are widely used to conduct such protection.

Based on the aforementioned statement, we can see that the performance of federated learning largely depends on the aggregation algorithm in the server end. Formally, the goal of federated learning is to optimize the following objective function

wherewğ‘¤wis the weights of DNNs,Lâ€‹(w)ğ¿ğ‘¤L(w)is the global loss function andLiâ€‹(w)subscriptğ¿ğ‘–ğ‘¤L_{i}(w)is the local loss function in theitâ€‹hsubscriptğ‘–ğ‘¡â„i_{th}client.fisubscriptğ‘“ğ‘–f_{i}represents the importance of theitâ€‹hsubscriptğ‘–ğ‘¡â„i_{th}client andâˆ‘k=1N=1superscriptsubscriptğ‘˜1ğ‘1\sum_{k=1}^{N}=1. In federated learning, the aggregation algorithm determines the value allocation forfisubscriptğ‘“ğ‘–f_{i}. Many research papers that try to improve the accuracy performance of federated learning are focused on this aspect.

Different from traditional centralized learning or distributed learning, federated learning faces the following key challenges:

Heterogeneity problem.In federated learning, the heterogeneity comes from three aspects:(1) Data heterogeneity. Considering that each participator collects data from its local end, the overall data distribution inevitably conforms to the non-independent identically distribution (non-iid) situation. For example, the same object image collected from different environments, or the same activity coming from different people, can lead to different data distributions, which will further affect the performance of federated aggregation(Zhao
etÂ al.,2018). (2) Model heterogeneity. In real-world scenarios, it is hard to limit the federated clients to use an identical model architecture. Instead, each client may prefer a distinctive model architecture for improved task performance. Therefore, how to aggregate these heterogeneous models is challenging in practical federated learning conditions. (3) System heterogeneity. Because of the variability in hardware, different parties may have different storage space, computation power, and communication capabilities. As a result, the server end needs to decide whether to wait for all parties to upload their models for better accuracy or remove stragglers (i.e., the parties with weak hardware performance) for accelerating the federation process.

Privacy leakage.The key idea of federated learning is to achieve collaborative learning in a privacy-preserving manner, which differs from the traditional paradigm that exchanges data or other sensitive information. Keeping data in the local end and transferring corresponding models is the original privacy protection design in federated learning. However, the parameters of the uploaded models may also be exploited by attackers to infer the user privacy information(Zhu etÂ al.,2019). So we require more rigorous encryption or obfuscation methods to ensure privacy.

Unfairness.In traditional centralized learning or distributed learning, the unfairness problem does not exist since the participants belong to a same organization. However, the participants in federated learning come from various parties with different data resources. According to a previous work(Dwork etÂ al.,2012), if individuals with similar preferences and characteristics receive substantially different outcomes, then we say that the model violates individual fairness. Thus, it is necessary to generate federated models that go beyond average accuracy to further consider the fairness performance.

In this section, we first present a taxonomy of federated learning and allocate different federated approaches into different categories according to the taxonomy. Then for each category, we describe in detail how various methods achieve their goal.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3.Our taxonomy of different federated learning methods.

In this survey, we propose a new taxonomy to classify the existing federated learning methods (Fig.3).
Our taxonomy is motivated by the pipeline and challenges in federated learning. As stated in the previous section, the key step in the federated learning pipeline is the aggregation algorithm and the key challenges come from three different aspects. Therefore, in our taxonomy, federated learning approaches can be summarized into four cases: aggregation optimization, heterogeneous federated learning, secure federated learning and fair federated learning.

Aggregation optimization.Considering that the number of participants in a federated learning system is usually large, it is essential to implement an effective aggregation optimization for outputting a better global model compared to the ones with local training. This survey investigates various aggregation methods such as FedAvg(McMahan etÂ al.,2017; Nagalapatti and
Narayanam,2021; Zhao
etÂ al.,2018), FedMA(Wang etÂ al.,2020b)and FedProx(Li etÂ al.,2020c), with a focus on how to combine local models into an improved global model.

Heterogeneous federated learning.In real-world scenarios, federated clients may come from different environments or equip with various hardware, leading to the heterogeneity problem. In the following sections, we respectively explore how related research efforts address the issue of data heterogeneity, model heterogeneity and system heterogeneity. In particular, techniques such as meta-learning(Fallah
etÂ al.,2020; Acar etÂ al.,2021; Jiang etÂ al.,2019; Khodak
etÂ al.,2019; Zheng
etÂ al.,2021; Chen
etÂ al.,2018), multi-task learning(Smith
etÂ al.,2017; Vanhaesebrouck etÂ al.,2017; Corinzia
etÂ al.,2019; Zantedeschi
etÂ al.,2020; Huang etÂ al.,2021a; Li
etÂ al.,2021a; Marfoq etÂ al.,2021; He etÂ al.,2021; Zhou
etÂ al.,2022; Chen and Zhang,2022), transfer learning(Wang etÂ al.,2019b; Yu
etÂ al.,2020; Peterson
etÂ al.,2019; Ozkara
etÂ al.,2021)and clustering(Sattler
etÂ al.,2020; Ghosh
etÂ al.,2020,2019; Zhang etÂ al.,2020c; Ruan and Joe-Wong,2022; Lubana
etÂ al.,2022)are incorporated to achieve our goal.

Secure federated learning.Although traditional federated learning has attempted to protect data privacy by only exchanging parameters of the local trained models, malicious attackers can still design some scheme to infer the properties of raw data. In our survey, we first summarize a series of attacks targeting federated learning, where we describe how backdoor attack(Bagdasaryan etÂ al.,2020; Sun
etÂ al.,2019; Wang etÂ al.,2020a; Xie
etÂ al.,2020; Zhang etÂ al.,2022d; Xie
etÂ al.,2021; Ozdayi
etÂ al.,2021), gradients attack(Zhu etÂ al.,2019; Lam
etÂ al.,2021; Hitaj
etÂ al.,2017; Zhao
etÂ al.,2020; Yin etÂ al.,2021; Li
etÂ al.,2022b; Zhu and Blaschko,2021; Geiping etÂ al.,2020)and poison attack(Bhagoji etÂ al.,2019; Sun etÂ al.,2021; Panda etÂ al.,2022; Wu
etÂ al.,2022)are applied to compromise federated learning. Then we introduce how to combine federated learning, differential privacy (DP)(Wei etÂ al.,2020; Geyer
etÂ al.,2017; McMahan
etÂ al.,2018; Kairouz
etÂ al.,2021a; Agarwal
etÂ al.,2021; Zhang
etÂ al.,2022a; Zheng
etÂ al.,2021; Wang etÂ al.,2020b; Girgis etÂ al.,2021), homomorphic encryption (HE)(Hardy etÂ al.,2017; Zhang
etÂ al.,2020b), trusted execution environment (TEE)(Mo etÂ al.,2021,2020)and other algorithms(Baruch
etÂ al.,2019; Xie
etÂ al.,2021; Huang
etÂ al.,2021b; Tang
etÂ al.,2022)to defend aforementioned attacks.

Fair federated learning.During federated learning, it is possible that the performance of the global model varies significantly across the devices, resulting in the fairness problem. This survey reviews literature about how to ensure fair federated learning, such as designing minimax optimization strategies(Sharma
etÂ al.,2022; Tarzanagh etÂ al.,2022)and sample reweighting approaches(Zhao and Joshi,2022; Enthoven and
Al-Ars,2021).

The goal of aggregation optimization is to improve the performance of the final global model, which is the core output in federated learning. There have been a large number of aggregation algorithms proposed to combine these local models to a better global one. In the following parts, we will describe in detail how different types of aggregation methods work.

A typical and prevalent weight-level aggregation method called FedAvg(McMahan etÂ al.,2017)is mostly adopted by developers. The key idea of FedAvg is to aggregate these local models in a coordinate-based weight averaging manner, which can be denoted as

where N is the number of federated clients.wksubscriptğ‘¤ğ‘˜w_{k}denotes the weight parameters of thektâ€‹hsubscriptğ‘˜ğ‘¡â„k_{th}client andWgrsuperscriptsubscriptğ‘Šğ‘”ğ‘ŸW_{g}^{r}is the final aggregated model at thertâ€‹hsubscriptğ‘Ÿğ‘¡â„r_{th}round. Researchers have shown the remarkable performance of FedAvg on a variety of public datasets (e.g., MNIST(LeCun
etÂ al.,1998)and CIFAR-10(Krizhevsky
etÂ al.,2009)) and provided some theoretical analyses to prove why FedAvg works well(Li
etÂ al.,2020a).

Despite being widely applied, FedAvg still suffers from the weight divergence problem(Zhao
etÂ al.,2018): the weight in the same coordinates (e.g., same layer or same filter) may have a large mismatching due to the highly skewed data distribution in each distinctive client/party. Therefore, directly averaging them will degrade the accuracy of the generated global model. To solve the issue, researchers leverage a particular DNN principle,weight permutation invariance, which has been mentioned and discussed by recent works(Wang etÂ al.,2020b; Yurochkin etÂ al.,2018,2019b). The key idea of this principle is that the weights in a DNN can be specially shuffled without incurring much accuracy drop. Concretely, supposeljsubscriptğ‘™ğ‘—l_{j}andlj+1subscriptğ‘™ğ‘—1l_{j+1}are the weight of two continuous layers in a DNN model, where the output function can be denoted as

whereIğ¼Iis the input andOj+1subscriptğ‘‚ğ‘—1O_{j+1}is the output of thej+1tâ€‹hğ‘—subscript1ğ‘¡â„{j+1}_{th}layer. Note that for each weight matrixlğ‘™l, it can be further decomposed as follows

whereÎ Î \Pirepresents the permutation matrix. In terms of this equation, we can transform Eq.4to the following form

Based on Eq.6, we can clearly see that the original layer weight can be losslessly transformed with a pair of well-designed permutation matrices, which we call itweight permutation invariance.

In federated learning, traditional aggregation methods fuse local models according to their weight location, which may be sub-optimal since theweight permutation invarianceprinciple indicates that we can change the weight value in a specific location while ensuring the same performance. Thus, the location-based aggregation cannot achieve accurate knowledge fusion, leading to the weight mismatching problem.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: Figure 4.The illustration of PFNM(Yurochkin etÂ al.,2019b).

To address this problem, a large number of federated optimation works attempt to achieve weight-level alignment. For example, Yurochkinet al.(Yurochkin etÂ al.,2019b)developed Probabilistic Federated Neural Matching (PFNM). As shown in Fig.4, the key idea is to identify subsets of
neurons in each local model that matches neurons in other local models and then combine the matched neurons to an improved global model by leveraging Bayesian nonparametric machinery. For single-layer neural matching, they presented a Beta Bernoulli Process(Zhang
etÂ al.,2013)based model of MLP weight parameters, where the corresponding neurons in the
output layer are used to convert the neurons in each batch and form a cost matrix. Then the matched neurons can be
aggregated to generate the final global model. For multilayer neural matching, they extended the single strategy by defining a generative
model of deep neural network weights from outputs back to inputs. In this way, they could adopt a greedy inference procedure that first infers the matching of the top layer and then proceeds down the layers of the model.

Unfortunately, PFNM only performs well on simple architectures (e.g. fully connected feedforward networks). For more complex CNNs and LSTMs, it just receives minor improvements over location-based methods (e.g., FedAvg). To further achieve the weight alignment goal, Wanget al.(Wang etÂ al.,2020b)proposed Federated Matched Averaging (FedMA) to effectively align advanced CNNs and LSTMs in a layer-wise manner. The key idea is to search for the best permutation matrices by addressing the following optimization problem

whereÎ¸isubscriptğœƒğ‘–\theta_{i}is theitâ€‹hsubscriptğ‘–ğ‘¡â„i_{th}neuron in the current global model,wjâ€‹lsubscriptğ‘¤ğ‘—ğ‘™w_{jl}is the output weights processed by permutation matrixÏ€lâ€‹ijsuperscriptsubscriptğœ‹ğ‘™ğ‘–ğ‘—\pi_{li}^{j}.câ€‹()ğ‘c()is the distance metric served as determining the similarity between neurons. To solve this optimization problem, unlike PFNM that used heuristic choices, FedMA addressed it by the Hungarian matching algorithm(Kuhn,1955).

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: Figure 5.Comparison between FedAvg andFâ€‹eâ€‹d2ğ¹ğ‘’superscriptğ‘‘2Fed^{2}(Yu etÂ al.,2021).

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: Figure 6.The illustration ofFâ€‹eâ€‹d2ğ¹ğ‘’superscriptğ‘‘2Fed^{2}(Yu etÂ al.,2021).

Despite effectiveness, the performance of weight-level aggregation/alignment largely depends on the selection of distance metric, which may not fully reflect the inherent feature information embedded in the neurons. In addition, the computation cost of the matching process is significantly heavy. To address these limitations, Yuet al.(Yu etÂ al.,2021)designed a feature-level alignment method, namedFâ€‹eâ€‹d2ğ¹ğ‘’superscriptğ‘‘2Fed^{2}which is composed of a feature-oriented structure adaptation and a model fusion algorithm. As shown in Fig.5, compared with traditional weight alignment,Fâ€‹eâ€‹d2ğ¹ğ‘’superscriptğ‘‘2Fed^{2}paid more attention to the neuron features and then aggregated the corresponding neurons. As a result, similar knowledge can be fused to achieve better performance.

Concretely, the authors developed two schemes to accomplish feature-based federated learning. Fig.6shows the pipeline of the proposedFâ€‹eâ€‹d2ğ¹ğ‘’superscriptğ‘‘2Fed^{2}. The first scheme ismodel structure adaptation, whereFâ€‹eâ€‹d2ğ¹ğ‘’superscriptğ‘‘2Fed^{2}takes advantage of the group-convolution technique to allocate and learn the distinctive neuron features. Next, a feature paired averaging policy is presented to aggregate different neurons according to the partitioned group features. In this way,Fâ€‹eâ€‹d2ğ¹ğ‘’superscriptğ‘‘2Fed^{2}enables more accurate feature alignment as well as avoiding the expensive distance-based optimization.

The aforementioned works mainly focus on alignment, in fact, there are also many other literatures targeting federated aggregation. For example, Yinet al.(Yin
etÂ al.,2018)proposed a robust aggregation method for distributed learning. In the beginning, this work mainly analyzed two robust distributed gradient descent (GD) algorithms, including the coordinate-wise median and the coordinate-wise trimmed mean. They proved statistical error rates for three kinds of population loss functions: strongly convex, non-strongly convex, and smooth non-convex. Furthermore, to reduce the communication cost, the authors designed a median-based distributed algorithm and demonstrate its effectiveness by extensive experiments. Chenet al.(Chen
etÂ al.,2020a)further considered the federated learning scenario, and found that heterogeneous data in different nodes will harm the training convergence to some degree. Based on this observation, they developed a novel gradient correction mechanism that can perturb the local gradients with noise. The main advantage of the proposed scheme is that it offers a provable convergence guarantee even when data are non-iid.

Besides, Yurochkinet al.(Yurochkin etÂ al.,2019a)leveraged Bayesian nonparametrics to design a meta-model that can potentially capture the global structure through statistical parameter matching. The authors pointed out that their approach is model-independent and is applicable to a wide range of model types.
Chenet al.(Chen and Chao,2020)proposed FEDBE, a novel method to apply bayesian model ensemble into conventional federated learning, aiming at making the aggregation more robust. Motivated by prior work(Maddox etÂ al.,2019), the authors utilized bayesian inference to construct an improved global model. In addition, stochastic weight average (SWA)(Izmailov etÂ al.,2018)is also used to further boost the performance.

Heterogeneous federated learning aims to effectively aggregate models generated from heterogeneous environments. Here the heterogeneous property could be reflected from data, models or device systems. We will dive into each aspect in the next parts.

Data heterogeneity indicates that collaborative clients might be in different situations, resulting in various data distributions. For example, the dog images collected from indoors and outdoors display highly heterogeneous data distribution. To address the issue, the research community borrows the idea from other AI techniques to alleviate the heterogeneity influence, which we list as follows.

Multi-task learning based methods.Multi-task learning enables learning models for multiple related tasks at the same time(Rebuffi
etÂ al.,2017; Bilen and Vedaldi,2017; Mallya and
Lazebnik,2018). The core design principle is to capture the relationship among tasks and leverage the relationship to facilitate the learning process. In federated learning, clients with different data distributions could also be considered as a type of multi-task learning, where each task has a distinctive statistical representation(Vanhaesebrouck etÂ al.,2017; Zantedeschi
etÂ al.,2020; TÂ Dinh
etÂ al.,2020; Hanzely etÂ al.,2020; Hanzely and
RichtÃ¡rik,2020; Huang etÂ al.,2021a).
For instance, Smithet al.(Smith
etÂ al.,2017)first proposed to combine federated learning and multi-task learning. By a series of concept formulations and theoretical analyses, they suggested multi-task learning is a natural choice to handle the statistical problem in the federated setting. Based on the combination, they further developed a novel approach MOCHA, in order to accomplish their goal. Specifically, the authors formulated the problem as a dual optimization problem as follows

whereltâˆ—superscriptsubscriptğ‘™ğ‘¡l_{t}^{*}andâ„›âˆ—superscriptâ„›\mathcal{R}^{*}are the conjugate dual functions ofltsubscriptğ‘™ğ‘¡l_{t}andâ„›â„›\mathcal{R}, respectively. To solve9, they carefully designed the quadratic approximation of the dual problem to separate computation across the nodes.

Despite federated multi-task learning being demonstrated effective, it has been applied only on convex models. To address the limitation, Corinziaet al.(Corinzia
etÂ al.,2019)proposed a more general approach, named VIRTUAL, to achieve federation on non-convex models.
The key idea is to construct a hierarchical Bayesian network in terms of the central server and the clients, such that the inference could be performed with variational methods. In this way, each client can obtain a task-specific model that benefits from the server model in a transfer learning manner.

Marfoqet al.(Marfoq etÂ al.,2021)further proposed to study federated multi-task learning under the flexible assumption that each local data distribution is a mixture of unknown underlying distributions, which is a more challenging and practical scenario. In the
beginning, the authors showed the fact that t federated learning is impossible without
assumptions on local data distributions. Then they made the flexible assumption and developed Federated Expectation-Maximization to accomplish their objective. Besides, the proposed approach is proven generalizable to unseen clients.

Meta-learning based methods.Meta-learning is commonly considered as learning to learn(Thrun and Pratt,2012). Compared with conventional deep learning algorithms that learn specific feature knowledge, meta-learning focus more on learning the learning ability. In the field of federated learning, meta-learning techniques can also be applied to generate a more personalized federation model. Jianget al.(Jiang etÂ al.,2019)first proposed to combine them, where they believed meta-learning had a number of similarities with the objective of addressing the statistical challenge in FL. Concretely, they developed a novel algorithm to further combine FedAvg(McMahan etÂ al.,2017)and Reptile(Nichol and
Schulman,2018), with two modifications: the first one is to decrease the local learning rate to make training more stable; another is to design a fine-tuning stage based on Reptile with smaller K and Adam as the server optimizer, which could improve the initial model as well as preserving and stabilizing the personalized model.

Khodaket al.(Khodak
etÂ al.,2019)built a theoretical framework to further characterize meta-learning methods and apply them into federated learning. They introduced Average Regret-Upper-Bound Analysis (ARUBA), which enables meta-learning to leverage more sophisticated structures. With ARUBA, researchers could improve the results of many ML
tasks, including adapting to the task-similarity, adapting to dynamic environments, adapting to the inter-task geometry and statistical learning-to-Learn. Towards FL, they improved meta-test-time performance on few-shot learning
and effectively added user-personalization to FedAvg.

Fallahet al.(Fallah
etÂ al.,2020)aims to find an initial shared model that can be easily fitted to their local data with one or a few steps
of gradient descent. They achieved their objective by incorporating Model-Agnostic Meta-Learning (MAML)(Finn
etÂ al.,2017,2018)into current FL pipelines. Specifically, the authors proposed a personalized variant of the FedAvg algorithm, named Per-FedAvg, which can be formulated as optimizing the following equation

wherenğ‘›nis the number of clients andÎ±ğ›¼\alphais the learning rate. The detailed solution for the optimization problem can be seen in the paper if readers have an interest.

Acaret al.(Acar etÂ al.,2021)further modified meta-learning to benefit federated learning. As shown in Fig.7, they proposed PFL, a gradient correction method based on prior works, which explicitly de-biased the
meta-model in the distributed heterogeneous data setting to learn a more personalized device model. During the process, convergence guarantees of PFL for strongly convex, convex and nonconvex meta objectives are provided.

Transfer learning based methods.Transfer learning aims to transfer the information learned from a source task to a target task(Pan and Yang,2009). A large number of research works have been proposed to advance this promising field(Yosinski
etÂ al.,2014; Li
etÂ al.,2018,2019b; Zhang etÂ al.,2022b). In federated learning, transferring the knowledge of the federated model to each client model will significantly facilitate the personalization performance under the data heterogeneity environment. Wanget al.(Wang etÂ al.,2019b)proposed to use fine-tuning, a typical transfer learning algorithm to achieve personalization. They first conducted traditional FL to obtain a global model. Then the federated model is regarded as the source model and further retrained using individual clientâ€™s training cache data. In this way, each client model can acquire and benefit the transferred knowledge, outputting an improved customized model.

Based on the aforementioned work, Yuet al.(Yu
etÂ al.,2020)extended the simple fine-tuning strategy. They investigated how three adaptation mechanisms: fine-tuning,
multi-task learning, and knowledge distillation affect the personalization performance. The authors characterized these mechanisms aslocal adaptation. In addition, different model protection techniques such as differential privacy and robust aggregation were applied to further validate the effectiveness of local adaptation. Finally, they used both CV and NLP datasets to demonstrate the superiority and necessity to conduct local adaptation.

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: Figure 7.The illustration of PFL(Acar etÂ al.,2021).

Penget al.(Peng
etÂ al.,2020)considered a new FL+TL scenario beyond fine-tuning. Instead, they paid more attention to domain shift, which means that the labeled data collected by source nodes statistically differ from the target nodeâ€™s unlabeled data. Based on this setting, they proposed the problem of federated domain adaptation and address it by Federated Adversarial Domain Adaptation (FADA). The key idea is to apply adversarial adaptation and representation disentanglement to FL settings.

Ozkaraet al.(Ozkara
etÂ al.,2021)introduced a quantized and personalized FL algorithm to deal with the data issue. The quantized training process is conducted via knowledge distillation (KD) among clients who have access to heterogeneous data and resources. Besides, they developed an alternating proximal gradient update to address this compressed personalization challenge and analyzed its convergence properties.

[å›¾ç‰‡: images\image_8.png]
å›¾ç‰‡è¯´æ˜: Figure 8.The illustration of IFCA(Ghosh
etÂ al.,2020).

Clustering-based methods.Clustering-based FL attempts to tackle the data heterogeneity issue via partitioning clients into different clusters, each of which conforms to a similar distribution. In terms of this key idea, much research effort is made to explore cluster-based FL. Sattleret al.(Sattler
etÂ al.,2020)proposed Clustered Federated Learning (CFL), to utilize geometric properties of the FL loss surface, in order to group
the client population into clusters with jointly trainable data distributions. It is worth noting that CFL is orthogonal to the current FL communication protocol and can be applied to
general non-convex objectives beyond DNNs.

Ghoshet al.(Ghosh
etÂ al.,2020)proposed the Iterative Federated Clustering Algorithm
(IFCA), which alternately estimated the cluster identities of the users and optimized model parameters for the user clusters via gradient descent. As shown in Fig.8, the server broadcasted models and the workers dynamically identified their cluster memberships and run local updates. This process will continue to operate until the clusters become stable.

To train high-quality cluster models, Ruanet al.(Ruan and Joe-Wong,2022)suggested FedSoft, which uses proximal updates to restrict client burden by asking a subset of clients to complete just one optimization task per communication round.

Liuet al.(Liu
etÂ al.,2021d)proposed a framework to accomplish privacy-preserving federated adaptation. The key idea is to group the clients with similar distribution to collaboratively adapt the federated model, rather than just adapting it with the data in a single device. PFA leveraged the sparsity property of neural networks to generate privacy-preserving representations and used them to efficiently identify clients with similar data distributions. In this way, PFA can conduct an FL process in a group-wise way on the federated model to achieve adaptation.

Besides, in order to achieve clustering without uploading any extra information, Liuet al.(Liu etÂ al.,2021b)further proposed DistFL, targeting at finishing accurate, automated and efficient cluster-based FL in terms of distribution feature. Specifically, they extracted the distribution knowledge from the uploaded model via existing synthesis techniques(Mahendran and
Vedaldi,2015)and then compared them to obtain the clustering results. Finally, they aggregated models in each cluster, getting rid of the influence of heterogeneous data.

Model heterogeneity means that the federated model might not be identical due to the different hardware and data distributions of clients. For example, in order to fit various
computation capabilities of clients, we require deploying different model architectures to match each client. On the other hand, NAS techniques(Zoph and Le,2016)have been widely used to search a crafted architecture based on the data in each device, thus leading to the model heterogeneity situation.

[å›¾ç‰‡: images\image_9.png]
å›¾ç‰‡è¯´æ˜: Figure 9.The illustration of HeteroFL(Diao
etÂ al.,2021).

To tackle the problem, Liet al.(Li and Wang,2019)used transfer learning
and knowledge distillation to develop a universal framework, which enabled federated
learning with uniquely designed models. Linet al.(Lin
etÂ al.,2020)further proposed a distillation framework for robust federated model fusion and leveraged entropy-reduction to accelerate convergence. Diaoet al.(Diao
etÂ al.,2021)designed HeteroFL to address heterogeneous clients equipped with highly
different computation and communication capabilities. As shown in Fig.9, the federation is achieved by aggregating parameters on the same location while unlearning the other non-overlapping area.

System heterogeneity is a practical property in FL scenarios because different clients/parties naturally own heterogeneous hardware and memory limitation. Therefore, how to accomplish FL under the condition of system heterogeneity is worth exploring.

[å›¾ç‰‡: images\image_10.png]
å›¾ç‰‡è¯´æ˜: Figure 10.The illustration of Oort(Lai
etÂ al.,2021b).

A key design for system acceleration is to develop different client selection strategies for avoiding the influence of latency stragglers. Here stragglers refer to the clients with weak computing power and thus could slow down the overall FL process. Laiet al.(Lai
etÂ al.,2021b)proposed Oort, a system to improve the performance
of federated training and testing with guided participant selection. As shown in Fig.10, Oort cherry-picked participants according to the tradeoff between statistical and system efficiency. Specifically, they defined â€Client Statistical Utilityâ€ to measure the importance of each client. Shinet al.(Shin
etÂ al.,2022)developed FedBalancer, a framework to actively select clientsâ€™ training samples in terms of the more â€œinformativeâ€ data. Besides, they introduced an adaptive deadline control scheme
to predict the optimal deadline for each round, in order to further speed up global training. Liet al.(Li
etÂ al.,2022a)observed that current client selection was coarse-grained due to their under-exploitation on the clientsâ€™ data
and system heterogeneity. Based on this finding, they proposed PyramidFL, a fine-grained client selection framework to speed up the FL training. The key idea is to not only focus on the divergence of those selected participants but also fully exploited the data and system heterogeneity within selected clients to profile their utility
more efficiently. As a result, PyramidFL is able to achieve better performance compared to other baselines.

The original design of federated learning considers the security problem via exchanging parameters while keeping raw data in their own devices. However, recent studies have proved that attackers might steal the privacy information from the uploaded models. Therefore, more rigorous secure FL should be investigated. In the following parts, we will introduce the attack methods and defense methods in FL scenarios.

[å›¾ç‰‡: images\image_11.png]
å›¾ç‰‡è¯´æ˜: Figure 11.The illustration of model replacement(Bagdasaryan etÂ al.,2020).

Backdoor attack.The goal of backdoor attacks is to manipulate a subset of training data by injecting adversarial triggers such that DNN models will
output incorrect prediction on the test set when the same trigger occurs. In federated learning, directly applying current backdoor attacks is unsuitable since the aggregation process might destroy the triggers. Bagdasaryanet al.(Bagdasaryan etÂ al.,2020)is the first to backdoor federated learning. They achieved their objective by proposing model replacement, which means the backdoor is injected to the joint model rather than raw data. As shown in Fig.11, the attacker trained a model on the backdoor data using the constrain-and-scale technique. In this way, the averaging function is largely affected by this attack model. Wanget al.(Wang etÂ al.,2020a)proposed edge-case backdoors, which forced a model
to misclassify on seemingly easy inputs that are unlikely to be part of the
training or testing data. For example, they may exist on the tail of the input distribution. As a result, it is extremely hard to detect them. Xieet al.(Xie
etÂ al.,2020)further developed distributed backdoor attack (DBA) to compromise FL. They mainly took advantage of the distributed nature of FL, decomposing a
global trigger pattern into separate local patterns and introducing them into the training set of different adversarial parties respectively. Therefore, DBA is more persistent and stealthy compared to centralized ones.
In FL models, backdoors can be inserted, but these backdoors are often not durable, i.e., they do not remain in the model after poisoned updates stop being uploaded. Since training occurs gradually in FL systems, an inserted backdoor may not survive until deployment. Zhanget al.(Zhang etÂ al.,2022d)proposed Neurotoxin, which is a simple modification to existing backdoor attacks that target parameters that are not changed in magnitude as much during training.

Gradients attack.Gradients attack targets at reverse some privacy information from gradients. In federated learning, exchanging gradients is a typical step for knowledge update and aggregation. Therefore, gradient attack poses a high risk to the federal participants. Zhuet al.(Zhu etÂ al.,2019)found since training occurs gradually in FL systems, an inserted backdoor may not survive until deployment. that it is possible to obtain the
private training data from the publicly shared gradients. They first randomly generated a pair of â€œdummyâ€ inputs and labels and used them to compute corresponding gradients. Then the gradients were compared to the shared ones and continually optimize the dummy inputs and labels to minimize the distance between them. As a result, the dummy data are close to the original ones and can peek into user privacy.
Lamet al.(Lam
etÂ al.,2021)further realized gradients attack from the aggregated model updates/gradients. The authors leveraged the summary information from device analytics and reconstructed the user participant matrix, which invalided the current secure aggregation protocols(Bonawitz etÂ al.,2017).
Zhuet al.(Zhu and Blaschko,2021)proposed Recursive Gradient Attack on Privacy (R-GAP), an approach to analyze how and when the target gradients can lead to the unique recovery of original data. Concretely, the authors designed a recursive, depth-wise algorithm for recovering training data from the gradient
information, which is the first closed-form algorithm that works on both CNN layers and FC layers.
Liet al.(Li
etÂ al.,2022b)found that under certain defense settings, generative gradient leakage can still leak private training information.

Model poison attack.The goal of poison attacks is to induce the FL model to output the target label specified by the adversary. For example, Tolpeginet al.(Tolpegin
etÂ al.,2020)implemented data poison attack by flipping the labels of training data from one class to another class in the local training epoch to mislead the global model output. Although the aggregation process in FL can mitigate the attack to some extent, when the number of malicious clients becomes large, FL is inevitably poisoned.
Fanget al.(Fang
etÂ al.,2020)conducted the first systematic study
on local model poisoning attacks to federated learning. Based on this study, they proposed local model poisoning attacks to Byzantine robust federated learning via manipulating the
local model parameters on compromised worker devices during the learning process. Besides, the authors further stated two defense strategies and test their performance on the proposed attack.

DP-based defense.Differential privacy (DP)(Dwork,2008)has been widely used to prevent information leakage. The key idea is to add some noises to obfuscate the original information. As a result, attackers are hard to infer the privacy properties. Federated learning also requires this type of protection since the uploaded model parameters can be easily exploited to extract sensitive information.
Weiet al.(Wei etÂ al.,2020)proposed NbAFL, a framework that applied DP into FL. Specifically, they added noises to parameters of the local model at the client side before aggregation. Besides, the authors theoretically analyzed the convergence property of differentially private FL algorithms and proved the effectiveness of the proposed framework.

Kairouzet al.(Kairouz
etÂ al.,2021a)presented a comprehensive end-to-end
system, where they discretized the data and added discrete Gaussian noise before conducting secure aggregation. In addition, the authors provided a novel privacy analysis for sums of discrete Gaussians and carefully analyzed the effects of data quantization and modular summation arithmetic. Experiments demonstrated that their method can achieve comparable performance with 16 bits of precision per value.
Agarwalet al.(Agarwal
etÂ al.,2021)proposed a multi-dimensional Skellam mechanism, where two independent poisson random
variables are used to measure the difference. The authors applied their mechanism to FL and provided a novel algorithm that
appropriately discretized the data and used the Skellam mechanism along with modular arithmetic to bound the range of the data and communication costs before secure aggregation. As a result, they could achieve better privacy-accuracy trade-offs in a more efficient manner.

[å›¾ç‰‡: images\image_12.png]
å›¾ç‰‡è¯´æ˜: Figure 12.The illustration of BatchCrypt(Zhang
etÂ al.,2020b).

[å›¾ç‰‡: images\image_13.png]
å›¾ç‰‡è¯´æ˜: Figure 13.The illustration of PPFL(Mo etÂ al.,2021).

HE-based defense.HE-based FL aims to combine traditional Homomorphic Encryption (HE) and FL in a more suitable way. By applying HE, FL is able to aggregate client models without revealing the information of the concrete model parameters. Therefore, it is impossible to infer user privacy from the model.
Hardyet al.(Hardy etÂ al.,2017)proposed to encrypt FL with the homomorphic scheme in the field of privacy-preserving entity resolution and federated logistic regression. They bounded the difference between the empirical loss of their classifier on the true data and showed an improved convergence speed. Besides, their experiments found that even rates for generalization cannot be significantly affected by entity resolution.
Liuet al.(Liu
etÂ al.,2019a)designed a secure FL framework through leveraging the additive property of partial homomorphic encryption, which effectively avoids the exposure of client models at the server side. Besides, the authors introduced two optimization mechanisms to further enhance efficiency.
Zhanget al.(Zhang
etÂ al.,2020b)proposed BatchCrypt, an efficient homomorphic encryption system for cross-Silo federated learning. As shown in Fig.12, there exist five typical steps to achieve a cross-silo FL system. In the beginning, the aggregator needed to select a client to generate an HE key-pair and distribute it to others. Then for each iteration, clients conducted local gradient updates and further encrypted them by the public key. These encrypted parameters were uploaded to the server where aggregation happened and the aggregated model is transferred to each client. Finally, the client side decrypted the received information and implemented the local training as the next round. BatchCrypt proposed two novel schemes to further improve efficiency. First, a feasible batch encryption scheme was presented to directly sum up the ciphertexts of two batches. Second, an efficient analytical model dACIQ was presented to choose optimal clipping thresholds with
the minimum cumulative error. As a result, BatchCrypt achieved 23Ã—-93Ã— training speedup while reducing the communication overhead by 66Ã—-101Ã—.

TEE-based defense.The aforementioned secure FL approaches provide security guarantee mainly from the perspective of software. In real-world scenarios, hardware protection is also widely applied by designing crafted architecture. Trusted Execution Environment (TEE) is
a trusted component that establishes an isolated region on the main processor to ensure the confidentiality and integrity of data and programs(ARM,2009; Costan and
Devadas,2016). Compared to traditional encryption schemes such as homomorphic encryption, TEE is more efficient with respect to the computation cost since it only requires some simple operations to connect the trusted and untrusted part in OS. Recently there have been a large number of works targeting at applying TEE to deep/federated learning, in order to achieve protection from hardware level. For example, Moet al.(Mo etÂ al.,2020)proposed DarkneTZ that enabled executing DNNs more secure with TEE in an edge device. They partitioned DNNs into a set of non-sensitive layers and sensitive layers, which are respectively processed by TEE or normal OS. Here the partition choice is based on the underlying systemâ€™s CPU execution time, memory usage, and accurate power consumption of different DNN layers. Besides, the authors developed a threat model to validate DarkneTZâ€™s robustness under the membership inference attack and the results showed that DarkneTZ could defend against this type of attack with negligible performance overhead.

Based on the combination of DNNs and TEE, Moet al.(Mo etÂ al.,2021)further attempted to apply TEEs to federated learning. Specifically, they proposed PPFL, a framework that limited privacy leakages in federated learning via implementing local training in TEEs. As shown in Fig.13, to address the challenge of limited memory size of TEEs, the authors designed a greedy layer-wise training to conduct local updates until convergence. In this way, this approach could support sophisticated settings such as training one or more layers (block) each time, which potentially speed up the training process.
Zhanget al.(Zhang
etÂ al.,2022c)proposed TEESlice, a system to provide a strong security guarantee while maintaining low inference latency with the help of TEEs. Concretely, TEESlice executed the more private model slices on TEEs and others on normal AI accelerators. As a result, TEESlice can achieve more than 10Ã— throughput promotion with the same level of strong security guarantee.

Existing works of federated learning pay more attention to improving learning performance based on the accuracy of the model and the time of learning task completion. However, the interests of the FL clients are often ignored and this may lead to unfairness. The problem of fairness can occur in the whole FL training process, including client selection, model optimization, incentive distribution, and contribution evaluation. The unfairness can have a negative impact on both the FL clients and the FL server, as clients are discouraged to join FL training, and servers are less likely to attract potentially high-quality clients. Recently, to achieve fairness from different angles, various Fairness-Aware Federated Learning (FAFL) approaches have been proposed. In this section, we will discuss recent FAFL methods in detail.

Unfairness in FL Client Selection mainly consists of three types, over-representation, under-representation, and never-representation. Suppose an FL system prefers to select clients with high performance (such as a faster GPU), and clients with the highest performance may be selected much more than any other clients (i.e., over-representation), while clients with poor performance may be selected just a few times (i.e., under-representation). At the same time, the client with the lowest performance may never be selected (i.e., never-representation). Additionally, due to the heterogeneity among clients, fairness does not indicate giving everyone the same possibility to be selected. It is important to balance the interests of the server and the interests of the clients. If clients from specific groups are oversampled, the global FL model will be partial to their data, so the modelâ€™s performance will deteriorate(Cho
etÂ al.,2020). Existing FAFL client selection methods can be partitioned into two categories, considering fairness factors and customization for each client.

1) Fairness factors.Fairness factors are designed to allow rarely selected clients, such as clients with lower computational abilities or smaller datasets, to join the FL training more frequently. Yanget al.(Yang
etÂ al.,2021)proposed a client selection algorithm based on the Combinatorial Multi-Armed Bandit (CMAB) framework to reduce the class imbalance effect. Inspired by(Li etÂ al.,2019a), Huanget al.(Huang
etÂ al.,2020)converts the original offline problem to an online Lyapunov optimization problem and uses dynamic queues to quantify the long-term guarantee of the client participation rate. Moreover, Huang introduces a long-term fairness constraint to make sure the average clientâ€™s long-term chosen rate is above a constant. After(Huang
etÂ al.,2020), Huanget al.(Huang
etÂ al.,2022)improves the performance by replacing dynamic queues to the Exp3 algorithms(Auer
etÂ al.,2002), and the fairness parameter determining the selection possibility in each round can be different. However, these works all design the fairness factor without considering the real-time contribution of individual clients. Songet al.(Song
etÂ al.,2021)addresses this problem and proposed a client selection policy with fairness constraints based on reputation, using a fairness parameter to balance reputation and the number of successful transmissions.

2) Client customization.This approach pays attention to customized model settings or customized model procedures. Clients often receive the same initial models at the first training round in most current FL paradigms. Therefore, clients with lower capabilities, such as bad network connections, require more time to complete each training round and are likely to be kept out of subsequent rounds, leading to under-presented and never-presented problems. To alleviate this problem, dynamically adapting the FL model framework or the training procedure based on client capabilities is often used.

Caldaset al.(Caldas etÂ al.,2018)proposed Federated Dropout (FD), which distributes sub-models with sizes suitable for each client based on their computational resources. The process of FD is shown in Fig14. Although FD diminishes communication and local computation costs largely, it uses dropout operations and treats the neural networks as black-box functions. Bouaciadaet al.noticed this problem and proposed Adaptive Federated Dropout (AFD)(Bouacida
etÂ al.,2021). AFD keeps an activation score map to generate the best-fit sub-model for each client. FD and AFD both make sure clients with low capabilities could participate in FL training, but they do not provide custom pruned submodels to different clients. To address this limitation, Horvathet al.(Horvath etÂ al.,2021)augmented FD to Ordered Dropout (OD). Different from FD, OD drops neighboring components of the model despite random neurons. OD divides clients with comparable computational capabilities into clusters, and clients in the same cluster apply the same dropout rate. Moreover, OD applies the knowledge distillation method(Hinton
etÂ al.,2015)to enhance feature extraction for smaller submodels.

[å›¾ç‰‡: images\image_14.png]
å›¾ç‰‡è¯´æ˜: Figure 14.The summary of the Federated Dropout (FD) training procedure(Caldas etÂ al.,2018).

[å›¾ç‰‡: images\image_15.png]
å›¾ç‰‡è¯´æ˜: Figure 15.The illustration of ThrowRightAway (TRA) scheme(Zhou
etÂ al.,2021).

Clientsâ€™ communication capabilities can also affect client selection. A poor network may cause too much retransmission and lead to extra delays in FL model training, which makes clients with a poorer network less likely to aggregate their model updates into the final model and leads to model bias. To deal with this issue, Zhouet al.(Zhou
etÂ al.,2021)proposed ThrowRightAway (TRA), a loss-tolerant FL framework that makes the FL training faster by ignoring few lost packets. As is shown in Fig15, at first every participating FL client reports their network conditions to the FL server, and the server divides the clients into two categories: sufficient type and insufficient type. Only the clients in the sufficient type can get a re-transmission request and then re-transmit their loss packets. Apparently, the method can only be effective when the category is accurate.

This method means assigning less work to clients with lower capabilities to make them available to pass threshold-based FL client selection. Liet al.(Li
etÂ al.,2020b)proposed FedProx which allowed each client performed partial training based on its accessible resources. FedProx allows various local epochs, and thus more clients are encouraged to join the training process.

In the optimization during FL model training, the model may discriminate against definite preserved groups, or overfit some clients at the expense of others. Recent works dealing with this issue can be approximately divided into two types: 1) objective function-based and 2) gradient-based.

1) Objective function-based methods:Objective function-based methods focus on the global/local objectives of the FL model, such as minimizing the loss function. Mohriet al.(Mohri
etÂ al.,2019)proposed AFL, which aims to prevent the model overfitting any specific client at the expense of others. AFL just optimizes the global model for the target distribution made up of a mixture of clients. However, this method only works for a small number of clients. Zhouet al.(Zhou
etÂ al.,2021)proposed q-FFL to diminish the scalability limitation of AFL. q-FFL adds parameter q to reweigh the aggregate loss. To improve the model robustness and maintain good-intent fairness at the same time, Huet al.(Hu
etÂ al.,2022)proposed fedMGDA+ which optimizes each FL clientâ€™s loss function respectively and simultaneously. Addressing the same issue, Liet al.(Li
etÂ al.,2021a)proposed Ditto, which improves fairness and robustness at the same time.

While the methods mentioned all pay attention to the accuracy parity notion of fairness, there are also many kinds of research focusing on group fairness. Duet al.(Du
etÂ al.,2021)proposed AgnosticFair, which incorporates an agnostic fairness constraint. Although it has good accuracy and fairness on unknown testing data distribution, it needs prior knowledge to design the re-weighting function, which limits its application in dynamic systems. Cuiet al.(Cui
etÂ al.,2021)proposed FCFL, a multi-objective optimization framework that achieves good-intent fairness and group fairness at the same time. Different from AFL, it minimizes the loss of the client with the worst performance and uses a smooth surrogate maximum function considering all clients. A fairness constraint is also added to calculate the disparities among all clients.

2) Gradient-based approaches:Here, gradient means the local updated gradient of each client in every local iteration. Wanget al.(Wang
etÂ al.,2021)proposed the federated fair averaging (FedFV) algorithm, which aims to average clientsâ€™ gradients after mitigating potential conflicts among clients. FedFV detects gradient conflicts through the cosine similarity and modifies both the direction and magnitude of the gradients by iteratively eliminating such conflicts. However, the estimated gradients may be incompatible with the latest updates.

Contribution evaluation in FL learning indicates that an FL system can evaluate the contribution of different clients without accessing data from the clients. Many methods designed for non-privacy machine learning environments cannot be applied to FL scenarios directly. A general method is to evaluate each clientâ€™s model contribution to the aggregated FL model, and a fair evaluation is critical. Unfairness in contribution evaluation may lead to the free-rider issue(Hardin and
Cullity,2003), which implies that clients contribute little but can get similar benefits as the clients who contribute more. In this part we will introduce five types of existing FL contribution evaluation methods with their typical works.

1) Self-reported information:This method of evaluation contribution is based on clients reporting their information actively. Most works based on this method believe their clients are reliable, which is not always correct in practice. Proposed by Zhanget al.(Zhang etÂ al.,2020a), Hierarchically fair federated learning (HFFL) follows the idea of â€™contribute more, get more rewardâ€™, which is proved effective in social psychology(Tornblom and
Jonsson,1985), game theory(Rabin,1993)and bandwidth allocation(Li etÂ al.,2008). Hence, itâ€™s critical to figure out how to evaluate a clientâ€™s contribution and how much proportion of reward a client should get to ensure fairness. Data Shapley can be used to evaluate contribution in machine learning, but Shapley value is model-dependent(Ghorbani and Zou,2019)and incompatible with FL tasks. As a result, Zhang proposes evaluating contributions based on publicly verifiable factors of clients, such as cost of data collection, data volume, and data quality, to avoid the inconsistency of model-dependent methods. To distribute proportional rewards to clients, Zhang introduces hierarchically fair federated learning (HFFL), as is shown in Figure16. The publicly verifiable factors determined by the clientsâ€™ consensus about each client are reported to the FL server, and the FL server then uses the information to rate each client, which at the same level are supposed to contribute to the model equally and will get the equal reward.

[å›¾ç‰‡: images\image_16.png]
å›¾ç‰‡è¯´æ˜: Figure 16.The illustration of hierarchically fair federated learning (HFFL)(Zhang etÂ al.,2020a).

2) Individual evaluation:Individual evaluation implies evaluating contribution through performance on specific tasks and pays more attention to individual performance instead of global performance. The method often adopts two assumptions that both the server and the client are reliable and clients with a similar model to others are regarded to supply more contribution, which is not always feasible. To achieve fairness without sacrificing the model performance, Lyuet al.(Lyu
etÂ al.,2020a)proposed a Collaborative Fair Federated Learning (CFFL) framework based on reputation, which uses a reputation mechanism to achieve collaborative fairness. Lyu definites collaborative fairness as the reward is proportional to the clientâ€™s contribution. Different standard FL process, CFFL allows clients to receive only the allocated aggregated updates according to their reputations, and the server is in charge of a reputation list which is updated in each communication round relying on the quality of the uploaded gradients of each participant.

3) Utility game:The utility game(Gollapudi etÂ al.,2017)refers to a game where each player chooses an available team to maximize their payoffs, while the universal social welfare is the total utility produced by all the teams cumulatively. FL contribution evaluation methods based on utility games have a deep connection with profit-sharing schemes, and there are three diffusely used profit-sharing schemes: (1) Egalitarian: any part of the utility produced by a team is separated equally between the members.
(2) Marginal gain: the payoff of a player in a team is equal to the team gained when the player joined.
(3) Marginal loss: the payoff of a player in a team is equal to the team will lose if the player leaves.

Among the three types above, the marginal loss scheme is the most commonly adopted. Wanget al.(Wang
etÂ al.,2019a)proposed a deletion method to evaluate contributions in horizontal federated learning. This evaluation method consists of removing the instances supplied from one definite party, retraining the model, calculating the difference between the original model and the new model, and using this difference to define the contribution of this party. Wang formulates the influence measure as follows,

where n is the size of the dataset,y^jsubscript^ğ‘¦ğ‘—\hat{y}_{j}is the model trained on all data prediction onjth instance, andy^jâˆ’isuperscriptsubscript^ğ‘¦ğ‘—ğ‘–\hat{y}_{j}^{-i}s the model trained without theith instance prediction onjth instance.

Then Huang defines a partyâ€™s contribution as the total influence of all instances it possesses.

For vertical horizontal learning, Huang uses shapley value which will be introduced in the next part.

4) Shapley value:Shapley value (SV) was first introduced in cooperative game theory(Shapley,1997). Different from marginal loss, SV-based FL contribution evaluation approaches can reflect the contribution of a clientâ€™s own data, in spite of its joining order, and can produce a fairer evaluation. However, SVâ€™s computational complexity isOâ€‹(2n)ğ‘‚superscript2ğ‘›O(2^{n}), so many approaches have been proposed to improve efficiency.

In this section, we will introduce several prevalent frameworks of federated learning, including FedLab, Flower, FedML, FATE, and FedScale.

FedLab.Since most FL schemes follow the same basic steps and just a few changes in some steps are needed in different scenarios, Zenget al.(Zeng
etÂ al.,2021)proposed FedLab(SMILELab-FL,2021), which is designed flexible and customizable, offers essential functional modules, and has highly customizable interfaces. Two main roles in FL settings are provided: Server and Client, and both of them are made up of two components, NetworkManager and ParameterServerHandler/Trainer. The design focuses more on communication efficiency and FL algorithm effectiveness. To support methods improving Communication Efficiency, FedLab uses tensor-based communication, supports customizable communication agreement, and implements both Synchronous and Asynchronous communication patterns according to Federated Optimization algorithms. For Optimization Effectiveness, FedLab applies a â€high-cohesion and low-couplingâ€ optimization module which provides aggregation and data partition methods. Additionally, FedLab can be used in various scenarios, such as Standalone, Cross-process and Hierarchical FL simulation.

Flower.Due to the lack of frameworks that are able to support scalably executing FL methods on mobile and edge devices, Beutelet al.(Beutel etÂ al.,2020)proposed Flower(adap,2020), which can run large-scale FL experiments on different FL device scenarios. Flower makes it possible to smoothly transition from experimental research to system research on a large group of real edge devices. Designed to be scalable, client-agnostic, communication-agnostic, privacy-agnostic, and flexible, Flower has extensive implementations, such as communication stack, serialization, ClientProxy, and Virtual Client Engine(VCE).

FedML.Proposed by Heet al.(He etÂ al.,2020), FedML(FedML-AI,2020)aims to solve the lack of support for diverse FL computing paradigms, support of diverse FL configurations, and standardized FL algorithm implementations and benchmarks. FedML library is mainly made up of high-level API FedML-API and low-level API FedML-core. To support FL on real-world hardware platforms, FedML offers on-device FL testbeds called FedML-Mobile and FedML-IoT which are built upon real-world hardware platforms. FedML programming interface allows worker/client-oriented programming, message definition beyond gradient and model, topology management, trainer and coordinator, privacy, security, and robustness, so users can just pay attention to algorithms implementations and ignore the backend details.

FATE.Since most open-sourced frameworks are research-oriented and lack the implementation on industry, Liuet al.(Liu
etÂ al.,2021c)proposed FATE(Federated AI Technology Enabler)(WeBank,2019), which is the first production-oriented platform. Built on FederatedML, FATE provides Private Set Intersection(PSI), and uses distributed computation framework Eggroll to improve computation efficiency. FATE provides three main components, scheduling system FATE-Flow, visualization tool FATE-Board, and high-performance inference platform FATE-Serving. In addition, kinds of deployments are supported, including building FATE on top of Kubernetes in data centers through KubeFATE, manual or docker deployments on Mac and Linux, and cross-cloud deployment and management through FATE-cloud.

FedScale.Laiet al.(Lai
etÂ al.,2021a)proposed FedScale(SymbioticLab,2021), which contains many realistic FL datasets for different tasks, and FedScale Runtime which is an automated evaluation platform aiming to simplify and standardize FL evaluation in more realistic environments. The raw data of FedScale datasets are collected from various sources, processed into consistent formats, sorted into different FL use cases and packed into standardized APIs for users to easily use in other frameworks. The evaluation platform, FedScale Runtime, is equipped with both mobile and cluster backends to enable both on-device FL evaluation on smartphones, and FL evaluations in real deployments and in-cluster simulations.

This section summarizes some limitations of current FL approaches and discusses possible future directions.

Dynamic federated learning.Current federated learning approaches assume that data in each client are stable and unchanged. However, in real-world scenarios, clients may be in an ever-changing environment, where the local data are continuously observed and processed by sensors. Under this condition, directly conducting conventional training and aggregation will suffer from the catastrophic forgetting problem, which indicates that the prior knowledge learned by the model might be forgotten as new data arrive. Incremental learning(Castro etÂ al.,2018; Wu
etÂ al.,2019; Lomonaco
etÂ al.,2021)is a hot research topic to address the issue, targeting at learning new knowledge
while maintaining the ability to recognize previous ones. In the future, how to effectively combine federated learning and incremental learning is worth exploring.

Decentralized federated learning.A central server is of vital importance to traditional federated learning since aggregation needs to be conducted in this side. Considering that the third-party server may not be honest, uploading parameters or gradients to it potentially exists security risks. Therefore, it is necessary to achieve federated learning without a server involved.
Although Heet al.(He
etÂ al.,2019)has made a preliminary attempt to decentralized FL, they only target logistic regression and the experiments are insufficient.
How to accomplish general decentralized FL still remains an open problem.

Scalability of federated learning.Recent FL papers paid more attention to designing new algorithms to improve FL performance under different conditions. However, they ignore the scalability property, which determines whether we could operate large-scale FL. In many cooperation scenarios, there might be a huge number of parties and we should provide guidance to the cooperation improvement as the number of participants increases. In a word, FL scalability deserves future investigation.

Unified benchmark.Although a large number of datasets have been used for evaluating the performance of FL, there is still a lack of a unified benchmark to align the results for a fair comparison. On one hand, in order to achieve different federated goals (e.g., personalization, robustness), researchers use different datasets to test the performance. On the other hand, two typical types of FL, horizontal FL and vertical FL, also apply distinctive datasets to demonstrate the performance of different FL types. Thus a unified benchmark will definitely benefit the FL community.

Federated learning has gained more and more attention due to its ability of collaboratively generating a global model without leaking sensitive information. Recent surveys have summarized many related works devoted to offering a comprehensive understanding to developers and readers in this community. However, most of them focus on a specific aspect of FL or fail to catch the latest progress of this hot research topic. This paper provides a systematic survey, which investigates recent development on federated learning. By analyzing the pipeline and challenges of FL, we propose a taxonomy with different FL aspects involved. In addition, we also explore some practical FL frameworks and characterize their features. Finally, some limitations and future direction are concluded in order to promote the evolution of the FL community.

[å›¾ç‰‡: images\image_17.png]

[å›¾ç‰‡: images\image_18.png]

