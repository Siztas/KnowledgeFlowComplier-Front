æ ‡é¢˜ï¼šUnderstanding and Measuring Robustness of Multimodal Learning

The modern digital world is increasingly becoming multimodal. Although multimodal learning has recently revolutionized the state-of-the-art performance in multimodal tasks, relatively little is known about the robustness of multimodal learning in an adversarial setting. In this paper, we introduce a comprehensive measurement of the adversarial robustness of multimodal learning by focusing on thefusionof input modalities in multimodal models, via a framework called MUROAN (MUltimodal RObustness ANalyzer). We first present a unified view of multimodal models in MUROAN and identify the fusion mechanism of multimodal models as a key vulnerability. We then introduce a new type of multimodal adversarial attacks called decoupling attack in MUROAN that aims to compromise multimodal models by decoupling their fused modalities. We leverage the decoupling attack of MUROAN to measure several state-of-the-art multimodal models and find that the multimodal fusion mechanism in all these models is vulnerable to decoupling attacks. We especially demonstrate that, in the worst case, the decoupling attack of MUROAN achieves an attack success rate of 100% by decoupling just 1.16% of the input space. Finally, we show that traditional adversarial training is insufficient to improve the robustness of multimodal models with respect to decoupling attacks. We hope our findings encourage researchers to pursue improving the robustness of multimodal learning.

Multimodal learning has been gradually gaining focus of the research community over the past few years. The approaches for multimodal learning have come a long way from simple models re-purposed for multimodal tasks, to deep learning-based models that are specifically designed for multimodal tasks (referred to as Deep Multimodal Models or DMMs throughout this paper). For example, recent advances in this field have led to several state-of-the-art DMMs, such as ViLBERT(Lu etÂ al.2019), VisualBERT(Li etÂ al.2019), MMBT(Kiela etÂ al.2019), and Pythia(Jiang etÂ al.2018), while also engendering the collection of several multimodal datasets, such as Hateful Memes(Kiela etÂ al.2020), Visual Question Answering (VQA)(Goyal etÂ al.2017), and Visual Commonsense Reasoning (VCR)(Zellers etÂ al.2019). Due to the success of these DMMs on standard benchmarks, there have been many encouraging attempts to adopt them to real-world and safety-critical scenarios, such as assistance to blind people(Gurari etÂ al.2018), hate-speech moderation on social media(Kiela etÂ al.2020), as well as emerging domains, such as Google MUM search(goo2021). However, in spite of the recent advances, the robustness of DMMs remains poorly understood.

A significant difference between DMMs and their unimodal counterparts is thefusionmechanism in DMMs. This fusion mechanism fuses multiple input modalities to learn their joint representation, which is then processed by several fully connected layers to predict classification scores depending on the nature of the corresponding downstream tasks. Different DMMs(Lu etÂ al.2019; Kiela etÂ al.2019; Li etÂ al.2019; Jiang etÂ al.2018)employ different strategies to learn strong fusion embeddings of their input modalities. This fusion mechanism presents new challenges towards studying the adversarial robustness of these models.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1:By decoupling the input modalities through removal of a few datapoints in the image via MUROAN framework, the multimodal model predicts a wrong answer class:Nothing, indicating that decoupling attack can easily compromise multimodal models.

Recently, several unimodal adversarial attacks for deep unimodal models have been formulated to study their robustness. For example, unimodal adversarial images(Szegedy etÂ al.2013; Madry etÂ al.2017; Papernot etÂ al.2016; Wicker, Huang, and Kwiatkowska2018; Carlini and Wagner2017; Wang etÂ al.2020)and unimodal adversarial text(Alzantot etÂ al.2018; Li etÂ al.2018; Jin etÂ al.2019; Alzantot etÂ al.2018; Ren etÂ al.2019)have been widely studied, which have exposed numerous vulnerabilities in the deep unimodal models. However, these attacks cannot be directly employed to study the robustness of their deep multimodal counterparts. First, since these attacks can only be applied to single modalities, they do not affect the fusion mechanism that is fundamental to DMMs. Second, since DMMs combine several different types of modalities (e.g. image, text, speech, etc.), a single unimodal attack cannot be used for all those modalities. We note that formulating comprehensive methods to study the robustness of DMMs is of utmost importance to adopting them in real-world systems, such as VQA.

To address these challenges, in this work, we first highlight how multimodal adversarial attacks based on decoupling the input modalities in DMMs can easily compromise these models. Then, we introduce a framework called MUROAN to study the robustness of DMMs based on decoupling of modalities, thereby revealing vulnerabilities in the fusion mechanism of existing DMMs. MUROAN uses a unified view of DMMs to expose its key vulnerability. Then, we introduce a new type of adversarial attacks called decoupling attack in MUROAN, wherein the objective of its attack algorithm is to decouple the input modalities of multimodal models to induce a misclassification. As depicted in Figure1, a decoupling of the image and text modalities through occlusion of a few datapoints in the image induces a misclassification. In addition, we leverage the MUROAN framework to measure several state-of-the-art DMMs. We find that the seemingly straightforward decoupling attack of MUROAN is in fact highly effective in compromising DMMs.

Our contributions in this work are as follows.

We present a unified view of DMMs to explore their vulnerabilities, and identify the fusion mechanism of these models as a critical component for their robustness analysis.

We propose a novel framework called MUROAN that consists of the unified view to exploit the fusion mechanism and a decoupling attack algorithm for comprehensively studying the adversarial robustness of DMMs. MUROAN directly focuses on the fusion mechanism of DMMs by decoupling the input modalities that are fused together.

We use MUROAN for a comprehensive robustness analysis of state-of-the-art DMMs under several dataset and model settings. Our experiments show that, in the worst case, the decoupling attack in MUROAN can achieve an attack success rate of 100% after decoupling of 1.16% of input modalities of DMMs, while the unimodal adversarial attacks overestimate the robustness of DMMs.

We are open-sourcing our code to encourage research in training DMMs robust to decoupling attacks:http://github.com/SecurityAndPrivacyResearch/mda.

In the following, we give an overview of the field of multimodal learning as well as the state-of-the-art unimodal adversarial attacks used for the robustness analysis of unimodal models.

The renewed interest in multimodal learning can be attributed to more powerful models(Devlin etÂ al.2018; Vaswani etÂ al.2017)that can learn strong fusion of input modalities and the availability of several multimodal datasets(Goyal etÂ al.2017; Zellers etÂ al.2019; Kiela etÂ al.2020). These models and datasets have resulted in DMMs achieving impressive results on standard benchmarks. Much of the DMMs that have achieved impressive performances can be categorized under the following categories.

Traditional Fusion-based Models.Several DMMs have attempted to address how to effectively combine multimodal information(BaltruÅ¡aitis, Ahuja, and
Morency2018; Bruni, Tran, and Baroni2014; Lazaridou, Pham, and Baroni2015). Feature concatenation is one of the most preferred fusion techniques in these models, while some of the models use other feature fusion techniques such as element-wise product. Since these models showed impressive performances on several multimodal benchmarks, they are considered strong baselines for many multimodal tasks.

Transformer-based Fusion Models.Recently, the BERT model(Devlin etÂ al.2018), a type of transformer(Vaswani etÂ al.2017), has been shown to achieve state-of-the-art performance(Kiela etÂ al.2019; Li etÂ al.2019; Su etÂ al.2019)on multimodal benchmarks, by learning the interaction between the input modalities via self-attention over many different layers. For example the MMBT(Kiela etÂ al.2019)model fuses image embeddings in the form of pooled filter maps from a ResNet model and word tokens as two segments of BERT(Devlin etÂ al.2018). Similarly, the VL-BERT(Su etÂ al.2019)model fuses regions of interest (ROIs) of an image with word tokens as two segments of BERT. As shown by these works, the transformer based DMMs outperform their unimodal counterparts in multimodal tasks by quite a large margin.

The discovery of unimodal adversarial attacks has engendered active research in the safety and robustness of unimodal deep learning models. In this section, we discuss important unimodal adversarial attacks on images and text.

Unimodal Adversarial Image.A large body of adversarial attacks have been introduced in recent times that mainly focus towards robustness analysis of computer vision models. For example, several works, such as fast-gradient attacks(Goodfellow, Shlens, and Szegedy2014; Liu etÂ al.2016), optimization-based methods(Szegedy etÂ al.2013; Carlini and Wagner2017), and other such methods(Papernot etÂ al.2016; Nguyen, Yosinski, and Clune2015), have been proposed successfully. Recently, ensemble based attacks(Croce and Hein2020)have been show to more deeply reveal vulnerabilities in unimodal models. Furthermore, alarmingly critical real-world attacks such as adversarial patches(Brown etÂ al.2017)have been introduced recently, which cast serious questions on the safety of these vision models.

Unimodal Adversarial Text.Recently, some works have focused on unimodal adversarial text to study robustness of Natural Language Processing (NLP) models. While earlier works(Li etÂ al.2018; Gao etÂ al.2018; Eger etÂ al.2019)effectively employed character level perturbations to perform adversarial attacks, more recent works have found word replacement strategies(Jin etÂ al.2019; Alzantot etÂ al.2018; Ren etÂ al.2019)to be largely effective in compromising these models. Recent works(Iyyer etÂ al.2018; Zhao, Dua, and Singh2017; Ribeiro, Singh, and Guestrin2018)have also demonstrated how sentences can be merely reconfigured to pose serious adversarial threats.

Recently, some studies have emerged that discuss adversarial attacks on DMMs(Tian and Xu2021; Li etÂ al.2020). However, these studies do not focus on exploring the vulnerabilities of the fusion mechanism to adversarial attacks. In this work, we specifically focus on comprehensively studying the adversarial robustness of DMMs via a type of multimodal adversarial attack called decoupling attack, that focuses on decoupling the input modalities of a DMM to compromise the fusion mechanism of these DMMs.

Our primary objective in this section is to demonstrate how easily decoupling of input modalities can compromise DMMs. To this end, we performed a preliminary experiment for comparing the effect of decoupling attacks on DMMs against traditional unimodal adversarial attacks.

We randomly selected 100 samples from the VQA dataset(Antol etÂ al.2015)and the pretrained Pythia DMM(Jiang etÂ al.2018)to conduct our preliminary study. Several previous unimodal attacks(Goodfellow, Shlens, and Szegedy2014; Madry etÂ al.2017; Kurakin, Goodfellow, and Bengio2016; Moosavi-Dezfooli, Fawzi, and Frossard2016; Papernot etÂ al.2016; Xie etÂ al.2019; Dong etÂ al.2018)have revealed the nature of different vulnerabilities in traditional unimodal model. In this experiment, we use the state-of-the-art attack called PGD attack(Madry etÂ al.2017)as the traditional, unimodal adversarial attacks. Next, we manually studied the 100 samples and occluded datapoints that we considered as participating in the fusion mechanism. Our objective from this step was to manually decouple the DMMs to study whether decoupling could be considered as an effective means to create adversarial attacks that can be comparable against strong and popular unimodal attacks in terms of their effectiveness in fooling the DMM.

We found that just manual decoupling was able to effectively fool 50% of the samples considered in the experiment. But more importantly, we found that on average, manual decoupling only affected 7.25% of the datapoints in the image of each sample. On the other hand, we found that although the PGD attack was quite effective in compromising the DMM with close to 100% success rate, 96.47% of the datapoints on average were affected by PGD. What this experiment shows is that unimodal adversarial attacks are not able to identify the optimum datapoints to perturb. Thus, unimodal attacks are not sufficiently suitable for studying robustness of DMMs. Furthermore, since unimodal attacks do not seem to take the fusion mechanism into consideration, they do not reveal the vulnerabilities specific to DMMs. In the sections that follow, we show how MUROAN decoupling attack algorithm can optimally find the exact datapoints involved in fusion, so that the adversarial robustness through decoupling can be studied.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2:Overview of our approach.

In this section, we discuss our approach for the robustness analysis of DMMs via MUROAN framework. In this regard, we first discuss a unified view of DMMs to explore the vulnerabilities of the fusion mechanism of DMMs, and then introduce our algorithm to decouple the fused modalities of DMMs. The overview of our approach is depicted in Figure2.

We consider a DMMD:Xâ†’Y:ğ·â†’ğ‘‹ğ‘ŒD:X\rightarrow Yto be a function that maps a domainXğ‘‹Xto a co-domainYğ‘ŒY. An input is a set of vectors of different modalitiesx={x01â€‹â€¦â€‹xn1,x02â€‹â€¦â€‹xm2,â€¦}ğ‘¥subscriptsuperscriptğ‘¥10â€¦subscriptsuperscriptğ‘¥1ğ‘›subscriptsuperscriptğ‘¥20â€¦subscriptsuperscriptğ‘¥2ğ‘šâ€¦x=\{x^{1}_{0}\dots x^{1}_{n},\,x^{2}_{0}\dots x^{2}_{m},\,\dots\}(Figure2, Step (a)). We considerYğ‘ŒYto be the set of possible classes for a multimodal inputxâˆˆXğ‘¥ğ‘‹x\in X. The output of the DMM for a multimodal inputxğ‘¥xis considered to beDâ€‹(x)=yğ·ğ‘¥ğ‘¦D(x)=y, for someyâˆˆYğ‘¦ğ‘Œy\in Y. We denote the confidence of the DMM for a multimodal classification probability on inputxğ‘¥xand classyğ‘¦yasDyâ€‹(x)subscriptğ·ğ‘¦ğ‘¥D_{y}(x). Lastly, we denote the cardinality of a set as|â‹…||\cdot|, which represents the number of elements in the set.

Although DMMs have several different architectural configurations, we need a unified view (or representation) of them for a uniform vulnerability analysis of all these different multimodal architectures. To achieve this, we unify these different architectural approaches into a single view, in which we consider a DMM as a generator of the fusion embedding of multiple input modalities (Figure2, Step (b)), followed by several fully connected layers that are specific for downstream tasks. In other words, we break down a DMM into two functions: the first generates a latent representation (i.e., the fusion embedding) of the multimodal inputs and the second performs classification based on the fusion embedding. We consider the fusion embedding of a multimodal inputxğ‘¥xasZâ€‹(x)=zğ‘ğ‘¥ğ‘§Z(x)=z, wherezğ‘§zis thedğ‘‘d-dimensional fusion embedding vector. Next, we considery=Mâ€‹(z)ğ‘¦ğ‘€ğ‘§y=M(z)to represent classification based on the fusion embedding from fully connected layers that are specific to downstream tasks. Therefore, the original DMM is broken down into two functions, represented asMâ€‹(Zâ€‹(x))ğ‘€ğ‘ğ‘¥M(Z(x)). We further discuss this process for two typical DMM architectures: traditional architectures and transformer-based architectures.

Traditional Multimodal Architectures.Traditional DMM architectures are composed of separate neural networks that are specific to each input modality, whose outputs are combined using fusion techniques such as element-wise multiplication, addition or concatenation. For example, the Pythia(Jiang etÂ al.2018)architecture is composed of a convolutional neural network that learns the embedding of the image modality, and a recurrent network that learns the embedding of the text modality, which are then combined using element-wise multiplication. This combination represents the fusion embedding.

Transformer-based Multimodal Architectures.These architectures use the transformer(Vaswani etÂ al.2017)for learning a strong fusion embedding of the input modalities. The input modalities are first converted into embeddings, which are then combined using the transformer, which performs several self-attentions across many layers. The first token embedding then constitutes the fusion embedding, which is subsequently processed by fully connected layers for classification.

We note that the traditional methods of adversarial attacks are not suitable for DMMs for two specific reasons. First, most key methods of crafting adversarial attacks use either thelâˆsubscriptğ‘™l_{\infty}orl2subscriptğ‘™2l_{2}norm. Optimization with respect to these kinds of manipulations induces a perturbation in all (or almost all) of the datapoints of an input modality by a small valueÂ±Ïµplus-or-minusitalic-Ïµ\pm\epsilon. This is not suitable in case of multimodal inputs because different modalities have different compositions, and not all modalities support this type of manipulation. For example, image-based inputs are continuous and thus suitable for such manipulations, but text-based inputs are discrete, thus not suitable for such manipulations. Furthermore, for DMMs, such adversarial manipulations are not suitable for robustness analysis processes since the core weaknesses of these models should be examined in the fusion mechanism of these models, which is not achieved by these manipulations. Since we are interested in studying the effect of decoupling fused modalities, we employl0subscriptğ‘™0l_{0}-norm optimization attack algorithm, wherein anl0subscriptğ‘™0l_{0}-norm attack optimizes for the number of changes made to the inputs for a successful decoupling attack.

Removal of salient datapoints from inputs has been shown to be an important factor for considering the robustness and safety of a decision model(Mathias etÂ al.2013; Wicker and Kwiatkowska2019; Noh etÂ al.2018). However, the key difference between the traditional unimdoal domains and the multimodal domain is that such datapoints are in fact parts of separate modalities that are coupled together by the multimodal fusion mechanism. Thus, it is imperative to study the cases, in which some parts of the input modalities are removed, so as to render this fusion as unsuccessful.

For a multimodal inputxğ‘¥x, we consider coupled datapoints as somexâ€²âŠ‚xsuperscriptğ‘¥â€²ğ‘¥x^{\prime}\subset x. Our objective is to find the minimum subset via the following optimization.

However, it is impractical to solve the optimization in Equation1, due to a large number of such datapoints in the multimodal input space. Thus, to solve this optimization, we use the notion of the fusion embedding to compute a salient points set first,Snsubscriptğ‘†ğ‘›S_{n}(Figure2, Step (c)). Such sets of critical or salient points have been previously utilized to inspect deep learning-based models(Qi etÂ al.2017). We use the salient datapoints set to study the weaknesses of DMMs, by defining it as follows.

In Equation2, the salient datapoints set contains those datapoints that affect the fusion embedding upon removal (wherex/xiğ‘¥subscriptğ‘¥ğ‘–x/x_{i}denotes removal of a datapoint). For example in the transformer-based DMMs, a datapointxiâˆˆSnxsubscriptğ‘¥ğ‘–superscriptsubscriptğ‘†ğ‘›ğ‘¥x_{i}\in S_{n}^{x}ifâˆ€iâ‰ j,ziâ‰¥zjformulae-sequencefor-allğ‘–ğ‘—subscriptğ‘§ğ‘–subscriptğ‘§ğ‘—\forall i\neq j,z_{i}\geq z_{j}due to the transformer pooling layer. Next, we find the set of coupled datapoints (Figure2, Step (d)) from the salient datapoints set, by computing permutations of all datapoints of a maximum size equal to the size of the salient datapoints set (denoted byâˆproduct\prodin Equation3).

Now that we have computed the coupled datapoints set, we propose the MUROAN Decoupling Attack Algorithm (Algorithm1) to iteratively refine the decoupling attack. In our algorithm, first the salient datapoints set is computed based on the process described in Equation2. Then, the GetCoupledSet procedure is called, which performs two functions. First, the coupled datapoints are computed as described in Equation3. Then, they are ordered based on the size of the datapoints, so as to satisfy Equation1. We encode the termination of our algorithm as a boolean functionfğ‘“f, to support multiple adversarial requirements. For example, adversarial requirements for crafting untargeted attacks (Dâ€‹(xâ€²)â‰ yğ·superscriptğ‘¥â€²ğ‘¦D(x^{\prime})\neq y) or targeted attacks (Dâ€‹(xâ€²)=yâ€²ğ·superscriptğ‘¥â€²superscriptğ‘¦â€²D(x^{\prime})=y^{\prime}) can be supported (Figure2, StepÂ (e)).
Lastly, we propose the following theorem to use our decoupling attack algorithm as a robustness verification technique to find adversarial examples in DMMs if one exists.

For a multimodal modelDğ·Dthat satisfies our unified view and a given multimodal inputxğ‘¥x, the MUROAN decoupling attack algorithm will find the optimum adversarial example that satisfies Equation1.

Proof.If an adversarial example exists for inputxğ‘¥x, it can be found by an exhaustive search of the input space. The GetCoupledSet function returns all possible permutations of the coupled datapoints and thefğ‘“ffunction andmaxitrcan be set such that the algorithm does not terminate until a satisfactory adversarial permutation is found. Furthermore, since the permutations in the coupled datapoints set are ordered, thus, a permutation that is found by our algorithm to be adversarial is minimal.

In our evaluation, we use MUROAN to analyze the robustness of state-of-the-art DMMs trained on popular multimodal datasets to show how decoupling attack can easily compromise these models, thereby enabling us to understand their robustness. We also consider some unimodal adversarial attack baselines in our evaluation only to show how easily decoupling attack can compromise DMMs. Our objective is not to make a direct comparison of our approach against these existing attacks, but to highlight how decoupling of input modalities can be easily used to attack the fusion mechanism of DMMs. Our findings highlight the need for rigorous safety analysis of DMMs against decoupling attacks, and lay down important groundwork for their deployment in real-world applications. We first summarize the DMMs, datasets, and unimodal adversarial baselines that are used in our experiments.

Pythia. The Pythia(Jiang etÂ al.2018)is a state-of-the-art model in the VQA challenge task. This models is composed of a convolutional network to compute an image embedding and a recurrent network to compute a sentence embedding, which are fused using element-wise multiplication.

Late Fusion. We consider the late-fusion architecture based DMM in(Antol etÂ al.2015)as a strong baseline model. In this model, image embeddings from a convolutional neural network and text embeddings from a recurrent network are fused using element-wise sum, and then the fusion embedding is processed through multiple classification layers to generate a probability score.

MMBT. The MMBT model(Kiela etÂ al.2019)is a state-of-the-art DMM that utilizes the BERT(Devlin etÂ al.2018)to learn multimodal embeddings by the implicit alignment of image and text features with the self-attention mechanism of transformers(Vaswani etÂ al.2017), for a wide range of visual-linguistic tasks. The query vector of this model, which is treated as the fusion embedding, is processed through a classifier head for downstream tasks.

Hateful Memes. The Hateful Memes(Kiela etÂ al.2020)dataset consists of image and text pairs pertaining to hateful memes, a recent phenomenon that poses a serious threat societal threat in todayâ€™s day and age. The objective is classification into two categories: â€œhatefulâ€ or â€œnon-hatefulâ€.

Visual Question Answering (VQA). The VQA dataset(Antol etÂ al.2015)consists of images with multiple associate natural language questions. Each image and question pair expects a list of answers. The objective is to predict the best answer from the list of answers for each image-question pair.

CW Attack. We use the Carlini and Wagner(Carlini and Wagner2017)attack algorithm as baseline for unimodal adversarial images for image-based modality.

Genetic Attack. We use the Genetic Attack(Alzantot etÂ al.2018)algorithm (referred to as â€œGeneticâ€ in this paper) as baseline for unimodal adversarial text for text-based modality.

We have implemented our attack using the PyTorch(Paszke etÂ al.2019)library. For the VQA dataset we used 1000 samples and for Hateful Memes dataset, we used 250 samples to conduct our experiments. We used pretrained models(pre2021)published by the original authors for all the DMMs that we have evaluated in our experiments. In the MUROAN decoupling attack algorithm, we used a maximum iteration limit of 500 epochs, post which we report the attack as unsuccessful. We ran our experiments on a single NVIDIA V100 GPU enabled eight core machine.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3:Three samples depict three types of minimum coupled datapoints in the VQA and Hateful Memes dataset. In sample (a), the minimum coupled datapoints are in the image only (indicated by red circles), and it is enough to only make changes to a those datapoints to decouple the sample. In sample (b), the minimum coupled datapoints are in the text only (indicated by red font), it is enough to make changes to the text only to decouple the sample. In sample (c), the coupled datapoints consist of both image and text, therefore both need to be changed to decouple this sample.

In this section, we used our framework to analyze the robustness of state-of-the-art DMMs under various attack conditions to show that the robustness of these DMMs are largely overestimated.

We studied the percentage of points changed by MUROAN decoupling attack algorithm in comparison with the CW attack for a successful misclassification. We used the same cutoff of 500 epochs for both the algorithms in all the tests, post which we reported a failure. We have depicted the results of this experiment in Figure4.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: (a)

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: (a)

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: (a)

Figure4depicts the CDF of the average percentage of datapoints changed in both the attacks under consideration. We found that the unimodal adversarial images (i.e., the CW attack) vastly overestimated the robustness of all the three DMMs. For the Pythia-VQA, it was observed that the CW attack changed 93.99% of the input datapoints, whereas MUROAN decoupling attack algorithm changed 1.16% of input datapoints. This difference of a large margin showed that the baseline unimodal adversarial images vastly overestimated the robustness of models for the VQA task. This finding may have important implications on using VQA in real-world applications, such as visual question answering for the blind(Gurari etÂ al.2018). Next, we discuss another important application domain, namely Hateful Memes. For the Late Fusion- Hateful Memes model, it was again observed that the CW attack changed 99.86% of datapoints, whereas MUROAN decoupling attack algorithm changed an average of 16.93% of input datapoints, a significant difference. For the MMBT-Hateful Memes model, it was observed that the CW attack changed 94.92% of datapoints, whereas MUROAN decoupling attack algorithm changed an average of 5.73% of input datapoints. In both cases, the unimodal adversarial images overestimated the robustness of the DMMs trained for the Hateful Memes task.

Next, we compared the Attack Success Rate (ASR) of MUROAN decoupling attack algorithm with respect to the unimodal adversarial images and text baselines, namely the CW(Carlini and Wagner2017)attack and the Genetic(Alzantot etÂ al.2018)attack respectively. The results of this experiment have been depicted in Table2. We first discuss the impact of the unimodal adversarial images on the DMMs. In all the three DMMs, we found that the unimodal adversarial images could affect these DMMs. However, they vastly overestimated their robustness in all three cases, when we compared the ASRs of MUROAN decoupling attack algorithm. For the Pythia-VQA model, the CW attack achieved an ASR of 79.77%, although the ASR achieved by MUROAN decoupling attack algorithm was 100%. For the two DMMs for hateful memes (i.e., Late Fusion-Hateful Memes and MMBT-Hateful Memes), a similar observation was made, although the CW attack achieved significantly lower ASR for both DMMs. Next, we took a closer look at the impact of the unimodal adversarial text (i.e., Genetic attack) on the DMMs, in comparison with MUROAN. For the Pythia-VQA, it was observed that the Genetic attack has little effect when compared to MUROAN decoupling attack algorithm, and even to the CW attack, wherein both these attacks outperformed the unimodal adversarial text baseline by a large margin. In case of the hateful memes DMMs (i.e., Late Fusion-Hateful Memes and MMBT-Hateful Memes) this margin was found to be even larger. It was observed that the unimodal adversarial text had no significant effect on the DMMs for hateful memes.

Thus, we observed that the safety and robustness of these DMMs need to be deeply examined, specifically from the perspective of decoupling attacks. In this regard, our experiments indicate that our attack exposes the vulnerabilities in the fusion mechanism of DMMs, and the robustness of this mechanism needs significant improvement, especially if DMMs are to be deployed in real-world systems.

In this section, we provide a qualitative analysis of the decoupled samples that our the MUROAN decoupling attack algorithm generated. Upon observation of such samples in the two baseline datasets (i.e., VQA and Hateful Memes), we discuss certain aspects of the nature of decoupling pertaining to our observations. In Figure3111Note: samples (b) and (c) are from the Hateful Memes dataset(Kiela etÂ al.2020), which some readers may find distressing., we depict three samples from our robustness analysis experiments. Figure3(a) is from the VQA dataset, and Figures3(b) and (c) are from the Hateful Memes dataset. These three samples represent the three levels of decoupling we observed in our experiments. In Figure3(a), the minimum coupled datapoints were found in the image only, therefore it is sufficient to decouple just the single image modality. In the VQA dataset, since questions are asked about certain parts of an image, this observation is intuitive since it should be sufficient to only affect the relevant parts of the image. In Figure3(b), the minimum coupled datapoints were only found in the text modality, since intuitively we cannot see why this sample could be a hateful meme from the image alone. In Figure3(c), the minimum coupled datapoints consist of both the image and the text modalities. In this case, both the input modalities need to be affected for decoupling this fusion. Therefore, we note that vulnerabilities in the DMMs are of a very different nature when compared to their unimodal counterparts.

Our experiments in SectionRobustness Analysisraise an important question: how can we defend against decoupling attacks? We performed a preliminary experiment to see if
adversarial training(Goodfellow, Shlens, and Szegedy2014), a popular technique to improve adversarial robustness, can be used to reduce the attack success rate. We performed adversarial training using the MMBT model for the hateful memes classification. We generated 247 adversarial examples via MUROAN framework and trained the model on these samples combined with the original dataset from scratch. We observed that the adversarial trained DMM was still vulnerable to newly crafted decoupled samples, despite the model achieving near 100% accuracy classifying adversarial examples included in the training set. These results demonstrate the difficulty in defending against decoupling attacks using traditional adversarial training. We hope these results inspire further work in increasing the robustness of DMMs.

In this section, we discuss some limitations, potential negative societal impacts, and some future directions of our work.

In this work, we have focused on DMMs that mainly operate on image and text modalities as inputs. We chose this type of DMMs since it could represent different compositions of inputs (i.e., a continuous input and a discrete input). Our approach however can be generalized to incorporate any other types of DMMs, considering compositions of other inputs including speech and video modalities.

Our major research objective is to improve the robustness of DMMs by showing their vulnerabilities to decoupling attacks, so that adversarial attacks on real-world multimodal systems can be mitigated. A potential negative societal impact of our work is that it might be used to craft adversarial attacks against DMMs.
As future work, we will investigate potential techniques to improve the robustness of DMMs. We hope our findings encourage more researchers to pursue improving the robustness of DMMs.

In conclusion, we have studied the robustness of DMMs against multimodal decoupling attacks that are aimed at compromising the fusion mechanism of DMMs. We have introduced a new framework called MUROAN for studying the robustness of DMMs, which consists of a unified view of the DMMs that exposes the fusion embedding, and an algorithm for decoupling the input modalities. Our experiment regarding adversarial training shows that it does not improve the robustness against our decoupling attacks. MUROAN paves the way for studying the robustness of DMMs via decoupling input modalities in the future.

In this section, we provide additional qualitative examples of our attack against the MMBT-Hateful Memes model and the Pythia-VQA model in Figure5and Figure6, respectively. In SectionQualitative Analysis of MUROAN, we discussed a few samples from MUROAN from the Hateful Memes dataset. We further discuss more samples from the VQA dataset in addition to some samples from the Hateful Memes dataset in this section.

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: Figure 5:Additional Samples from the Hateful Memes dataset.

[å›¾ç‰‡: images\image_8.png]
å›¾ç‰‡è¯´æ˜: Figure 6:Additional Samples from the VQA dataset.

Figure5depicts three samples from the MMBT-Hateful Memes baseline. The first sample depicts the case where only the text is needed to be manipulated to decouple the input modalities in a sample. The second example depicts the case where only a part of the image needs to be manipulated to decouple the modalities in a sample. The third example depicts the case where both image and the text need to be manipulated to decouple the modalities in a sample.

Figure6depicts three samples from the Pythia-VQA baseline. In this case, the objective is to fool the DMM so as to output a wrong answer (as opposed to a wrong label in the Hateful Memes case). We observed a similar trend in case of VQA as well, as noted in SectionQualitative Analysis of MUROAN. In some cases (such as the first sample and the second sample in Figure6), it was sufficient to only manipulate one of the input modalities to decouple the input modalities in a sample. In some cases though, both modalities had to be manipulated for decoupling them (such as the third sample in Figure6).

We have discussed in SectionRobustness Analysisabout how our attack can be used to study the robustness of several DMMs. In this section, we use our attack to study and compare the robustness of two baseline DMMs, Late Fusion and MMBT, discussed in our paper.

[å›¾ç‰‡: images\image_9.png]
å›¾ç‰‡è¯´æ˜: (a)

[å›¾ç‰‡: images\image_10.png]
å›¾ç‰‡è¯´æ˜: (a)

In this experiment, our objective is to compare the two DMMs that are trained for the same task to determine which DMM is more robust against our attack. In this way, we can use MUROAN to additionally compare DMMs in terms of their robustness. We study and compare the robustness of the two DMMs both trained on the Hateful Memes dataset based on therobustness metricÏˆğœ“\psi(Yu etÂ al.2019). Model robustness is defined as follows.

Equation4uses the Kullbackâ€“Leibler divergence loss (DKâ€‹Lsubscriptğ·ğ¾ğ¿D_{KL})(Kullback1997)to depict the divergence between the probability distributions of the original samples and the adversarial samples generated by MUROAN decoupling attack algorithm. In other words, theDKâ€‹Lsubscriptğ·ğ¾ğ¿D_{KL}is higher for a model, for which the adversarial samples are further from the original distribution, indicating stronger robustness. In this experiment, we compared the robustness of the MMBT model to the Late Fusion model, where both DMMs were trained on the same Hateful Memes dataset. The distribution of the robustness the two DMMs as calculated by Equation4based on our attack is depicted in Figures7aand7b, respectively. We found that the MMBT model is significantly more robust than the Late Fusion model, as can be observed from the Figure7. The mean robustness of the MMBT model was found to beÏˆ=0.65ğœ“0.65\psi=0.65and the mean robustness of the Late Fusion model was found to beÏˆ=0.003ğœ“0.003\psi=0.003. The higher robustness of the MMBT model could be attributed to the way the fusion is achieved in this DMM, using the more sophisticated self-attention mechanism of the transformer(Vaswani etÂ al.2017), while the Late Fusion model uses the element-wise addition. Thus, the robustness metric in this experiment could also indicate the strength of the fusion mechanism. In this way, the robustness of the state-of-the-art DMMs can be quantitatively measured using MUROAN.

[å›¾ç‰‡: images\image_11.png]

[å›¾ç‰‡: images\image_12.png]

