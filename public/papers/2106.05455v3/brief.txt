###你删除的冗余通道可能是提升GNN表现的关键


AKE-GNN 是一种用于图学习的新型图神经网络（GNN）框架，旨在通过多视图间的自适应知识交换来提升模型的表现力。论文指出，在训练良好的 GNN 模型中存在大量冗余的通道（weight channels），传统方法通常选择移除这些冗余通道以提高效率，但本文提出了一种新的思路：将这些冗余通道替换为更具信息量的通道，从而增强模型的表示能力。为此，AKE-GNN 利用图增强技术生成多个图视图，并在这些视图对应的多个 GNN 之间进行逐层的通道级参数交换。具体来说，AKE-GNN 包含两个阶段：个体学习阶段和知识交换阶段。在个体学习阶段，每个 GNN 独立地从一个图视图中学习信息；在知识交换阶段，系统以逐层的方式将一个 GNN 中的冗余通道替换为另一个 GNN 中的信息通道。该方法无需修改现有 GNN 的结构或损失函数，因此可以无缝集成多种主流 GNN 模型。实验结果显示，AKE-GNN 在包括节点分类、链接预测和图分类在内的多项任务中均优于现有的 GNN 模型及其集成方法，在15个公共数据集上的平均绝对准确率提升了1.9%∼3.9%。此外，论文还进行了广泛的消融实验和分析，验证了所提知识交换策略的有效性。AKE-GNN 的优势还包括不引入额外推理开销以及对多种图任务和领域的广泛适用性。