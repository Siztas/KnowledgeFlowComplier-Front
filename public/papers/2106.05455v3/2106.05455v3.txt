标题：AKE-GNN: Effective Graph Learning withAdaptive Knowledge Exchange

Graph Neural Networks (GNNs) have already been widely used in various graph mining tasks. However, recent works reveal that the learned weights (channels) in well-trained GNNs are highly redundant, which inevitably limits the performance of GNNs. Instead of removing these redundant channels for efficiency consideration, we aim to reactivate them to enlarge the representation capacity of GNNs for effective graph learning.
In this paper, we propose to substitute these redundant channels with other informative channels to achieve this goal.
We introduce a novel GNN learning framework named AKE-GNN, which performs theAdaptiveKnowledgeExchange strategy among multiple graph views generated by graph augmentations.
AKE-GNN first trains multiple GNNs each corresponding to one graph view to obtain informative channels. Then, AKE-GNN iteratively exchanges redundant channels in the weight parameter matrix of one GNN with informative channels of another GNN in a layer-wise manner.
Additionally, existing GNNs can be seamlessly incorporated into our framework. AKE-GNN achieves superior performance compared with various baselines across a suite of experiments on node classification, link prediction, and graph classification. In particular, we conduct a series of experiments on 15 public benchmark datasets, 8 popular GNN models, and 3 graph tasks and show that AKE-GNN consistently outperforms existing popular GNN models and even their ensembles. Extensive ablation studies and analyses on knowledge exchange methods validate the effectiveness of AKE-GNN.

[图片: images\image_1.png]
图片说明: Figure 1.The illustrative schematic diagram of the proposed AKE-GNN framework. (a) Two generated graph views (i.e., masking node features and dropping edges). (b) Adaptive knowledge exchange by exchanging channel-wise parameters among two graph views (in one layer for illustration). (c) AKE-GNN in the multiple GNN case (best viewed in color).

Graph Neural Networks (GNNs), as the powerful tool for modeling relational inductive bias(Battaglia et al.,2018; Barabási,2013)to jointly encode graph structure and node features of the input graph(Hamilton,2020), have been widely employed for analyzing graph-mining tasks, including node classification(Kipf and Welling,2016; Veličković et al.,2017; Hamilton et al.,2017; Chen et al.,2020; Wu et al.,2019), link prediction(Zhang and Chen,2018; Ying et al.,2018), and graph classification(Xu et al.,2019a; Errica et al.,2020).
Despite the prevalence and effectiveness of GNN models, as discussed in recent works(Chen et al.,2021; Jin et al.,2022), there exist redundant channels of the weight parameter matrix in a well-trained GNN model. These redundant channels can be removed without performance degradation.
Existing works mainly remove these redundant channels from the perspective of efficiency.
However, non-structured channel pruning methods are not hardware-friendly(Han et al.,2015)and thus suffer from limited efficiency improvement(Li et al.,2017).
Moreover, these methods often improve efficiency of the model with a slight sacrifice of effectiveness(Chen et al.,2021; Frankle and Carbin,2018).
Therefore, from a novel and practical perspective of effectiveness, we propose to substitute these redundant channels with informative channels to enrich knowledge of GNN models for effective graph learning.
To achieve this goal, we need to tackle two unique technical challenges: 1) How to obtain informative channels? 2) How to exchange redundant channels with informative channels effectively?

For obtaining informative channels, we are inspired by recent advances in multi-view GNNs, whose multiple graph views generated by graph augmentations can provide complementary information of a graph from different aspects. Most GNN models are trained in an end-to-end supervised manner to learn effective node/graph representations in a single-view graph.
As argued in some recent works(Wang et al.,2020; Xie et al.,2020; Xu et al.,2019b), such training methods can only capture partial information from the complex input, hence may not generalize well on unseen nodes/graphs.
As a result, researchers propose new training algorithms that generate multiple views from the input graph and then build multi-view GNNs(Wang et al.,2020; Cheng et al.,2020). The idea is that each view captures knowledge from one certain aspect, and knowledge learned from different views is fused to enhance node/graph representation. Representative models include AM-GCN(Wang et al.,2020)and MGAT(Xie et al.,2020)which utilize multi-head attention modules to fuse feature and topology knowledge, and MAGCN(Cheng et al.,2020)which develops multi-view attribute graph convolution encoders.

For exchanging channels effectively, we propose a novel GNN learning framework, calledAdaptiveKnowledgeExchange GNNs (AKE-GNN), which fuses diverse knowledge learned from multiple graph views generated by graph augmentations. AKE-GNN adaptively exchanges parameters among those graph views.
The advantage of AKE-GNN is that we do not need to modify the model architecture or training loss functions(Jin et al.,2022), and thus existing GNN models can be seamlessly incorporated into our framework.

AKE-GNN contains two training phases: an individual learning phase and a knowledge exchange phase. In the individual learning phase, we construct multiple views by stochastic graph augmentation functions(Zhu et al.,2021), and GNNs sharing the same backbone model learn those graph views independently to obtain informative channels. In the knowledge exchange phase, we design a channel-wise adaptive exchange method that repeatedly replaces redundant channels in one GNN with the informative channels from another GNN in a layer-wise manner.
Furthermore, we show the extension of AKE-GNN to more than two graph views.
Comprehensive experiments show that AKE-GNN consistently achieves superior performance over existing popular GNN models and their ensembles on representative graph tasks including node classification, link predictions, and graph classification, and across various domains including bioinformatics (e.g., to predict the property of the protein) and social networks (e.g., to predict the co-authorship).
In shot, our maincontributionsare:

We present a novel GNN learning framework, namely AKE-GNN, which adaptively exchanges knowledge from multiple GNNs learned on diverse graph views for effective graph learning.

Existing backbone GNN models can be seamlessly incorporated into AKE-GNN without modifying the original configurations such as the learning rate and the number of layers. Moreover, AKE-GNN introduces no extra computational overheads to the inference stage.

We extensively evaluate the effectiveness of AKE-GNN on 15 public datasets, 8 popular GNN models, and 3 graph tasks. AKE-GNN consistently outperforms corresponding GNN backbone models by an average of 1.9%∼similar-to\sim3.9% in terms of absolute accuracy improvements and even their ensembles.
In addition, extensive ablation studies and analyses on the proposed knowledge exchange method also validate the effectiveness of AKE-GNN.

Multi-view joint learning aims to jointly model (generated) multiple graph views to improve the generalization performance(Wang et al.,2020; Xie et al.,2020; Ma et al.,2020; Xu et al.,2019b; Cheng et al.,2020). Most existing works leverage graph augmentations to generate multiple views from the original graph, and then design specific architectures to collaboratively fuse knowledge learned from different graph views to enhance their ability of graph representation learning.
AM-GCN(Wang et al.,2020)explicitly constructs the node feature graph view and the topology graph view, and then employs two GNNs with attention mechanisms to extract knowledge from these two aspects.
MGAT(Xie et al.,2020)automatically generates multiple views via graph augmentations and then designs an attention-based architecture to collaboratively integrate multiple types of knowledge in different views.
MAGCN(Cheng et al.,2020)develops multi-view attribute graph convolution encoders with attention mechanisms for learning graph embeddings from multi-view graph data.
Different from these methods, our method retains the benefits of joint modeling multiple views via an adaptive knowledge exchange framework while not requiring dedicated architecture designs.
Additionally, graph contrastive learning (GCL) methods(Hassani and Khasahmadi,2020; You et al.,2020; Qiu et al.,2020; Zhu et al.,2021)leverage generated multiple graph views to maximize the feature consistency among these views. However, GCL methods operate within a self-supervised learning regime, where label information is not available during the training phase. In contract, AKE-GNN is grounded in a supervised learning setting to facilitate knowledge exchange of parameters from informative to redundant channels.

Our adaptive knowledge exchange framework is conceptually connected to weight re-activating methods.
Grafting(Meng et al.,2020)improves the network performance by grafting external information (weights) on the same data source to re-activate invalid filters in computer vision tasks. DeCorr(Jin et al.,2022)introduces the explicit feature dimension decorrelation term into the loss objective to tackle the feature overcorrelation issue in GNNs.
In contrast, our work aims at fusing different knowledge from GNNs trained on multiple (generated) graph views.
Since different graph views share different parts of knowledge that should not be repeated in just one GNN, we propose an adaptive approach to iteratively exchange complementary knowledge from different graph views for more effective graph learning.

In this section, we first present a preliminary study to investigate the redundancy issue of the weight parameter matrix in GNNs (Sec.3.1). Then we introduce the AKE-GNN framework on two graph views with its two training phases (Sec.3.2&3.3).
We finally extend AKE-GNN to the multiple GNN case (Sec.3.4).
The overall framework of AKE-GNN is shown in Fig.1.

Let𝒢=(𝒱,ℰ,𝑿)𝒢𝒱ℰ𝑿{\mathcal{G}}=\left({\mathcal{V}},{\mathcal{E}},{\bm{X}}\right)denote a graph, where𝒱𝒱{\mathcal{V}}is a set ofN𝑁Nnodes, andℰℰ{\mathcal{E}}is a set of edges between nodes.𝑿=[𝒙1,𝒙2,⋯,𝒙N]T𝑿superscriptsubscript𝒙1subscript𝒙2⋯subscript𝒙𝑁𝑇{\bm{X}}=\left[{\bm{x}}_{1},{\bm{x}}_{2},\cdots,{\bm{x}}_{N}\right]^{T}∈ℝn×dabsentsuperscriptℝ𝑛𝑑\in{\mathbb{R}}^{n\times d}represents the node feature matrix and𝒙i∈ℝdsubscript𝒙𝑖superscriptℝ𝑑{\bm{x}}_{i}\in{\mathbb{R}}^{d}is the feature vector of nodevisubscript𝑣𝑖v_{i}, whered𝑑dis the number of channels in the feature matrix𝑿𝑿{\bm{X}}. The adjacency matrix𝑨∈{0,1}N×N𝑨superscript01𝑁𝑁{\bm{A}}\in\{0,1\}^{N\times N}is defined by𝑨i,j=1subscript𝑨𝑖𝑗1{\bm{A}}_{i,j}=1if(vi,vj)∈ℰsubscript𝑣𝑖subscript𝑣𝑗ℰ\left(v_{i},v_{j}\right)\in{\mathcal{E}}and00otherwise.
We denote the (generated) multiple views as𝒢ℳ={𝒢1,⋯,𝒢V}subscript𝒢ℳsubscript𝒢1⋯subscript𝒢𝑉{\mathcal{G}}_{{\mathcal{M}}}=\{{\mathcal{G}}_{1},\cdots,{\mathcal{G}}_{V}\}, where𝒢vsubscript𝒢𝑣{\mathcal{G}}_{v}is thev𝑣v-th view of the original input graph𝒢𝒢{\mathcal{G}}. Note that all of the graph views share the same node set.

Jin et al.(2022)have empirically found that the weight parameter matrix of GNNs has a high tendency to contain redundant channels resulting from standard GNN training, i.e., high Pearson correlation among channels in the weight matrix.
We verified this phenomenon by conducting experiments on Cora with GCN. We successively find a pair of output channels with the highest Pearson correlation in the weight matrix and then prune these two channels and re-train the resultant GCN model, starting from hidden size 16.
In Table1, we find that several channels have minor impacts on the output, and pruning these redundant channels does not degrade the performance.
This preliminary study inspires us that GNN models indeed contain highly correlated channels in the weight matrix, which cannot introduce extra useful information.
It naturally spurs a question: can we further improve the performance of GNN models by adaptively exchanging knowledge contained in their learned weights (channels)?
Herein, we need to tackle two unique technical challenges: 1) How to obtain informative channels (Sec.3.2)?
2) How to exchange channels effectively (Sec.3.3)?

[图片: images\image_2.png]
图片说明: Figure 2.The illustration of the pipeline of parameter updating in AKE-GNN with two GNNs.

In this phase, we first generate the multiple graph views by graph augmentations. Then, we train multiple GNNs each corresponding to a generated graph view to obtain informative channels of the weight parameter matrix in GNNs.

To capture different views of the original graph, following previous work(Veličković et al.,2018; Zhu et al.,2021), we apply stochasticaugmentation functionsto generate multiple views of the original graph and then feed them into GNNs.
Formally, a different view of the original graph(𝑿,𝑨)𝑿𝑨\left({\bm{X}},{\bm{A}}\right)is obtained by(𝑿~,𝑨~)=𝒞​(𝑿,𝑨)~𝑿~𝑨𝒞𝑿𝑨\left(\widetilde{{\bm{X}}},\widetilde{{\bm{A}}}\right)=\mathcal{C}\left({\bm{X}},{\bm{A}}\right), where𝒞​(⋅)𝒞⋅\mathcal{C}(\cdot)is anaugmentation function.
We leverage four commonly-used augmentation functions to generate multiple graph views in AKE-GNN(Zhu et al.,2021; Veličković et al.,2018; Feng et al.,2020), which aremasking node features,corrupting node features,dropping edges, andextracting subgraphs.

Masking node features.Randomly mask a fraction of node attributes with zeros. Formally, the generated matrix of node features𝑿~~𝑿\widetilde{{\bm{X}}}is computed by

where𝒎∈{0,1}d𝒎superscript01𝑑{\bm{m}}\in\{0,1\}^{d}is a random vector, which is drawn from a Bernoulli distribution,[⋅,⋅]⋅⋅[\cdot\;,\cdot]is the concatenation operator, and⊙direct-product\odotis the element-wise multiplication.

Corrupting node features.Randomly replace a fraction of node attributes with Gaussian noise. Formally, it can be calculated by

where𝒎i∈ℝdsubscript𝒎𝑖superscriptℝ𝑑{\bm{m}}_{i}\in{\mathbb{R}}^{d}is a random vector drawn from a Gaussian distribution𝒩​(μ​(𝒙i),1)𝒩𝜇subscript𝒙𝑖1\mathcal{N}\left(\mu({\bm{x}}_{i}),1\right)independently andμ​(⋅)𝜇⋅\mu(\cdot)denotes the mean value of a vector.

Dropping edges.Randomly remove edges in the graph. Formally, we sample a modified subsetℰ~~ℰ\widetilde{{\mathcal{E}}}from the original edge setℰℰ{\mathcal{E}}with the probability defined as follows:

where(u,v)∈ℰ𝑢𝑣ℰ\left(u,v\right)\in{\mathcal{E}}andpu​vesuperscriptsubscript𝑝𝑢𝑣𝑒p_{uv}^{e}is the probability of removing(u,v)𝑢𝑣\left(u,v\right).

Extracting subgraphs.Extract the induced subgraphs𝒢′=(𝒱′,ℰ′)superscript𝒢′superscript𝒱′superscriptℰ′{\mathcal{G}}^{\prime}=\left({\mathcal{V}}^{\prime},{\mathcal{E}}^{\prime}\right)containing the nodes ang the corresponding edges in a given subset(Veličković et al.,2018), i.e.,𝒱′⊆𝒱superscript𝒱′𝒱{\mathcal{V}}^{\prime}\subseteq{\mathcal{V}}andℰ′⊆ℰsuperscriptℰ′ℰ{\mathcal{E}}^{\prime}\subseteq{\mathcal{E}}.

Note that AKE-GNN does not require specific graph augmentation functions, and thus other graph augmentation methods can be seamlessly incorporated into our framework.

For any existing GNN model, it can be directly applied in the AKE-GNN framework without modifying its original implementations such as the learning rate and the number of layers.
We denote a parametrized GNN asℱ𝜽:𝕏→𝕐:subscriptℱ𝜽→𝕏𝕐\mathcal{F}_{\bm{\theta}}:{\mathbb{X}}\rightarrow{\mathbb{Y}}with the initial parameter𝜽(0)superscript𝜽0\bm{\theta}^{\left(0\right)}, where𝕏𝕏{\mathbb{X}}and𝕐𝕐{\mathbb{Y}}are the input space and output space.
GivenNtrainsubscript𝑁trainN_{\text{train}}paired training data{(𝒙t,yt)}t=1Ntrain⊂𝕏×𝕐superscriptsubscriptsubscript𝒙𝑡subscript𝑦𝑡𝑡1subscript𝑁train𝕏𝕐\{\left({\bm{x}}_{t},y_{t}\right)\}_{t=1}^{N_{\text{train}}}\subset{\mathbb{X}}\times{\mathbb{Y}}, the networkℱ𝜽subscriptℱ𝜽\mathcal{F}_{\bm{\theta}}is optimized with a supervised lossℒℒ\mathcal{L}as follows:

where𝜽(1)superscript𝜽1\bm{\theta}^{\left(1\right)}is the parameters of a GNN after optimization.

Earlier works identify that knowledge is contained in the updated parameter values of a neural network(Hinton et al.,2015).
After the individual learning phase, GNNs trained with multiple views have learned knowledge stored in their updated parameters and can take a further step to interact with each other for knowledge exchange.
In the knowledge exchange phase, we take𝜽𝒢1(1)superscriptsubscript𝜽subscript𝒢11\bm{\theta}_{{\mathcal{G}}_{1}}^{\left(1\right)}and𝜽𝒢2(1)superscriptsubscript𝜽subscript𝒢21\bm{\theta}_{{\mathcal{G}}_{2}}^{\left(1\right)}of the two GNNs as input, exchange knowledge, and produce𝜽𝒢1(1)′\bm{\theta}_{{\mathcal{G}}_{1}}^{{}^{\prime}\left(1\right)}and𝜽𝒢2(1)′\bm{\theta}_{{\mathcal{G}}_{2}}^{{}^{\prime}\left(1\right)}as output, where𝒢1subscript𝒢1{\mathcal{G}}_{1}and𝒢2subscript𝒢2{\mathcal{G}}_{2}are two corresponding graph views.
Then, we re-train the output parameters𝜽𝒢1(1)′\bm{\theta}_{{\mathcal{G}}_{1}}^{{}^{\prime}\left(1\right)}and𝜽𝒢2(1)′\bm{\theta}_{{\mathcal{G}}_{2}}^{{}^{\prime}\left(1\right)}and obtain the final parameters𝜽𝒢1(2)superscriptsubscript𝜽subscript𝒢12\bm{\theta}_{{\mathcal{G}}_{1}}^{\left(2\right)}and𝜽𝒢2(2)superscriptsubscript𝜽subscript𝒢22\bm{\theta}_{{\mathcal{G}}_{2}}^{\left(2\right)}.
The illustration of the pipeline of parameter updating in AKE-GNN is shown in Fig.2.
To exchange knowledge among multiple GNNs, we need to answer the following two questions: 1) How to measure information (knowledge) inside the parameters (connection weights)? 2) How to adaptively perform knowledge exchange among multiple GNNs?

We leverage entropy to measure information in one layer of a well-trained GNN. As suggested in(Meng et al.,2020), the higher entropy the weight matrix has, the more variation (the less redundant information) the model contains, and then the potentially better performance of the final prediction.
Let𝜽𝒢kl∈ℝdi×di+1superscriptsubscript𝜽subscript𝒢𝑘𝑙superscriptℝsubscript𝑑𝑖subscript𝑑𝑖1\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}\in{\mathbb{R}}^{d_{i}\times d_{i+1}}denote the parameters of thel𝑙l-th layer in the corresponding GNN whose input is𝒢ksubscript𝒢𝑘{\mathcal{G}}_{k}, wheredisubscript𝑑𝑖d_{i}is the number of channels in thel𝑙l-th layer.
Following(Meng et al.,2020; Cheng et al.,2019),
we calculate entropy by dividing the range of values in𝜽𝒢klsuperscriptsubscript𝜽subscript𝒢𝑘𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}intoB𝐵Bdifferent bins. Denote the number of values that fall into theb𝑏b-th bin asnbsubscript𝑛𝑏n_{b}. We usepb≜nb(n1+⋯+nm)≜subscript𝑝𝑏subscript𝑛𝑏subscript𝑛1⋯subscript𝑛𝑚p_{b}\triangleq\frac{n_{b}}{\left(n_{1}+\cdots+n_{m}\right)}to approximate the probability of theb𝑏b-th bin, wheren1+⋯+nB=di×di+1subscript𝑛1⋯subscript𝑛𝐵subscript𝑑𝑖subscript𝑑𝑖1n_{1}+\cdots+n_{B}=d_{i}\times d_{i+1}. Then, the entropy of𝜽𝒢klsuperscriptsubscript𝜽subscript𝒢𝑘𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}can be calculated as follows:

A larger value ofH​(𝜽𝒢kl)𝐻superscriptsubscript𝜽subscript𝒢𝑘𝑙H\left(\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}\right)usually indicates richer information in the parameters of thel𝑙l-th layer in the corresponding GNN whose input is𝒢ksubscript𝒢𝑘{\mathcal{G}}_{k}, and vice versa. For example, if each element of𝜽𝒢klsuperscriptsubscript𝜽subscript𝒢𝑘𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}takes the same value (entropy is 0),𝜽𝒢klsuperscriptsubscript𝜽subscript𝒢𝑘𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}cannot discriminate which part of the input is more important.

Given quantitative measurements of information in each layer of a GNN, we then consider how to adaptively exchange information among multiple GNNs.
Since GNNs follow the message passing scheme to iteratively aggregate information from neighbor nodes, thek𝑘k-th layer makes use of the subtree structures of heightk𝑘krooted at every node. Thus, we only exchange information of the same layer to preserve the consistency of information between two GNNs.
Let parameters of the source and the target GNN be𝜽𝒢ssubscript𝜽subscript𝒢𝑠\bm{\bm{\theta}}_{{\mathcal{G}}_{s}}and𝜽𝒢tsubscript𝜽subscript𝒢𝑡\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}, respectively.
We denote parameters in thel𝑙l-th GNN layer trained on thek𝑘k-th graph view as𝜽𝒢kl=[𝜽𝒢kl,1,⋯,𝜽𝒢kl,Cl+1]∈ℝCl×Cl+1superscriptsubscript𝜽subscript𝒢𝑘𝑙superscriptsubscript𝜽subscript𝒢𝑘𝑙1⋯superscriptsubscript𝜽subscript𝒢𝑘𝑙subscript𝐶𝑙1superscriptℝsubscript𝐶𝑙subscript𝐶𝑙1\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}=\small[\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l,1},\cdots,\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l,C_{l+1}}\small]\in{\mathbb{R}}^{C_{l}\times C_{l+1}}, where the input channel isClsubscript𝐶𝑙C_{l}, the output channel isCl+1subscript𝐶𝑙1C_{l+1}, and𝜽𝒢kl,j∈ℝClsuperscriptsubscript𝜽subscript𝒢𝑘𝑙𝑗superscriptℝsubscript𝐶𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l,j}\in{\mathbb{R}}^{C_{l}}is thej𝑗j-th output channel vector.
In each exchange step, our target is to adaptively exchange a redundant output channel in𝜽𝒢tsubscript𝜽subscript𝒢𝑡\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}with another informative output channel in𝜽𝒢ssubscript𝜽subscript𝒢𝑠\bm{\bm{\theta}}_{{\mathcal{G}}_{s}}.
To exchange the redundant channel, we first calculate the values ofPearson correlationamong all possible output channel pairs in𝜽𝒢tlsuperscriptsubscript𝜽subscript𝒢𝑡𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l}, and then obtain a pair of redundant channels with the highest correlation, i.e.,idx1subscriptidx1\text{idx}_{1}andidx2subscriptidx2\text{idx}_{2}.
We select an output channel from𝜽𝒢slsuperscriptsubscript𝜽subscript𝒢𝑠𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{s}}^{l}to substitute𝜽𝒢tl,idx1superscriptsubscript𝜽subscript𝒢𝑡𝑙subscriptidx1\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l,\text{idx}_{1}}or𝜽𝒢tl,idx2superscriptsubscript𝜽subscript𝒢𝑡𝑙subscriptidx2\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l,\text{idx}_{2}}with the purpose to maximize entropy of the new weight parameter matrix𝜽𝒢tlsuperscriptsubscript𝜽subscript𝒢𝑡𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l}. Formally, letf​(𝑨j,𝒙)𝑓superscript𝑨𝑗𝒙f\left({\bm{A}}^{j},{\bm{x}}\right)be the operator to substitute thej𝑗j-th output channel of the matrix𝑨𝑨{\bm{A}}with a vector𝒙𝒙{\bm{x}}. We find the informative output channeli∈[Cl+1]𝑖delimited-[]subscript𝐶𝑙1i\in\left[C_{l+1}\right]in the source network𝒢ssubscript𝒢𝑠{\mathcal{G}}_{s}and the redundant output channelr∈{idx1,idx2}𝑟subscriptidx1subscriptidx2r\in\{\text{idx}_{1},\text{idx}_{2}\}in the target network𝒢tsubscript𝒢𝑡{\mathcal{G}}_{t}at thel𝑙l-th layer as follows:

Finally, we exchange parameters between𝜽𝒢sl,isuperscriptsubscript𝜽subscript𝒢𝑠𝑙𝑖\bm{\bm{\theta}}_{{\mathcal{G}}_{s}}^{l,i}and𝜽𝒢tl,rsuperscriptsubscript𝜽subscript𝒢𝑡𝑙𝑟\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l,r}. By repeating the above exchange step forM𝑀Mtimes,𝜽𝒢tlsuperscriptsubscript𝜽subscript𝒢𝑡𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l}can accept the part of information (M𝑀Mchannels) from𝜽𝒢slsuperscriptsubscript𝜽subscript𝒢𝑠𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{s}}^{l}while retaining the useful information in the original network𝜽𝒢tlsuperscriptsubscript𝜽subscript𝒢𝑡𝑙\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l}. We illustrate the procedure in Fig.1(b), where𝜽𝒢1subscript𝜽subscript𝒢1\bm{\bm{\theta}}_{{\mathcal{G}}_{1}}and𝜽𝒢2subscript𝜽subscript𝒢2\bm{\bm{\theta}}_{{\mathcal{G}}_{2}}perform adaptive channel-wise parameter exchange in one layer as aforementioned. As a result,𝜽𝒢1subscript𝜽subscript𝒢1\bm{\bm{\theta}}_{{\mathcal{G}}_{1}}exchanges the second channel in the weight matrix with the first channel in𝜽𝒢2subscript𝜽subscript𝒢2\bm{\bm{\theta}}_{{\mathcal{G}}_{2}}. Through this procedure, both networks contain information from two graph views. Finally, we re-train two GNNs with the same number of epochs as introduced in Sec.3.2to obtain the output predictions.
The complete algorithm of AKE-GNN with two GNNs is summarized in Algorithm1.

[图片: images\image_3.png]
图片说明: Figure 3.The illustration of exchanging output channels (a) compared with point-wise exchange (b), exchanging input channels (c), and self-exchange output channels (d). The newly replaced value in the weight matrix is depicted in purple.
We only consider features of one node (one row in𝒛lsuperscript𝒛𝑙{\bm{z}}^{l}) and omit the input feature vector𝒛lsuperscript𝒛𝑙{\bm{z}}^{l}in the bottom half of the figure for brevity.

To further explain the rationale of our proposed adaptive knowledge exchange method, we present an illustrated example in Fig.3.
We denote the input and the output feature in thel𝑙l-th GNN layer as𝒛lsuperscript𝒛𝑙{\bm{z}}^{l}and𝒛l+1superscript𝒛𝑙1{\bm{z}}^{l+1}, respectively. Letθlsuperscript𝜃𝑙\theta^{l}be the weight matrix of thel𝑙l-th GNN layer. Each value in𝒛lsuperscript𝒛𝑙{\bm{z}}^{l}/𝒛l+1superscript𝒛𝑙1{\bm{z}}^{l+1}represents the certain feature dimension.
Each feature dimension in𝒛l+1superscript𝒛𝑙1{\bm{z}}^{l+1}is a function of𝒛lsuperscript𝒛𝑙{\bm{z}}^{l}, which is parameterized by output channels inθlsuperscript𝜃𝑙\theta^{l}.
Thus, exchanging output channels can produce partially modified features in𝒛l+1superscript𝒛𝑙1{\bm{z}}^{l+1}.
Comparing Fig.3(a) with (b) and (c), exchanging certain output channels can alter the corresponding features in𝒛l+1superscript𝒛𝑙1{\bm{z}}^{l+1}(e.g., “40”→→\rightarrow“68”) while keeping other features unchanged (e.g., “46”), which explicitly contain information of both the original and the other new network.
Comparing (a) with (d), self-exchange among output channels in the network itself cannot bring new information and can only obtain repeated features (e.g., repeated “46” in Fig.4(d)). However, exchanging output channels among multiple GNNs can introduce extra information from the other weight matrix and obtain new features (e.g., “68” in Fig.4(a)).
We also present a detailed ablation study on AKE-GNN to compare adaptive channel exchange with other parameter exchange methods such as randomly exchanging parameters (without the adaptive exchange strategy), exchanging parameters with a randomly initialized model (without a well-trained GNN model), and exchanging output channels in the network itself (only disturbing channels) in Sec.4.2.
We empirically find that adaptive knowledge exchange learns more effective graph representations and achieves the highest accuracy 82.8% on the Cora dataset.

AKE-GNN can be easily extended to the multiple GNN case, as illustrated in Fig.1(c).
In each iteration of the knowledge exchange phase, each GNN model𝒢ksubscript𝒢𝑘{\mathcal{G}}_{k}accepts the knowledge from𝒢k−1subscript𝒢𝑘1{\mathcal{G}}_{k-1}. After certain iterations of knowledge exchange, each GNN model contains the knowledge from all the other GNN models trained on the multiple graph views.
We list the complete algorithm of AKE-GNN for multiple GNNs in Appendix.

(a) Results of accuracy (%) on graph classification tasks.

(b) Results of accuracy (%) on link prediction tasks.

(c) Results of accuracy (%) on OGBn-Arxiv.

To show the generalization ability of the proposed AKE-GNN framework, we conduct experiments on 15 public benchmark datasets and 4 learning tasks.
(1)Node classification:Citation network(Yang et al.,2016): Cora, CiteSeer, and PubMed;Wikipedia network(Rozemberczki et al.,2021): Chameleon, and Squirrel;Actor co-occurrence network(Tang et al.,2009): Actor;WebKB(Pei et al.,2020): Cornell, Texas, and Wisconsin.
(2)Link prediction:Citation network(Yang et al.,2016): Cora, CiteSeer, and PubMed.
(3)Graph classification:Chemical compounds(Morris et al.,2020): DD, NCI1, and PROTEIN;Social network(Morris et al.,2020): IMDB and REDDIT.
(4)Large scale dataset: Academic citation network in the open graph benchmark (OGB)(Hu et al.,2020): OGBn-Arxiv.

[图片: images\image_4.png]
图片说明: Figure 4.Comparison of different knowledge (weight matrix) exchange methods. We exchange the same number of parameters in the following methods for fair comparison and present the exchange scheme with one output/input channel for ease of understanding. (a) Adaptively exchange output channels (ours). (b) Randomly exchange output channels. (c) Exchange output channels in order. (d)(e)(f) Exchange input channels. (g) Exchange the weight matrix in a point-wise manner. (h) Randomly exchange output channels with a randomly initialized GNN. (i) Self-exchange output channels in the target GNN. (j) AKE-GNN without graph augmentations.
The experimental results are performed on Cora based on the GCN model.

(1)GNN backbone models:
①Node classification: GCN(Kipf and Welling,2016), GAT(Veličković et al.,2017), APPNP(Klicpera et al.,2019), JKNET with concatenation and maximum aggregation scheme(Xu et al.,2018), GRAND(Feng et al.,2020), and a recent deep GNN model GCNII(Chen et al.,2020).
②Link prediction: GCN.
③Graph classification: GCN, and GIN(Xu et al.,2019a);
(2)Weight re-activating methods: Adaptive Weighting(Meng et al.,2020), and Decorr(Jin et al.,2022);
(3)Three variants for ablation analyses:
① Further training (FT) trains GNN backbone models based on augmented graph views (we report the best result among four graph views) with the same number of epochs as AKE-GNN to exclude the influence of longer training epochs and graph augmentations.
② The multiple GNNs ensemble (Ensemble) first trains GNNs on the generated views individually and then ensembles their outputs by majority voting.
③ The multiple GNNs ensemble+further training (Ensemble+FT) not only ensembles the output of multiple GNNs on different views, but also trains each GNN with the same number of epochs as AKE-GNN.
The original GNN baseline models are denoted by their names directly.

As a learning framework (rather than a specific GNN architecture), AKE-GNN is implementedbased ona backbone GNN model.
For generating multiple views, we adopt 4 graph augmentation methods, i.e.,Masking node features,Corrupting node features,Dropping edges, andExtracting subgraphs.
We setB=10𝐵10B=10as the number of bins in entropy calculation,N=12𝑁12N=12(#iterations=3iterations3\text{iterations}=3) as the iteration steps, andM=5𝑀5M=5as the number of exchange channels in each layer of GNNs.
Data preparation follows the standard experimental settings, including feature preprocessing and data splitting(Feng et al.,2020; Hu et al.,2020).
We use accuracy (%) with standard deviation averaged over 100 runs with different random seeds as the metric, except for the result on the large-scaleOGBn-Arxivdataset, which is averaged over 10 runs.
Since each GNN in AKE-GNN interacts with all the other GNNs trained on different views, after the process of parameter exchange, the performance of different GNNs is similar.
Thus, in what follows, we always record the performance of the first GNN model.

We implement AKE-GNN and theEnsemblevariant based on GRAND(Feng et al.,2020).
The comparison with baseline models on Cora, CiteSeer, and PubMed is reported in Table2.
We find that AKE-GNN consistently outperforms both the single-view GNNs and multi-view GNNs, which demonstrates the effectiveness of adaptive knowledge exchange in modeling the relationship of multiple views.
Notably, AKE-GNN achieves the state-of-the-art results with 85.9% accuracy on the Cora semi-supervised node classification dataset. AKE-GNN improves GCN by an average 3.9% in terms of test accuracy on Cora.
The outperformance over theEnsembleshows that the adaptive integration of multiple views in AKE-GNN is more effective than the simple ensemble of GNNs trained on different views.
Contrary toEnsemble, which requires simultaneous inference of the multiple models, the inference cost of AKE-GNN is the same as a single-view model.
Moreover, we compare our methods with the adaptive weighting method followingMeng et al.(2020).
AKE-GNN consistently surpasses this method, which suggests that our fine-grained method to exchange part of the knowledge in each GNN is more effective for multi-view GNNs.

To further evaluate the effectiveness of AKE-GNN, we implement AKE-GNN and compare it with the 3 variants of the baseline GNN model. The baseline models include GCN, GAT, APPNP, JKNet, and GCNII(Kipf and Welling,2016; Veličković et al.,2017; Klicpera et al.,2019; Xu et al.,2018; Chen et al.,2020).
In Table3, the experimental results of baselines are reproduced based on their official codes.
It shows that AKE-GNN consistently outperforms baselines by 1.9%∼similar-to\sim3.8% (absolute improvements) on average.
The outperformance of AKE-GNN overFT,Ensemble, andEnsemble+FTshows that the expressiveness of the adaptive knowledge exchange comes from neither extra training epochs, nor the larger model capacity of multi-graph GNNs.
We find that AKE-GNN andEnsembleoutperform the baseline model by a large margin, which suggests the integration of multiple views on the small and medium-sized datasets helps to obtain better performance.
So, conductingFTorEnsembleamong them brings inferior results. Nevertheless, AKE-GNN consistently achieves the best performance, which indicates its effectiveness to integrate informative information from multiple graph views.
Besides, in contrast to the ensemble methods, AKE-GNN utilizes only one network during inference, which is more computationally efficient.

As shown in Table4, AKE-GNN consistently outperforms the original GNN models by a large margin.
Meanwhile, AKE-GNN achieves a higher accuracy over the three variants, further showing the superiority of our adaptive parameter exchange method.
We notice that the performance improvement is marginal on the PubMed dataset of link prediction tasks.
We postulate the reason is that the connection density (#edges / #nodes) of the PubMed dataset is higher than Cora and CiteSeer, which makes the model easier to complete the missing edges via message aggregation of neighbors in the single-view graph(Pei et al.,2020).
Thus, AKE-GNN extracts less extra information behind the multiple generated graph views on PubMed than the other datasets, which hinders the model performance improvement.

To validate that AKE-GNN can scale to large graphs, we further conduct experiments on the large citation datasetOGBn-Arxiv.
We select four top-ranked GNN models from the leaderboard of OGB(Hu et al.,2020), and then perform AKE-GNN based on them with the same GNN architectures and hyperparameters.
As shown in Table4, our method outperforms the original methods and even their ensembles, which demonstrates the effectiveness of AKE-GNN on the large scale dataset.

[图片: images\image_5.png]
图片说明: Figure 5.Performance comparisons of AKE-GNN with GCN on Cora.Left:the measurement of over-smoothing in terms of test accuracy (%).Right:test accuracy (%) on the few-shot setting.

To verify the effectiveness of our adaptive exchange method, we compare AKE-GNN with other possible knowledge exchange approaches, as shown in Fig.4.
These comparisons can be categorized into six groups: ① AKE-GNN (a) v.s. output channel exchange (b)(c), where (b) randomly chooses output channels in the target network and (c) swaps the firstM𝑀Moutput channels; ② output channel exchange (a)(b)(c) v.s. input channel exchange (d)(e)(f); ③ AKE-GNN (a) v.s. point-wise exchange (g); ④ AKE-GNN (a) v.s. adaptive exchange with a randomly initialized model (h); ⑤ AKE-GNN(a) v.s. adaptive exchange within the same network (i); ⑥ AKE-GNN (a) v.s. AKE-GNN without graph augmentations (j).

All the experiments are conducted on Cora using GCN(Kipf and Welling,2016)as the backbone model.
For consistency, the same number of parameters are exchanged in all the experiments.
In Fig.4, results are directly shown below the illustrations of the corresponding exchange methods.
We find that our proposed adaptive parameter exchange method along the output channel consistently outperforms all other approaches.
From Fig.4, we can conclude that:
1) Comparing (a) with (b) and (c), the adaptive approach is more effective to substitute the redundant channel as it uses the entropy maximization heuristic;
2) Comparing (a) (b) (c) and (d) (e) (f), we find that exchanging the output channel is more effective than exchanging the input one.
As illustrated in Fig.3, each output hidden feature is solely determined by the corresponding output channel’s parameters in the last layer. Thus, such a scheme may exchange the extra information from the other weight parameter matrix of GNN models, which enriches the model’s representation ability;
3) The result in (g) suggests that random point-wise exchange without considering the input or output channel decreases accuracy;
4) Comparing (a) with (h), we can properly draw the conclusion that exchanging parameters with another well-trained GNN can incorporate knowledge of another graph view, and thus achieve better performance;
5) Self-exchanging cannot bring benefits as shown in (i) since it does introduce extra knowledge;
6) Since exchanging knowledge without graph augmentations cannot introduce external information from other graph views, it achieves similar results as the baseline GCN and performs much worse than AKE-GNN, as shown in (j).

[图片: images\image_6.png]
图片说明: Figure 6.Learning curves (accuracy (left) and loss (right)) on OGBn-Arxiv with the GCNII backbone model.

[图片: images\image_7.png]
图片说明: Figure 7.Hyperparameter analyses on the Cora dataset.

Most current GNN models are shallow due to the over-smoothing issue, where node features become indistinguishable as we increase the feature propagation steps(Liu et al.,2020).
We present the results of GCN(Kipf and Welling,2016)by increasing the propagation steps (layers), and implement AKE-GNN based on GCN for comparison. In Fig.5, we empirically find that AKE-GNN can mitigate the over-smoothing issue compared to the original GCN.
As the number of layers increases, the accuracy of the original GNN decreases dramatically from 0.8 to 0.1.
In contrast, the accuracy of AKE-GNN decreases much slower. We find that AKE-GNN can make the propagation layer at least 50% deeper (propagation layer from 4 to 10) than the original GCN model without sacrificing the learning performance.
This suggests that AKE-GNN equipped with the adaptive knowledge exchange method provides an effective way to extend model capacity with relatively large layer numbers.
As suggested in(Rong et al.,2020), dropping edges in some generated graph views may help mitigate over-smoothing issues. We conjecture that removing certain edges makes node connections more sparse, and hence avoids over-smoothing to some extent when AKE-GNN goes very deep.

Following prior work(Wan et al.,2021), we further evaluate the effectiveness of AKE-GNN under the few-shot setting.
Taking Cora as the representative dataset, we manually vary the number of labeled nodes per class from 1 to 50 in the training phase, and keep the validation and test dataset unchanged. As shown in Fig.5, AKE-GNN consistently outperforms GCN.
Specifically, the relative improvements on accuracy are 4.0/3.3/4.2/0.9/2.2 on average for 1/5/10/20/50 labeled nodes per class, which shows that exchanging information from multiple generated graph views is more efficient when utilizing limited supervision.

We plot the training loss and accuracy curves to verify that AKE-GNN can improve the training process compared with the backbone GNN model (GCNII).
Fig6shows the accuracy and the loss curve of AKE-GNN in the re-training phase and GCNII onOGBn-Arxiv.
We can observe that the blue line (AKE-GNN) is above the orange line (GCNII) in Fig.6(a), while the blue line (AKE-GNN) is under the orange line (GCNII) in Fig.6(b).
It demonstrates that the backbone GNN model equipped with our proposed AKE-GNN framework indeed converges faster.

We study the sensitivity of hyperparameters of our framework AKE-GNN, and conduct experiments on Cora based on the GCN model. We have two hyperparameters in the knowledge exchange phase of AKE-GNN: the iteration stepsN𝑁Nand the exchanging channelsM𝑀M.
Taking AKE-GNN on 4 graph views as an example, we first present a study on the number of iterations by varying it from111to555(N=4×#𝑁4#N=4\times\#Iterations) while using the default valueM=5𝑀5M=5. As shown in Fig.7(a), adaptively exchanging parameters with only a few iterations (##\#Iterations=3) can achieve satisfying performance.
We further study the number of exchange channelsM𝑀Mby varying it from 1 to 15 (the hidden size of the GCN is 16) while fixing##\#Iterations=3 in Fig.7(b).
The best performance is achieved by exchanging part of channels rather than all parameters in a layer, demonstrating that adaptively exchanging information in a complementary way can bring more benefits.
Finally, we study the number of graph views from 1 to 5 while using the default value##\#Iterations=3 andM=5𝑀5M=5. We successively add the following graph views in AKE-GNN:masking node features,corrupting node features,dropping edges,extracting subgraphs,the original graph. As shown in Fig.7(c), adding more graph views indeed improves performance. However, the performance stagnates when we add the number of graph views to 5. We assume the cause might be that the network receives too much information from other graph views which may affect its self-information for learning.
In all, we find that the performance of our framework is relatively stable across different hyperparameters, and thus does not rely on heavy and case-by-case hyperparameter tuning to achieve satisfactory results.

The computational complexity of AKE-GNN isO​(N​L​d3)𝑂𝑁𝐿superscript𝑑3O\left(NLd^{3}\right), whereN𝑁Nis the number of iterations,L𝐿Lis the number of GNN layers, andd𝑑dis the embedding dimension. Note thatN𝑁Nis usually small, and we set 3 in our experiments. The time cost of AKE-GNN is acceptable compared with multi-epoch training of GNNs, because the adaptive parameter exchange only executes once before the re-training of GNNs.
We conduct ablations on the large paper citation network (OGBn-Arxiv) using GCNII(Chen et al.,2020)to investigate the time consumption overhead of AKE-GNN. We set the hidden dimension sizes as 64, 128, and 256. As shown in Table5, the time cost of the adaptive parameter exchange is substantially less than that in the training phase, which indicates that the bottleneck of AKE-GNN still depends on the training of GNNs rather than the adaptive parameter exchange.

In this paper, we propose a novel framework named AKE-GNN, which performs the adaptive knowledge exchange strategy on the multiple GNNs each corresponding to a generated graph view. In AKE-GNN, we iteratively exchange redundant channels in the weight matrix of one GNN with informative channels of another GNN in a layer-wise manner.
Moreover, existing GNNs can be seamlessly integrated into our framework. Comprehensive experiments show that our proposed learning framework consistently outperforms the existing popular GNN models and even their ensembles. This work focuses on exchanging knowledge of views with different graph augmentations. However, how to generate diverse views for better knowledge exchange is still under exploration, which leaves for future work.
We hope our work can inspire new ideas in exploring new learning mechanisms on the multi-view graphs.

Liang Zeng, Jin Xu, and Jian Li are supported in part by the National Natural Science Foundation of China Grant 62161146004.

[图片: images\image_8.png]

[图片: images\image_9.png]

