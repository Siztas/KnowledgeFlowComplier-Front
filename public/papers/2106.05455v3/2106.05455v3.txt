æ ‡é¢˜ï¼šAKE-GNN: Effective Graph Learning withAdaptive Knowledge Exchange

Graph Neural NetworksÂ (GNNs) have already been widely used in various graph mining tasks. However, recent works reveal that the learned weightsÂ (channels) in well-trained GNNs are highly redundant, which inevitably limits the performance of GNNs. Instead of removing these redundant channels for efficiency consideration, we aim to reactivate them to enlarge the representation capacity of GNNs for effective graph learning.
In this paper, we propose to substitute these redundant channels with other informative channels to achieve this goal.
We introduce a novel GNN learning framework named AKE-GNN, which performs theAdaptiveKnowledgeExchange strategy among multiple graph views generated by graph augmentations.
AKE-GNN first trains multiple GNNs each corresponding to one graph view to obtain informative channels. Then, AKE-GNN iteratively exchanges redundant channels in the weight parameter matrix of one GNN with informative channels of another GNN in a layer-wise manner.
Additionally, existing GNNs can be seamlessly incorporated into our framework. AKE-GNN achieves superior performance compared with various baselines across a suite of experiments on node classification, link prediction, and graph classification. In particular, we conduct a series of experiments on 15 public benchmark datasets, 8 popular GNN models, and 3 graph tasks and show that AKE-GNN consistently outperforms existing popular GNN models and even their ensembles. Extensive ablation studies and analyses on knowledge exchange methods validate the effectiveness of AKE-GNN.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1.The illustrative schematic diagram of the proposed AKE-GNN framework. (a) Two generated graph viewsÂ (i.e.,Â masking node features and dropping edges). (b) Adaptive knowledge exchange by exchanging channel-wise parameters among two graph views (in one layer for illustration). (c) AKE-GNN in the multiple GNN caseÂ (best viewed in color).

Graph Neural NetworksÂ (GNNs), as the powerful tool for modeling relational inductive bias(Battaglia etÂ al.,2018; BarabÃ¡si,2013)to jointly encode graph structure and node features of the input graph(Hamilton,2020), have been widely employed for analyzing graph-mining tasks, including node classification(Kipf and Welling,2016; VeliÄkoviÄ‡ etÂ al.,2017; Hamilton etÂ al.,2017; Chen etÂ al.,2020; Wu etÂ al.,2019), link prediction(Zhang and Chen,2018; Ying etÂ al.,2018), and graph classification(Xu etÂ al.,2019a; Errica etÂ al.,2020).
Despite the prevalence and effectiveness of GNN models, as discussed in recent works(Chen etÂ al.,2021; Jin etÂ al.,2022), there exist redundant channels of the weight parameter matrix in a well-trained GNN model. These redundant channels can be removed without performance degradation.
Existing works mainly remove these redundant channels from the perspective of efficiency.
However, non-structured channel pruning methods are not hardware-friendly(Han etÂ al.,2015)and thus suffer from limited efficiency improvement(Li etÂ al.,2017).
Moreover, these methods often improve efficiency of the model with a slight sacrifice of effectiveness(Chen etÂ al.,2021; Frankle and Carbin,2018).
Therefore, from a novel and practical perspective of effectiveness, we propose to substitute these redundant channels with informative channels to enrich knowledge of GNN models for effective graph learning.
To achieve this goal, we need to tackle two unique technical challenges: 1) How to obtain informative channels? 2) How to exchange redundant channels with informative channels effectively?

For obtaining informative channels, we are inspired by recent advances in multi-view GNNs, whose multiple graph views generated by graph augmentations can provide complementary information of a graph from different aspects. Most GNN models are trained in an end-to-end supervised manner to learn effective node/graph representations in a single-view graph.
As argued in some recent works(Wang etÂ al.,2020; Xie etÂ al.,2020; Xu etÂ al.,2019b), such training methods can only capture partial information from the complex input, hence may not generalize well on unseen nodes/graphs.
As a result, researchers propose new training algorithms that generate multiple views from the input graph and then build multi-view GNNs(Wang etÂ al.,2020; Cheng etÂ al.,2020). The idea is that each view captures knowledge from one certain aspect, and knowledge learned from different views is fused to enhance node/graph representation. Representative models include AM-GCN(Wang etÂ al.,2020)and MGAT(Xie etÂ al.,2020)which utilize multi-head attention modules to fuse feature and topology knowledge, and MAGCN(Cheng etÂ al.,2020)which develops multi-view attribute graph convolution encoders.

For exchanging channels effectively, we propose a novel GNN learning framework, calledAdaptiveKnowledgeExchange GNNs (AKE-GNN), which fuses diverse knowledge learned from multiple graph views generated by graph augmentations. AKE-GNN adaptively exchanges parameters among those graph views.
The advantage of AKE-GNN is that we do not need to modify the model architecture or training loss functions(Jin etÂ al.,2022), and thus existing GNN models can be seamlessly incorporated into our framework.

AKE-GNN contains two training phases: an individual learning phase and a knowledge exchange phase. In the individual learning phase, we construct multiple views by stochastic graph augmentation functions(Zhu etÂ al.,2021), and GNNs sharing the same backbone model learn those graph views independently to obtain informative channels. In the knowledge exchange phase, we design a channel-wise adaptive exchange method that repeatedly replaces redundant channels in one GNN with the informative channels from another GNN in a layer-wise manner.
Furthermore, we show the extension of AKE-GNN to more than two graph views.
Comprehensive experiments show that AKE-GNN consistently achieves superior performance over existing popular GNN models and their ensembles on representative graph tasks including node classification, link predictions, and graph classification, and across various domains including bioinformaticsÂ (e.g., to predict the property of the protein) and social networksÂ (e.g., to predict the co-authorship).
In shot, our maincontributionsare:

We present a novel GNN learning framework, namely AKE-GNN, which adaptively exchanges knowledge from multiple GNNs learned on diverse graph views for effective graph learning.

Existing backbone GNN models can be seamlessly incorporated into AKE-GNN without modifying the original configurations such as the learning rate and the number of layers. Moreover, AKE-GNN introduces no extra computational overheads to the inference stage.

We extensively evaluate the effectiveness of AKE-GNN on 15 public datasets, 8 popular GNN models, and 3 graph tasks. AKE-GNN consistently outperforms corresponding GNN backbone models by an average of 1.9%âˆ¼similar-to\sim3.9% in terms of absolute accuracy improvements and even their ensembles.
In addition, extensive ablation studies and analyses on the proposed knowledge exchange method also validate the effectiveness of AKE-GNN.

Multi-view joint learning aims to jointly modelÂ (generated) multiple graph views to improve the generalization performance(Wang etÂ al.,2020; Xie etÂ al.,2020; Ma etÂ al.,2020; Xu etÂ al.,2019b; Cheng etÂ al.,2020). Most existing works leverage graph augmentations to generate multiple views from the original graph, and then design specific architectures to collaboratively fuse knowledge learned from different graph views to enhance their ability of graph representation learning.
AM-GCN(Wang etÂ al.,2020)explicitly constructs the node feature graph view and the topology graph view, and then employs two GNNs with attention mechanisms to extract knowledge from these two aspects.
MGAT(Xie etÂ al.,2020)automatically generates multiple views via graph augmentations and then designs an attention-based architecture to collaboratively integrate multiple types of knowledge in different views.
MAGCN(Cheng etÂ al.,2020)develops multi-view attribute graph convolution encoders with attention mechanisms for learning graph embeddings from multi-view graph data.
Different from these methods, our method retains the benefits of joint modeling multiple views via an adaptive knowledge exchange framework while not requiring dedicated architecture designs.
Additionally, graph contrastive learningÂ (GCL) methods(Hassani and Khasahmadi,2020; You etÂ al.,2020; Qiu etÂ al.,2020; Zhu etÂ al.,2021)leverage generated multiple graph views to maximize the feature consistency among these views. However, GCL methods operate within a self-supervised learning regime, where label information is not available during the training phase. In contract, AKE-GNN is grounded in a supervised learning setting to facilitate knowledge exchange of parameters from informative to redundant channels.

Our adaptive knowledge exchange framework is conceptually connected to weight re-activating methods.
Grafting(Meng etÂ al.,2020)improves the network performance by grafting external informationÂ (weights) on the same data source to re-activate invalid filters in computer vision tasks. DeCorr(Jin etÂ al.,2022)introduces the explicit feature dimension decorrelation term into the loss objective to tackle the feature overcorrelation issue in GNNs.
In contrast, our work aims at fusing different knowledge from GNNs trained on multiple (generated) graph views.
Since different graph views share different parts of knowledge that should not be repeated in just one GNN, we propose an adaptive approach to iteratively exchange complementary knowledge from different graph views for more effective graph learning.

In this section, we first present a preliminary study to investigate the redundancy issue of the weight parameter matrix in GNNsÂ (Sec.3.1). Then we introduce the AKE-GNN framework on two graph views with its two training phasesÂ (Sec.3.2&3.3).
We finally extend AKE-GNN to the multiple GNN caseÂ (Sec.3.4).
The overall framework of AKE-GNN is shown in Fig.1.

Letğ’¢=(ğ’±,â„°,ğ‘¿)ğ’¢ğ’±â„°ğ‘¿{\mathcal{G}}=\left({\mathcal{V}},{\mathcal{E}},{\bm{X}}\right)denote a graph, whereğ’±ğ’±{\mathcal{V}}is a set ofNğ‘Nnodes, andâ„°â„°{\mathcal{E}}is a set of edges between nodes.ğ‘¿=[ğ’™1,ğ’™2,â‹¯,ğ’™N]Tğ‘¿superscriptsubscriptğ’™1subscriptğ’™2â‹¯subscriptğ’™ğ‘ğ‘‡{\bm{X}}=\left[{\bm{x}}_{1},{\bm{x}}_{2},\cdots,{\bm{x}}_{N}\right]^{T}âˆˆâ„nÃ—dabsentsuperscriptâ„ğ‘›ğ‘‘\in{\mathbb{R}}^{n\times d}represents the node feature matrix andğ’™iâˆˆâ„dsubscriptğ’™ğ‘–superscriptâ„ğ‘‘{\bm{x}}_{i}\in{\mathbb{R}}^{d}is the feature vector of nodevisubscriptğ‘£ğ‘–v_{i}, wheredğ‘‘dis the number of channels in the feature matrixğ‘¿ğ‘¿{\bm{X}}. The adjacency matrixğ‘¨âˆˆ{0,1}NÃ—Nğ‘¨superscript01ğ‘ğ‘{\bm{A}}\in\{0,1\}^{N\times N}is defined byğ‘¨i,j=1subscriptğ‘¨ğ‘–ğ‘—1{\bm{A}}_{i,j}=1if(vi,vj)âˆˆâ„°subscriptğ‘£ğ‘–subscriptğ‘£ğ‘—â„°\left(v_{i},v_{j}\right)\in{\mathcal{E}}and00otherwise.
We denote the (generated) multiple views asğ’¢â„³={ğ’¢1,â‹¯,ğ’¢V}subscriptğ’¢â„³subscriptğ’¢1â‹¯subscriptğ’¢ğ‘‰{\mathcal{G}}_{{\mathcal{M}}}=\{{\mathcal{G}}_{1},\cdots,{\mathcal{G}}_{V}\}, whereğ’¢vsubscriptğ’¢ğ‘£{\mathcal{G}}_{v}is thevğ‘£v-th view of the original input graphğ’¢ğ’¢{\mathcal{G}}. Note that all of the graph views share the same node set.

Jin etÂ al.(2022)have empirically found that the weight parameter matrix of GNNs has a high tendency to contain redundant channels resulting from standard GNN training, i.e.,Â high Pearson correlation among channels in the weight matrix.
We verified this phenomenon by conducting experiments on Cora with GCN. We successively find a pair of output channels with the highest Pearson correlation in the weight matrix and then prune these two channels and re-train the resultant GCN model, starting from hidden size 16.
In Table1, we find that several channels have minor impacts on the output, and pruning these redundant channels does not degrade the performance.
This preliminary study inspires us that GNN models indeed contain highly correlated channels in the weight matrix, which cannot introduce extra useful information.
It naturally spurs a question: can we further improve the performance of GNN models by adaptively exchanging knowledge contained in their learned weights (channels)?
Herein, we need to tackle two unique technical challenges: 1) How to obtain informative channelsÂ (Sec.3.2)?
2) How to exchange channels effectivelyÂ (Sec.3.3)?

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2.The illustration of the pipeline of parameter updating in AKE-GNN with two GNNs.

In this phase, we first generate the multiple graph views by graph augmentations. Then, we train multiple GNNs each corresponding to a generated graph view to obtain informative channels of the weight parameter matrix in GNNs.

To capture different views of the original graph, following previous work(VeliÄkoviÄ‡ etÂ al.,2018; Zhu etÂ al.,2021), we apply stochasticaugmentation functionsto generate multiple views of the original graph and then feed them into GNNs.
Formally, a different view of the original graph(ğ‘¿,ğ‘¨)ğ‘¿ğ‘¨\left({\bm{X}},{\bm{A}}\right)is obtained by(ğ‘¿~,ğ‘¨~)=ğ’â€‹(ğ‘¿,ğ‘¨)~ğ‘¿~ğ‘¨ğ’ğ‘¿ğ‘¨\left(\widetilde{{\bm{X}}},\widetilde{{\bm{A}}}\right)=\mathcal{C}\left({\bm{X}},{\bm{A}}\right), whereğ’â€‹(â‹…)ğ’â‹…\mathcal{C}(\cdot)is anaugmentation function.
We leverage four commonly-used augmentation functions to generate multiple graph views in AKE-GNN(Zhu etÂ al.,2021; VeliÄkoviÄ‡ etÂ al.,2018; Feng etÂ al.,2020), which aremasking node features,corrupting node features,dropping edges, andextracting subgraphs.

Masking node features.Randomly mask a fraction of node attributes with zeros. Formally, the generated matrix of node featuresğ‘¿~~ğ‘¿\widetilde{{\bm{X}}}is computed by

whereğ’âˆˆ{0,1}dğ’superscript01ğ‘‘{\bm{m}}\in\{0,1\}^{d}is a random vector, which is drawn from a Bernoulli distribution,[â‹…,â‹…]â‹…â‹…[\cdot\;,\cdot]is the concatenation operator, andâŠ™direct-product\odotis the element-wise multiplication.

Corrupting node features.Randomly replace a fraction of node attributes with Gaussian noise. Formally, it can be calculated by

whereğ’iâˆˆâ„dsubscriptğ’ğ‘–superscriptâ„ğ‘‘{\bm{m}}_{i}\in{\mathbb{R}}^{d}is a random vector drawn from a Gaussian distributionğ’©â€‹(Î¼â€‹(ğ’™i),1)ğ’©ğœ‡subscriptğ’™ğ‘–1\mathcal{N}\left(\mu({\bm{x}}_{i}),1\right)independently andÎ¼â€‹(â‹…)ğœ‡â‹…\mu(\cdot)denotes the mean value of a vector.

Dropping edges.Randomly remove edges in the graph. Formally, we sample a modified subsetâ„°~~â„°\widetilde{{\mathcal{E}}}from the original edge setâ„°â„°{\mathcal{E}}with the probability defined as follows:

where(u,v)âˆˆâ„°ğ‘¢ğ‘£â„°\left(u,v\right)\in{\mathcal{E}}andpuâ€‹vesuperscriptsubscriptğ‘ğ‘¢ğ‘£ğ‘’p_{uv}^{e}is the probability of removing(u,v)ğ‘¢ğ‘£\left(u,v\right).

Extracting subgraphs.Extract the induced subgraphsğ’¢â€²=(ğ’±â€²,â„°â€²)superscriptğ’¢â€²superscriptğ’±â€²superscriptâ„°â€²{\mathcal{G}}^{\prime}=\left({\mathcal{V}}^{\prime},{\mathcal{E}}^{\prime}\right)containing the nodes ang the corresponding edges in a given subset(VeliÄkoviÄ‡ etÂ al.,2018), i.e.,ğ’±â€²âŠ†ğ’±superscriptğ’±â€²ğ’±{\mathcal{V}}^{\prime}\subseteq{\mathcal{V}}andâ„°â€²âŠ†â„°superscriptâ„°â€²â„°{\mathcal{E}}^{\prime}\subseteq{\mathcal{E}}.

Note that AKE-GNN does not require specific graph augmentation functions, and thus other graph augmentation methods can be seamlessly incorporated into our framework.

For any existing GNN model, it can be directly applied in the AKE-GNN framework without modifying its original implementations such as the learning rate and the number of layers.
We denote a parametrized GNN asâ„±ğœ½:ğ•â†’ğ•:subscriptâ„±ğœ½â†’ğ•ğ•\mathcal{F}_{\bm{\theta}}:{\mathbb{X}}\rightarrow{\mathbb{Y}}with the initial parameterğœ½(0)superscriptğœ½0\bm{\theta}^{\left(0\right)}, whereğ•ğ•{\mathbb{X}}andğ•ğ•{\mathbb{Y}}are the input space and output space.
GivenNtrainsubscriptğ‘trainN_{\text{train}}paired training data{(ğ’™t,yt)}t=1NtrainâŠ‚ğ•Ã—ğ•superscriptsubscriptsubscriptğ’™ğ‘¡subscriptğ‘¦ğ‘¡ğ‘¡1subscriptğ‘trainğ•ğ•\{\left({\bm{x}}_{t},y_{t}\right)\}_{t=1}^{N_{\text{train}}}\subset{\mathbb{X}}\times{\mathbb{Y}}, the networkâ„±ğœ½subscriptâ„±ğœ½\mathcal{F}_{\bm{\theta}}is optimized with a supervised lossâ„’â„’\mathcal{L}as follows:

whereğœ½(1)superscriptğœ½1\bm{\theta}^{\left(1\right)}is the parameters of a GNN after optimization.

Earlier works identify that knowledge is contained in the updated parameter values of a neural network(Hinton etÂ al.,2015).
After the individual learning phase, GNNs trained with multiple views have learned knowledge stored in their updated parameters and can take a further step to interact with each other for knowledge exchange.
In the knowledge exchange phase, we takeğœ½ğ’¢1(1)superscriptsubscriptğœ½subscriptğ’¢11\bm{\theta}_{{\mathcal{G}}_{1}}^{\left(1\right)}andğœ½ğ’¢2(1)superscriptsubscriptğœ½subscriptğ’¢21\bm{\theta}_{{\mathcal{G}}_{2}}^{\left(1\right)}of the two GNNs as input, exchange knowledge, and produceğœ½ğ’¢1(1)â€²\bm{\theta}_{{\mathcal{G}}_{1}}^{{}^{\prime}\left(1\right)}andğœ½ğ’¢2(1)â€²\bm{\theta}_{{\mathcal{G}}_{2}}^{{}^{\prime}\left(1\right)}as output, whereğ’¢1subscriptğ’¢1{\mathcal{G}}_{1}andğ’¢2subscriptğ’¢2{\mathcal{G}}_{2}are two corresponding graph views.
Then, we re-train the output parametersğœ½ğ’¢1(1)â€²\bm{\theta}_{{\mathcal{G}}_{1}}^{{}^{\prime}\left(1\right)}andğœ½ğ’¢2(1)â€²\bm{\theta}_{{\mathcal{G}}_{2}}^{{}^{\prime}\left(1\right)}and obtain the final parametersğœ½ğ’¢1(2)superscriptsubscriptğœ½subscriptğ’¢12\bm{\theta}_{{\mathcal{G}}_{1}}^{\left(2\right)}andğœ½ğ’¢2(2)superscriptsubscriptğœ½subscriptğ’¢22\bm{\theta}_{{\mathcal{G}}_{2}}^{\left(2\right)}.
The illustration of the pipeline of parameter updating in AKE-GNN is shown in Fig.2.
To exchange knowledge among multiple GNNs, we need to answer the following two questions: 1) How to measure information (knowledge) inside the parametersÂ (connection weights)? 2) How to adaptively perform knowledge exchange among multiple GNNs?

We leverage entropy to measure information in one layer of a well-trained GNN. As suggested in(Meng etÂ al.,2020), the higher entropy the weight matrix has, the more variationÂ (the less redundant information) the model contains, and then the potentially better performance of the final prediction.
Letğœ½ğ’¢klâˆˆâ„diÃ—di+1superscriptsubscriptğœ½subscriptğ’¢ğ‘˜ğ‘™superscriptâ„subscriptğ‘‘ğ‘–subscriptğ‘‘ğ‘–1\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}\in{\mathbb{R}}^{d_{i}\times d_{i+1}}denote the parameters of thelğ‘™l-th layer in the corresponding GNN whose input isğ’¢ksubscriptğ’¢ğ‘˜{\mathcal{G}}_{k}, wheredisubscriptğ‘‘ğ‘–d_{i}is the number of channels in thelğ‘™l-th layer.
Following(Meng etÂ al.,2020; Cheng etÂ al.,2019),
we calculate entropy by dividing the range of values inğœ½ğ’¢klsuperscriptsubscriptğœ½subscriptğ’¢ğ‘˜ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}intoBğµBdifferent bins. Denote the number of values that fall into thebğ‘b-th bin asnbsubscriptğ‘›ğ‘n_{b}. We usepbâ‰œnb(n1+â‹¯+nm)â‰œsubscriptğ‘ğ‘subscriptğ‘›ğ‘subscriptğ‘›1â‹¯subscriptğ‘›ğ‘šp_{b}\triangleq\frac{n_{b}}{\left(n_{1}+\cdots+n_{m}\right)}to approximate the probability of thebğ‘b-th bin, wheren1+â‹¯+nB=diÃ—di+1subscriptğ‘›1â‹¯subscriptğ‘›ğµsubscriptğ‘‘ğ‘–subscriptğ‘‘ğ‘–1n_{1}+\cdots+n_{B}=d_{i}\times d_{i+1}. Then, the entropy ofğœ½ğ’¢klsuperscriptsubscriptğœ½subscriptğ’¢ğ‘˜ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}can be calculated as follows:

A larger value ofHâ€‹(ğœ½ğ’¢kl)ğ»superscriptsubscriptğœ½subscriptğ’¢ğ‘˜ğ‘™H\left(\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}\right)usually indicates richer information in the parameters of thelğ‘™l-th layer in the corresponding GNN whose input isğ’¢ksubscriptğ’¢ğ‘˜{\mathcal{G}}_{k}, and vice versa. For example, if each element ofğœ½ğ’¢klsuperscriptsubscriptğœ½subscriptğ’¢ğ‘˜ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}takes the same valueÂ (entropy is 0),ğœ½ğ’¢klsuperscriptsubscriptğœ½subscriptğ’¢ğ‘˜ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}cannot discriminate which part of the input is more important.

Given quantitative measurements of information in each layer of a GNN, we then consider how to adaptively exchange information among multiple GNNs.
Since GNNs follow the message passing scheme to iteratively aggregate information from neighbor nodes, thekğ‘˜k-th layer makes use of the subtree structures of heightkğ‘˜krooted at every node. Thus, we only exchange information of the same layer to preserve the consistency of information between two GNNs.
Let parameters of the source and the target GNN beğœ½ğ’¢ssubscriptğœ½subscriptğ’¢ğ‘ \bm{\bm{\theta}}_{{\mathcal{G}}_{s}}andğœ½ğ’¢tsubscriptğœ½subscriptğ’¢ğ‘¡\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}, respectively.
We denote parameters in thelğ‘™l-th GNN layer trained on thekğ‘˜k-th graph view asğœ½ğ’¢kl=[ğœ½ğ’¢kl,1,â‹¯,ğœ½ğ’¢kl,Cl+1]âˆˆâ„ClÃ—Cl+1superscriptsubscriptğœ½subscriptğ’¢ğ‘˜ğ‘™superscriptsubscriptğœ½subscriptğ’¢ğ‘˜ğ‘™1â‹¯superscriptsubscriptğœ½subscriptğ’¢ğ‘˜ğ‘™subscriptğ¶ğ‘™1superscriptâ„subscriptğ¶ğ‘™subscriptğ¶ğ‘™1\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l}=\small[\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l,1},\cdots,\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l,C_{l+1}}\small]\in{\mathbb{R}}^{C_{l}\times C_{l+1}}, where the input channel isClsubscriptğ¶ğ‘™C_{l}, the output channel isCl+1subscriptğ¶ğ‘™1C_{l+1}, andğœ½ğ’¢kl,jâˆˆâ„Clsuperscriptsubscriptğœ½subscriptğ’¢ğ‘˜ğ‘™ğ‘—superscriptâ„subscriptğ¶ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{k}}^{l,j}\in{\mathbb{R}}^{C_{l}}is thejğ‘—j-th output channel vector.
In each exchange step, our target is to adaptively exchange a redundant output channel inğœ½ğ’¢tsubscriptğœ½subscriptğ’¢ğ‘¡\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}with another informative output channel inğœ½ğ’¢ssubscriptğœ½subscriptğ’¢ğ‘ \bm{\bm{\theta}}_{{\mathcal{G}}_{s}}.
To exchange the redundant channel, we first calculate the values ofPearson correlationamong all possible output channel pairs inğœ½ğ’¢tlsuperscriptsubscriptğœ½subscriptğ’¢ğ‘¡ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l}, and then obtain a pair of redundant channels with the highest correlation, i.e.,idx1subscriptidx1\text{idx}_{1}andidx2subscriptidx2\text{idx}_{2}.
We select an output channel fromğœ½ğ’¢slsuperscriptsubscriptğœ½subscriptğ’¢ğ‘ ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{s}}^{l}to substituteğœ½ğ’¢tl,idx1superscriptsubscriptğœ½subscriptğ’¢ğ‘¡ğ‘™subscriptidx1\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l,\text{idx}_{1}}orğœ½ğ’¢tl,idx2superscriptsubscriptğœ½subscriptğ’¢ğ‘¡ğ‘™subscriptidx2\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l,\text{idx}_{2}}with the purpose to maximize entropy of the new weight parameter matrixğœ½ğ’¢tlsuperscriptsubscriptğœ½subscriptğ’¢ğ‘¡ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l}. Formally, letfâ€‹(ğ‘¨j,ğ’™)ğ‘“superscriptğ‘¨ğ‘—ğ’™f\left({\bm{A}}^{j},{\bm{x}}\right)be the operator to substitute thejğ‘—j-th output channel of the matrixğ‘¨ğ‘¨{\bm{A}}with a vectorğ’™ğ’™{\bm{x}}. We find the informative output channeliâˆˆ[Cl+1]ğ‘–delimited-[]subscriptğ¶ğ‘™1i\in\left[C_{l+1}\right]in the source networkğ’¢ssubscriptğ’¢ğ‘ {\mathcal{G}}_{s}and the redundant output channelrâˆˆ{idx1,idx2}ğ‘Ÿsubscriptidx1subscriptidx2r\in\{\text{idx}_{1},\text{idx}_{2}\}in the target networkğ’¢tsubscriptğ’¢ğ‘¡{\mathcal{G}}_{t}at thelğ‘™l-th layer as follows:

Finally, we exchange parameters betweenğœ½ğ’¢sl,isuperscriptsubscriptğœ½subscriptğ’¢ğ‘ ğ‘™ğ‘–\bm{\bm{\theta}}_{{\mathcal{G}}_{s}}^{l,i}andğœ½ğ’¢tl,rsuperscriptsubscriptğœ½subscriptğ’¢ğ‘¡ğ‘™ğ‘Ÿ\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l,r}. By repeating the above exchange step forMğ‘€Mtimes,ğœ½ğ’¢tlsuperscriptsubscriptğœ½subscriptğ’¢ğ‘¡ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l}can accept the part of informationÂ (Mğ‘€Mchannels) fromğœ½ğ’¢slsuperscriptsubscriptğœ½subscriptğ’¢ğ‘ ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{s}}^{l}while retaining the useful information in the original networkğœ½ğ’¢tlsuperscriptsubscriptğœ½subscriptğ’¢ğ‘¡ğ‘™\bm{\bm{\theta}}_{{\mathcal{G}}_{t}}^{l}. We illustrate the procedure in Fig.1(b), whereğœ½ğ’¢1subscriptğœ½subscriptğ’¢1\bm{\bm{\theta}}_{{\mathcal{G}}_{1}}andğœ½ğ’¢2subscriptğœ½subscriptğ’¢2\bm{\bm{\theta}}_{{\mathcal{G}}_{2}}perform adaptive channel-wise parameter exchange in one layer as aforementioned. As a result,ğœ½ğ’¢1subscriptğœ½subscriptğ’¢1\bm{\bm{\theta}}_{{\mathcal{G}}_{1}}exchanges the second channel in the weight matrix with the first channel inğœ½ğ’¢2subscriptğœ½subscriptğ’¢2\bm{\bm{\theta}}_{{\mathcal{G}}_{2}}. Through this procedure, both networks contain information from two graph views. Finally, we re-train two GNNs with the same number of epochs as introduced in Sec.3.2to obtain the output predictions.
The complete algorithm of AKE-GNN with two GNNs is summarized in Algorithm1.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3.The illustration of exchanging output channelsÂ (a) compared with point-wise exchangeÂ (b), exchanging input channelsÂ (c), and self-exchange output channelsÂ (d). The newly replaced value in the weight matrix is depicted in purple.
We only consider features of one node (one row inğ’›lsuperscriptğ’›ğ‘™{\bm{z}}^{l}) and omit the input feature vectorğ’›lsuperscriptğ’›ğ‘™{\bm{z}}^{l}in the bottom half of the figure for brevity.

To further explain the rationale of our proposed adaptive knowledge exchange method, we present an illustrated example in Fig.3.
We denote the input and the output feature in thelğ‘™l-th GNN layer asğ’›lsuperscriptğ’›ğ‘™{\bm{z}}^{l}andğ’›l+1superscriptğ’›ğ‘™1{\bm{z}}^{l+1}, respectively. LetÎ¸lsuperscriptğœƒğ‘™\theta^{l}be the weight matrix of thelğ‘™l-th GNN layer. Each value inğ’›lsuperscriptğ’›ğ‘™{\bm{z}}^{l}/ğ’›l+1superscriptğ’›ğ‘™1{\bm{z}}^{l+1}represents the certain feature dimension.
Each feature dimension inğ’›l+1superscriptğ’›ğ‘™1{\bm{z}}^{l+1}is a function ofğ’›lsuperscriptğ’›ğ‘™{\bm{z}}^{l}, which is parameterized by output channels inÎ¸lsuperscriptğœƒğ‘™\theta^{l}.
Thus, exchanging output channels can produce partially modified features inğ’›l+1superscriptğ’›ğ‘™1{\bm{z}}^{l+1}.
Comparing Fig.3(a) with (b) and (c), exchanging certain output channels can alter the corresponding features inğ’›l+1superscriptğ’›ğ‘™1{\bm{z}}^{l+1}(e.g., â€œ40â€â†’â†’\rightarrowâ€œ68â€) while keeping other features unchangedÂ (e.g., â€œ46â€), which explicitly contain information of both the original and the other new network.
Comparing (a) with (d), self-exchange among output channels in the network itself cannot bring new information and can only obtain repeated featuresÂ (e.g., repeated â€œ46â€ in Fig.4(d)). However, exchanging output channels among multiple GNNs can introduce extra information from the other weight matrix and obtain new featuresÂ (e.g., â€œ68â€ in Fig.4(a)).
We also present a detailed ablation study on AKE-GNN to compare adaptive channel exchange with other parameter exchange methods such as randomly exchanging parameters (without the adaptive exchange strategy), exchanging parameters with a randomly initialized model (without a well-trained GNN model), and exchanging output channels in the network itself (only disturbing channels) in Sec.4.2.
We empirically find that adaptive knowledge exchange learns more effective graph representations and achieves the highest accuracy 82.8% on the Cora dataset.

AKE-GNN can be easily extended to the multiple GNN case, as illustrated in Fig.1(c).
In each iteration of the knowledge exchange phase, each GNN modelğ’¢ksubscriptğ’¢ğ‘˜{\mathcal{G}}_{k}accepts the knowledge fromğ’¢kâˆ’1subscriptğ’¢ğ‘˜1{\mathcal{G}}_{k-1}. After certain iterations of knowledge exchange, each GNN model contains the knowledge from all the other GNN models trained on the multiple graph views.
We list the complete algorithm of AKE-GNN for multiple GNNs in Appendix.

(a) Results of accuracyÂ (%) on graph classification tasks.

(b) Results of accuracyÂ (%) on link prediction tasks.

(c) Results of accuracyÂ (%) on OGBn-Arxiv.

To show the generalization ability of the proposed AKE-GNN framework, we conduct experiments on 15 public benchmark datasets and 4 learning tasks.
(1)Node classification:Citation network(Yang etÂ al.,2016): Cora, CiteSeer, and PubMed;Wikipedia network(Rozemberczki etÂ al.,2021): Chameleon, and Squirrel;Actor co-occurrence network(Tang etÂ al.,2009): Actor;WebKB(Pei etÂ al.,2020): Cornell, Texas, and Wisconsin.
(2)Link prediction:Citation network(Yang etÂ al.,2016): Cora, CiteSeer, and PubMed.
(3)Graph classification:Chemical compounds(Morris etÂ al.,2020): DD, NCI1, and PROTEIN;Social network(Morris etÂ al.,2020): IMDB and REDDIT.
(4)Large scale dataset: Academic citation network in the open graph benchmarkÂ (OGB)(Hu etÂ al.,2020): OGBn-Arxiv.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: Figure 4.Comparison of different knowledge (weight matrix) exchange methods. We exchange the same number of parameters in the following methods for fair comparison and present the exchange scheme with one output/input channel for ease of understanding.Â (a)Â Adaptively exchange output channels (ours).Â (b)Â Randomly exchange output channels.Â (c)Â Exchange output channels in order.Â (d)(e)(f)Â Exchange input channels.Â (g)Â Exchange the weight matrix in a point-wise manner.Â (h)Â Randomly exchange output channels with a randomly initialized GNN.Â (i)Â Self-exchange output channels in the target GNN.Â (j)Â AKE-GNN without graph augmentations.
The experimental results are performed on Cora based on the GCN model.

(1)GNN backbone models:
â‘ Node classification: GCN(Kipf and Welling,2016), GAT(VeliÄkoviÄ‡ etÂ al.,2017), APPNP(Klicpera etÂ al.,2019), JKNET with concatenation and maximum aggregation scheme(Xu etÂ al.,2018), GRAND(Feng etÂ al.,2020), and a recent deep GNN model GCNII(Chen etÂ al.,2020).
â‘¡Link prediction: GCN.
â‘¢Graph classification: GCN, and GIN(Xu etÂ al.,2019a);
(2)Weight re-activating methods: Adaptive Weighting(Meng etÂ al.,2020), and Decorr(Jin etÂ al.,2022);
(3)Three variants for ablation analyses:
â‘  Further trainingÂ (FT) trains GNN backbone models based on augmented graph viewsÂ (we report the best result among four graph views) with the same number of epochs as AKE-GNN to exclude the influence of longer training epochs and graph augmentations.
â‘¡ The multiple GNNs ensembleÂ (Ensemble) first trains GNNs on the generated views individually and then ensembles their outputs by majority voting.
â‘¢ The multiple GNNs ensemble+further trainingÂ (Ensemble+FT) not only ensembles the output of multiple GNNs on different views, but also trains each GNN with the same number of epochs as AKE-GNN.
The original GNN baseline models are denoted by their names directly.

As a learning framework (rather than a specific GNN architecture), AKE-GNN is implementedbased ona backbone GNN model.
For generating multiple views, we adopt 4 graph augmentation methods, i.e.,Masking node features,Corrupting node features,Dropping edges, andExtracting subgraphs.
We setB=10ğµ10B=10as the number of bins in entropy calculation,N=12ğ‘12N=12(#iterations=3iterations3\text{iterations}=3) as the iteration steps, andM=5ğ‘€5M=5as the number of exchange channels in each layer of GNNs.
Data preparation follows the standard experimental settings, including feature preprocessing and data splitting(Feng etÂ al.,2020; Hu etÂ al.,2020).
We use accuracy (%) with standard deviation averaged over 100 runs with different random seeds as the metric, except for the result on the large-scaleOGBn-Arxivdataset, which is averaged over 10 runs.
Since each GNN in AKE-GNN interacts with all the other GNNs trained on different views, after the process of parameter exchange, the performance of different GNNs is similar.
Thus, in what follows, we always record the performance of the first GNN model.

We implement AKE-GNN and theEnsemblevariant based on GRAND(Feng etÂ al.,2020).
The comparison with baseline models on Cora, CiteSeer, and PubMed is reported in Table2.
We find that AKE-GNN consistently outperforms both the single-view GNNs and multi-view GNNs, which demonstrates the effectiveness of adaptive knowledge exchange in modeling the relationship of multiple views.
Notably, AKE-GNN achieves the state-of-the-art results with 85.9% accuracy on the Cora semi-supervised node classification dataset. AKE-GNN improves GCN by an average 3.9% in terms of test accuracy on Cora.
The outperformance over theEnsembleshows that the adaptive integration of multiple views in AKE-GNN is more effective than the simple ensemble of GNNs trained on different views.
Contrary toEnsemble, which requires simultaneous inference of the multiple models, the inference cost of AKE-GNN is the same as a single-view model.
Moreover, we compare our methods with the adaptive weighting method followingMeng etÂ al.(2020).
AKE-GNN consistently surpasses this method, which suggests that our fine-grained method to exchange part of the knowledge in each GNN is more effective for multi-view GNNs.

To further evaluate the effectiveness of AKE-GNN, we implement AKE-GNN and compare it with the 3 variants of the baseline GNN model. The baseline models include GCN, GAT, APPNP, JKNet, and GCNII(Kipf and Welling,2016; VeliÄkoviÄ‡ etÂ al.,2017; Klicpera etÂ al.,2019; Xu etÂ al.,2018; Chen etÂ al.,2020).
In Table3, the experimental results of baselines are reproduced based on their official codes.
It shows that AKE-GNN consistently outperforms baselines by 1.9%âˆ¼similar-to\sim3.8% (absolute improvements) on average.
The outperformance of AKE-GNN overFT,Ensemble, andEnsemble+FTshows that the expressiveness of the adaptive knowledge exchange comes from neither extra training epochs, nor the larger model capacity of multi-graph GNNs.
We find that AKE-GNN andEnsembleoutperform the baseline model by a large margin, which suggests the integration of multiple views on the small and medium-sized datasets helps to obtain better performance.
So, conductingFTorEnsembleamong them brings inferior results. Nevertheless, AKE-GNN consistently achieves the best performance, which indicates its effectiveness to integrate informative information from multiple graph views.
Besides, in contrast to the ensemble methods, AKE-GNN utilizes only one network during inference, which is more computationally efficient.

As shown in Table4, AKE-GNN consistently outperforms the original GNN models by a large margin.
Meanwhile, AKE-GNN achieves a higher accuracy over the three variants, further showing the superiority of our adaptive parameter exchange method.
We notice that the performance improvement is marginal on the PubMed dataset of link prediction tasks.
We postulate the reason is that the connection densityÂ (#edges / #nodes)Â of the PubMed dataset is higher than Cora and CiteSeer, which makes the model easier to complete the missing edges via message aggregation of neighbors in the single-view graph(Pei etÂ al.,2020).
Thus, AKE-GNN extracts less extra information behind the multiple generated graph views on PubMed than the other datasets, which hinders the model performance improvement.

To validate that AKE-GNN can scale to large graphs, we further conduct experiments on the large citation datasetOGBn-Arxiv.
We select four top-ranked GNN models from the leaderboard of OGB(Hu etÂ al.,2020), and then perform AKE-GNN based on them with the same GNN architectures and hyperparameters.
As shown in Table4, our method outperforms the original methods and even their ensembles, which demonstrates the effectiveness of AKE-GNN on the large scale dataset.

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: Figure 5.Performance comparisons of AKE-GNN with GCN on Cora.Left:the measurement of over-smoothing in terms of test accuracyÂ (%).Right:test accuracyÂ (%) on the few-shot setting.

To verify the effectiveness of our adaptive exchange method, we compare AKE-GNN with other possible knowledge exchange approaches, as shown in Fig.4.
These comparisons can be categorized into six groups: â‘  AKE-GNNÂ (a) v.s. output channel exchangeÂ (b)(c), where (b) randomly chooses output channels in the target network and (c) swaps the firstMğ‘€Moutput channels; â‘¡ output channel exchangeÂ (a)(b)(c) v.s. input channel exchangeÂ (d)(e)(f); â‘¢ AKE-GNN (a) v.s. point-wise exchange (g); â‘£ AKE-GNNÂ (a) v.s. adaptive exchange with a randomly initialized modelÂ (h); â‘¤ AKE-GNN(a) v.s. adaptive exchange within the same networkÂ (i); â‘¥ AKE-GNNÂ (a) v.s. AKE-GNN without graph augmentationsÂ (j).

All the experiments are conducted on Cora using GCN(Kipf and Welling,2016)as the backbone model.
For consistency, the same number of parameters are exchanged in all the experiments.
In Fig.4, results are directly shown below the illustrations of the corresponding exchange methods.
We find that our proposed adaptive parameter exchange method along the output channel consistently outperforms all other approaches.
From Fig.4, we can conclude that:
1) Comparing (a) with (b) and (c), the adaptive approach is more effective to substitute the redundant channel as it uses the entropy maximization heuristic;
2) Comparing (a) (b) (c) and (d) (e) (f), we find that exchanging the output channel is more effective than exchanging the input one.
As illustrated in Fig.3, each output hidden feature is solely determined by the corresponding output channelâ€™s parameters in the last layer. Thus, such a scheme may exchange the extra information from the other weight parameter matrix of GNN models, which enriches the modelâ€™s representation ability;
3) The result in (g) suggests that random point-wise exchange without considering the input or output channel decreases accuracy;
4) Comparing (a) with (h), we can properly draw the conclusion that exchanging parameters with another well-trained GNN can incorporate knowledge of another graph view, and thus achieve better performance;
5) Self-exchanging cannot bring benefits as shown in (i) since it does introduce extra knowledge;
6) Since exchanging knowledge without graph augmentations cannot introduce external information from other graph views, it achieves similar results as the baseline GCN and performs much worse than AKE-GNN, as shown in (j).

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: Figure 6.Learning curves (accuracyÂ (left) and lossÂ (right)) on OGBn-Arxiv with the GCNII backbone model.

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: Figure 7.Hyperparameter analyses on the Cora dataset.

Most current GNN models are shallow due to the over-smoothing issue, where node features become indistinguishable as we increase the feature propagation steps(Liu etÂ al.,2020).
We present the results of GCN(Kipf and Welling,2016)by increasing the propagation steps (layers), and implement AKE-GNN based on GCN for comparison. In Fig.5, we empirically find that AKE-GNN can mitigate the over-smoothing issue compared to the original GCN.
As the number of layers increases, the accuracy of the original GNN decreases dramatically from 0.8 to 0.1.
In contrast, the accuracy of AKE-GNN decreases much slower. We find that AKE-GNN can make the propagation layer at least 50% deeperÂ (propagation layer from 4 to 10) than the original GCN model without sacrificing the learning performance.
This suggests that AKE-GNN equipped with the adaptive knowledge exchange method provides an effective way to extend model capacity with relatively large layer numbers.
As suggested in(Rong etÂ al.,2020), dropping edges in some generated graph views may help mitigate over-smoothing issues. We conjecture that removing certain edges makes node connections more sparse, and hence avoids over-smoothing to some extent when AKE-GNN goes very deep.

Following prior work(Wan etÂ al.,2021), we further evaluate the effectiveness of AKE-GNN under the few-shot setting.
Taking Cora as the representative dataset, we manually vary the number of labeled nodes per class from 1 to 50 in the training phase, and keep the validation and test dataset unchanged. As shown in Fig.5, AKE-GNN consistently outperforms GCN.
Specifically, the relative improvements on accuracy are 4.0/3.3/4.2/0.9/2.2 on average for 1/5/10/20/50 labeled nodes per class, which shows that exchanging information from multiple generated graph views is more efficient when utilizing limited supervision.

We plot the training loss and accuracy curves to verify that AKE-GNN can improve the training process compared with the backbone GNN modelÂ (GCNII).
Fig6shows the accuracy and the loss curve of AKE-GNN in the re-training phase and GCNII onOGBn-Arxiv.
We can observe that the blue lineÂ (AKE-GNN) is above the orange lineÂ (GCNII) in Fig.6(a), while the blue lineÂ (AKE-GNN) is under the orange lineÂ (GCNII) in Fig.6(b).
It demonstrates that the backbone GNN model equipped with our proposed AKE-GNN framework indeed converges faster.

We study the sensitivity of hyperparameters of our framework AKE-GNN, and conduct experiments on Cora based on the GCN model. We have two hyperparameters in the knowledge exchange phase of AKE-GNN: the iteration stepsNğ‘Nand the exchanging channelsMğ‘€M.
Taking AKE-GNN on 4 graph views as an example, we first present a study on the number of iterations by varying it from111to555(N=4Ã—#ğ‘4#N=4\times\#Iterations) while using the default valueM=5ğ‘€5M=5. As shown in Fig.7(a), adaptively exchanging parameters with only a few iterationsÂ (##\#Iterations=3) can achieve satisfying performance.
We further study the number of exchange channelsMğ‘€Mby varying it from 1 to 15 (the hidden size of the GCN is 16) while fixing##\#Iterations=3 in Fig.7(b).
The best performance is achieved by exchanging part of channels rather than all parameters in a layer, demonstrating that adaptively exchanging information in a complementary way can bring more benefits.
Finally, we study the number of graph views from 1 to 5 while using the default value##\#Iterations=3 andM=5ğ‘€5M=5. We successively add the following graph views in AKE-GNN:masking node features,corrupting node features,dropping edges,extracting subgraphs,the original graph. As shown in Fig.7(c), adding more graph views indeed improves performance. However, the performance stagnates when we add the number of graph views to 5. We assume the cause might be that the network receives too much information from other graph views which may affect its self-information for learning.
In all, we find that the performance of our framework is relatively stable across different hyperparameters, and thus does not rely on heavy and case-by-case hyperparameter tuning to achieve satisfactory results.

The computational complexity of AKE-GNN isOâ€‹(Nâ€‹Lâ€‹d3)ğ‘‚ğ‘ğ¿superscriptğ‘‘3O\left(NLd^{3}\right), whereNğ‘Nis the number of iterations,Lğ¿Lis the number of GNN layers, anddğ‘‘dis the embedding dimension. Note thatNğ‘Nis usually small, and we set 3 in our experiments. The time cost of AKE-GNN is acceptable compared with multi-epoch training of GNNs, because the adaptive parameter exchange only executes once before the re-training of GNNs.
We conduct ablations on the large paper citation networkÂ (OGBn-Arxiv) using GCNII(Chen etÂ al.,2020)to investigate the time consumption overhead of AKE-GNN. We set the hidden dimension sizes as 64, 128, and 256. As shown in Table5, the time cost of the adaptive parameter exchange is substantially less than that in the training phase, which indicates that the bottleneck of AKE-GNN still depends on the training of GNNs rather than the adaptive parameter exchange.

In this paper, we propose a novel framework named AKE-GNN, which performs the adaptive knowledge exchange strategy on the multiple GNNs each corresponding to a generated graph view. In AKE-GNN, we iteratively exchange redundant channels in the weight matrix of one GNN with informative channels of another GNN in a layer-wise manner.
Moreover, existing GNNs can be seamlessly integrated into our framework. Comprehensive experiments show that our proposed learning framework consistently outperforms the existing popular GNN models and even their ensembles. This work focuses on exchanging knowledge of views with different graph augmentations. However, how to generate diverse views for better knowledge exchange is still under exploration, which leaves for future work.
We hope our work can inspire new ideas in exploring new learning mechanisms on the multi-view graphs.

Liang Zeng, Jin Xu, and Jian Li are supported in part by the National Natural Science Foundation of China Grant 62161146004.

[å›¾ç‰‡: images\image_8.png]

[å›¾ç‰‡: images\image_9.png]

