æ ‡é¢˜ï¼šExplicit Feature Interaction-aware Graph Neural Networks

Graph neural networks are powerful methods to handle graph-structured data.
However, existing graph neural networks only learn higher-order feature interactions implicitly.
Thus, they cannot capture information that occurred in low-order feature interactions.
To overcome this problem, we propose Explicit Feature Interaction-aware Graph Neural Network (EFI-GNN), which explicitly learns arbitrary-order feature interactions.
EFI-GNN can jointly learn with any other graph neural network.
We demonstrate that the joint learning method always enhances performance on the various node classification tasks.
Furthermore, since EFI-GNN is inherently a linear model, we can interpret the prediction result of EFI-GNN.
With the computation rule, we can obtain an any-order featureâ€™s effect on the decision.
By that, we visualize the effects of the first-order and second-order features as a form of a heatmap.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1.Feature interactions via hand-crafted features

Domain-specific feature engineering to learn feature interactions plays a significant role in machine learning systems(Wang
etÂ al.,2017).
However, feature engineering requires expertise domain knowledge, much effort, and is time-consuming.
Several methods that automatically learn certain-order feature interactions have been proposed to address this problem(Wang
etÂ al.,2017; Lian
etÂ al.,2018; Kim etÂ al.,2020a).
They show good performance in prediction problems such as recommendation(Lian
etÂ al.,2018)and regression(Kim etÂ al.,2020a).
However, they can only handle tabular data.

Recently, graph-structured data have been widely used in many fields such as citation networks(Kipf and Welling,2017), social networks(Fan
etÂ al.,2019), and natural language processing(Huang
etÂ al.,2019).
Therefore, graph representation learning has become a crucial task in machine learning.
Graph neural networks achieved great success in graph representation learning.
However, conventional graph neural networks only learn higher-order feature interactions implicitly, so they cannot capture information occurring in low-order feature interactions.

Interpreting the decision process of a machine learning model is essential to applying them to high-stakes domains.
Therefore, interpretable artificial intelligence (AI) has been actively studied in recent years(Agarwal etÂ al.,2020).
However, most interpretable models can only learn first-order feature interactions and perform more poorly than complex non-interpretable models.

To address the issues mentioned above, we propose Explicit Feature Interaction-aware Graph Neural Network (EFI-GNN).
EFI-GNN explicitly learns certain-order feature interactions on graphs.
Specifically, each layer of EFI-GNN learns different feature interactions.
For example, the first and second layers of EFI-GNN learn second-order and third-order feature interactions, respectively.
Despite linearity, EFI-GNN does not show significant performance degradation against any other graph neural network on node classification.
Furthermore, EFI-GNN can jointly learn with other graph neural networks to simultaneously learn explicit and implicit feature interactions.
We demonstrate that the joint learning method achieved state-of-the-art results.
In addition, we visualize the first-order and second-order feature interactions learned by EFI-GNN as a form of a heatmap.

The major contributions of this paper are as follows:

EFI-GNN is a linear and interpretable model and explicitly learns feature interactions on graphs. Thus, we can know not only single featuresâ€™ effects but also high-order featuresâ€™ effects using EFI-GNN.

Despite linearity, EFI-GNN does not show any performance degradation against any other graph neural networks. It shows that explicitly learning feature interactions on graphs is effective.

EFI-GNN can jointly learn with other graph neural networks. We demonstrate that the joint learning method achieved state-of-the-art results.

We visualize the first-order and second-order feature interactions learned by EFI-GNN using heat maps.

Feature interaction is the effect of a feature combination on the prediction value.
The effect of one feature may depend on other features.
Thus, learning feature interactions helps improve the performance of a machine learning model.
Traditionally, hand-crafted features are widely used to learn feature interactions.
Figure1shows the linear model that learns three kinds of feature interactions via hand-crafted features.
The model can only learn first-order feature interactions but explicitly generated hand-crafted features allow to learn high-order feature interactions.
However, making hand-crafted features requires expertise domain knowledge, much effort, and is time-consuming.

Deep neural networks aggregate information of all features implicitly.
Thus, deep neural network models can learn higher-order feature interactions and achieve high performance.
However, they are uninterpretable and cannot capture information included in low-order feature interactions.

In recent years, several methods that can learn multiple feature interactions without hand-crafted features have been proposed(He and Chua,2017; He
etÂ al.,2017; Kim etÂ al.,2020b; Kim and Lee,2021).
Factorization Machine (FM)(Rendle,2010)combines linear regression and feature factorization models to simultaneously learn first-order and second-order feature interactions.
Deep neural networks can only learn implicit higher-order feature interactions.
Wide & Deep Learning(Cheng
etÂ al.,2016)is a combination of linear regression and deep neural network models.
Thus, it can simultaneously learn explicit first-order and implicit higher-order feature interactions.
This strategy outperforms existing deep neural network methods.
DeepFM(Guo
etÂ al.,2017)is a combination of FM and deep neural network models.
However, existing explicit methods are restricted.
They can only learn fixed-order feature interactions.
To overcome this problem, multi-layer linear networks have been proposed.
Cross Network(Wang
etÂ al.,2017)applies feature crossing to each layer instead of an activation function.
Thus, it can explicitly learn feature interactions.
The layer depth determines the order of interactions.
Compressed Interaction Network (CIN)(Lian
etÂ al.,2018)and the explicit component of eXtreme Interaction Network (XIN)(Kim etÂ al.,2020a)are improvements of Cross Network with the vector-wise operations.

Graph neural networks are a special form of deep neural networks to handle graph-structured data.
Graph Neural Network (GNN)(Scarselli etÂ al.,2009)extends Recurrent Neural Network (RNN)(Rumelhart
etÂ al.,1986)to make them applicable to more general graphs such as undirected cyclic graphs.
Gated Graph Neural Network (GGNN)(Li
etÂ al.,2016)improve GNN using Gated Recurrent Unit (GRU)(Cho etÂ al.,2014).
These RNN-based graph neural networks can be classified into Recurrent Graph Neural Networks (RGNNs).
Planetoid(Yang
etÂ al.,2016)is a semi-supervised node embedding framework, which is trained to simultaneously predict node class labels and sub-graph contexts using a neural network.
Recently, Graph Convolutional Network (GCN)(Kipf and Welling,2017)and its variants(Hamilton
etÂ al.,2017; VeliÄkoviÄ‡ etÂ al.,2018)have achieved great success in node embedding task.
One problem with spectral methods like GCN is over-smoothing.
To alleviate the over-smoothing problem and train a deep GCN, several methods using residual connections have been proposed(Li
etÂ al.,2019b,2020).
Jumping Knowledge Network (JKNet)(Xu etÂ al.,2018)use the results of all layers of GCN to predict and alleviate the over-smoothing problem.
Sub-graph sampling methods for fast learning and inference of GCN have been proposed(Chen
etÂ al.,2018; Huang
etÂ al.,2018).
Simple Graph Convolutional Network (SGC)(Wu etÂ al.,2019)are a straightforward and linear version of GCN using powers of the adjacency matrix.
Link prediction methods for bipartite graphs using graph neural networks have been proposed(Berg
etÂ al.,2017; Zhang and Chen,2020).
Graph Transformers(Yun
etÂ al.,2019)automatically generate meta-paths and effectively deal with heterogeneous graphs.
All methods mentioned above (except SGCs) implicitly aggregate feature information.
Thus, they can only learn higher-order feature interactions and cannot capture the information occurring in low-order feature interactions.

Recently, several methods for learning feature interaction via graph neural networks have been proposed(Li
etÂ al.,2019a,2021; Liu
etÂ al.,2021; Zheng
etÂ al.,2021).
These methods model features as a graph and learn edge weight on feature graphs using graph neural networks.
The learned edge weights indicate interactions of features.
However, they cannot handle graph-structured data such as citation networks.
FI-GNN(Ding
etÂ al.,2019)is a joint learning method of graph neural network and feature factorization module.
However, the feature factorization module cannot directly handle graph-structured data and only learn second-order feature interactions.
Our proposed EFI-GNN can directly handle graph-structured data and learn arbitrary-order feature interactions.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2.EFI-GNN layer

This section presents Explicit Feature Interaction-aware Graph Neural Network (EFI-GNN) and the joint learning method to combine EFI-GNN with another graph neural network.
In addition, we show that EFI-GNN is an interpretable model.

EFI-GNN consists of multiple layers, and each layer has the feature crossing term.
It allows EFI-GNN to learn high-order feature interactions.
We need the sum of first-order feature interactions before making high-order feature interactions.
The sum of first-order feature interactions are defined as follows:

whereX(iâ€‹nâ€‹iâ€‹t)superscriptXğ‘–ğ‘›ğ‘–ğ‘¡\textbf{X}^{(init)}is the raw feature matrix, andW(0)superscriptW0\textbf{W}^{(0)}is the weight matrix.
An EFI-GNN layer is defined as follows:

whereA^=D~âˆ’1/2â€‹A~â€‹D~âˆ’1/2^Asuperscript~D12~Asuperscript~D12\hat{\textbf{A}}=\tilde{\textbf{D}}^{-1/2}\tilde{\textbf{A}}\tilde{\textbf{D}}^{-1/2},A~=A+I~AAI\tilde{\textbf{A}}=\textbf{A}+\textbf{I},Ais the adjacency matrix,D~~D\tilde{\textbf{D}}is the degree matrix ofA~~A\tilde{\textbf{A}},W(l)superscriptWğ‘™\textbf{W}^{(l)}is the trainable weight matrix inltâ€‹hsuperscriptğ‘™ğ‘¡â„l^{th}layer, andâŠ™direct-product\odotis the Hadamard product.
The adjacency matrix is multiplied to aggregate neighbor nodesâ€™ information, and the sum of first-order feature interactions is element-wise multiplied again in each layer.
Therefore, EFI-GNN can learn the next-order feature interactions considering neighbor nodesâ€™ feature interactions as the number of layers increases.
Note that an EFI-GNN layer has no non-linear transformation like activation function.
Thus, our EFI-GNN is a linear network.
Figure2shows the architecture of an EFI-GNN layer.

We show vector-level representation of an EFI-GNN layer to show how the layer learns feature interactions.
In eq.1,X(0)superscriptX0\textbf{X}^{(0)}is the linear transformation of the raw features.
In other words,X(0)superscriptX0\textbf{X}^{(0)}indicates the sum of first-order feature interactions.
Thus, we can rewrite eq.1in vector level as follows:

wherevğ‘£vis the node index, andMğ‘€Mis the number of raw features.Xv,m(iâ€‹nâ€‹iâ€‹t)â€‹W:,m(0)subscriptsuperscriptXğ‘–ğ‘›ğ‘–ğ‘¡ğ‘£ğ‘šsubscriptsuperscriptW0:ğ‘š\textbf{X}^{(init)}_{v,m}\textbf{W}^{(0)}_{:,m}indicates first-order feature interactions ofmtâ€‹hsuperscriptğ‘šğ‘¡â„m^{th}feature invtâ€‹hsuperscriptğ‘£ğ‘¡â„v^{th}node.
We can know thatXv(0)subscriptsuperscriptX0ğ‘£\textbf{X}^{(0)}_{v}is the sum of feature interactions through eq.3.
In eq.2,X(l)superscriptXğ‘™\textbf{X}^{(l)}is the linear transformation ofX(lâˆ’1)superscriptXğ‘™1\textbf{X}^{(l-1)}with the neighbor aggregation and feature crossing.
The neighbor aggregation allows an EFI-GNN layer to consider neighbor nodesâ€™ information, and the feature crossing allows an EFI-GNN layer to learn next-order feature interactions.
We can rewrite eq.2in vector level as follows:

whereKğ¾Kis the embedding size.Xi,k(lâˆ’1)â€‹W:,k(l)superscriptsubscriptXğ‘–ğ‘˜ğ‘™1subscriptsuperscriptWğ‘™:ğ‘˜\textbf{X}_{i,k}^{(l-1)}\textbf{W}^{(l)}_{:,k}indicates the sum ofltâ€‹hsuperscriptğ‘™ğ‘¡â„l^{th}-order feature interactions initâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}node.
The sum of first-order feature interactions is element-wise multiplied toltâ€‹hsuperscriptğ‘™ğ‘¡â„l^{th}-order feature interactions.
Thus, we can know thatX(l)superscriptğ‘‹ğ‘™X^{(l)}is the(l+1)tâ€‹hsuperscriptğ‘™1ğ‘¡â„(l+1)^{th}-order feature interactions through eq.4.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3.The overall architecture of EFI-GNN

The final output layer of EFI-GNN is defined as follows:

whereLğ¿Lis the number of layers, andâˆ¥parallel-to\parallelis the concatenation operator.
Each layer of EFI-GNN learns different feature interactions.
EFI-GNN predicts the output value with these rich feature interactions to improve performance.
Algorithm1shows the pseudo-code of forward propagation of EFI-GNN.
Figure3shows the overall architecture of EFI-GNN.

Our EFI-GNN can jointly learn with another graph neural network.
We present a joint learning method to learn explicit and implicit feature interactions simultaneously.
In EFI-GNN, each layer learns different feature interactions.
In existing graph neural networks, each layer learns higher-order feature interactions, but the upper layer can capture the complex pattern of the data more than the lower layer.
Both EFI-GNN and graph neural networks capture the different data patterns in each layer.
Therefore, using the results of all layers of EFI-GNN and graph neural networks is helpful for prediction performance.
We define the final representation matrix of EFI-GNN as follows:

whereâˆ¥parallel-to\parallelis the concatenation operator andLeâ€‹fâ€‹isubscriptğ¿ğ‘’ğ‘“ğ‘–L_{efi}is the number of layers of EFI-GNN.
Similarly, we define the final representation matrix of a graph neural network as follows:

whereLgâ€‹nâ€‹nsubscriptğ¿ğ‘”ğ‘›ğ‘›L_{gnn}is the number of layers of a graph neural network.
The final output layer of the joint learning method is defined as follows:

whereW(oâ€‹uâ€‹t)superscriptWğ‘œğ‘¢ğ‘¡\textbf{W}^{(out)}is the trainable output matrix.

Input:

normalized adjacency matrixA^^A\hat{\textbf{A}}

raw feature matrixX(iâ€‹nâ€‹iâ€‹t)superscriptXğ‘–ğ‘›ğ‘–ğ‘¡\textbf{X}^{(init)}

target labelsY

number of layersLğ¿L

Parameter:

trainable weightsW(0)superscriptW0\textbf{W}^{(0)},â€¦,W(L)superscriptWğ¿\textbf{W}^{(L)},W(oâ€‹uâ€‹t)superscriptWğ‘œğ‘¢ğ‘¡\textbf{W}^{(out)}

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: (a)EFI-GNNâˆ—

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: (a)EFI-GNNâˆ—

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: (a)EFI-GNNâˆ—

Since EFI-GNN is an inherently linear model, we can obtain the value of feature interactions in a layer if the layer is connected to the output layer.
It shows that our EFI-GNN is an interpretable model.
Interestingly, most interpretable or explainable methods sacrifice their performance for interpretability or explainability, while EFI-GNN is not.
EFI-GNN shows almost the same performance as Graph Convolutional Network (GCN).
Furthermore, combining EFI-GNN with another graph neural network can make higher performance.
We define the process for obtaining a feature interaction value.
The value of a first-order feature interaction is defined as follows:

wherean,i(1)subscriptsuperscripta1ğ‘›ğ‘–\textbf{a}^{(1)}_{n,i}is the representation vector of the first-order featureiğ‘–iin the nodenğ‘›n,Wi(0)subscriptsuperscriptW0ğ‘–\textbf{W}^{(0)}_{i}is the embedding weight vector of the featureiğ‘–i,W:u,c(oâ€‹uâ€‹t)subscriptsuperscriptWğ‘œğ‘¢ğ‘¡:absentğ‘¢ğ‘\textbf{W}^{(out)}_{:u,c}is the output weight vector of the classcğ‘cindexed by:u:absentğ‘¢:u,uğ‘¢uis the number of units in the layer,an,i(1)subscriptsuperscripta1ğ‘›ğ‘–\textbf{a}^{(1)}_{n,i}indicates the representation vector of the featureiğ‘–iin the nodenğ‘›n, andec,n,i(1)superscriptsubscriptğ‘’ğ‘ğ‘›ğ‘–1e_{c,n,i}^{(1)}indicates the direct effect of the first-order featureiğ‘–iin the nodenğ‘›nfor the classcğ‘c.
Similarly, the value of a second-order feature is defined as follows:

wherean,i,j(2)superscriptsubscriptağ‘›ğ‘–ğ‘—2\textbf{a}_{n,i,j}^{(2)}is the representation vector of the second-order featureiÃ—jğ‘–ğ‘—i\times j, andec,n,i,jsubscriptğ‘’ğ‘ğ‘›ğ‘–ğ‘—e_{c,n,i,j}is the direct effect of the second-order featureiÃ—jğ‘–ğ‘—i\times jfor the classcğ‘c.
Therefore, we can generalize the above equations as follows:

We compare the proposed EFI-GNN and the joint learning method with other state-of-the-art models using three representative citation network datasets, Cora, Citeseer, and PubMed.
In addition, we examine whether explicit feature interactions learned by EFI-GNN are effective for node embeddings using two Open Graph Benchmark (OGB)(Hu etÂ al.,2020)datasets.
Table1shows the statistics of the experimental datasets.
These five datasets are citation network datasets.
The goal of each dataset is to predict the class label of the nodes (i.e.paper).
Especially, ogbn-mag is a heterogeneous network dataset.
Thus, we made the homogeneous network using only (paper, cites, paper) relationships.
Cora, CiteSeer, PubMed, and obgn-mag are undirected networks, and ogbn-arxiv is a directed network.
Their features are the representations of the words.
In Cora and CiteSeer, the features of a node represent the zero/one encoding vector indicating the absent/present words.
In PubMed, the features of a node represent the TF/IDF score vector of the words.
In ogbn-arxiv and obgn-mag, the features of a node represent the average of the word2vec vectors of the present words.
All experimental datasets are publicly open.
The training/validation/test data for Cora, CiteSeer, and PubMed are split as the same scenario as(Chen
etÂ al.,2018), and training/validation/test data for ogbn-arxiv and ogbn-mag are split as the same scenario as public OGB test.

We implemented the proposed EFI-GNN and the joint learning method using PyTorch(Paszke
etÂ al.,2019)and PyTorch Geometric(Fey and Lenssen,2019).
We use L2 penalty and Dropout(Srivastava etÂ al.,2014)for all datasets.
Especially, we use Batch Normalization(Ioffe and Szegedy,2015)for PubMed, ogbn-arxiv and ogbn-mag additionally.
For the fast approximation, we train the model with Residual Connections(He
etÂ al.,2016)on PubMed.
For Cora, CiteSeer and PubMed, we optimize the other hyper-parameters such as learning rate using the greedy search algorithm.
Only the validation data was used for hyper-parameter optimization.
For a fair comparison, we used the same model structure and hyper-parameter setting on ogbn-arxiv and ogbn-mag across all models.
The model has trained over 200 epochs on Cora, CiteSeer, and PubMed, and 1000 epochs on ogbn-arxiv and ogbn-mag.
We use Cross-Entropy as the loss function and optimize the trainable parameters using Adam optimizer.

We compare the EFI-GNN with existing state-of-the-art methods using three representative citation network datasets, Cora, CiteSeer, and PubMed.
Table2shows the test accuracies of each model.
The reported accuracies of the existing graph neural networks are reproduced.
We report the average accuracies of experimental methods with ten random seeds.
All experimental methods concatenate all layers to make predictions.
Although our EFI-GNN is a linear model, it shows almost matched performance with GCN.
In addition, the joint learning method of GCN and EFI-GNN achieved state-of-the-art results in CiteSeer and PubMed.
The joint learning method of GCNII with EFI-GNN achieved the state-of-the-art result in Cora.
It shows that the feature interactions learned by EFI-GNN are effective.
Note that we did not use any sampling method during the training and prediction phase of the experiment models.

We conduct additional experiments using two OGC datasets to examine whether explicit feature interactions learned by EFI-GNN are effective.
Table3shows the test accuracies of the experimental methods.
GCNâˆ—and EFI-GNNâˆ—only use the last layer to make predictions and learn implicit higher-order and explicit higher-order feature interactions.
GCN and EFI-GNN use the concatenate of all layers to make predictions and respectively learn multiple implicit higher-order and explicit various-orders feature interactions.
GCN & EFI-GNN is the joint learning method of GCN and EFI-GNN.
It can simultaneously learn multiple implicit higher-order and explicit various-orders feature interactions.
Figure4shows the brief architecture of EFI-GNNâˆ—, EFI-GNN, and GCN & EFI-GNN
For a fair comparison, we use GCN & GCN, which is the joint learning method of two GCN, and EFI-GNN & EFI-GNN, which is the joint learning method of two EFI-GNNs.
They have the same number of trainable parameters as GCN & EFI-GNN.
GCN and EFI-GNN outperform GCNâˆ—and EFI-GNNâˆ—, respectively.
It shows that simultaneously learning multiple feature interactions is effective for node representation learning.
Furthermore, GCN & EFI-GNN outperforms GCN & GCN and EFI-GNN & EFI-GNN.
It indicates that simultaneously learning both explicit and implicit feature interactions is more effective than learning only explicit or implicit feature interactions.

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: (a)first-order features

[å›¾ç‰‡: images\image_8.png]
å›¾ç‰‡è¯´æ˜: (a)first-order features

Our EFI-GNN is a linear and interpretable model.
Thus, we can compute arbitrary-order feature interactions learned by EFI-GNN.
The computed value of a feature interaction indicates the direct effect of a feature on the prediction value.
We visualize the first-order and second-order feature interactions of the node111for the class111in the Cora dataset.
Figure5shows the feature interaction value of the first-order and second-order features.
The node has nine activated features.
Thus, we only visualize the interactions for these nine features.
In the figure, a red cell indicates a feature with a positive effect, and a blue cell indicates a feature with a negative effect.
The positive features increase the probability that the node111belongs to the class111.
Contrary, the negative features decrease the probability that the node111belongs to the class111.

We proposed a novel graph-based feature interaction method named EFI-GNN.
EFI-GNN can learn arbitrary-order feature interactions explicitly without any manually crafted features on graphs and is a linear and interpretable model.
Although EFI-GNN is a linear model, it does not shows performance degradation compared to Graph Convolutional Network (GCN) on node classification.
Furthermore, EFI-GNN can jointly learn with another graph neural network.
The join learning method achieved state-of-the-art results.
We visualized the first-order and second-order feature interactions learned by EFI-GNN.
The value of any-order feature interaction can be computed with our interpretability computation rule.
Our EFI-GNN can alleviate efforts about domain-specific feature engineering.
Although we have only experimented on node classification tasks, we expect EFI-GNN will show performance enhancement in fields that need expert domain-specific feature engineering, such as recommendation.
High-stakes domains such as healthcare and bioinformatics need interpretable machine learning models.
We can obtain the effects of any-order features with EFI-GNN.
The interpretability of EFI-GNN can help apply machine learning models to high-stakes domains.

We optimize the hyper-parameters of experimental models with the greedy search algorithm.
Table4shows the hyper-parameter settings on Cora, CiteSeer, and PubMed.
For a fair comparison, we use the same hyper-parameter setting on OGB datasets.
Table5shows the hyper-parameter setting on OGB datasets.

We showed that our EFI-GNN is an interpretable AI model and visualized the feature interactions.
We additionally visualize first-order and second-order feature interactions with different hyper-parameter settings.
Figure6, Figure7, and Figure8are the visualizations of the first-order feature interactions on the different number of units, layers, and different seeds, respectively.
The first-order feature interactions have the same trend.
It indicates that EFI-GNN pays attention to the same features across the different hyper-parameter settings and shows EFI-GNN is reliable.
Figure9is the visualization of the second-order feature interactions on the different numbers of units.
Unlike first-order feature interactions, it is hard to find any patterns or trends in second-order feature interactions.
We anticipate that the node aggregation in an EFI-GNN layer causes this problem.
An EFI-GNN layer aggregates the features of neighboring nodes.
This may make it more challenging to capture stable patterns of feature interactions.
Our future work is to find out exactly why this problem is caused.

[å›¾ç‰‡: images\image_9.png]
å›¾ç‰‡è¯´æ˜: (a)Units 32

[å›¾ç‰‡: images\image_10.png]
å›¾ç‰‡è¯´æ˜: (a)Units 32

[å›¾ç‰‡: images\image_11.png]
å›¾ç‰‡è¯´æ˜: (a)Units 32

[å›¾ç‰‡: images\image_12.png]
å›¾ç‰‡è¯´æ˜: (a)Units 32

[å›¾ç‰‡: images\image_13.png]
å›¾ç‰‡è¯´æ˜: (a)Units 32

[å›¾ç‰‡: images\image_14.png]
å›¾ç‰‡è¯´æ˜: (a)Layers 1

[å›¾ç‰‡: images\image_15.png]
å›¾ç‰‡è¯´æ˜: (a)Layers 1

[å›¾ç‰‡: images\image_16.png]
å›¾ç‰‡è¯´æ˜: (a)Layers 1

[å›¾ç‰‡: images\image_17.png]
å›¾ç‰‡è¯´æ˜: (a)Layers 1

[å›¾ç‰‡: images\image_18.png]
å›¾ç‰‡è¯´æ˜: (a)Layers 1

[å›¾ç‰‡: images\image_19.png]
å›¾ç‰‡è¯´æ˜: (a)Seed 1

[å›¾ç‰‡: images\image_20.png]
å›¾ç‰‡è¯´æ˜: (a)Seed 1

[å›¾ç‰‡: images\image_21.png]
å›¾ç‰‡è¯´æ˜: (a)Seed 1

[å›¾ç‰‡: images\image_22.png]
å›¾ç‰‡è¯´æ˜: (a)Seed 1

[å›¾ç‰‡: images\image_23.png]
å›¾ç‰‡è¯´æ˜: (a)Seed 1

[å›¾ç‰‡: images\image_24.png]
å›¾ç‰‡è¯´æ˜: (a)Units 32

[å›¾ç‰‡: images\image_25.03225v2]
å›¾ç‰‡è¯´æ˜: (a)Units 32

[å›¾ç‰‡: images\image_26.png]
å›¾ç‰‡è¯´æ˜: (a)Units 32

[å›¾ç‰‡: images\image_27.png]
å›¾ç‰‡è¯´æ˜: (a)Units 32

[å›¾ç‰‡: images\image_28.png]
å›¾ç‰‡è¯´æ˜: (a)Units 32

[å›¾ç‰‡: images\image_29.png]

[å›¾ç‰‡: images\image_30.png]

