###传统GNN又慢又贵？试试G-CoS带来的“结构+硬件”协同加速




论文《G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency》提出了一种全新的图神经网络（GNN）与加速器协同搜索框架——G-CoS，旨在同时提升GNN的推理精度与硬件效率。尽管GNN已成为处理图结构数据的最先进方法，其在大规模图数据上的推理成本依旧极高，严重限制了其在实际应用中的推广。为此，G-CoS尝试打破当前算法与硬件各自为政的优化方式，通过统一的协同搜索机制，自动寻找最适配的GNN结构与硬件加速器配置，以实现高精度与高效率的双重最优化。G-CoS包含两个关键组成部分：（1）一个通用的GNN加速器搜索空间，兼容多种GNN结构；（2）一种一轮式的协同搜索算法，能够同时并高效地完成结构与加速器的联合搜索。实验表明，G-CoS在Cora等数据集上仅需几小时即可完成搜索，其生成的GNN结构和加速器在精度和效率方面均优于现有最先进的方案。

在深入分析GNN推理高成本的原因时，作者指出三大瓶颈：图规模巨大且邻接关系复杂导致运算量剧增、真实图的幂律分布使邻接矩阵高度不规则、节点特征维度极高（如CiteSeer中达3703维）。为应对这些挑战，已有方法从算法压缩（如剪枝、低精度计算）与加速器设计（如HyGCN、AWB-GCN等）两个方向出发，提升了GNN在单一方面的效率。然而，这些方法未能解决结构与硬件之间的适配性问题。受CNN领域中算法与加速器协同搜索（co-search）研究的启发，G-CoS首次将此思想扩展到GNN领域，提出了可自动探索多种GNN结构及对应加速器的框架，推动了GNN高效推理的系统化设计。

在具体实现上，G-CoS以空间域的GNN为重点研究对象，如GraphSAGE、GAT、GCN和GIN，并构建统一的特征表示和硬件建模框架。在建模上，图可表示为 $G=(V, E)$，其中 $v_i \in V$，$(v_i, v_j) \in E$，节点数为 $N=|V|$，边数为 $M=|E|$。节点度数为 $d=\{d_1, d_2, \cdots, d_N\}$，对应度矩阵为对角矩阵 $D$，图的邻接关系通过邻接矩阵 $A \in \mathbb{R}^{N\times N}$ 表示。每一层 $l$ 的节点特征为 $\{x_1^{(l)}, x_2^{(l)}, \cdots, x_N^{(l)}\} = X^{(l)} \in \mathbb{R}^{N\times F_{(l)}}$，其中 $F_{(l)}$ 为特征维度。GNN层的计算可形式化为：

$$
\hat{A} = D^{-\frac{1}{2}} A D^{-\frac{1}{2}}, \quad \text{输出} = \text{ACT}_{(l)}([\hat{A}X^{(l)}]W^{(l)}), \quad W^{(l)} \in \mathbb{R}^{F_{(l)} \times K_{(l)}}
$$

其中 $\hat{A}X^{(l)}$ 为 Aggregation，相当于邻居特征聚合操作；$[\hat{A}X^{(l)}]W^{(l)}$ 为 Combination，对聚合结果进行变换以获得新的表示。在最终分类层，通常应用 softmax 函数：

$$
\textit{softmax}(x_i^{(l)}) = \frac{\text{exp}(x_i^{(l)})}{\sum_{i} \text{exp}(x_i^{(l)})}
$$

并使用交叉熵损失函数进行训练，其中 $\mathcal{Y}_{N}$ 为有标签节点集合，$Y_{nf}$ 表示真实标签，$\Theta_{nf}$ 表示预测概率。

G-CoS框架结合了图神经结构搜索（GNAS）与硬件感知神经架构搜索（HA-NAS）的优势。尽管NAS和HA-NAS在CNN领域取得显著成果，但由于GNN结构异质性强、数据访问不规律，现有方法难以适应。因此，G-CoS通过定义更通用的搜索空间并引入高效的一轮式搜索机制，在不牺牲性能的前提下大幅降低了搜索成本。此外，G-CoS还对比分析了主流GNN结构（如GCN、GAT、GIN、GraphSAGE）在聚合函数、注意力机制和采样方法上的差异，进一步增强了系统的通用性和可拓展性。

综上，G-CoS是首个支持GNN结构与加速器协同搜索的系统框架，在硬件设计与算法建模之间架起桥梁，为高效图神经网络推理提供了新的范式。在精度与效率的双重提升下，其有望在未来大规模图计算任务中发挥关键作用。
