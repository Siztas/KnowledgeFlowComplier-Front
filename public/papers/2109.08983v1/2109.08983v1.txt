æ ‡é¢˜ï¼šG-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency

Graph Neural Networks (GNNs) have emerged as the state-of-the-art (SOTA) method for graph-based learning tasks. However, it still remains prohibitively challenging to inference GNNs over large graph datasets, limiting their application to large-scale real-world tasks. While end-to-end jointly optimizing GNNs and their accelerators is promising in boosting GNNsâ€™ inference efficiency and expediting the design process, it is still underexplored due to the vast and distinct design spaces of GNNs and their accelerators. In this work, we propose G-CoS, a GNN and accelerator co-search framework that can automatically search for matched GNN structures and accelerators to maximize both task accuracy and acceleration efficiency. Specifically, G-CoS integrates two major enabling components: (1)a generic GNN accelerator search spacewhich is applicable to various GNN structures and (2)a one-shot GNN and accelerator co-search algorithmthat enables simultaneous and efficient search for optimal GNN structures and their matched accelerators. To the best of our knowledge, G-CoS is the first co-search framework for GNNs and their accelerators. Extensive experiments and ablation studies show that the GNNs and accelerators generated by G-CoS consistently outperform SOTA GNNs and GNN accelerators in terms of both task accuracy and hardware efficiency, while only requiring a few hours for the end-to-end generation of the best matched GNNs and their accelerators.

Graph neural networks (GNNs)[1]have gained an increased popularity recently as they demonstrated the state-of-the-art (SOTA) performance in various graph-based learning tasks, including node classification[1],
graph classification[2], and recommendation systems[3]. However, GNNs often suffer from prohibitive inference cost, limiting their potential to handle large-scale real-world graph applications.
For example, a 2-layer GNN model requires 19G FLOPs (FLOPs: floating point operations) to inference the Reddit graph[4], which requires a latency of2.942.942.94E+555milliseconds when being executed on an Intel Xeon E5-2680 CPU platform[5], i.e., its required FLOPs and latency are 2Ã—\timesand 5000Ã—\timesof a 50-layer convolutional neural network (CNN), ResNet-50[6].

The giant computational cost of GNN inference results from three aspects.First, graphs are often very large as exacerbated by their intertwined complex neighbor connections, e.g., a total of 232,965 nodes in the Reddit graph with each node having about 50 neighbors[7].Second, real-world graphs tend to follow the power-law distribution and therefore have highly irregular adjacent matrices, resulting in prohibitive inefficiencies in both data processing and movements.Third, the dimension of GNNsâ€™ node feature vectors can be very high, e.g., each node in the CiteSeer graph has 3703 features.

To tackle GNNsâ€™ prohibitive inference cost, various efficient GNN inference techniques have been developed. On the algorithm level, several pioneering GNN compression techniques have been developed. For instance, two concurrent GNN pruning works[8,9]aim to sparsify the connections in GNNsâ€™ graph adjacent matrices; and[4]for the first time shows the feasibility of adopting 8-bit integer arithmetic for GNN inference without sacrificing the accuracy. Another paralleled trend is to search for efficient GNN architectures[10,11]. On the hardware level, various GNN accelerators have been proposed. For example, HyGCN[12]proposes hybrid execution patterns of GNNs for leveraging their intra-vertex and inter-vertex parallelisms to handle the irregularity in the aggregation phase and reusability in the combination phase, respectively;
Later, AWB-GCN[5]identifies the workload imbalance problem in the aggregation phase, and proposes auto-tuning workload balancing techniques, achieving an average speedup of 7.4Ã—\timesover HyGCN. On the development tool level, pioneering works have attempted to characterize the design space of dataflows and micro-architectures for GNN accelerators[13], and develop an automated framework to generate GNN accelerators[14].

Despite the promising performance of existing efficient GNN inference solutions, their achievable efficiency is still not sufficient for enabling extensive GNN inference applications due to GNNsâ€™ extremely dynamic and irregular data accesses and thus excessive acceleration cost. Motivated by the great success of algorithm-accelerator co-exploration works for CNN accelerations[15,16,17,18,19,20,21], this work targetsto co-optimize both the GNN structures and their accelerators with boosted development speed, and makes the following contributions:

We propose G-CoS, a GNN and accelerator co-search framework that can automatically search for the matched GNN structures and accelerators to maximize both task accuracy and acceleration efficiency. To the best of our knowledge, G-CoS is the first co-search framework for GNNs and their accelerators.

G-CoS integrates two enabling components: (1)a generic GNN accelerator search spacewhich is applicable to various GNN structures and (2)a one-shot GNN and accelerator co-search algorithmthat enables simultaneous and efficient search for optimal GNN structures and their matched accelerators, both of which can facilitate the algorithmic exploration of efficient GNN solutions.

Extensive hardware/algorithm experiments and ablation studies validate G-CoSâ€™s effectiveness and advantage: G-CoS generated networks/accelerators consistently outperform SOTA GNNs/accelerators, while requiring only a few hours for the end-to-end search,(e.g., 4 GPU hours for the Cora dataset).

Graph neural networks (GNNs).GNNs have achieved great success in graph-based learning tasks[22,23]. Depending on their graph representation domains, GNNs can be categorized into spectral and spatial GNNs. Specifically, spectral GNNs model the representation in the graph Fourier transform domain based on eigen-decomposition and usually handle the whole graph simultaneously[24,25]. However, it becomes impractical for spectral GNNs to process large graphs and difficult for them to take advantage of parallel processing[10,26]. On the other hand, spatial GNNs[7,27], which directly perform the computation in the graph domain by aggregating the neighbor nodesâ€™ features, have undergone rapid development. Recently,[28]introduced an attention mechanism to further improve the performance of spatial GNNs; and[29]utilized mini-batch training to improve GNNsâ€™ scalability to handling large graphs. Combined with sampling strategies, the whole graph is no longer required during aggregation, leaving much room for potential hardware acceleration[7,30]. Therefore, in this work, we will primarily focus on the spatial GNNs for their advantages on scalability and potential hardware acceleration.

Graph neural architecture search (GNAS).Neural architecture search (NAS) has become a popular approach to designing neural networks[31,32,33], which can significantly relieve human efforts from manually designing complex networks. The recent NAS success and the large distinction among GNN structures for different tasks have motivated the use of NAS for GNNs (denoted as GNAS). For example,[10,34]used reinforcement learning (RL) methods along with parameter sharing to efficiently search for GNNs;[35]adopted an evolutionary search algorithm; and[36]proposed a more generic GNN design space and a standardized evaluation method for GNNs across various graph learning tasks. Despite the preliminary success, existing works still heavily rely on excessive rounds of sampling and retraining, limiting their scalability to more generic search spaces.

Hardware-aware architecture search (HA-NAS).To ensure the searched networksâ€™ hardware efficiency, hardware-aware NAS (HA-NAS) proposes to incorporate hardware metrics, e.g., the latency on mobile phones, into the search process. Early works, e.g.,[37,38,39], utilized RL-based methods, and thus suffered from substantial search time and costs, limiting their scalability to larger and more diverse search spaces. Inspired by DARTS[40], differentiable HA-NAS[41,42,43,21]has emerged to greatly improve both the search and hardware efficiency.
However, restricted by the large difference among different GNN structures and thus the difficulty for hardware acceleration, HA-NAS targeting GNNs has rarely been explored. Furthermore, existing HA-NAS methods have not yet fully explored the hardware design space. As the acceleration efficiency is determined by both the network structures and their accelerators, it is thus desirable to jointly search for both the networks and their accelerators.

GNN inference accelerators.GNNsâ€™ ultra-sparse graph matrices, corresponding to extremely dynamic and irregular data accesses as well as distinct execution patterns from DNNs, have fueled a growing interest in developing dedicated GNN accelerators[44]. For instance, HyGCN[12]explored both intra/inter-vertex parallelisms to separately handle the irregularity in the aggregation phase and reusability in the combination phase.
Later, aiming to boost the overall hardware utilization, AWB-GCN[5]proposed to balance the workload during runtime with an auto-tuning algorithm and to increase the data locality by regionally clustering the non-zero values (i.e., connected neighbors) within the adjacency matrices; EnGN[45]proposed a ring-edge-reduce dataflow to handle graphs with arbitrary dimensions and increase the acceleratorâ€™s scalability to large graphs; and GRIP[46]employed fine-grained vertex-tiling to reduce the weight bandwidth requirements;
In parallel, to reduce the human efforts in designing GNN accelerators and democratize the process, pioneering works have attempted to characterize the design space of dataflows and micro-architectures for GNN accelerators[13], and developed an automated framework to generate GNN accelerators[14]. Nevertheless, existing automated frameworks for GNNs still have limited support to various GNN structures and thus suffer from low hardware utilization and achievable efficiency on certain tasks.

Software/Hardware Co-exploration.Jointly exploring the networks and their accelerators has shown promising results[15,16,17,18,19,20,21,47].
For instance,[16,20]conducted RL-based search to jointly optimize the networks and some design parameters of FPGA-based accelerators;[15]developed the first differentiable network and accelerator co-search framework to boost both the task accuracy and acceleration efficiency; and[18]co-searches for networks, bitwidths, and accelerators to achieve superior performance. However, co-optimizing the GNN structures and their accelerators has not been studied.

A typical GNN graph can be represented as,G=(V,E)ğºğ‘‰ğ¸G=(V,E), whereviâˆˆVsubscriptğ‘£ğ‘–ğ‘‰v_{i}\in Vand(vi,vj)âˆˆEsubscriptğ‘£ğ‘–subscriptğ‘£ğ‘—ğ¸(v_{i},v_{j})\in Edenote the nodes and edges, respectively; andN=|V|ğ‘ğ‘‰N=|V|andM=|E|ğ‘€ğ¸M=|E|denote the total number of nodes and edges, respectively. The node degree is denoted asd={d1,d2,â‹¯,dN}ğ‘‘subscriptğ‘‘1subscriptğ‘‘2â‹¯subscriptğ‘‘ğ‘d=\{d_{1},d_{2},\cdots,d_{N}\}wheredisubscriptğ‘‘ğ‘–d_{i}indicates the number of neighbors connected to nodevisubscriptğ‘£ğ‘–v_{i}. We defineDğ·Das the degree matrix whose diagonal elements are formed usingdğ‘‘d. The connectivity information is encoded within the adjacency matrixAâˆˆâ„NÃ—Nğ´superscriptâ„ğ‘ğ‘A\in\mathbb{R}^{N\times N}, where the non-zero entries represent the existed connections among different nodes. For each layerlğ‘™lof a GNN, the nodes are encoded by their feature vectors{x1(l),x2(l),â‹¯,xN(l)}=X(l)superscriptsubscriptğ‘¥1ğ‘™superscriptsubscriptğ‘¥2ğ‘™â‹¯superscriptsubscriptğ‘¥ğ‘ğ‘™superscriptğ‘‹ğ‘™\{x_{1}^{(l)},x_{2}^{(l)},\cdots,x_{N}^{(l)}\}=X^{(l)}, whereX(l)âˆˆâ„NÃ—F(l)superscriptğ‘‹ğ‘™superscriptâ„ğ‘subscriptğ¹ğ‘™X^{(l)}\in\mathbb{R}^{N\times F_{(l)}}andF(l)subscriptğ¹ğ‘™F_{(l)}denotes the feature dimension used to encode the nodes at layerlğ‘™l. Thus, a GNN layer[24]can be formulated as:

whereA^^ğ´\hat{A}is a normalized version ofAğ´A:A^=Dâˆ’12â€‹Aâ€‹Dâˆ’12^ğ´superscriptğ·12ğ´superscriptğ·12\hat{A}=D^{-\frac{1}{2}}AD^{-\frac{1}{2}},ACT(l)subscriptACTğ‘™\text{ACT}_{(l)}represents the activation function of layerlğ‘™landW(l)âˆˆâ„F(l)âˆ—K(l)superscriptğ‘Šğ‘™superscriptâ„subscriptğ¹ğ‘™subscriptğ¾ğ‘™W^{(l)}\in\mathbb{R}^{F_{(l)}*K_{(l)}}represents the weights in layerlğ‘™l, withK(l)subscriptğ¾ğ‘™K_{(l)}denoting the hidden/weight dimension at layerlğ‘™l.
The whole GNN inference can thus be viewed as two separated phases:AggregationandCombination.

AggregationA^â€‹X(l)^ğ´superscriptğ‘‹ğ‘™\hat{A}X^{(l)}: For each node in the graph, a GNN aggregates its 1-hop neighbor nodesâ€™ features into a unified feature vector, corresponding to the multiplication between the adjacent and feature matrix, i.e.,A^â€‹X^ğ´ğ‘‹\hat{A}X.

Combination[A^â€‹X(l)]â€‹W(l)delimited-[]^ğ´superscriptğ‘‹ğ‘™superscriptğ‘Šğ‘™[\hat{A}X^{(l)}]W^{(l)}: The aggregated feature vector,A^â€‹X(l)^ğ´superscriptğ‘‹ğ‘™\hat{A}X^{(l)}, will be further transformed to another feature vector via an MLP network (shared among nodes) with weightsW(l)superscriptğ‘Šğ‘™W^{(l)}for learning better representations, corresponding to the multiplication between the aggregated feature matrix and weight matrix, i.e.,[A^â€‹X(l)]â€‹W(l)delimited-[]^ğ´superscriptğ‘‹ğ‘™superscriptğ‘Šğ‘™[\hat{A}X^{(l)}]W^{(l)}.

In the final/prediction layer of GNNs, after the feature vectorsâ€™ update, a softmax function is usually applied in a row-wise manner, i.e.,softmaxâ€‹(xi(l))=expâ€‹(xi(l))/âˆ‘iexpâ€‹(xi(l))softmaxsuperscriptsubscriptğ‘¥ğ‘–ğ‘™expsuperscriptsubscriptğ‘¥ğ‘–ğ‘™subscriptğ‘–expsuperscriptsubscriptğ‘¥ğ‘–ğ‘™\textit{softmax}(x_{i}^{(l)})=\text{exp}(x_{i}^{(l)})/\sum_{i}\text{exp}(x_{i}^{(l)})[24]. For semi-supervised multiclass classification, the loss function captures the cross-entropy errors over all labeled examples:

whereğ’´Nsubscriptğ’´ğ‘\mathcal{Y}_{N}is the set of node indices that have labels,Ynâ€‹fsubscriptğ‘Œğ‘›ğ‘“Y_{nf}is the ground truth label matrix, andÎ˜nâ€‹fsubscriptÎ˜ğ‘›ğ‘“\Theta_{nf}denotes the predicted possibilities of nodenğ‘›nbelonging to classfğ‘“f. During GNN training,W(l)superscriptğ‘Šğ‘™W^{(l)}is updated via gradient descents.

Many advanced GNN variants have recently been proposed to consider different aggregation functions and introduce additional attention modules or sampling functions.
Without loss of generality, we summarize four popular GNN architectures:GCN[24],GAT[28],GIN[48], andGraphSAGE[7]. We analyze the difference among them as compared with vanilla GNNs below,aiming to generally support them in G-CoS.

GCN[24]: During inference, each node can be written asxi(l+1)=âˆ‘jâˆˆğ’©â€‹(i)âˆªi(1diâ€‹djâ€‹W(l)â€‹xj(l))superscriptsubscriptğ‘¥ğ‘–ğ‘™1subscriptğ‘—ğ’©ğ‘–ğ‘–1subscriptğ‘‘ğ‘–subscriptğ‘‘ğ‘—superscriptğ‘Šğ‘™superscriptsubscriptğ‘¥ğ‘—ğ‘™x_{i}^{(l+1)}=\sum_{j\in\mathcal{N}(i)\cup i}(\frac{1}{d_{i}d_{j}}W^{(l)}x_{j}^{(l)}), wherelğ‘™lis the layer index andğ’©â€‹(i)ğ’©ğ‘–\mathcal{N}(i)represents theiğ‘–i-th nodeâ€™s neighbor set. Thus, compared with the vanilla GNNs, the only difference lies in the entries of the adjacency matrix where each node is encoded as1diâ€‹dj1subscriptğ‘‘ğ‘–subscriptğ‘‘ğ‘—\frac{1}{d_{i}d_{j}}andcan be filled offline before the processing.

GAT[28]: An attention module is introduced, i.e.,xi(l+1)=Î±i,iâ€‹W(l)â€‹xi(l)superscriptsubscriptğ‘¥ğ‘–ğ‘™1subscriptğ›¼ğ‘–ğ‘–superscriptğ‘Šğ‘™superscriptsubscriptğ‘¥ğ‘–ğ‘™x_{i}^{(l+1)}=\alpha_{i,i}W^{(l)}x_{i}^{(l)}+âˆ‘jâˆˆğ’©â€‹(i)(Î±i,jâ€‹W(l)â€‹xj(l))subscriptğ‘—ğ’©ğ‘–subscriptğ›¼ğ‘–ğ‘—superscriptğ‘Šğ‘™superscriptsubscriptğ‘¥ğ‘—ğ‘™+\sum_{j\in\mathcal{N}(i)}(\alpha_{i,j}W^{(l)}x_{j}^{(l)}), whereÎ±ğ›¼\alphadenotes the attention coefficients for the neighbor nodes and can be viewed as the elements to replace the original adjacency matrixâ€™s entries. Adapting from the formulation of GAT[28,49],Î±i,jsubscriptğ›¼ğ‘–ğ‘—\alpha_{i,j}can be calculated as:

whereACTdenotes the activation used in the attention module and(w1(l),w2(l))âˆˆ(â„F(l)Ã—1,â„F(l)Ã—1)superscriptsubscriptğ‘¤1ğ‘™superscriptsubscriptğ‘¤2ğ‘™superscriptâ„subscriptğ¹ğ‘™1superscriptâ„subscriptğ¹ğ‘™1(w_{1}^{(l)},w_{2}^{(l)})\in(\mathbb{R}^{F_{(l)}\times 1},\mathbb{R}^{F_{(l)}\times 1})denotes the weights of the attention module. The whole set ofÎ±ğ›¼\alphacan then be calculated by introducing an additional layer of matrix multiplication ofX(l)âˆ—[w1(l)||w2(l)]X^{(l)}*[w_{1}^{(l)}||w_{2}^{(l)}]along with the element-wise activation and multiplication with the original adjacency matrix. Thus,replacing the original adjacency matrix withÎ±ğ›¼\alphacaptures the functionality of the attention module.

GIN[48]: An information-lossless aggregation function is adopted, i.e.,xi(l+1)=MLPâ€‹((1+Ïµ)â€‹xi(l)+âˆ‘jâˆˆğ’©â€‹(i)xj(l))superscriptsubscriptğ‘¥ğ‘–ğ‘™1MLP1italic-Ïµsuperscriptsubscriptğ‘¥ğ‘–ğ‘™subscriptğ‘—ğ’©ğ‘–superscriptsubscriptğ‘¥ğ‘—ğ‘™x_{i}^{(l+1)}=\text{MLP}((1+\epsilon)x_{i}^{(l)}+\sum_{j\in\mathcal{N}(i)}x_{j}^{(l)}), where MLP denotes an MLP network andÏµitalic-Ïµ\epsilonis a learnable constant. AsÏµitalic-Ïµ\epsilonis trained and then fixed during inference, GIN can be realized byfusingÏµitalic-Ïµ\epsiloninto the original adjacency matrix and incorporating an additional MLP layer into the aggregation phase of vanilla GNNs.

GraphSAGE[7]: Uniform neighbor sampling is introduced to alleviate the extreme memory consumption during training, i.e.,xi(l+1)=Meanâ€‹(W(l)â€‹xj(l))superscriptsubscriptğ‘¥ğ‘–ğ‘™1Meansuperscriptğ‘Šğ‘™superscriptsubscriptğ‘¥ğ‘—ğ‘™x_{i}^{(l+1)}=\textit{Mean}(W^{(l)}x_{j}^{(l)}),jâˆˆ{i}âˆª{ğ’®â€‹(i)}ğ‘—ğ‘–ğ’®ğ‘–j\in\{i\}\cup\{\mathcal{S}(i)\}, whereğ’®â€‹(i)ğ’®ğ‘–\mathcal{S}(i)denotes the sampled neighbors, which can be easily supported byintroducing an additional node sampling layer and using mean aggregationto the original GNN formulation as in Eq.1.

[å›¾ç‰‡: images\image_1.jpg]
å›¾ç‰‡è¯´æ˜: Figure 1:FPGA measured Frame-Per-Second (FPS; see the left axis) on a VCU128 FPGA[50]and accuracy on Cora dataset (see the right colorbar) of 300 randomly sampled fully trained GNNs from supernet as defined in Sec.IV-D, when each of the networks is accelerated by 300 randomly sampled accelerators from the accelerator design space (see Sec.IV-C), leading to a total of999E+444randomly sampled data points. Designs withAâ€‹Câ€‹C>0.7ğ´ğ¶ğ¶0.7ACC>0.7andFâ€‹Pâ€‹S>5ğ¹ğ‘ƒğ‘†5FPS>5E+555are circled out in red, which are extremely sparse.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2:An overview of our G-CoS GNN-accelerator co-search framework, where Accel. denotes accelerators.

This section describes our G-CoS framework by first providing an overview and the problem formulation, and then G-CoSâ€™s two major enablers: a generic GNN accelerator design space and network structure design space, followed by G-CoSâ€™s efficient one-shot evolutionary co-search algorithm.

To maximize both task accuracy and hardware efficiency, G-CoS jointly searches for the best matched GNN structures and accelerators, under the specified datasets, resource constraints, and optimizing metrics (e.g., accuracy and latency), as shown in Fig.2.

To enable effective GNN-accelerator co-search, there existthree major challenges, including (1) the prohibitively large and distinct joint space versus very sparse optima excelling at both accuracy and efficiency, as shown in Fig.1, (2) the excessive retraining cost during GNAS, and (3) the lack of either generic GNN structure or accelerator search space description. To tackle the first two aforementioned challenges, G-CoS employsa one-shot evolutionary GNN-accelerator co-search algorithm, as introduced in Sec.IV-E. G-CoS first one-shot pre-trains the proposed GNN supernet to avoid the necessity of cumbersome retraining in the GNN-accelerator co-search phase, and then utilizes an evolutionary search algorithm to efficiently navigate the joint network-accelerator space to locate the optimal GNN-accelerator pairs based on the feedback of the estimated inference accuracy and hardware efficiency. For the third challenge, G-CoS integrates (1)a generic GNN network spacedescription which is compatible with its one-shot search algorithm and (2)a generic GNN accelerator design spacewhich includes accelerators with high hardware utilization across various GNN structures.

G-CoSâ€™s co-optimization process can be formulated as:

whereÏ‰ğœ”\omegadenotes the GNN weights;Ltâ€‹râ€‹aâ€‹iâ€‹nsubscriptğ¿ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›L_{train},Lvâ€‹aâ€‹lsubscriptğ¿ğ‘£ğ‘ğ‘™L_{val}, andLcâ€‹oâ€‹sâ€‹tsubscriptğ¿ğ‘ğ‘œğ‘ ğ‘¡L_{cost}are the task loss during training, task loss during validation, and hardware-cost, respectively, given the GNN structure, the accelerator parameter set, and the specified metrics; andgâ€‹nâ€‹eâ€‹tğ‘”ğ‘›ğ‘’ğ‘¡gnetandhâ€‹wâ„ğ‘¤hware the selected GNN structure and accelerator to be optimized, respectively. Note that the hardware-cost is co-determined by both the GNN structure and accelerator.

To comprehensively cover potential parallelism and reuse opportunities for various GNN structures, we propose a generic GNN accelerator template along with a set of corresponding searchable parameters, forming a design space
featuring a total ofâˆ¼similar-to\sim111E+151515GNN accelerator choices with different micro-architectures and dataflows.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3:An illustration of G-CoSâ€™s accelerator micro-architecture template.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: Figure 4:The choices of kernel modes for each sub-accelerator, where different modes represent different spatial mapping/temporal mapping methods. For better visual clarity, the operation order for each mode is numbered in each sub-figure, and the corresponding properties on off-chip bandwidth consumption, on-chip buffer consumption, and potential parallelism opportunities are summarized on the right corner of each sub-figure.

The micro-architecture overview.To maximize the acceleration throughput while at the same time minimizing the latency, we adopt a multi-accelerator micro-architecture template to accelerate both the combination and aggregation phases, as shown in Fig.3. This template has two overall advantages:

Latency friendly:When working on either of the two phases, all the hardware components will be instantiated and subsequently reused for the other phase, thus reducing the startup latency of GNN inference which would otherwise be much higher if a pipeline structure was employed for the two phases as in[5].

Utilization friendly:Given the workload allocation scheme as introduced later in Sec.IV-C, all sub-accelerators work on different parts of the feature/weight data in parallel; the hardware utilization and latency can thus be further improved as each sub-accelerator is configured to better fit the corresponding data structure and thus can utilize more parallelism/reuse opportunities.

In particular, the aforementioned template is composed of (1) multiple sub-accelerators which are able to handle both the sparse and dense matrix multiplications, (2) the off-chip memory which holds the data that cannot be stored entirely on chip, and (3) a host CPU to manage the states of different sub-accelerators. Each sub-accelerator has local buffers assigned to the intermediate features (i.e., adjacency matrices), the index for sparse features (assuming a COO format[51]), and the weights and intermediate outputs, respectively. The buffers among sub-accelerators can be configured to be inter-connected so that the buffered data can be shared to minimize the costly off-chip memory accesses, as presented in Fig.3.

The GNN parsing & sparsity analysis auxiliaries.Here we briefly describe GNN parsing & sparsity analysis auxiliaries for a given GNN, e.g., from PyG[52].
As shown in Fig.2, a pre-trained GNN choice is first passed through the blocks of GNN parser and sparsity analyzer before being loaded for G-CoSâ€™s automated accelerator generation.
Specifically, (1) the GNN parser helps to extract the GNN structure parameters (e.g., the dimensions of the weights and features) and (2) the sparsity analyzer analyzes the sparsity of each adjacency matrix row. They together help the accelerator design space generator produce all the possible choices and balance the workloads for each sub-accelerator.

Flexible workload allocation.As different GNN structures can have drastically different sparsity patterns and feature/weight dimensions, G-CoS employstwo flexible workload allocation schemesto ensure each sub-acceleratorâ€™s assigned workload better fit its micro-architecture, e.g., the processing element (PE) arrayâ€™s dimensions and tiling sizes, to achieve high hardware utilization and thus efficiency. The main allocation principle is that the assigned workload is proportional to each sub-acceleratorâ€™s capacity which is characterized by its number of PEs.
The two schemes balance the workload with (1)the number of feature (i.e., adjacency matrix) rowswhich will be scaled with the pre-analyzed sparsity of the adjacency matrix and (2)the number of weight columns, respectively.

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: Figure 5:The choices of tiling modes for each sub-accelerator, where the black box represents the temporal tiles, each arrow color represents the first direction that the tile will move towards, and the color gradient represents the output process from start to complete. The three arrow choices favor weight reuses, feature reuses, and output reuses, respectively. For instance, when tiles first move along the red arrow direction, the tiles of the weights can stay stationary.

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: Figure 6:The GNN supernet of the proposed G-CoS which covers a comprehensive range of GNN options and is compatible with one-shot NAS methods.

The sub-accelerator design.Based on G-CoSâ€™s micro-architecture template, the sub-accelerators are auto-generated according to different tiling/kernel modes (as introduced below) to be equipped with different functional components for (1) reflecting different data reuse strategies, (2) favoring different resource trade-offs, and (3) supporting the special operations from various GNN structures, aiming to maximize the hardware efficiency on a wide range of GNNs. Specifically, the sub-accelerators consider:

Tiling modes/sizes:The data per assigned workload may not fit the on-chip memory of each sub-accelerator. As shown in Fig.5, temporal tiling is then enabled to process the workload temporally. With different tiling modes, a sub-accelerator features more reuses of the features (adjacency matrices), weights, and outputs, respectively. The tiling sizes are defined by K, M, and N (see Fig.5).

Kernel modes:As illustrated in Fig.4, with different data mapping and processing patterns for the PE array, each sub-accelerator design would have different off-chip bandwidth consumption, on-chip buffer consumption, and parallelism opportunities.

Moreover, each sub-accelerators is equipped with:

Dedicated Buffersto facilitate local reuse opportunities.

Dense/SpMM Hybrid Enginewhich supports both dense and sparse matrix multiplication within one unit aided with an configurable PE array as in[49].

Element-wise Activation Unitsto process the non-linear activation operations.

Sampling Unitsto schedule the node sampling.

To further increase on-chip reuse opportunities and reduce off-chip accesses, the sub-accelerators can support (1)weight buffer sharingwhich inter-connects all the on-chip weight buffers for weight reuses to reduce off-chip accesses and (2)buffer re-purposingwhere the feature, weight and output buffers are inter-changeable, so no/reduced off-chip accesses are necessary for the intermediate results between the combination and aggregation phases and/or between layers. Considering the controlling complexity and limited on-chip memory size, if either of these two options are enabled, the number of sub-accelerators will be restricted (e.g., 5 in this work).

The searchable accelerator parameters.Based on the accelerator template (see Fig.3), we extract the searchable parameters, of which different combinations lead to different accelerators and form a generic GNN accelerator space to be used by the automated co-search of G-CoS, as summarized in Tab.I. All design choices can be configured differently for each sub-accelerator, except when eitherbuffer re-purposingorweight buffer sharingis enabled. Otherwise, the tiling and kernel modes are fixed for all sub-accelerators for the ease of controlling and scheduling. The tiling size can range from about 10 to 100 for each sub-accelerator, depending on the given GNN structure. Together, these design choices lead to a hardware design space size of111E+101010âˆ¼similar-to\sim111E+151515.

The GNN supernet.To avoid the retraining cost during co-search, G-CoS incorporates a GNN supernet as its GNN design space which is compatible with the adopted one-shot NAS method and able to produce subnetworks covering a comprehensive range of GNN structures, as shown in Fig.6. Specifically, the GNN supernet is composed offiveblocks denoting the attention, combination, sampling, aggregation, and activation blocks. Each block will also have multiple attributes to be determined from a wide range of options as specified in Tab.II. For instance, an attention block may assume a GAT-sym structure and have two heads as in Fig.6; The combination and aggregation blocks share the same attributes for their hidden dimensions; The attention, sampling, and activation blocks all have a â€™skipâ€™ option to cover GNNs devoid of these modules. For better generality, each layer of the GNNs assumes this format of supernet, with attribute choices different among layers. For the final/prediction layer, the hidden dimensions and activations are fixed according to the given dataset. Combining all the possible choices in Tab.II, the GNN supernet in G-CoS is able to produceâˆ¼similar-to\sim111E+999choices for a 2 layer GNN, leading to a joint GNN-accelerator space with more than111E+191919choices.

The subnetwork sampling.The subnetwork is sampled by choosing an option for each attributes of the blocks, e.g., red boxes in Fig.6.
In particular, G-CoS employs uniform random sampling during the pre-training stage, and samples the subnetworks based on the proposed evolutionary algorithm (see Sec.IV-E) during the exploration stage.

To tackle the aforementioned challenge of excessively large GNN-accelerator joint search space and costly retraining rooted in many NAS methods[10,35], we propose to employ an one-shot based search approach as inspired by[53], to decouple the supernet pre-training and GNN-accelerator co-search processes, along with an evolutionary algorithm to efficiently navigate through the large joint space to locate optimal GNN-accelerator design pairs for boosting both task accuracy and hardware efficiency. In particular, we only pre-train the GNN supernet once and only inference on the validation set as needed during the exploration stage. To the best of our knowledge, we are the first to study the effectiveness of one-shot NAS within the scope of GNNs.

The supernet pre-training.During supernet pre-training, a random subnetwork is uniformly sampled from the GNN supernet by selecting the attribute options from each block and then the subnetwork weightsÏ‰ğœ”\omegaare updated via back-propagation. As[53]pointed out, uniform sampling can decouple the weights among possible subnetworks and thus provide a better estimate for their individual fully trained accuracy when these subnetworks are inferenced on the validation dataset during the GNN structure exploration.

The weight sharing.For more effective pre-training, G-CoS adopts a weight sharing strategy as inspired by[53,10]during pre-training, such that different subnetworks share certain slices of weights. In particular, the weights for combination and aggregation will be shared and retrieved according to the chosen # of hidden dimensions when a subnetwork is sampled. For the attention block, only the options belonging to the same attention types share their weights and are retrieved according to the number of attention heads.

The evolutionary co-search algorithm.As illustrated in Alg.1, a specially tailored evolutionary algorithm is developed to efficiently search for best satisfied GNN-accelerator pairs, characterized by(gâ€‹nâ€‹eâ€‹t,hâ€‹w)ğ‘”ğ‘›ğ‘’ğ‘¡â„ğ‘¤(gnet,hw). Overall, it operates by constantly generating new designs around the good design options stored in a poolPğ‘ƒP, filtering out the inferior designs, and then outputting the topN2subscriptğ‘2N_{2}performing designs. Specifically, the algorithm inputs include: (1) the fitness function fit(), which evaluates the designs given the GNN-accelerator specs(gâ€‹nâ€‹eâ€‹t,hâ€‹w)ğ‘”ğ‘›ğ‘’ğ‘¡â„ğ‘¤(gnet,hw), (2) mutation function mut() which randomly changes the attributes of good designs to generate new designs, (3) the largest samples pool sizepmâ€‹aâ€‹xsubscriptğ‘ğ‘šğ‘ğ‘¥p_{max}, (4) the birth/dying rate which controls how many designs will be generated/filtered out, and (5) performance targetTğ‘‡Twhich determines the terminating conditions and follows the same units as the fitness function. After search, the algorithm outputs the topN2subscriptğ‘2N_{2}performing designs withinPğ‘ƒP. More input specification choices are provided in Sec.V-A.

In this section, we first introduce the experiment setups in Sec.V-A, and then benchmark the proposed G-CoS with SOTA GCN accelerators, GNAS and handcrafted GNNs in Sec.V-B, Sec.V-Cand Sec.V-D, respectively.

Baselines and datasets.For evaluating G-CoSover SOTA GNN accelerators, we consider three baselines: HyGCN[12], AWB-GCN[5],
and Deepburning-GL[14], respectively. For evaluating G-CoSover SOTA GNAS, we consider three GNAS baselines: GraphNAS[10], Auto-GNN[34], and AutoGraph[35]. For benchmarkingover SOTA handcrafted GNNs, we consider four baselines: GCN[24], GAT[28], LGCN[30], and GraphSAGE[7],covering the most common GNN variants. Our experiments are conducted onfour datasets: three citation graph datasets (Cora, CiteSeer and Pumbed)[54], and the Reddit post dataset[7]), respectively.

GNN training setup.For the GNN training, we follow the same dataset splits as[24,7,55].
The GNN supernet is trained using an Adam optimizer[56]with a learning rate of 0.001 for 1000 epochs, L2 regularization, and dropout. After the design space exploration is finished, the final derived models are trained with additional 400 epochs from scratch under the same configurations.

Evolutionary search algorithm setup.For the evolutionary search algorithm presented in Alg.1, we use a set of generic configurations. Specifically, the fitness function is set as the weighted sum of (inverse)latency and task accuracy, such that latency and task accuracy contributes similarly to the fitness score. The mutation function is set to have a 50% chance, i.e., the selected GNN-accelerator design pairs have half of the randomly picked design attributes changed.The birth/dying rate (sğ‘ s) is set to 0.2.

Hardware experiment setup.To evaluate G-CoSâ€™s generated accelerators, we adopt standard the FPGA evaluation and implementation flows in Vivado 2020.2[57]. For the platform, we picked the Xilinx VCU128 FPGA board[50], which is equipped with 9024 DSPs, 42MB on-chip memory and 460GB/s HBM of-chip memory. For a fair comparison with other baselines, we limit the DSP consumption to be less than 4096 throughout the design. The generated GNN accelerators are clocked at 330MHz and adopt a 16-bit fixed point precision. For the accelerator template, we fix the number of sub-accelerators to 5 (unless otherwise specified). Note that the design principles of G-CoS introduced in Sec.IV-Cis platform agnostic, i.e., G-CoS can be flexibly extended to other platforms such as ASIC. During the design space exploration, to quickly go over numerous design options, we design and implement anin-house performancesimulator to measure the execution time (i.e., the number of cycles).

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: Figure 7:Accuracy vs. Latency of G-CoS over the SOTA GNAS and handcrafted GNNs across Cora, CiteSeer and Pubmed datasets.

[å›¾ç‰‡: images\image_8.png]
å›¾ç‰‡è¯´æ˜: Figure 8:The normalized inference speedups (w.r.t. PyG-CPU) achieved by G-CoS over the four SOTA baseline platforms on three GNN models and four representative graph datasets, where KCU1500 and Alveo U50 are two implementation platforms for Deepburning-GL[14]

In this set of experiments, we compare G-CoS with existing SOTA GNN accelerators: HyGCN[12], AWB-GCN[5], and Deepburning-GL[14], in terms of latency and bandwidth consumption. For a fair comparison, we fix the GNN structures and datasets in G-CoS to be the same as the baselines and only search for the accelerator parameters. Since most of them do not provide absolute performance values while reporting the relative speedups over PyG-CPU on an Intel Xeon E5-2680 v3 CPU instead. We also measure and verify the latency on the same CPUand calculate the FPGA speedups over it, so that PyG-CPU is a common baseline for all methods, and then we can analyze the relative improvements as elaborated below:

(1) Speedup.
As shown in Fig.8, G-CoS achieves an average of 5.52Ã—\times, 1.92Ã—\times, 35.98Ã—\times, and 21.54Ã—\timesspeedups over HyGCN, AWB-GCN, Deepburning-GL-KCU1500, and Deepburning-GL-Alveo U50, respectively. G-CoSâ€™s much reduced latency is mostly attributed to its enabling better hardware(DSP) utilization as the multi-sub-accelerator scheme of G-CoS can better cover GNNsâ€™ high irregularity and the wide range of searchable accelerator parameters make G-CoSâ€™s searched accelerators matching different variation in GNNs.(2) Off-chip memory Bandwidth Consumption. G-CoS only requires an average of 50% off-chip memory bandwidth as compared to HyGCN. The high bandwidth of HyGCN is due to to its high-degree parallelism while G-CoSâ€™s versatile kernel mode as illustrated in Sec.IV-Calleviates such an off-chip bandwidth pressure.

In this set of experiments, we benchmark G-CoS with the SOTA GNAS works: GraphNAS[10], Auto-GNN[34], and AutoGraph[35]based on the metrics of task accuracy and latency. For G-CoS, we co-optimize both the GNN and accelerator design parameters as introduced in Sec.IV-Cand Sec.IV-D, respectively. For a fair comparison, we also accelerate the baselinesâ€™ generated GNNs under the same hardware platform and optimize the corresponding accelerator parameters. To demonstrate the tradeoff between the hardware efficiency and task accuracy, we restricted the baseline generated GNNs with different flops and then select the designs with the lowest latency; for G-CoS, we simply decrease/increase the weighting coefficient of the latency in the search metrics to achieve flexible tradeoffs. The results are presented in Fig.7: G-CoS consistently maintains a better performance frontier, i.e., a higher accuracy and lower latency. In particular, G-CoS achieves a 1.60%, 1.3% and 2.11% increase in task accuracy with a similar or lower latency, and a 2.54Ã—\times, 2.15Ã—\timesand 2.98Ã—\timeslatency reduction with a similar or higher accuracy, as compared with the SOTA GNAS works on the Cora, CiteSeer and Pubmed datasets, respectively. Although, considering flops during the search can offer some guidance for more hardware-friendly GNNs, it can not fully capture the compatibility between the searched GNNs and the specific hardware platform, resulting the searched GNNs which satisfy the flops requirement but might be hard to accelerate. Thus, co-optimizing the GNN-accelerator pairs can excel both in terms of accuracy and hardware efficiency against traditional GNAS works by offering better guidance and fully customized architecture to every single searched network.The entire process takes as low as 4 GPU hours depending on the dataset.

We also compare the performance of the proposed G-CoS against the SOTA handcrafted GNNs: GCN[24], GAT[28], LGCN[30], and GraphSAGE[7]. For G-CoS, we co-optimize the GNN-accelerator design pairs. For the baselines GNNs, we optimize their accelerator parameters for fair comparison. As shown in Fig.7, G-CoSâ€™s generated GNN-accelerator design pairs consistently achieve better accuracy and lower latency at the same time. Specifically, the G-CoS generated designs achieve up to 2.7%, 3.22% and 1.01% increase in accuracy while having 1.91Ã—\times, 1.08Ã—\timesand 1.19Ã—\timesreduction in latency.

We propose G-CoS, a GNN and accelerator co-search framework to automatically search for matched GNN structures and accelerators to maximize both task accuracy and acceleration efficiency. To the best of our knowledge, G-CoS is the first co-search framework for GNNs and their accelerators. Extensive experiments show that the GNNs and accelerators generated by G-CoS consistently outperform SOTA GNNs and GNN accelerators, while only requiring a few hours for the end-to-end generation of the matched GNNs and their accelerators. We believe that our G-CoS has made an important heuristic step towards boosted GNN acceleration efficiency and fast development of efficient GNN solutions.

This work was supported in part by theNational Institutes of Health under Award R01HL144683, National Science
Foundation under CAREER-2048183.

[å›¾ç‰‡: images\image_9.png]

[å›¾ç‰‡: images\image_10.png]

