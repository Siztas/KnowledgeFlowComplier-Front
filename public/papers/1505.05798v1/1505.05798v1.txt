æ ‡é¢˜ï¼šSafe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret

Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time,sublinear regretfor lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.

Reinforcement learning (RL)(Busoniu etÂ al.,2010; Sutton & Barto,1998)often requires substantial experience before achieving acceptable performance on individual control problems. One major contributor to this issue is thetabula-rasaassumption of typical RL methods, which learn from scratch on each new task. In these settings, learning performance is directly correlated with the quality of the acquired samples. Unfortunately, the amount of experience necessary for high-quality performance increases exponentially with the tasksâ€™ degrees of freedom, inhibiting the application of RL to high-dimensional control problems.

When data is in limited supply, transfer learning can significantly improve model performance on new tasks by reusing previous learned knowledge during training(Taylor & Stone,2009; GheshlaghiÂ Azar etÂ al.,2013; Lazaric,2011; Ferrante etÂ al.,2008; Bou Ammar etÂ al.,2012). Multi-task learning (MTL) explores another notion of knowledge transfer, in which task models are trained simultaneously and share knowledge during the joint learning process(Wilson etÂ al.,2007; Zhang etÂ al.,2008).

In thelifelong learningsetting(Thrun & Oâ€™Sullivan,1996a,b), which can be framed
as an online MTL problem, agents acquire knowledge incrementally
by learning multiple tasks consecutively over their lifetime.
Recently, based on the work ofRuvolo & Eaton (2013)on supervised lifelong learning,Bou Ammar etÂ al. (2014)developed a lifelong learner for policy gradient RL. To ensure efficient learning over consecutive tasks, these works employ a second-order Taylor expansion around the parameters that are (locally) optimal for each task without transfer. This assumption simplifies
the MTL objective into a weighted quadratic form for online learning, but since it is based on single-task learning, this technique can lead to parameters far from globally optimal.
Consequently, the success of these methods for RL highly depends on the policy initializations, which must lead to near-optimal trajectories for meaningful updates. Also, since their objective functions average loss over all tasks, these methods exhibit non-vanishing regrets of the formğ’ªâ€‹(R)ğ’ªğ‘…\mathcal{O}(R), whereRğ‘…Ris the total number of rounds in a non-adversarial setting.

In addition, these methods may produce control policies with unsafe behavior (i.e., capable
of causing damage to the agent or environment, catastrophic failure,
etc.). This is a critical issue in robotic control, where unsafe control
policies can lead to physical damage or user injury. This problem is caused by using constraint-free optimization
over the shared knowledge during the transfer process, which may lead to uninformative or
unbounded policies.

In this paper, we address these issues by proposing the firstsafe lifelong learnerfor policy gradient RL operating in an adversarial framework. Our approach rapidly learns
high-performancesafe control policiesbased on the agentâ€™s
previously learned knowledge and safety constraints on each task, accumulating knowledge over multiple consecutive tasks to optimize overall performance. We theoretically analyze the regret
exhibited by our algorithm, showingsublineardependency of
the formğ’ªâ€‹(R)ğ’ªğ‘…\mathcal{O}(\sqrt{R})forRğ‘…Rrounds, thus outperforming current methods. We then evaluate our approach empirically on a set of dynamical systems.

An RL agent sequentially chooses actions to minimize its expected
cost. Such problems are formalized as Markov decision processes (MDPs)âŸ¨ğ’³,ğ’°,ğ’«,ğ’„,Î³âŸ©ğ’³ğ’°ğ’«ğ’„ğ›¾\left\langle\mathcal{X},\mathcal{U},\mathcal{P},\bm{c},\gamma\right\rangle,
whereğ’³âŠ‚â„dğ’³superscriptâ„ğ‘‘\mathcal{X}\subset\mathbb{R}^{d}is the (potentially infinite)
state space,ğ’°âˆˆâ„dağ’°superscriptâ„subscriptğ‘‘ğ‘\mathcal{U}\in\mathbb{R}^{d_{a}}is the set of all
possible actions,ğ’«:ğ’³Ã—ğ’°Ã—ğ’³â†’[0,1]:ğ’«â†’ğ’³ğ’°ğ’³01\mathcal{P}:\mathcal{X}\times\mathcal{U}\times\mathcal{X}\rightarrow[0,1]is a state transition probability describing the systemâ€™s dynamics,ğ’„:ğ’³Ã—ğ’°Ã—ğ’³â†’â„:ğ’„â†’ğ’³ğ’°ğ’³â„\bm{c}:\mathcal{X}\times\mathcal{U}\times\mathcal{X}\rightarrow\mathbb{R}is the cost function measuring the agentâ€™s performance, andÎ³âˆˆ[0,1]ğ›¾01\gamma\in[0,1]is a discount factor. At each time stepmğ‘šm, the agent is in stateğ’™mâˆˆğ’³subscriptğ’™ğ‘šğ’³\bm{x}_{m}\in\mathcal{X}and must choose an actionğ’–mâˆˆğ’°subscriptğ’–ğ‘šğ’°\bm{u}_{m}\in\mathcal{U},
transitioning it to a new stateğ’™m+1âˆ¼ğ’«â€‹(ğ’™m+1|ğ’™m,ğ’–m)similar-tosubscriptğ’™ğ‘š1ğ’«conditionalsubscriptğ’™ğ‘š1subscriptğ’™ğ‘šsubscriptğ’–ğ‘š\bm{x}_{m+1}\sim\mathcal{P}\left(\bm{x}_{m+1}|\bm{x}_{m},\bm{u}_{m}\right)and yielding a costğ’„m+1=ğ’„â€‹(ğ’™m+1,ğ’–m,ğ’™m)subscriptğ’„ğ‘š1ğ’„subscriptğ’™ğ‘š1subscriptğ’–ğ‘šsubscriptğ’™ğ‘š\bm{c}_{m+1}=\bm{c}(\bm{x}_{m+1},\bm{u}_{m},\bm{x}_{m}).
The sequence of state-action pairs forms a trajectoryğ‰=[ğ’™0:Mâˆ’1,ğ’–0:Mâˆ’1]ğ‰subscriptğ’™:0ğ‘€1subscriptğ’–:0ğ‘€1\bm{\tau}=\left[\bm{x}_{0:M-1},\bm{u}_{0:M-1}\right]over a (possibly infinite) horizonMğ‘€M. A policyÏ€:ğ’³Ã—ğ’°â†’[0,1]:ğœ‹â†’ğ’³ğ’°01\pi:\mathcal{X}\times\mathcal{U}\rightarrow[0,1]specifies a probability distribution over state-action pairs, whereÏ€â€‹(ğ’–|ğ’™)ğœ‹conditionalğ’–ğ’™\pi\left(\bm{u}|\bm{x}\right)represents the probability of selecting
an actionğ’–ğ’–\bm{u}in stateğ’™ğ’™\bm{x}. The goal of RL is to find
an optimal policyÏ€â‹†superscriptğœ‹â‹†\pi^{\star}that minimizes the total expected cost.

Policy search methodshave shown success in solving high-dimensional
problems, such as robotic control(Kober & Peters,2011; Peters & Schaal,2008a; Sutton etÂ al.,2000).
These methods represent the policyÏ€ğœ¶â€‹(ğ’–|ğ’™)subscriptğœ‹ğœ¶conditionalğ’–ğ’™\pi_{\bm{\alpha}}(\bm{u}|\bm{x})using a vectorğœ¶âˆˆâ„dğœ¶superscriptâ„ğ‘‘\bm{\alpha}\in\mathbb{R}^{d}of control parameters.
The optimal policyÏ€â‹†superscriptğœ‹â‹†\pi^{\star}is found by determining the parametersğœ¶â‹†superscriptğœ¶â‹†\bm{\alpha}^{\star}that minimize the expected average cost:

wherenğ‘›nis the total number of trajectories, andpğœ¶â€‹(ğ‰(k))subscriptğ‘ğœ¶superscriptğ‰ğ‘˜p_{\bm{\alpha}}\!\left(\bm{\tau}^{(k)}\right)andğ‘ªâ€‹(ğ‰(k))ğ‘ªsuperscriptğ‰ğ‘˜\bm{C}\!\left(\bm{\tau}^{(k)}\right)are the probability and
cost of trajectoryğ‰(k)superscriptğ‰ğ‘˜\bm{\tau}^{(k)}:

with an initial state distributionğ’«0:ğ’³â†’[0,1]:subscriptğ’«0â†’ğ’³01\mathcal{P}_{0}:\mathcal{X}\rightarrow[0,1]. We handle a constrained version of policy search, in which optimality not only corresponds to minimizing the total expected cost, but also to ensuring that the policy satisfies safety constraints. These constraints vary between applications, for example corresponding to maximum joint torque or prohibited physical positions.

In this paper, we employ a special form ofregret minimization games, which we briefly review here. A regret minimization game is a tripleâŸ¨ğ’¦,â„±,RâŸ©ğ’¦â„±ğ‘…\langle\mathcal{K},\mathcal{F},R\rangle, whereğ’¦ğ’¦\mathcal{K}is a non-empty decision set,â„±â„±\mathcal{F}is the set of moves of the adversary which contains bounded convex functions fromâ„nsuperscriptâ„ğ‘›\mathbb{R}^{n}toâ„â„\mathbb{R}, andRğ‘…Ris the total number of rounds. The game proceeds in rounds, where at each roundj=1,â€¦,Rğ‘—1â€¦ğ‘…j=1,\dots,R, the agent chooses a predictionğœ½jâˆˆğ’¦subscriptğœ½ğ‘—ğ’¦\bm{\theta}_{j}\in\mathcal{K}and the environment (i.e., the adversary) chooses a loss functionljâˆˆâ„±subscriptğ‘™ğ‘—â„±l_{j}\in\mathcal{F}. At the end of the round, the loss functionljsubscriptğ‘™ğ‘—l_{j}is revealed to the agent and the decisionğœ½jsubscriptğœ½ğ‘—\bm{\theta}_{j}is revealed to the environment. In this paper, we handle the full-information case, where the agent may observe the entire loss functionljsubscriptğ‘™ğ‘—l_{j}as its feedback and can exploit this in making decisions. The goal is to minimize the cumulative regretâˆ‘j=1Rljâ€‹(ğœ½j)âˆ’infğ’–âˆˆğ’¦â€‹[âˆ‘j=1Rljâ€‹(ğ’–)]superscriptsubscriptğ‘—1ğ‘…subscriptğ‘™ğ‘—subscriptğœ½ğ‘—subscriptinfğ’–ğ’¦delimited-[]superscriptsubscriptğ‘—1ğ‘…subscriptğ‘™ğ‘—ğ’–\sum_{j=1}^{R}l_{j}(\bm{\theta}_{j})-\text{inf}_{\bm{u}\in\mathcal{K}}\left[\sum_{j=1}^{R}l_{j}(\bm{u})\right].
When analyzing the regret of our methods, we use a variant of this definition to handle the lifelong RL case:

whereltjâ€‹(â‹…)subscriptğ‘™subscriptğ‘¡ğ‘—â‹…l_{t_{j}}(\cdot)denotes the loss of tasktğ‘¡tat roundjğ‘—j.

For our framework, we adopt a variant of regret minimization called â€œFollow the Regularized Leader,â€ which minimizes regret in two steps. First, the unconstrained solutionğœ½~~ğœ½\tilde{\bm{\theta}}is determined (see Sect.4.1) by solving an unconstrained optimization over the accumulated losses observed so far.
Givenğœ½~~ğœ½\tilde{\bm{\theta}}, the constrained solution is then determined by learning a projection into the constraint set via Bregman projections (seeAbbasi-Yadkori etÂ al. (2013)).

We adopt a lifelong learning framework in which the agent learns multiple
RL tasks consecutively, providing it the opportunity to transfer knowledge
between tasks to improve learning. Letğ’¯ğ’¯\mathcal{T}denote the set
of tasks, each element of which is an MDP. At any time, the learner
may face any previously seen task, and so must strive to maximize
its performance across all tasks. The goal is to learn optimal policiesÏ€ğœ¶1â‹†â‹†,â€¦,Ï€ğœ¶|ğ’¯|â‹†â‹†superscriptsubscriptğœ‹superscriptsubscriptğœ¶1â‹†â‹†â€¦superscriptsubscriptğœ‹superscriptsubscriptğœ¶ğ’¯â‹†â‹†\pi_{\bm{\alpha}_{1}^{\star}}^{\star},\dots,\pi_{\bm{\alpha}_{{|\mathcal{T}|}}^{\star}}^{\star}for all tasks, where policyÏ€ğœ¶tâ‹†â‹†superscriptsubscriptğœ‹superscriptsubscriptğœ¶ğ‘¡â‹†â‹†\pi_{\bm{\alpha}_{t}^{\star}}^{\star}for tasktğ‘¡tis parameterized byğœ¶tâ‹†âˆˆâ„dsuperscriptsubscriptğœ¶ğ‘¡â‹†superscriptâ„ğ‘‘\bm{\alpha}_{t}^{\star}\in\mathbb{R}^{d}.
In addition, each task is equipped with safety constraints to ensure
acceptable policy behavior:ğ‘¨tâ€‹ğœ¶tâ‰¤ğ’ƒtsubscriptğ‘¨ğ‘¡subscriptğœ¶ğ‘¡subscriptğ’ƒğ‘¡\bm{A}_{{t}}\bm{\alpha}_{{t}}\leq\bm{b}_{{t}},
withğ‘¨tâˆˆâ„dÃ—dsubscriptğ‘¨ğ‘¡superscriptâ„ğ‘‘ğ‘‘\bm{A}_{{t}}\in\mathbb{R}^{d\times d}andğ’ƒtâˆˆâ„dsubscriptğ’ƒğ‘¡superscriptâ„ğ‘‘\bm{b}_{{t}}\in\mathbb{R}^{d}representing the allowed policy combinations. The precise form of
these constraints depends on the application domain, but this formulation supports constraints on (e.g.) joint torque, acceleration, position,
etc.

At each roundjğ‘—j, the learner observes a set ofntjsubscriptğ‘›subscriptğ‘¡ğ‘—n_{{t_{j}}}trajectories{ğ‰tj(1),â€¦,ğ‰tj(ntj)}superscriptsubscriptğ‰subscriptğ‘¡ğ‘—1â€¦superscriptsubscriptğ‰subscriptğ‘¡ğ‘—subscriptğ‘›subscriptğ‘¡ğ‘—\left\{\bm{\tau}_{{t_{j}}}^{(1)},\dots,\bm{\tau}_{{t_{j}}}^{(n_{{t_{j}}})}\right\}from a tasktjâˆˆğ’¯subscriptğ‘¡ğ‘—ğ’¯{t_{j}}\in\mathcal{T}, where each trajectory has
lengthMtjsubscriptğ‘€subscriptğ‘¡ğ‘—M_{{t_{j}}}. To support knowledge transfer between tasks,
we assume that each taskâ€™s policy parametersğœ¶tjâˆˆâ„dsubscriptğœ¶subscriptğ‘¡ğ‘—superscriptâ„ğ‘‘\bm{\alpha}_{{t_{j}}}\in\mathbb{R}^{d}at roundjğ‘—jcan be written as a linear combination of a shared latent
basisğ‘³âˆˆâ„dÃ—kğ‘³superscriptâ„ğ‘‘ğ‘˜\bm{L}\in\mathbb{R}^{d\times k}with coefficient vectorsğ’”tjâˆˆâ„ksubscriptğ’”subscriptğ‘¡ğ‘—superscriptâ„ğ‘˜\bm{s}_{{t_{j}}}\in\mathbb{R}^{k}; therefore,ğœ¶tj=ğ‘³â€‹ğ’”tjsubscriptğœ¶subscriptğ‘¡ğ‘—ğ‘³subscriptğ’”subscriptğ‘¡ğ‘—\bm{\alpha}_{{t_{j}}}=\bm{L}\bm{s}_{{t_{j}}}.
Each column ofğ‘³ğ‘³\bm{L}represents a chunk of transferrable knowledge;
this task construction has been used successfully in previous multi-task
learning work(Kumar & DaumÃ© III,2012; Ruvolo & Eaton,2013; Bou Ammar etÂ al.,2014).
Extending this previous work, we ensure that the shared knowledge
repository is â€œinformativeâ€ by incorporating bounding constraints
on the Frobenius normâˆ¥â‹…âˆ¥ğ–¥\|\cdot\|_{\mathsf{F}}ofğ‘³ğ‘³\bm{L}. Consequently, the optimization
problem after observingrğ‘Ÿrrounds is:

wherepğ‘pandqğ‘qare the constraints onâ€–ğ‘³â€–ğ–¥subscriptnormğ‘³ğ–¥\|\bm{L}\|_{\mathsf{F}},Î·tjâˆˆâ„subscriptğœ‚subscriptğ‘¡ğ‘—â„\eta_{{t_{j}}}\in\mathbb{R}are design weighting parameters111We describe later how to set theÎ·ğœ‚\etaâ€™s later in Sect.5to obtain regret bounds, and leave them as variables now for generality.,â„r={t1,â€¦,tr}subscriptâ„ğ‘Ÿsubscriptğ‘¡1â€¦subscriptğ‘¡ğ‘Ÿ\mathcal{I}_{r}=\left\{t_{1},\dots,t_{r}\right\}denotes
the set of all tasks observed so far through roundrğ‘Ÿr, andğ‘ºğ‘º\bm{S}is the collection of all coefficients

The loss functionltjâ€‹(ğœ¶tj)subscriptğ‘™subscriptğ‘¡ğ‘—subscriptğœ¶subscriptğ‘¡ğ‘—l_{{t_{j}}}(\bm{\alpha}_{{t_{j}}})in Eq.Â (4)
corresponds to a policy gradient learner for tasktjsubscriptğ‘¡ğ‘—{t_{j}}, as
defined in Eq.Â (1). Typical policy gradient methods(Kober & Peters,2011; Sutton etÂ al.,2000)maximize a lower bound of the expected costltjâ€‹(ğœ¶tj)subscriptğ‘™subscriptğ‘¡ğ‘—subscriptğœ¶subscriptğ‘¡ğ‘—l_{{t_{j}}}\!\left(\bm{\alpha}_{{t_{j}}}\right),
which can be derived by taking the logarithm and applying Jensenâ€™s
inequality:

Therefore, our goal is to minimize the following objective:

The optimization problem above can be mapped to the standard online
learning framework by unrollingğ‘³ğ‘³\bm{L}andğ‘ºğ‘º\bm{S}into a vectorğœ½=[vecâ€‹(ğ‘³)â€‹vecâ€‹(ğ‘º)]ğ–³âˆˆâ„dâ€‹k+kâ€‹|ğ’¯|ğœ½superscriptdelimited-[]vecğ‘³vecğ‘ºğ–³superscriptâ„ğ‘‘ğ‘˜ğ‘˜ğ’¯\bm{\theta}=[\text{vec}(\bm{L})\ \text{vec}(\bm{S})]^{\mathsf{T}}\in\mathbb{R}^{dk+k{|\mathcal{T}|}}. Choosingğ›€0â€‹(ğœ½)=Î¼2â€‹âˆ‘i=1dâ€‹kğœ½i2+Î¼1â€‹âˆ‘i=dâ€‹k+1dâ€‹k+kâ€‹|ğ’¯|ğœ½i2subscriptğ›€0ğœ½subscriptğœ‡2superscriptsubscriptğ‘–1ğ‘‘ğ‘˜superscriptsubscriptğœ½ğ‘–2subscriptğœ‡1superscriptsubscriptğ‘–ğ‘‘ğ‘˜1ğ‘‘ğ‘˜ğ‘˜ğ’¯superscriptsubscriptğœ½ğ‘–2\bm{\Omega}_{0}(\bm{\theta})=\mu_{2}\sum_{i=1}^{dk}\bm{\theta}_{i}^{2}+\mu_{1}\sum_{i=dk+1}^{dk+k{|\mathcal{T}|}}\bm{\theta}_{i}^{2}\enspace, andğ›€jâ€‹(ğœ½)=ğ›€jâˆ’1â€‹(ğœ½)+Î·tjâ€‹ltjâ€‹(ğœ½)subscriptğ›€ğ‘—ğœ½subscriptğ›€ğ‘—1ğœ½subscriptğœ‚subscriptğ‘¡ğ‘—subscriptğ‘™subscriptğ‘¡ğ‘—ğœ½\bm{\Omega}_{j}(\bm{\theta})=\bm{\Omega}_{j-1}(\bm{\theta})+\eta_{{t_{j}}}l_{{t_{j}}}(\bm{\theta}), we can write the safe lifelong policy search problem (Eq.Â (6)) as:

whereğ’¦âŠ†â„dâ€‹k+kâ€‹|ğ’¯|ğ’¦superscriptâ„ğ‘‘ğ‘˜ğ‘˜ğ’¯\mathcal{K}\subseteq\mathbb{R}^{dk+k{|\mathcal{T}|}}is the set of allowable policies under the given safety constraints.
Note that the loss for tasktjsubscriptğ‘¡ğ‘—{t_{j}}can be written as a bilinear
product inğœ½ğœ½\bm{\theta}:

We see that the problem in Eq.Â (7) is equivalent
to Eq.Â (6) by noting
that atrğ‘Ÿrrounds,ğ›€r=âˆ‘j=1rÎ·tjâ€‹ltjâ€‹(ğœ½)+ğ›€0â€‹(ğœ½)subscriptğ›€ğ‘Ÿsuperscriptsubscriptğ‘—1ğ‘Ÿsubscriptğœ‚subscriptğ‘¡ğ‘—subscriptğ‘™subscriptğ‘¡ğ‘—ğœ½subscriptğ›€0ğœ½\bm{\Omega}_{r}=\sum_{j=1}^{r}\eta_{{t_{j}}}l_{{t_{j}}}(\bm{\theta})+\bm{\Omega}_{0}(\bm{\theta}).

We solve Eq.Â (7)
in two steps. First, we determine the unconstrained solutionğœ½~r+1subscript~ğœ½ğ‘Ÿ1\tilde{\bm{\theta}}_{r+1}whenğ’¦=â„dâ€‹k+kâ€‹|ğ’¯|ğ’¦superscriptâ„ğ‘‘ğ‘˜ğ‘˜ğ’¯\mathcal{K}=\mathbb{R}^{dk+k{|\mathcal{T}|}}(see Sect.4.1). Givenğœ½~r+1subscript~ğœ½ğ‘Ÿ1\tilde{\bm{\theta}}_{r+1},
we derive the constrained solutionğœ½^r+1subscript^ğœ½ğ‘Ÿ1\hat{{\bm{\theta}}}_{r+1}by learning a projectionProjğ›€r,ğ’¦â€‹(ğœ½~r+1)subscriptProjsubscriptğ›€ğ‘Ÿğ’¦subscript~ğœ½ğ‘Ÿ1\text{Proj}_{\bm{\Omega}_{r},\mathcal{K}}\left(\tilde{\bm{\theta}}_{r+1}\right)to the constraint setğ’¦âŠ†â„dâ€‹k+kâ€‹|ğ’¯|ğ’¦superscriptâ„ğ‘‘ğ‘˜ğ‘˜ğ’¯\mathcal{K}\subseteq\mathbb{R}^{dk+k{|\mathcal{T}|}},
which amounts to minimizing the Bregman divergence overğ›€râ€‹(ğœ½)subscriptğ›€ğ‘Ÿğœ½\bm{\Omega}_{r}(\bm{\theta})(see Sect.4.2)222In Sect.4.2, we linearize the
loss around the constrained solution of the previous round
to increase stability and ensure convergence. Given the linear losses,
it suffices to solve the Bregman divergence over the regularizer, reducing the
computational cost.. The complete approach is given in Algorithm1and is available as a software implementation on the authorsâ€™ websites.

Although Eq.Â (6)
is not jointly convex in bothğ‘³ğ‘³\bm{L}andğ‘ºğ‘º\bm{S}, it is separably convex (for log-concave policy distributions). Consequently,
we follow an alternating optimization approach, first computingğ‘³ğ‘³\bm{L}while holdingğ‘ºğ‘º\bm{S}fixed, and then updatingğ‘ºğ‘º\bm{S}given the acquiredğ‘³ğ‘³\bm{L}. We detail this process for two popular PG learners, eREINFORCE(Williams,1992)and eNAC(Peters & Schaal,2008b). The derivations of the update rules below can be found in AppendixA.

These updates are governed by learning ratesÎ²ğ›½\betaandÎ»ğœ†\lambdathat decay over time;Î²ğ›½\betaandÎ»ğœ†\lambdacan be chosen using line-search methods as discussed byBoyd & Vandenberghe (2004). In our experiments, we adopt a simple yet effective strategy, whereÎ²=câ€‹jâˆ’1ğ›½ğ‘superscriptğ‘—1\beta=cj^{-1}andÎ»=câ€‹jâˆ’1ğœ†ğ‘superscriptğ‘—1\lambda=cj^{-1}, with0<c<10ğ‘10<c<1.

Step 1: UpdatingLğ¿\bm{L}Holdingğ‘ºğ‘º\bm{S}fixed, the
latent repository can be updated according to:

with learning rateÎ·ğ‘³Î²âˆˆâ„superscriptsubscriptğœ‚ğ‘³ğ›½â„\eta_{\bm{L}}^{\beta}\in\mathbb{R},
andğ‘®âˆ’1â€‹(ğ‘³,ğ‘º)superscriptğ‘®1ğ‘³ğ‘º\bm{G}^{-1}(\bm{L},\bm{S})as the inverse of the Fisher
information matrix(Peters & Schaal,2008b).

In the special case of Gaussian policies, the update forğ‘³ğ‘³\bm{L}can be derived in a closed form asğ‘³Î²+1=ğ’ğ‘³âˆ’1â€‹ğ’—ğ‘³subscriptğ‘³ğ›½1superscriptsubscriptğ’ğ‘³1subscriptğ’—ğ‘³\bm{L}_{\beta+1}=\bm{Z}_{\bm{L}}^{-1}\bm{v}_{\bm{L}},
where

Ïƒtj2superscriptsubscriptğœsubscriptğ‘¡ğ‘—2\sigma_{{t_{j}}}^{2}is the covariance of the Gaussian
policy for a tasktjsubscriptğ‘¡ğ‘—{t_{j}}, andğš½=ğš½â€‹(ğ’™m(k,tj))ğš½ğš½superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—\bm{\Phi}=\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)denotes the state features.

Step 2: UpdatingSğ‘†\bm{S}Given the fixed basisğ‘³ğ‘³\bm{L}, the coefficient
matrixğ‘ºğ‘º\bm{S}is updated column-wise for alltjâˆˆâ„rsubscriptğ‘¡ğ‘—subscriptâ„ğ‘Ÿ{t_{j}}\in\mathcal{I}_{r}:

with learning rateÎ·ğ‘ºÎ»âˆˆâ„superscriptsubscriptğœ‚ğ‘ºğœ†â„\eta_{\bm{S}}^{\lambda}\in\mathbb{R}. For Gaussian policies, the closed-form of the update
isğ’”tj=ğ’ğ’”tjâˆ’1â€‹ğ’—ğ’”tjsubscriptğ’”subscriptğ‘¡ğ‘—superscriptsubscriptğ’subscriptğ’”subscriptğ‘¡ğ‘—1subscriptğ’—subscriptğ’”subscriptğ‘¡ğ‘—\bm{s}_{{t_{j}}}=\bm{Z}_{\bm{s}_{{t_{j}}}}^{-1}\bm{v}_{\bm{s}_{{t_{j}}}}, where

Once we have obtained the unconstrained solutionğœ½~r+1subscript~ğœ½ğ‘Ÿ1\tilde{\bm{\theta}}_{r+1}(which
satisfies Eq.Â (7), but can lead to policy parameters
in unsafe regions), we then derive the constrained solution to ensure safe policies. We learn a projectionProjğ›€r,ğ’¦â€‹(ğœ½~r+1)subscriptProjsubscriptğ›€ğ‘Ÿğ’¦subscript~ğœ½ğ‘Ÿ1\text{Proj}_{\bm{\Omega}_{r},\mathcal{K}}\left(\tilde{\bm{\theta}}_{r+1}\right)fromğœ½~r+1subscript~ğœ½ğ‘Ÿ1\tilde{\bm{\theta}}_{r+1}to the constraint set:

whereâ„¬ğ›€r,ğ’¦â€‹(ğœ½,ğœ½~r+1)subscriptâ„¬subscriptğ›€ğ‘Ÿğ’¦ğœ½subscript~ğœ½ğ‘Ÿ1\mathcal{B}_{\bm{\Omega}_{r},\mathcal{K}}\!\left(\!\bm{\theta},\tilde{\bm{\theta}}_{r+1}\!\right)is the Bregman divergence overğ›€rsubscriptğ›€ğ‘Ÿ\bm{\Omega}_{r}:

Solving Eq.Â (8) is computationally
expensive sinceğ›€râ€‹(ğœ½)subscriptğ›€ğ‘Ÿğœ½\bm{\Omega}_{r}(\bm{\theta})includes
the sum back to the original round. To remedy this problem, ensure the
stability of our approach, and guarantee that the constrained solutions
for all observed tasks lie within a bounded region, we linearize the
current-round loss functionltrâ€‹(ğœ½)subscriptğ‘™subscriptğ‘¡ğ‘Ÿğœ½l_{{t_{r}}}(\bm{\theta})around
theconstrainedsolution of the previous roundğœ½^rsubscript^ğœ½ğ‘Ÿ\hat{\bm{\theta}}_{r}:

where

Given the above linear form, we can rewrite the optimization problem
in Eq.Â (8) as:

Consequently, determiningsafe policiesfor lifelong policy
search reinforcement learning amounts to solving:

To solve the optimization problem above, we start by converting the
inequality constraints to equality constraints by introducing slack variablesğ’„tjâ‰¥0subscriptğ’„subscriptğ‘¡ğ‘—0\bm{c}_{{t_{j}}}\geq 0. We also guarantee that these slack variables
are bounded by incorporatingâ€–ğ’„tjâ€–â‰¤ğ’„max,âˆ€tjâˆˆ{1,â€¦,|ğ’¯|}formulae-sequencenormsubscriptğ’„subscriptğ‘¡ğ‘—subscriptğ’„maxfor-allsubscriptğ‘¡ğ‘—1â€¦ğ’¯\|\bm{c}_{{t_{j}}}\|\leq\bm{c}_{\text{max}},\ \forall{t_{j}}\in\{1,\dots,{|\mathcal{T}|}\}:

With this formulation, learningProjğ›€r,ğ’¦â€‹(ğœ½~r+1)subscriptProjsubscriptğ›€ğ‘Ÿğ’¦subscript~ğœ½ğ‘Ÿ1\text{Proj}_{\bm{\Omega}_{r},\mathcal{K}}\left(\tilde{\bm{\theta}}_{r+1}\right)amounts to solving second-order cone and semi-definite programs.

This section determines the constrained projection of the shared
basisğ‘³ğ‘³\bm{L}given fixedğ‘ºğ‘º\bm{S}andğ‘ªğ‘ª\bm{C}. We show thatğ‘³ğ‘³\bm{L}can be acquired efficiently, since this step can be relaxed to solving
a semi-definite program inğ‘³â€‹ğ‘³ğ–³ğ‘³superscriptğ‘³ğ–³\bm{L}\bm{L}^{\mathsf{T}}(Boyd & Vandenberghe,2004).
To formulate the semi-definite program, note that

From the constraint set, we recognize:

Sincespectrumâ€‹(ğ‘³â€‹ğ‘³ğ–³)=spectrumâ€‹(ğ‘³ğ–³â€‹ğ‘³)spectrumğ‘³superscriptğ‘³ğ–³spectrumsuperscriptğ‘³ğ–³ğ‘³\text{spectrum}\left(\bm{L}\bm{L}^{\mathsf{T}}\right)=\text{spectrum}\left(\bm{L}^{\mathsf{T}}\bm{L}\right),
we can write:

Having determinedğ‘³ğ‘³\bm{L}, we can acquireğ‘ºğ‘º\bm{S}and
updateğ‘ªğ‘ª\bm{C}by solving a second-order cone
program(Boyd & Vandenberghe,2004)of the following form:

This section quantifies the performance of our approach by providing
formal analysis of the regret afterRğ‘…Rrounds. We
show that the safe lifelong reinforcement learner exhibitssublinearregret in the total number of rounds. Formally, we prove the following
theorem:

AfterRğ‘…Rrounds and choosingâˆ€tjâˆˆâ„RÎ·tj=Î·=1Rformulae-sequencefor-allsubscriptğ‘¡ğ‘—subscriptâ„ğ‘…subscriptğœ‚subscriptğ‘¡ğ‘—ğœ‚1ğ‘…\forall{t_{j}}\in\mathcal{I}_{R}\ \ \eta_{{t_{j}}}\!=\eta=\frac{1}{\sqrt{R}},ğ‹|ğ›‰^1=diagğ¤â€‹(Î¶)evaluated-atğ‹subscript^ğ›‰1subscriptdiagğ¤ğœ\bm{L}\Big{|}_{\hat{\bm{\theta}}_{1}}=\text{diag}_{\bm{k}}(\zeta),
withdiagğ¤â€‹(â‹…)subscriptdiagğ¤â‹…\text{diag}_{\bm{k}}(\cdot)being a diagonal matrix among
theğ¤ğ¤\bm{k}columns ofğ‹ğ‹\bm{L},pâ‰¤Î¶2â‰¤qğ‘superscriptğœ2ğ‘p\leq\zeta^{2}\leq q, andğ’|ğ›‰^1=ğŸkÃ—|ğ’¯|evaluated-atğ’subscript^ğ›‰1subscript0ğ‘˜ğ’¯\bm{S}\Big{|}_{\hat{\bm{\theta}}_{1}}=\bm{0}_{k\times{|\mathcal{T}|}},
the safe lifelong reinforcement learner exhibits sublinear regret
of the form:

Proof Roadmap:The remainder of this section completes our proof of Theorem1; further details are given in AppendixB.
We assume linear losses for all tasks in the constrained case in accordance
with Sect.4.2. Although linear losses for
policy search RL are too restrictive given a single
operating point, as discussed previously, we remedy this problem by
generalizing to the case of piece-wise linear losses, where the linearization
operating point is a resultant of the optimization problem.
To bound the regret, we need to bound the dual Euclidean norm
(which is the same as the Euclidean norm) of the gradient of the loss function,
then prove Theorem1by bounding: (1) tasktjsubscriptğ‘¡ğ‘—{t_{j}}â€™s
gradient loss (Sect.5.1), and (2) linearized
losses with respect toğ‘³ğ‘³\bm{L}andğ‘ºğ‘º\bm{S}(Sect.5.2).

We start by stating essential lemmas for Theorem1; due to space constraints, proofs for all lemmas are available in the supplementary material.
Here, we bound the gradient of a loss functionltjâ€‹(ğœ½)subscriptğ‘™subscriptğ‘¡ğ‘—ğœ½l_{{t_{j}}}(\bm{\theta})at roundrğ‘Ÿrunder Gaussian policies333Please note that derivations for other forms of log-concave policy
distributions could be derived in similar manner. In this work, we
focus on Gaussian policies since they cover a broad spectrum of real-world
applications..

We assume that the policy for a tasktjsubscriptğ‘¡ğ‘—{t_{j}}is Gaussian, the action setğ’°ğ’°\mathcal{U}is bounded byğ®maxsubscriptğ®\bm{u}_{\max},
and the feature set is upper-bounded byğš½maxsubscriptğš½\bm{\Phi}_{\max}.

Assume tasktjsubscriptğ‘¡ğ‘—{t_{j}}â€™s policy at roundrğ‘Ÿris given byÏ€ğ›‚tj(tj)â€‹(ğ®m(k,tj)|ğ±m(k,tj))|ğ›‰^r=ğ’©â€‹(ğ›‚tjğ–³|ğ›‰^râ€‹ğš½â€‹(ğ±m(k,tj)),ğ›”tj)evaluated-atsuperscriptsubscriptğœ‹subscriptğ›‚subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘—conditionalsuperscriptsubscriptğ®ğ‘šğ‘˜subscriptğ‘¡ğ‘—superscriptsubscriptğ±ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscript^ğ›‰ğ‘Ÿğ’©evaluated-atsuperscriptsubscriptğ›‚subscriptğ‘¡ğ‘—ğ–³subscript^ğ›‰ğ‘Ÿğš½superscriptsubscriptğ±ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğ›”subscriptğ‘¡ğ‘—\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\!\left(\!\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\!\right)\!\Big{|}_{\hat{\bm{\theta}}_{r}}\!=\mathcal{N}\!\left(\!\bm{\alpha}_{{t_{j}}}^{\mathsf{T}}\Big{|}_{\hat{\bm{\theta}}_{r}}\bm{\Phi}\left(\!\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\!,\bm{\sigma}_{{t_{j}}}\!\right),
for statesğ±m(k,tj)âˆˆğ’³tjsuperscriptsubscriptğ±ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğ’³subscriptğ‘¡ğ‘—\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{X}_{{t_{j}}}and actionsğ®m(k,tj)âˆˆğ’°tjsuperscriptsubscriptğ®ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğ’°subscriptğ‘¡ğ‘—\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{U}_{{t_{j}}}. Forltjâ€‹(ğ›‚tj)=âˆ’1ntjâ€‹âˆ‘k=1ntjâˆ‘m=0Mtjâˆ’1logâ¡[Ï€ğ›‚tj(tj)â€‹(ğ®m(k,tj)|ğ±m(k,tj))]subscriptğ‘™subscriptğ‘¡ğ‘—subscriptğ›‚subscriptğ‘¡ğ‘—1subscriptğ‘›subscriptğ‘¡ğ‘—superscriptsubscriptğ‘˜1subscriptğ‘›subscriptğ‘¡ğ‘—superscriptsubscriptğ‘š0subscriptğ‘€subscriptğ‘¡ğ‘—1superscriptsubscriptğœ‹subscriptğ›‚subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘—conditionalsuperscriptsubscriptğ®ğ‘šğ‘˜subscriptğ‘¡ğ‘—superscriptsubscriptğ±ğ‘šğ‘˜subscriptğ‘¡ğ‘—l_{{t_{j}}}\!\!\left(\bm{\alpha}_{{t_{j}}}\right)=-\frac{1}{n_{{t_{j}}}}\displaystyle\sum_{k=1}^{n_{{t_{j}}}}\!\sum_{m=0}^{M_{{t_{j}}}-1}\!\!\!\log\left[\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\!\left(\!\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\!\right)\!\right],Â the gradientâˆ‡ğ›‚tjltjâ€‹(ğ›‚tj)|ğ›‰^revaluated-atsubscriptâˆ‡subscriptğ›‚subscriptğ‘¡ğ‘—subscriptğ‘™subscriptğ‘¡ğ‘—subscriptğ›‚subscriptğ‘¡ğ‘—subscript^ğ›‰ğ‘Ÿ\nabla_{\!\bm{\alpha}_{{t_{j}}}}l_{{t_{j}}}\!\!\left(\!\bm{\alpha}_{{t_{j}}}\!\right)\!\Big{|}_{\hat{\bm{\theta}}_{r}}satisfies||âˆ‡ğ›‚tjltjâ€‹(ğ›‚tj)|ğ›‰^r||2â‰¤evaluated-atsubscriptsubscriptâˆ‡subscriptğ›‚subscriptğ‘¡ğ‘—subscriptğ‘™subscriptğ‘¡ğ‘—subscriptğ›‚subscriptğ‘¡ğ‘—subscript^ğ›‰ğ‘Ÿ2absent\left|\left|\nabla_{\!\bm{\alpha}_{{t_{j}}}}l_{{t_{j}}}\!\!\left(\!\bm{\alpha}_{{t_{j}}}\!\right)\!\Big{|}_{\hat{\bm{\theta}}_{r}}\right|\right|_{2}\leq

for all trajectories and all tasks, withumax=maxk,mâ¡{|ğ®m(k,tj)|}subscriptğ‘¢subscriptğ‘˜ğ‘šsuperscriptsubscriptğ®ğ‘šğ‘˜subscriptğ‘¡ğ‘—u_{\max}\!=\!\displaystyle\max_{k,m}\left\{\left|\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\right|\right\}andğš½max=maxk,mâ¡{â€–ğš½â€‹(ğ±m(k,tj))â€–2}subscriptğš½subscriptğ‘˜ğ‘šsubscriptnormğš½superscriptsubscriptğ±ğ‘šğ‘˜subscriptğ‘¡ğ‘—2\bm{\Phi}_{\max}\!=\!\displaystyle\max_{k,m}\left\{\left|\left|\bm{\Phi}\left(\!\bm{x}_{m}^{\left(k,\ {t_{j}}\!\right)}\right)\right|\right|_{2}\right\}.

As discussed previously, we linearize the
loss of tasktrsubscriptğ‘¡ğ‘Ÿ{t_{r}}around the constraint
solution of the previous roundğœ½^rsubscript^ğœ½ğ‘Ÿ\hat{\bm{\theta}}_{r}.
To acquire the regret bounds in Theorem1, the next
step is to bound the dual norm,â€–ğ’‡^tr|ğœ½^râˆ¥2â‹†=â€–ğ’‡^tr|ğœ½^râˆ¥2evaluated-atsubscriptdelimited-â€–|subscript^ğ’‡subscriptğ‘¡ğ‘Ÿsubscript^ğœ½ğ‘Ÿ2â‹†evaluated-atsubscriptdelimited-â€–|subscript^ğ’‡subscriptğ‘¡ğ‘Ÿsubscript^ğœ½ğ‘Ÿ2\left\|\hat{\bm{f}}_{{t_{r}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2}^{\star}=\left\|\hat{\bm{f}}_{{t_{r}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2}of Eq.Â (9). It can be easily seen

Since|ltr(ğœ½)|ğœ½^r|\left|l_{{t_{r}}}\left(\bm{\theta}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}\right|can be bounded byğœ¹ltrsubscriptğœ¹subscriptğ‘™subscriptğ‘¡ğ‘Ÿ\bm{\delta}_{l_{{t_{r}}}}(see Sect.2),the next step is to boundâ€–âˆ‡ğœ½ltrâ€‹(ğœ½)|ğœ½^râˆ¥2evaluated-atsubscriptdelimited-â€–|subscriptâˆ‡ğœ½subscriptğ‘™subscriptğ‘¡ğ‘Ÿğœ½subscript^ğœ½ğ‘Ÿ2\left\|\nabla_{\bm{\theta}}l_{{t_{r}}}\left(\bm{\theta}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2},
andâ€–ğœ½^râ€–2subscriptnormsubscript^ğœ½ğ‘Ÿ2\|\hat{\bm{\theta}}_{r}\|_{2}.

The norm of the gradient
of the loss function evaluated atğ›‰^rsubscript^ğ›‰ğ‘Ÿ\hat{\bm{\theta}}_{r}satisfies

To finalize the bound ofâ€–ğ’‡^tr|ğœ½^râˆ¥2evaluated-atsubscriptdelimited-â€–|subscript^ğ’‡subscriptğ‘¡ğ‘Ÿsubscript^ğœ½ğ‘Ÿ2\left\|\hat{\bm{f}}_{{t_{r}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2}as needed for deriving the regret, we must derive an upper-bound forâ€–ğœ½^râ€–2subscriptnormsubscript^ğœ½ğ‘Ÿ2\|\hat{\bm{\theta}}_{r}\|_{2}:

The L2norm of the constraint solution at roundrâˆ’1ğ‘Ÿ1r-1,â€–ğ›‰^râ€–22superscriptsubscriptnormsubscript^ğ›‰ğ‘Ÿ22\|\hat{\bm{\theta}}_{r}\|_{2}^{2}is bounded by

where|â„râˆ’1|subscriptâ„ğ‘Ÿ1\left|\mathcal{I}_{r-1}\right|is the number of
unique tasks observed so far.

Given the previous two lemmas, we can prove the bound
forâ€–ğ’‡^tr|ğœ½^râˆ¥2evaluated-atsubscriptdelimited-â€–|subscript^ğ’‡subscriptğ‘¡ğ‘Ÿsubscript^ğœ½ğ‘Ÿ2\left\|\hat{\bm{f}}_{{t_{r}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2}:

The L2norm of the linearizing
term ofltrâ€‹(ğ›‰)subscriptğ‘™subscriptğ‘¡ğ‘Ÿğ›‰l_{{t_{r}}}(\bm{\theta})aroundğ›‰^rsubscript^ğ›‰ğ‘Ÿ\hat{\bm{\theta}}_{r},â€–ğŸ^tr|ğ›‰^râˆ¥2evaluated-atsubscriptdelimited-â€–|subscript^ğŸsubscriptğ‘¡ğ‘Ÿsubscript^ğ›‰ğ‘Ÿ2\left\|\hat{\bm{f}}_{{t_{r}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2},
is bounded by

whereğ›…ltrsubscriptğ›…subscriptğ‘™subscriptğ‘¡ğ‘Ÿ\bm{\delta}_{l_{{t_{r}}}}is the constant upper-bound
on|ltr(ğ›‰)|ğ›‰^r|\left|l_{{t_{r}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}\right|,
and

Given the lemmas in the previous section, we now can derive the
sublinear regret bound given in Theorem1.
Using results developed byAbbasi-Yadkori etÂ al. (2013), it is easy to see that

From the convexity of the regularizer, we obtain:

We have:

Therefore, for anyğ’–âˆˆğ’¦ğ’–ğ’¦\bm{u}\in\mathcal{K}

Assuming thatâˆ€tjâ€‹Î·tj=Î·for-allsubscriptğ‘¡ğ‘—subscriptğœ‚subscriptğ‘¡ğ‘—ğœ‚\forall{t_{j}}\ \eta_{{t_{j}}}=\eta, we can derive:

The following lemma finalizes the proof of Theorem1:

AfterRğ‘…Rrounds withâˆ€tjâ€‹Î·tj=Î·=1Rfor-allsubscriptğ‘¡ğ‘—subscriptğœ‚subscriptğ‘¡ğ‘—ğœ‚1ğ‘…\forall{t_{j}}\ \eta_{{t_{j}}}=\eta=\frac{1}{\sqrt{R}},
for anyğ®âˆˆğ’¦ğ®ğ’¦\bm{u}\in\mathcal{K}we have thatâˆ‘j=1Rltjâ€‹(ğ›‰^j)âˆ’ltjâ€‹(ğ®)â‰¤ğ’ªâ€‹(R)superscriptsubscriptğ‘—1ğ‘…subscriptğ‘™subscriptğ‘¡ğ‘—subscript^ğ›‰ğ‘—subscriptğ‘™subscriptğ‘¡ğ‘—ğ®ğ’ªğ‘…\sum_{j=1}^{R}l_{{t_{j}}}(\hat{\bm{\theta}}_{j})-l_{{t_{j}}}(\bm{u})\leq\mathcal{O}\left(\sqrt{R}\right).

From Eq.Â (12), it follows that

withğœ¸3â€‹(R)=4â€‹ğœ¸12â€‹(R)+2â€‹maxtjâˆˆâ„Râˆ’1â¡ğœ¹tj2subscriptğœ¸3ğ‘…4superscriptsubscriptğœ¸12ğ‘…2subscriptsubscriptğ‘¡ğ‘—subscriptâ„ğ‘…1superscriptsubscriptğœ¹subscriptğ‘¡ğ‘—2\bm{\gamma}_{3}(R)=4\bm{\gamma}_{1}^{2}(R)+2\max_{{t_{j}}\in\mathcal{I}_{R-1}}\bm{\delta}_{{t_{j}}}^{2}.
Since|â„Râˆ’1|â‰¤|ğ’¯|subscriptâ„ğ‘…1ğ’¯|\mathcal{I}_{R-1}|\leq{|\mathcal{T}|}, we have thatâ€–ğ’‡^tj|Î¸^râˆ¥22â‰¤ğœ¸5â€‹(R)â€‹|ğ’¯|evaluated-atsubscriptdelimited-â€–|subscript^ğ’‡subscriptğ‘¡ğ‘—subscript^ğœƒğ‘Ÿ22subscriptğœ¸5ğ‘…ğ’¯\left\|\hat{\bm{f}}_{{t_{j}}}\Big{|}_{\hat{\theta}_{r}}\right\|_{2}^{2}\leq\bm{\gamma}_{5}(R){|\mathcal{T}|}withğœ¸5=8â€‹d/p2â€‹qâ€‹ğœ¸12â€‹(R)â€‹maxtkâˆˆâ„Râˆ’1â¡{â€–ğ‘¨tkâ€ â€–22â€‹(â€–ğ’ƒtkâ€–2+ğ’„max)2}subscriptğœ¸58ğ‘‘superscriptğ‘2ğ‘superscriptsubscriptğœ¸12ğ‘…subscriptsubscriptğ‘¡ğ‘˜subscriptâ„ğ‘…1superscriptsubscriptnormsuperscriptsubscriptğ‘¨subscriptğ‘¡ğ‘˜â€ 22superscriptsubscriptnormsubscriptğ’ƒsubscriptğ‘¡ğ‘˜2subscriptğ’„max2\displaystyle\bm{\gamma}_{5}=8\nicefrac{{d}}{{p^{2}}}q\bm{\gamma}_{1}^{2}(R)\max_{{t_{k}}\in\mathcal{I}_{R-1}}\left\{\|\bm{A}_{{t_{k}}}^{\dagger}\|_{2}^{2}\left(\|\bm{b}_{{t_{k}}}\|_{2}+\bm{c}_{\text{max}}\right)^{2}\right\}.

Given thatğ›€0â€‹(ğ’–)â‰¤qâ€‹d+ğœ¸5â€‹(R)â€‹|ğ’¯|subscriptğ›€0ğ’–ğ‘ğ‘‘subscriptğœ¸5ğ‘…ğ’¯\bm{\Omega}_{0}(\bm{u})\leq qd+\bm{\gamma}_{5}(R){|\mathcal{T}|},
withğœ¸5â€‹(R)subscriptğœ¸5ğ‘…\bm{\gamma}_{5}(R)being a constant, we have:

InitializingLğ¿\bm{L}andSğ‘†\bm{S}:We initializeğ‘³|ğœ½^1=diagğ’Œâ€‹(Î¶)evaluated-atğ‘³subscript^ğœ½1subscriptdiagğ’Œğœ\bm{L}\Big{|}_{\hat{\bm{\theta}}_{1}}\!=\text{diag}_{\bm{k}}(\zeta),
withpâ‰¤Î¶2â‰¤qğ‘superscriptğœ2ğ‘p\leq\zeta^{2}\leq qandğ‘º|ğœ½^1=ğŸkÃ—|ğ’¯|evaluated-atğ‘ºsubscript^ğœ½1subscript0ğ‘˜ğ’¯\bm{S}\Big{|}_{\hat{\bm{\theta}}_{1}}\!=\bm{0}_{k\times{|\mathcal{T}|}}to ensure the invertibility ofğ‘³ğ‘³\bm{L}and that the constraints are
met. This leads to

Choosingâˆ€tjâ€‹Î·tj=Î·=1/Rfor-allsubscriptğ‘¡ğ‘—subscriptğœ‚subscriptğ‘¡ğ‘—ğœ‚1ğ‘…\forall{t_{j}}\ \eta_{{t_{j}}}=\eta=\nicefrac{{1}}{{\sqrt{R}}},
we acquire sublinear regret, finalizing the statement of Theorem1:

âˆ

To validate the empirical performance of our method, we applied our safe online PG algorithm to learn multiple consecutive control tasks on three dynamical systems (Figure1). To generate multiple tasks, we varied the parameterization of each system, yielding a set of control tasks from each domain with varying dynamics. The optimal control policies for these systems vary widely with only minor changes in the system parameters, providing substantial diversity among the tasks within a single domain.

[å›¾ç‰‡: images\image_1.05798v1]
å›¾ç‰‡è¯´æ˜: Figure 1:Dynamical systems used in the experiments:a)simple mass system (left),b)cart-pole (middle), andc)quadrotor unmanned aerial vehicle (right).

Simple Mass Spring Damper:The simple mass (SM) system is
characterized by three parameters: the spring constantkğ‘˜kin N/m,
the damping constantdğ‘‘din Ns/m and the massmğ‘šmin kg. The systemâ€™s
state is given by the positionğ’™ğ’™\bm{x}andğ’™Ë™Ë™ğ’™\dot{\bm{x}}of the
mass, which varies according to a linear forceğ‘­ğ‘­\bm{F}. The goal is to train a policy for controlling the mass in a specific stateğ’ˆref=âŸ¨ğ’™ref,ğ’™Ë™refâŸ©subscriptğ’ˆrefsubscriptğ’™refsubscriptË™ğ’™ref\bm{g}_{\text{ref}}=\langle\bm{x}_{\text{ref}},\dot{\bm{x}}_{\text{ref}}\rangle.

Cart Pole:The cart-pole (CP) has been used extensively
as a benchmark for evaluating RL methods(Busoniu etÂ al.,2010). CP dynamics
are characterized by the cartâ€™s massmcsubscriptğ‘šğ‘m_{c}in kg, the poleâ€™s massmpsubscriptğ‘šğ‘m_{p}in kg, the poleâ€™s length in meters, and a damping parameterdğ‘‘din Ns/m. The state is given by the cartâ€™s positionğ’™ğ’™\bm{x}and velocityğ’™Ë™Ë™ğ’™\dot{\bm{x}}, as well as the poleâ€™s angleğœ½ğœ½\bm{\theta}and angular velocityğœ½Ë™Ë™ğœ½\dot{\bm{\theta}}. The goal is to
train a policy that controls the pole in an upright position.

We generated 10 tasks for each domain by varying the system
parameters to ensure a variety of tasks with diverse optimal policies,
including those with highly chaotic dynamics that are difficult to control.
We ran each experiment for a total ofRğ‘…Rrounds, varying from150150150for the simple mass to10,0001000010,000for the quadrotor to trainğ‘³ğ‘³\bm{L}andğ‘ºğ‘º\bm{S}, as well as for updating the PG-ELLA and PG models. At each roundjğ‘—j, the learner observed a tasktjsubscriptğ‘¡ğ‘—{t_{j}}through 50 trajectories of 150 steps and updatedğ‘³ğ‘³\bm{L}andğ’”tjsubscriptğ’”subscriptğ‘¡ğ‘—\bm{s}_{{t_{j}}}. The dimensionalitykğ‘˜kof the latent space was chosen independently for each domain via cross-validation over 3 tasks, and the learning step size for
each task domain was determined by a line search after gathering 10 trajectories of length 150. We used eNAC, a standard PG algorithm, as the base learner.

We compared our approach to both standard PG (i.e., eNAC) and PG-ELLA(Bou Ammar etÂ al.,2014), examining both the constrained and unconstrained variants of our algorithm. We also varied the number of iterations in our alternating optimization from101010to100100100to evaluate the effect of these inner iterations on the performance, as shown in Figures2and3. For the two MTL algorithms (our approach and PG-ELLA), the policy parameters for each tasktjsubscriptğ‘¡ğ‘—{t_{j}}were initialized
using the learned basis (i.e.,ğœ¶tj=ğ‘³â€‹ğ’”tjsubscriptğœ¶subscriptğ‘¡ğ‘—ğ‘³subscriptğ’”subscriptğ‘¡ğ‘—\bm{\alpha}_{{t_{j}}}=\bm{L}\bm{s}_{{t_{j}}}). We configured PG-ELLA as described byBou Ammar etÂ al. (2014), ensuring a fair comparison. For the standard PG learner, we provided additional trajectories in order to ensure a fair comparison, as described below.

For the experiments with policy constraints, we generated a set of constraints(ğ‘¨t,ğ’ƒt)subscriptğ‘¨ğ‘¡subscriptğ’ƒğ‘¡(\bm{A}_{t},\bm{b}_{t})for each task that restricted the policy parameters to pre-specified â€œsafeâ€ regions, as shown in Figures2(c)and2(d).
We also tested different values for the constraints onğ‘³ğ‘³\bm{L}, varyingpğ‘pandqğ‘qbetween0.10.10.1to101010; our approach showed robustness against this broad range, yielding similar average cost performance.

Figure2reports our results on the benchmark
simple mass and cart-pole systems. Figures2(a)and2(b)depicts the performance of the learned policy in a lifelong learning
setting over consecutive unconstrained tasks, averaged over all 10 systems over 100 different initial conditions. These results demonstrate that our approach is capable of outperforming both standard PG (which was provided with 50additionaltrajectories each iteration to ensure a more fair comparison) and PG-ELLA, both in terms of initial performance and learning speed. These figures also show that the performance of our method increases as it is given more alternating iterations per-round for fittingğ‘³ğ‘³\bm{L}andğ‘ºğ‘º\bm{S}.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: (a)Simple Mass

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: (a)Simple Mass

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: (a)Simple Mass

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: (a)Simple Mass

We evaluated the ability of these methods to respect safety constraints, as shown in
Figures2(c)and2(d). The thicker black lines in each figure depict the allowable â€œsafeâ€ region of the policy space. To enable online
learning per-task, the same tasktjsubscriptğ‘¡ğ‘—{t_{j}}was observed on each
round and the shared basisğ‘³ğ‘³\bm{L}and coefficientsğ’”tjsubscriptğ’”subscriptğ‘¡ğ‘—\bm{s}_{{t_{j}}}were updated using alternating optimization. We then plotted the change in the policy parameter
vectors per iterations (i.e.,ğœ¶tj=ğ‘³â€‹ğ’”tjsubscriptğœ¶subscriptğ‘¡ğ‘—ğ‘³subscriptğ’”subscriptğ‘¡ğ‘—\bm{\alpha}_{{t_{j}}}=\bm{L}\bm{s}_{{t_{j}}}) for each method, demonstrating that our approach abides by the safety constraints,
while standard PG and PG-ELLA can violate them (since they only solve an unconstrained optimization problem). In addition, these figures show that increasing the number of alternating iterations in our method causes it to take a more direct path to the optimal solution.

We also applied our approach to the more challenging domain of quadrotor control. The dynamics of the quadrotor system (Figure1) are influenced by inertial
constants aroundğ’†1,Bsubscriptğ’†1ğµ\bm{e}_{1,B},ğ’†2,Bsubscriptğ’†2ğµ\bm{e}_{2,B}, andğ’†3,Bsubscriptğ’†3ğµ\bm{e}_{3,B},
thrust factors influencing how the rotorâ€™s speed affects the overall
variation of the systemâ€™s state, and the lengths of the rods supporting
the rotors. Although the overall state of the system can be described
by a 12-dimensional vector, we focus on stability and so consider
only six of these state-variables. The quadrotor system has a high-dimensional
action space, where the goal is to control the four rotational velocities{wi}i=14superscriptsubscriptsubscriptğ‘¤ğ‘–ğ‘–14\{w_{i}\}_{i=1}^{4}of the rotors to stabilize the system. To ensure
realistic dynamics, we used the simulated model described by(Bouabdallah,2007; Voos & Bou Ammar,2010),
which has been verified and used in the control of physical quadrotors.

We generated 10 different quadrotor systems by varying the inertia around the
x, y and z-axes. We used a linear quadratic regulator, as described
byBouabdallah (2007), to initialize the policies in both the learning
and testing phases. We followed a similar experimental procedure to
that discussed above to update the models.

Figure3shows the performance of the unconstrained solution
as compared to standard PG and PG-ELLA. Again, our approach clearly outperforms
standard PG and PG-ELLA in both the initial performance
and learning speed. We also evaluated constrained tasks in a similar manner, again showing that our approach is capable of respecting constraints. Since the policy space is higher dimensional, we cannot visualize it as well as the benchmark systems, and so instead report the number of iterations it takes our approach to project the policy into the safe region. Figure4shows that our approach requires only one observation of the task to
acquire safe policies, which is substantially lower then standard PG or PG-ELLA (e.g., which require 545 and 510 observations, respectively,
in the quadrotor scenario).

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: Figure 3:Performance on quadrotor control.

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: Figure 4:Average number of task observations before acquiring policy parameters that abide by the constraints, showing that our approach immediately projects policies to safe regions.

We described the first lifelong PG learner that provides sublinear regretğ’ªâ€‹(R)ğ’ªğ‘…\mathcal{O}(\sqrt{R})withRğ‘…Rtotal rounds. In addition, our approach supports safety constraints on the learned policy, which are essential for robust learning in real applications. Our framework formalizes
lifelong learning as online MTL with limited
resources, and enables safe transfer by sharing policy parameters through
a latent knowledge base that is efficiently updated over time.

In this appendix, we derive the update equations forğ‘³ğ‘³\bm{L}andğ‘ºğ‘º\bm{S}in the special case of Gaussian policies. Please note that these derivations can be easily extended to other policy forms in higher dimensional action spaces.

For a tasktjsubscriptğ‘¡ğ‘—t_{j}, the policyÏ€ğœ¶tj(tj)â€‹(ğ’–m(k,tj)|ğ’™m(k,tj))subscriptsuperscriptğœ‹subscriptğ‘¡ğ‘—subscriptğœ¶subscriptğ‘¡ğ‘—conditionalsuperscriptsubscriptğ’–ğ‘šğ‘˜subscriptğ‘¡ğ‘—superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—\pi^{(t_{j})}_{\bm{\alpha}_{t_{j}}}\left(\bm{u}_{m}^{(k,t_{j})}|\bm{x}_{m}^{(k,t_{j})}\right)is given by:

Therefore, the safe lifelong reinforcement learning optimization objective can be written as:

To arrive at the update equations, we need to derive Eq.Â (13) with respect to eachğ‘³ğ‘³\bm{L}andğ‘ºğ‘º\bm{S}.

Starting with the derivative ofğ’†râ€‹(ğ‘³,ğ‘º)subscriptğ’†ğ‘Ÿğ‘³ğ‘º\bm{e}_{r}(\bm{L},\bm{S})with respect to the shared repositoryğ‘³ğ‘³\bm{L}, we can write:

To acquire the minimum, we set the above to zero:

Noting thatğ’”tjğ–³â€‹ğ‘³ğ–³â€‹ğš½â€‹(ğ’™m(k,tj))âˆˆâ„superscriptsubscriptğ’”subscriptğ‘¡ğ‘—ğ–³superscriptğ‘³ğ–³ğš½superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—â„\bm{s}_{t_{j}}^{\mathsf{T}}\bm{L}^{\mathsf{T}}\bm{\Phi}\left(\bm{x}_{m}^{(k,t_{j})}\right)\in\mathbb{R}, we can write:

To solve Eq.Â (14), we introduce the standardvecâ€‹(â‹…)vecâ‹…\text{vec}(\cdot)operator leading to:

Knowing that for a given set of matricesğ‘¨ğ‘¨\bm{A},ğ‘©ğ‘©\bm{B}, andğ‘¿ğ‘¿\bm{X},vecâ€‹(ğ‘¨â€‹ğ‘¿â€‹ğ‘©)=(ğ‘©ğ–³âŠ—ğ‘¨)â€‹vecâ€‹(ğ‘¿)vecğ‘¨ğ‘¿ğ‘©tensor-productsuperscriptğ‘©ğ–³ğ‘¨vecğ‘¿\text{vec}(\bm{A}\bm{X}\bm{B})=\left(\bm{B}^{\mathsf{T}}\otimes\bm{A}\right)\text{vec}(\bm{X}), we can write

By choosingğ’ğ‘³=2â€‹Î¼2â€‹ğ‘°dâ€‹kÃ—dâ€‹k+âˆ‘j=1rÎ·tjntjâ€‹Ïƒtj2â€‹âˆ‘k=1ntjâˆ‘m=0Mtjâˆ’1vecâ€‹(ğš½â€‹(ğ’™m(k,tj))â€‹ğ’”tjğ–³)â€‹(ğš½â€‹(ğ’™m(k,tj))âŠ—ğ’”tjğ–³)subscriptğ’ğ‘³2subscriptğœ‡2subscriptğ‘°ğ‘‘ğ‘˜ğ‘‘ğ‘˜superscriptsubscriptğ‘—1ğ‘Ÿsubscriptğœ‚subscriptğ‘¡ğ‘—subscriptğ‘›subscriptğ‘¡ğ‘—superscriptsubscriptğœsubscriptğ‘¡ğ‘—2superscriptsubscriptğ‘˜1subscriptğ‘›subscriptğ‘¡ğ‘—superscriptsubscriptğ‘š0subscriptğ‘€subscriptğ‘¡ğ‘—1vecğš½superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—superscriptsubscriptğ’”subscriptğ‘¡ğ‘—ğ–³tensor-productğš½superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—superscriptsubscriptğ’”subscriptğ‘¡ğ‘—ğ–³\bm{Z}_{\bm{L}}=2\mu_{2}\bm{I}_{dk\times dk}+\sum_{j=1}^{r}\frac{\eta_{t_{j}}}{n_{t_{j}}\sigma_{t_{j}}^{2}}\sum_{k=1}^{n_{t_{j}}}\sum_{m=0}^{M_{t_{j}}-1}\text{vec}\left(\bm{\Phi}\left(\bm{x}_{m}^{(k,t_{j})}\right)\bm{s}_{t_{j}}^{\mathsf{T}}\right)\left(\bm{\Phi}\left(\bm{x}_{m}^{(k,t_{j})}\right)\otimes\bm{s}_{t_{j}}^{\mathsf{T}}\right), andğ’—ğ‘³=âˆ‘j=1rÎ·tjntjâ€‹Ïƒtj2â€‹âˆ‘k=1ntjâˆ‘m=0Mtjâˆ’1vecâ€‹(ğ’–m(k,tj)â€‹ğš½â€‹(ğ’™m(k,tj))â€‹ğ’”tjğ–³)subscriptğ’—ğ‘³superscriptsubscriptğ‘—1ğ‘Ÿsubscriptğœ‚subscriptğ‘¡ğ‘—subscriptğ‘›subscriptğ‘¡ğ‘—superscriptsubscriptğœsubscriptğ‘¡ğ‘—2superscriptsubscriptğ‘˜1subscriptğ‘›subscriptğ‘¡ğ‘—superscriptsubscriptğ‘š0subscriptğ‘€subscriptğ‘¡ğ‘—1vecsuperscriptsubscriptğ’–ğ‘šğ‘˜subscriptğ‘¡ğ‘—ğš½superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—superscriptsubscriptğ’”subscriptğ‘¡ğ‘—ğ–³\bm{v}_{\bm{L}}=\sum_{j=1}^{r}\frac{\eta_{t_{j}}}{n_{t_{j}}\sigma_{t_{j}}^{2}}\sum_{k=1}^{n_{t_{j}}}\sum_{m=0}^{M_{t_{j}}-1}\text{vec}\left(\bm{u}_{m}^{(k,t_{j})}\bm{\Phi}\left(\bm{x}_{m}^{(k,t_{j})}\right)\bm{s}_{t_{j}}^{\mathsf{T}}\right), we can updateğ‘³=ğ’ğ‘³âˆ’1â€‹ğ’—ğ‘³ğ‘³superscriptsubscriptğ’ğ‘³1subscriptğ’—ğ‘³\bm{L}=\bm{Z}_{\bm{L}}^{-1}\bm{v}_{\bm{L}}.

To derive the update equations with respect toğ‘ºğ‘º\bm{S}, similar approach to that ofğ‘³ğ‘³\bm{L}can be followed. The derivative ofğ’†râ€‹(ğ‘³,ğ‘º)subscriptğ’†ğ‘Ÿğ‘³ğ‘º\bm{e}_{r}(\bm{L},\bm{S})with respect toğ‘ºğ‘º\bm{S}can be computed column-wise for all tasks observed so far:

Using a similar analysis to the previous section, choosing

we can updateğ’”tj=ğ’stjâˆ’1â€‹ğ’—ğ’”tjsubscriptğ’”subscriptğ‘¡ğ‘—superscriptsubscriptğ’subscriptğ‘ subscriptğ‘¡ğ‘—1subscriptğ’—subscriptğ’”subscriptğ‘¡ğ‘—\bm{s}_{t_{j}}=\bm{Z}_{s_{t_{j}}}^{-1}\bm{v}_{\bm{s}_{t_{j}}}.

In this appendix, we prove the claims and lemmas from the main paper, leading to sublinear regret (Theorem 1).

Assume the policy for a tasktjsubscriptğ‘¡ğ‘—{t_{j}}at a roundrğ‘Ÿrto be given byÏ€ğ›‚tj(tj)â€‹(ğ®m(k,tj)|ğ±m(k,tj))|ğ›‰^r=ğ’©â€‹(ğ›‚tjğ–³|ğ›‰^râ€‹ğš½â€‹(ğ±m(k,tj)),ğ›”tj)evaluated-atsuperscriptsubscriptğœ‹subscriptğ›‚subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘—conditionalsuperscriptsubscriptğ®ğ‘šğ‘˜subscriptğ‘¡ğ‘—superscriptsubscriptğ±ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscript^ğ›‰ğ‘Ÿğ’©evaluated-atsuperscriptsubscriptğ›‚subscriptğ‘¡ğ‘—ğ–³subscript^ğ›‰ğ‘Ÿğš½superscriptsubscriptğ±ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğ›”subscriptğ‘¡ğ‘—\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\left(\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}=\mathcal{N}\left(\bm{\alpha}_{{t_{j}}}^{\mathsf{T}}\Big{|}_{\hat{\bm{\theta}}_{r}}\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right),\bm{\sigma}_{{t_{j}}}\right),
forğ±m(k,tj)âˆˆğ’³tjsuperscriptsubscriptğ±ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğ’³subscriptğ‘¡ğ‘—\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{X}_{{t_{j}}}andğ®m(k,tj)âˆˆğ’°tjsuperscriptsubscriptğ®ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğ’°subscriptğ‘¡ğ‘—\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{U}_{{t_{j}}}withğ’³tjsubscriptğ’³subscriptğ‘¡ğ‘—\mathcal{X}_{{t_{j}}}andğ’°tjsubscriptğ’°subscriptğ‘¡ğ‘—\mathcal{U}_{{t_{j}}}representing
the state and action spaces, respectively. The gradientâˆ‡ğ›‚tjltjâ€‹(ğ›‚tj)|ğ›‰^revaluated-atsubscriptâˆ‡subscriptğ›‚subscriptğ‘¡ğ‘—subscriptğ‘™subscriptğ‘¡ğ‘—subscriptğ›‚subscriptğ‘¡ğ‘—subscript^ğ›‰ğ‘Ÿ\nabla_{\bm{\alpha}_{{t_{j}}}}l_{{t_{j}}}\left(\bm{\alpha}_{{t_{j}}}\right)\Big{|}_{\hat{\bm{\theta}}_{r}},
forltjâ€‹(ğ›‚tj)=âˆ’1/ntjâ€‹âˆ‘k=1ntjâˆ‘m=0Mtjâˆ’1logâ¡[Ï€ğ›‚tj(tj)â€‹(ğ®m(k,tj)|ğ±m(k,tj))]subscriptğ‘™subscriptğ‘¡ğ‘—subscriptğ›‚subscriptğ‘¡ğ‘—1subscriptğ‘›subscriptğ‘¡ğ‘—superscriptsubscriptğ‘˜1subscriptğ‘›subscriptğ‘¡ğ‘—superscriptsubscriptğ‘š0subscriptğ‘€subscriptğ‘¡ğ‘—1superscriptsubscriptğœ‹subscriptğ›‚subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘—conditionalsuperscriptsubscriptğ®ğ‘šğ‘˜subscriptğ‘¡ğ‘—superscriptsubscriptğ±ğ‘šğ‘˜subscriptğ‘¡ğ‘—l_{{t_{j}}}\left(\bm{\alpha}_{{t_{j}}}\right)=-\nicefrac{{1}}{{n_{{t_{j}}}}}\sum_{k=1}^{n_{{t_{j}}}}\sum_{m=0}^{M_{{t_{j}}}-1}\log\left[\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\left(\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\right]satisfies

withumax=maxk,mâ¡{|ğ®m(k,tj)|}subscriptğ‘¢maxsubscriptğ‘˜ğ‘šsuperscriptsubscriptğ®ğ‘šğ‘˜subscriptğ‘¡ğ‘—u_{\text{max}}=\max_{k,m}\left\{\left|\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\right|\right\}andğš½max=maxk,mâ¡{â€–ğš½â€‹(ğ±m(k,tj))â€–2}subscriptğš½maxsubscriptğ‘˜ğ‘šsubscriptnormğš½superscriptsubscriptğ±ğ‘šğ‘˜subscriptğ‘¡ğ‘—2\bm{\Phi}_{\text{max}}=\max_{k,m}\left\{\left|\left|\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\right|\right|_{2}\right\}for all trajectories and all tasks.

The proof of the above lemma will be provided as a
collection of claims. We start with the following:

Claim:GivenÏ€ğœ¶tj(tj)â€‹(ğ’–m(k)|ğ’™m(k))|ğœ½^r=ğ’©â€‹(ğœ¶tjğ–³|ğœ½^râ€‹ğš½â€‹(ğ’™m(k,tj)),ğˆtj)evaluated-atsuperscriptsubscriptğœ‹subscriptğœ¶subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘—conditionalsuperscriptsubscriptğ’–ğ‘šğ‘˜superscriptsubscriptğ’™ğ‘šğ‘˜subscript^ğœ½ğ‘Ÿğ’©evaluated-atsuperscriptsubscriptğœ¶subscriptğ‘¡ğ‘—ğ–³subscript^ğœ½ğ‘Ÿğš½superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğˆsubscriptğ‘¡ğ‘—\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\left(\bm{u}_{m}^{(k)}|\bm{x}_{m}^{(k)}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}=\mathcal{N}\left(\bm{\alpha}_{{t_{j}}}^{\mathsf{T}}\Big{|}_{\hat{\bm{\theta}}_{r}}\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right),\bm{\sigma}_{{t_{j}}}\right),
forğ’™m(k,tj)âˆˆğ’³tjsuperscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğ’³subscriptğ‘¡ğ‘—\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{X}_{{t_{j}}}andğ’–m(k,tj)âˆˆğ’°tjsuperscriptsubscriptğ’–ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğ’°subscriptğ‘¡ğ‘—\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{U}_{{t_{j}}},
andltjâ€‹(ğœ¶tj)=âˆ’1/ntjâ€‹âˆ‘k=1ntjâˆ‘m=0Mtjâˆ’1logâ¡[Ï€ğœ¶tj(tj)â€‹(ğ’–m(k,tj)|ğ’™m(k,tj))]subscriptğ‘™subscriptğ‘¡ğ‘—subscriptğœ¶subscriptğ‘¡ğ‘—1subscriptğ‘›subscriptğ‘¡ğ‘—superscriptsubscriptğ‘˜1subscriptğ‘›subscriptğ‘¡ğ‘—superscriptsubscriptğ‘š0subscriptğ‘€subscriptğ‘¡ğ‘—1superscriptsubscriptğœ‹subscriptğœ¶subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘—conditionalsuperscriptsubscriptğ’–ğ‘šğ‘˜subscriptğ‘¡ğ‘—superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—l_{{t_{j}}}\left(\bm{\alpha}_{{t_{j}}}\right)=-\nicefrac{{1}}{{n_{{t_{j}}}}}\sum_{k=1}^{n_{{t_{j}}}}\sum_{m=0}^{M_{{t_{j}}}-1}\log\left[\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\left(\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\right],||âˆ‡ğœ¶tjltjâ€‹(ğœ¶tj)|ğœ½^r||2evaluated-atsubscriptsubscriptâˆ‡subscriptğœ¶subscriptğ‘¡ğ‘—subscriptğ‘™subscriptğ‘¡ğ‘—subscriptğœ¶subscriptğ‘¡ğ‘—subscript^ğœ½ğ‘Ÿ2\left|\left|\nabla_{\bm{\alpha}_{{t_{j}}}}l_{{t_{j}}}\left(\bm{\alpha}_{{t_{j}}}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}\right|\right|_{2}satisfies

Proof:SinceÏ€ğœ¶tj(tj)â€‹(ğ’–m(k,tj)|ğ’™m(k,tj))|ğœ½^r=ğ’©â€‹(ğœ¶tjğ–³|ğœ½^râ€‹ğš½â€‹(ğ’™m(k,tj)),ğˆtj)evaluated-atsuperscriptsubscriptğœ‹subscriptğœ¶subscriptğ‘¡ğ‘—subscriptğ‘¡ğ‘—conditionalsuperscriptsubscriptğ’–ğ‘šğ‘˜subscriptğ‘¡ğ‘—superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscript^ğœ½ğ‘Ÿğ’©evaluated-atsuperscriptsubscriptğœ¶subscriptğ‘¡ğ‘—ğ–³subscript^ğœ½ğ‘Ÿğš½superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğˆsubscriptğ‘¡ğ‘—\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\left(\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}=\mathcal{N}\left(\bm{\alpha}_{{t_{j}}}^{\mathsf{T}}\Big{|}_{\hat{\bm{\theta}}_{r}}\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right),\bm{\sigma}_{{t_{j}}}\right),
we can write

Therefore:

Denotingmaxk,mâ¡{|ğ’–m(k,tj)|}=umaxsubscriptğ‘˜ğ‘šsuperscriptsubscriptğ’–ğ‘šğ‘˜subscriptğ‘¡ğ‘—subscriptğ‘¢max\max_{k,m}\left\{\left|\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\right|\right\}=u_{\text{max}}andmaxk,mâ¡{â€–ğš½â€‹(ğ’™m(k,tj))â€–2}=ğš½maxsubscriptğ‘˜ğ‘šsubscriptnormğš½superscriptsubscriptğ’™ğ‘šğ‘˜subscriptğ‘¡ğ‘—2subscriptğš½max\max_{k,m}\left\{\left|\left|\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\right|\right|_{2}\right\}=\bm{\Phi}_{\text{max}}for all trajectories and all tasks, we can write

Using the Cauchy-Shwarz inequality(Horn & Mathias,1990), we can upper
boundmaxk,m{|âŸ¨ğœ¶tj|ğœ½^r,ğš½(ğ’™m(k,tj))âŸ©|}\max_{k,m}\left\{\left|\left\langle\bm{\alpha}_{{t_{j}}}\Big{|}_{\hat{\bm{\theta}}_{r}},\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\right\rangle\right|\right\}as

Finalizing the statement of the claim, the overall bound on the norm
of the gradient ofltjâ€‹(ğœ¶tj)subscriptğ‘™subscriptğ‘¡ğ‘—subscriptğœ¶subscriptğ‘¡ğ‘—l_{{t_{j}}}(\bm{\alpha}_{{t_{j}}})can be
written as

â– â– \blacksquare

Claim:The norm of the gradient of the loss function satisfies:

Proof:As mentioned previously, we consider
the linearization of the loss functionltjsubscriptğ‘™subscriptğ‘¡ğ‘—l_{{t_{j}}}around the
constraint solution of the previous round,ğœ½^rsubscript^ğœ½ğ‘Ÿ\hat{\bm{\theta}}_{r}.
Sinceğœ½^rsubscript^ğœ½ğ‘Ÿ\hat{\bm{\theta}}_{r}satisfiesğ‘¨tkâ€‹ğœ¶tk=ğ’ƒtkâˆ’ğ’„tk,âˆ€tkâˆˆâ„râˆ’1formulae-sequencesubscriptğ‘¨subscriptğ‘¡ğ‘˜subscriptğœ¶subscriptğ‘¡ğ‘˜subscriptğ’ƒsubscriptğ‘¡ğ‘˜subscriptğ’„subscriptğ‘¡ğ‘˜for-allsubscriptğ‘¡ğ‘˜subscriptâ„ğ‘Ÿ1\bm{A}_{{t_{k}}}\bm{\alpha}_{{t_{k}}}=\bm{b}_{{t_{k}}}-\bm{c}_{{t_{k}}},\forall{t_{k}}\in\mathcal{I}_{r-1}.
Hence, we can write

Therefore

Combining the above results with those of Eq.Â (16)
we arrive at

â– â– \blacksquare

The previous result finalizes the statement of the lemma, bounding
the gradient of the loss function in terms of thesafetyconstraints.

âˆ

The norm of the gradient
of the loss function evaluated atğ›‰^rsubscript^ğ›‰ğ‘Ÿ\hat{\bm{\theta}}_{r}satisfies

The derivative ofltjâ€‹(ğœ½)|ğœ½^revaluated-atsubscriptğ‘™subscriptğ‘¡ğ‘—ğœ½subscript^ğœ½ğ‘Ÿl_{{t_{j}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}can be written as

The results of Lemma1boundâ€–âˆ‡ğœ¶tjltjâ€‹(ğœ½)|ğœ½^râˆ¥22evaluated-atsubscriptdelimited-â€–|subscriptâˆ‡subscriptğœ¶subscriptğ‘¡ğ‘—subscriptğ‘™subscriptğ‘¡ğ‘—ğœ½subscript^ğœ½ğ‘Ÿ22\left\|\nabla_{\bm{\alpha}_{{t_{j}}}}l_{{t_{j}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2}^{2}.

Now, we target to bound each of||ğ’”tj|ğœ½^r||22evaluated-atsubscriptsubscriptğ’”subscriptğ‘¡ğ‘—subscript^ğœ½ğ‘Ÿ22\Big{|}\Big{|}\bm{s}_{{t_{j}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{2}^{2}and||ğ‘³|ğœ½^r||ğ–¥2evaluated-atsubscriptğ‘³subscript^ğœ½ğ‘Ÿğ–¥2\Big{|}\Big{|}\bm{L}\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{\mathsf{F}}^{2}.

Considering the constraintğ‘¨tjâ€‹ğ‘³â€‹ğ’”tj+ğ’„tj=ğ’ƒtjsubscriptğ‘¨subscriptğ‘¡ğ‘—ğ‘³subscriptğ’”subscriptğ‘¡ğ‘—subscriptğ’„subscriptğ‘¡ğ‘—subscriptğ’ƒsubscriptğ‘¡ğ‘—\bm{A}_{{t_{j}}}\bm{L}\bm{s}_{{t_{j}}}+\bm{c}_{{t_{j}}}=\bm{b}_{{t_{j}}}for a tasktjsubscriptğ‘¡ğ‘—{t_{j}}, we realize thatğ’”tj=ğ‘³+â€‹(ğ‘¨tj+â€‹(ğ’ƒtjâˆ’ğ’„tj))subscriptğ’”subscriptğ‘¡ğ‘—superscriptğ‘³superscriptsubscriptğ‘¨subscriptğ‘¡ğ‘—subscriptğ’ƒsubscriptğ‘¡ğ‘—subscriptğ’„subscriptğ‘¡ğ‘—\bm{s}_{{{t_{j}}}}=\bm{L}^{+}\left(\bm{A}_{{t_{j}}}^{+}\left(\bm{b}_{{t_{j}}}-\bm{c}_{{t_{j}}}\right)\right).
Therefore,

Noting that

To relateâ€–ğ‘³+â€–ğŸ¤subscriptnormsuperscriptğ‘³2\left|\left|\bm{L}^{+}\right|\right|_{\mathsf{2}}toâ€–ğ‘³â€–ğ–¥subscriptnormğ‘³ğ–¥\left|\left|\bm{L}\right|\right|_{\mathsf{F}}, we need to boundâ€–(ğ‘³ğ–³â€‹ğ‘³)âˆ’1â€–2subscriptnormsuperscriptsuperscriptğ‘³ğ–³ğ‘³12\left|\left|\left(\bm{L}^{\mathsf{T}}\bm{L}\right)^{-1}\right|\right|_{2}in terms ofâ€–ğ‘³â€–ğ–¥subscriptnormğ‘³ğ–¥\|\bm{L}\|_{\mathsf{F}}. Denoting the spectrum ofğ‘³ğ–³â€‹ğ‘³superscriptğ‘³ğ–³ğ‘³\bm{L}^{\mathsf{T}}\bm{L}asspecâ€‹(ğ‘³ğ–³â€‹ğ‘³)={ğ€1,â€¦,ğ€k}specsuperscriptğ‘³ğ–³ğ‘³subscriptğ€1â€¦subscriptğ€ğ‘˜\text{spec}\left(\bm{L}^{\mathsf{T}}\bm{L}\right)=\left\{\bm{\lambda}_{1},\dots,\bm{\lambda}_{k}\right\}such that0<ğ€1â‰¤â‹¯â‰¤ğ€k0subscriptğ€1â‹¯subscriptğ€ğ‘˜0<\bm{\lambda}_{1}\leq\dots\leq\bm{\lambda}_{k}, thenspectâ€‹((ğ‘³ğ–³â€‹ğ‘³)âˆ’1)={1/ğ€1,â€¦,1/ğ€k}spectsuperscriptsuperscriptğ‘³ğ–³ğ‘³11subscriptğ€1â€¦1subscriptğ€ğ‘˜\text{spect}\left(\left(\bm{L}^{\mathsf{T}}\bm{L}\right)^{-1}\right)=\left\{\nicefrac{{1}}{{\bm{\lambda}_{1}}},\dots,\nicefrac{{1}}{{\bm{\lambda}_{k}}}\right\}such that1/ğ€kâ‰¤â‹¯â‰¤1/ğ€k1subscriptğ€ğ‘˜â‹¯1subscriptğ€ğ‘˜\nicefrac{{1}}{{\bm{\lambda}_{k}}}\leq\dots\leq\nicefrac{{1}}{{\bm{\lambda}_{k}}}.
Hence,â€–(ğ‘³ğ–³â€‹ğ‘³)âˆ’1â€–2=maxâ¡{specâ€‹((ğ‘³ğ–³â€‹ğ‘³)âˆ’1)}=1/ğ€1=1/Î»minâ€‹(ğ‘³ğ–³â€‹ğ‘³)subscriptnormsuperscriptsuperscriptğ‘³ğ–³ğ‘³12specsuperscriptsuperscriptğ‘³ğ–³ğ‘³11subscriptğ€11subscriptğœ†superscriptğ‘³ğ–³ğ‘³\left|\left|\left(\bm{L}^{\mathsf{T}}\bm{L}\right)^{-1}\right|\right|_{2}=\max\left\{\text{spec}\left(\left(\bm{L}^{\mathsf{T}}\bm{L}\right)^{-1}\right)\right\}=\nicefrac{{1}}{{\bm{\lambda}_{1}}}=\nicefrac{{1}}{{\lambda_{\min}\left(\bm{L}^{\mathsf{T}}\bm{L}\right)}}.
Noticing thatspecâ€‹(ğ‘³ğ–³â€‹ğ‘³)=specâ€‹(ğ‘³â€‹ğ‘³ğ–³)specsuperscriptğ‘³ğ–³ğ‘³specğ‘³superscriptğ‘³ğ–³\text{spec}\left(\bm{L}^{\mathsf{T}}\bm{L}\right)=\text{spec}\left(\bm{L}\bm{L}^{\mathsf{T}}\right),
we recognizeâ€–(ğ‘³ğ–³â€‹ğ‘³)âˆ’1â€–2=1/ğ€minâ€‹(ğ‘³â€‹ğ‘³ğ–³)â‰¤1/psubscriptnormsuperscriptsuperscriptğ‘³ğ–³ğ‘³121subscriptğ€minğ‘³superscriptğ‘³ğ–³1ğ‘\left|\left|\left(\bm{L}^{\mathsf{T}}\bm{L}\right)^{-1}\right|\right|_{2}=\nicefrac{{1}}{{\bm{\lambda}_{\text{min}}\left(\bm{L}\bm{L}^{\mathsf{T}}\right)}}\leq\nicefrac{{1}}{{p}}.
Therefore

Plugging the results of Eq.Â (18) into Eq.Â (17),
we arrive at

Finally, sinceğœ½^rsubscript^ğœ½ğ‘Ÿ\hat{\bm{\theta}}_{r}satisfies
the constraints, we note thatâ€–ğ‘³|ğœ½^râˆ¥ğ–¥2â‰¤qÃ—devaluated-atsubscriptdelimited-â€–|ğ‘³subscript^ğœ½ğ‘Ÿğ–¥2ğ‘ğ‘‘\left\|\bm{L}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{\mathsf{F}}^{2}\leq q\times d.
Consequently,

âˆ

The L2norm of the constraint solution at roundrâˆ’1ğ‘Ÿ1r-1,â€–ğ›‰^râ€–22superscriptsubscriptnormsubscript^ğ›‰ğ‘Ÿ22\|\hat{\bm{\theta}}_{r}\|_{2}^{2}is bounded by

with|â„râˆ’1|subscriptâ„ğ‘Ÿ1\left|\mathcal{I}_{r-1}\right|being the cardinality
ofâ„râˆ’1subscriptâ„ğ‘Ÿ1\mathcal{I}_{r-1}representing the number of
different tasks observed so-far.

Noting
thatğœ½^r=[ğœ½1,â€¦,ğœ½dâ€‹kâŸğ‘³|ğœ½^r,ğœ½dâ€‹k+1,â€¦âŸğ’”i1|Î¸^r,â€¦,â€¦âŸğ’”irâˆ’1|Î¸r^,â€¦,ğœ½dâ€‹k+kâ€‹Tâ‹†âŸğŸâ€™s: unobserved tasks]ğ–³subscript^ğœ½ğ‘ŸsuperscriptsubscriptâŸsubscriptğœ½1â€¦subscriptğœ½ğ‘‘ğ‘˜evaluated-atğ‘³subscript^ğœ½ğ‘ŸsubscriptâŸsubscriptğœ½ğ‘‘ğ‘˜1â€¦evaluated-atsubscriptğ’”subscriptğ‘–1subscript^ğœƒğ‘ŸsubscriptâŸâ€¦â€¦evaluated-atsubscriptğ’”subscriptğ‘–ğ‘Ÿ1^subscriptğœƒğ‘ŸsubscriptâŸâ€¦subscriptğœ½ğ‘‘ğ‘˜ğ‘˜superscriptğ‘‡â‹†ğŸâ€™s: unobserved tasksğ–³\hat{\bm{\theta}}_{r}=\Big{[}\underbrace{\bm{\theta}_{1},\dots,\bm{\theta}_{dk}}_{\bm{L}\Big{|}_{\hat{\bm{\theta}}_{r}}},\underbrace{\bm{\theta}_{dk+1},\dots}_{\bm{s}_{i_{1}}\Big{|}_{\hat{\theta}_{r}}},\underbrace{\dots,\dots}_{\bm{s}_{i_{r-1}}\Big{|}_{\hat{\theta_{r}}}},\underbrace{\dots,\bm{\theta}_{dk+kT^{\star}}}_{\text{$\bm{0}$'s: unobserved tasks}}\Big{]}^{\mathsf{T}},
it is easy to see

âˆ

The L2norm of the linearizing term ofltjâ€‹(ğ›‰)subscriptğ‘™subscriptğ‘¡ğ‘—ğ›‰l_{{t_{j}}}(\bm{\theta})aroundğ›‰^rsubscript^ğ›‰ğ‘Ÿ\hat{\bm{\theta}}_{r},â€–ğŸ^tj|ğ›‰^râˆ¥2evaluated-atsubscriptdelimited-â€–|subscript^ğŸsubscriptğ‘¡ğ‘—subscript^ğ›‰ğ‘Ÿ2\left\|\hat{\bm{f}}_{{t_{j}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2},
is bounded by

withğ›…ltjsubscriptğ›…subscriptğ‘™subscriptğ‘¡ğ‘—\bm{\delta}_{l_{{t_{j}}}}being the constant upper-bound
on|ltj(ğ›‰)|ğ›‰^r|\left|l_{{t_{j}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}\right|,
and

We have previously shown that||ğ’‡^tj|ğœ½^r||2â‰¤||âˆ‡ğœ½ltjâ€‹(ğœ½)|ğœ½^r||2+|ltjâ€‹(Î¸^r)|+||âˆ‡ğœ½ltjâ€‹(ğœ½)|ğœ½^r||2Ã—â€–ğœ½^râ€–2evaluated-atsubscriptsubscript^ğ’‡subscriptğ‘¡ğ‘—subscript^ğœ½ğ‘Ÿ2evaluated-atsubscriptsubscriptâˆ‡ğœ½subscriptğ‘™subscriptğ‘¡ğ‘—ğœ½subscript^ğœ½ğ‘Ÿ2subscriptğ‘™subscriptğ‘¡ğ‘—subscript^ğœƒğ‘Ÿevaluated-atsubscriptsubscriptâˆ‡ğœ½subscriptğ‘™subscriptğ‘¡ğ‘—ğœ½subscript^ğœ½ğ‘Ÿ2subscriptnormsubscript^ğœ½ğ‘Ÿ2\Big{|}\Big{|}\hat{\bm{f}}_{{t_{j}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{2}\leq\Big{|}\Big{|}\nabla_{\bm{\theta}}l_{{t_{j}}}\left(\bm{\theta}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{2}+\Big{|}l_{{t_{j}}}\left(\hat{\theta}_{r}\right)\Big{|}+\Big{|}\Big{|}\nabla_{\bm{\theta}}l_{{t_{j}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{2}\times\Big{|}\Big{|}\hat{\bm{\theta}}_{r}\Big{|}\Big{|}_{2}.
Using the previously derived lemmas we can upper-bound||ğ’‡^tj|ğœ½^r||2evaluated-atsubscriptsubscript^ğ’‡subscriptğ‘¡ğ‘—subscript^ğœ½ğ‘Ÿ2\Big{|}\Big{|}\hat{\bm{f}}_{{t_{j}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{2}as follows

Further,

Therefore

withğœ¹ltjsubscriptğœ¹subscriptğ‘™subscriptğ‘¡ğ‘—\bm{\delta}_{l_{{t_{j}}}}being the constant upper-bound
on|ltj(ğœ½)|ğœ½^r|\left|l_{{t_{j}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}\right|,
and

âˆ

AfterRğ‘…Rrounds and choosingÎ·t1=â‹¯=Î·tj=Î·=1Rsubscriptğœ‚subscriptğ‘¡1â‹¯subscriptğœ‚subscriptğ‘¡ğ‘—ğœ‚1ğ‘…\eta_{{t_{1}}}=\dots=\eta_{{t_{j}}}=\eta=\frac{1}{\sqrt{R}},ğ‹|ğ›‰^1=diagğ¤â€‹(Î¶)evaluated-atğ‹subscript^ğ›‰1subscriptdiagğ¤ğœ\bm{L}\Big{|}_{\hat{\bm{\theta}}_{1}}=\text{diag}_{\bm{k}}(\zeta),
withdiagğ¤â€‹(â‹…)subscriptdiagğ¤â‹…\text{diag}_{\bm{k}}(\cdot)being a diagonal matrix among
theğ¤ğ¤\bm{k}columns ofğ‹ğ‹\bm{L},pâ‰¤Î¶2â‰¤qğ‘superscriptğœ2ğ‘p\leq\zeta^{2}\leq q, andğ’|ğ›‰^1=ğŸkÃ—|ğ’¯|evaluated-atğ’subscript^ğ›‰1subscript0ğ‘˜ğ’¯\bm{S}\Big{|}_{\hat{\bm{\theta}}_{1}}=\bm{0}_{k\times{|\mathcal{T}|}},
for anyğ®âˆˆğ’¦ğ®ğ’¦\bm{u}\in\mathcal{K}our algorithm exhibits a sublinear regret of the form

Given the ingredients of the previous section, next
we derive the sublinear regret results which finalize the statement
of the theorem. First, it is easy to see that

Further, from strong convexity of the regularizer we obtain:

It can be seen that

Finally, for anyğ’–âˆˆğ’¦ğ’–ğ’¦\bm{u}\in\mathcal{K}, we have:

AssumingÎ·t1=â‹¯=Î·tj=Î·subscriptğœ‚subscriptğ‘¡1â‹¯subscriptğœ‚subscriptğ‘¡ğ‘—ğœ‚\eta_{{t_{1}}}=\dots=\eta_{{t_{j}}}=\eta, we can derive

The following lemma finalizes the statement of the theorem:

After T rounds and forÎ·t1=â‹¯=Î·tj=Î·=1Rsubscriptğœ‚subscriptğ‘¡1â‹¯subscriptğœ‚subscriptğ‘¡ğ‘—ğœ‚1ğ‘…\eta_{{t_{1}}}=\dots=\eta_{{t_{j}}}=\eta=\frac{1}{\sqrt{R}},
our algorithm exhibits, for anyğ®âˆˆğ’¦ğ®ğ’¦\bm{u}\in\mathcal{K}, a sublinear
regret of the form

It is then easy to see

Since|â„Râˆ’1|â‰¤|ğ’¯|subscriptâ„ğ‘…1ğ’¯|\mathcal{I}_{R-1}|\leq{|\mathcal{T}|}with|ğ’¯|ğ’¯{|\mathcal{T}|}being the total number of tasks available, then we can write

withğœ¸5=8â€‹d/p2â€‹qâ€‹ğœ¸12â€‹(R)â€‹maxtkâˆˆâ„Râˆ’1â¡{â€–ğ‘¨tkâ€ â€–22â€‹(â€–ğ’ƒtkâ€–2+ğ’„max)2}subscriptğœ¸58ğ‘‘superscriptğ‘2ğ‘superscriptsubscriptğœ¸12ğ‘…subscriptsubscriptğ‘¡ğ‘˜subscriptâ„ğ‘…1superscriptsubscriptnormsuperscriptsubscriptğ‘¨subscriptğ‘¡ğ‘˜â€ 22superscriptsubscriptnormsubscriptğ’ƒsubscriptğ‘¡ğ‘˜2subscriptğ’„max2\bm{\gamma}_{5}=8\nicefrac{{d}}{{p^{2}}}q\bm{\gamma}_{1}^{2}(R)\max_{{t_{k}}\in\mathcal{I}_{R-1}}\left\{\|\bm{A}_{{t_{k}}}^{\dagger}\|_{2}^{2}\left(\|\bm{b}_{{t_{k}}}\|_{2}+\bm{c}_{\text{max}}\right)^{2}\right\}.
Further, it is easy to see thatğ›€0â€‹(ğ’–)â‰¤qâ€‹d+ğœ¸5â€‹(R)â€‹|ğ’¯|subscriptğ›€0ğ’–ğ‘ğ‘‘subscriptğœ¸5ğ‘…ğ’¯\bm{\Omega}_{0}(\bm{u})\leq qd+\bm{\gamma}_{5}(R){|\mathcal{T}|}withğœ¸5â€‹(R)subscriptğœ¸5ğ‘…\bm{\gamma}_{5}(R)being a constant, which
leads to

InitializingLğ¿\bm{L}andSğ‘†\bm{S}:We initializeğ‘³|ğœ½^1=diagğ’Œâ€‹(Î¶)evaluated-atğ‘³subscript^ğœ½1subscriptdiagğ’Œğœ\bm{L}\Big{|}_{\hat{\bm{\theta}}_{1}}=\text{diag}_{\bm{k}}(\zeta),
withpâ‰¤Î¶2â‰¤qğ‘superscriptğœ2ğ‘p\leq\zeta^{2}\leq qandğ‘º|ğœ½^1=ğŸkÃ—|ğ’¯|evaluated-atğ‘ºsubscript^ğœ½1subscript0ğ‘˜ğ’¯\bm{S}\Big{|}_{\hat{\bm{\theta}}_{1}}=\bm{0}_{k\times{|\mathcal{T}|}}ensures the invertability ofğ‘³ğ‘³\bm{L}and that the constraints are
met. This leads us to

ChoosingÎ·t1=â‹¯=Î·tj=Î·=1/Rsubscriptğœ‚subscriptğ‘¡1â‹¯subscriptğœ‚subscriptğ‘¡ğ‘—ğœ‚1ğ‘…\eta_{{t_{1}}}=\dots=\eta_{{t_{j}}}=\eta=\nicefrac{{1}}{{\sqrt{R}}},
we acquire sublinear regret, finalizing the statement of the theorem:

withğœ¸5â€‹(R)subscriptğœ¸5ğ‘…\bm{\gamma}_{5}(R)being a constant. âˆ

âˆ

[å›¾ç‰‡: images\image_8.png]

[å›¾ç‰‡: images\image_9.png]

