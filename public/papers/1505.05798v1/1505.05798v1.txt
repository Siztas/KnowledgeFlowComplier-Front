标题：Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret

Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time,sublinear regretfor lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.

Reinforcement learning (RL)(Busoniu et al.,2010; Sutton & Barto,1998)often requires substantial experience before achieving acceptable performance on individual control problems. One major contributor to this issue is thetabula-rasaassumption of typical RL methods, which learn from scratch on each new task. In these settings, learning performance is directly correlated with the quality of the acquired samples. Unfortunately, the amount of experience necessary for high-quality performance increases exponentially with the tasks’ degrees of freedom, inhibiting the application of RL to high-dimensional control problems.

When data is in limited supply, transfer learning can significantly improve model performance on new tasks by reusing previous learned knowledge during training(Taylor & Stone,2009; Gheshlaghi Azar et al.,2013; Lazaric,2011; Ferrante et al.,2008; Bou Ammar et al.,2012). Multi-task learning (MTL) explores another notion of knowledge transfer, in which task models are trained simultaneously and share knowledge during the joint learning process(Wilson et al.,2007; Zhang et al.,2008).

In thelifelong learningsetting(Thrun & O’Sullivan,1996a,b), which can be framed
as an online MTL problem, agents acquire knowledge incrementally
by learning multiple tasks consecutively over their lifetime.
Recently, based on the work ofRuvolo & Eaton (2013)on supervised lifelong learning,Bou Ammar et al. (2014)developed a lifelong learner for policy gradient RL. To ensure efficient learning over consecutive tasks, these works employ a second-order Taylor expansion around the parameters that are (locally) optimal for each task without transfer. This assumption simplifies
the MTL objective into a weighted quadratic form for online learning, but since it is based on single-task learning, this technique can lead to parameters far from globally optimal.
Consequently, the success of these methods for RL highly depends on the policy initializations, which must lead to near-optimal trajectories for meaningful updates. Also, since their objective functions average loss over all tasks, these methods exhibit non-vanishing regrets of the form𝒪​(R)𝒪𝑅\mathcal{O}(R), whereR𝑅Ris the total number of rounds in a non-adversarial setting.

In addition, these methods may produce control policies with unsafe behavior (i.e., capable
of causing damage to the agent or environment, catastrophic failure,
etc.). This is a critical issue in robotic control, where unsafe control
policies can lead to physical damage or user injury. This problem is caused by using constraint-free optimization
over the shared knowledge during the transfer process, which may lead to uninformative or
unbounded policies.

In this paper, we address these issues by proposing the firstsafe lifelong learnerfor policy gradient RL operating in an adversarial framework. Our approach rapidly learns
high-performancesafe control policiesbased on the agent’s
previously learned knowledge and safety constraints on each task, accumulating knowledge over multiple consecutive tasks to optimize overall performance. We theoretically analyze the regret
exhibited by our algorithm, showingsublineardependency of
the form𝒪​(R)𝒪𝑅\mathcal{O}(\sqrt{R})forR𝑅Rrounds, thus outperforming current methods. We then evaluate our approach empirically on a set of dynamical systems.

An RL agent sequentially chooses actions to minimize its expected
cost. Such problems are formalized as Markov decision processes (MDPs)⟨𝒳,𝒰,𝒫,𝒄,γ⟩𝒳𝒰𝒫𝒄𝛾\left\langle\mathcal{X},\mathcal{U},\mathcal{P},\bm{c},\gamma\right\rangle,
where𝒳⊂ℝd𝒳superscriptℝ𝑑\mathcal{X}\subset\mathbb{R}^{d}is the (potentially infinite)
state space,𝒰∈ℝda𝒰superscriptℝsubscript𝑑𝑎\mathcal{U}\in\mathbb{R}^{d_{a}}is the set of all
possible actions,𝒫:𝒳×𝒰×𝒳→[0,1]:𝒫→𝒳𝒰𝒳01\mathcal{P}:\mathcal{X}\times\mathcal{U}\times\mathcal{X}\rightarrow[0,1]is a state transition probability describing the system’s dynamics,𝒄:𝒳×𝒰×𝒳→ℝ:𝒄→𝒳𝒰𝒳ℝ\bm{c}:\mathcal{X}\times\mathcal{U}\times\mathcal{X}\rightarrow\mathbb{R}is the cost function measuring the agent’s performance, andγ∈[0,1]𝛾01\gamma\in[0,1]is a discount factor. At each time stepm𝑚m, the agent is in state𝒙m∈𝒳subscript𝒙𝑚𝒳\bm{x}_{m}\in\mathcal{X}and must choose an action𝒖m∈𝒰subscript𝒖𝑚𝒰\bm{u}_{m}\in\mathcal{U},
transitioning it to a new state𝒙m+1∼𝒫​(𝒙m+1|𝒙m,𝒖m)similar-tosubscript𝒙𝑚1𝒫conditionalsubscript𝒙𝑚1subscript𝒙𝑚subscript𝒖𝑚\bm{x}_{m+1}\sim\mathcal{P}\left(\bm{x}_{m+1}|\bm{x}_{m},\bm{u}_{m}\right)and yielding a cost𝒄m+1=𝒄​(𝒙m+1,𝒖m,𝒙m)subscript𝒄𝑚1𝒄subscript𝒙𝑚1subscript𝒖𝑚subscript𝒙𝑚\bm{c}_{m+1}=\bm{c}(\bm{x}_{m+1},\bm{u}_{m},\bm{x}_{m}).
The sequence of state-action pairs forms a trajectory𝝉=[𝒙0:M−1,𝒖0:M−1]𝝉subscript𝒙:0𝑀1subscript𝒖:0𝑀1\bm{\tau}=\left[\bm{x}_{0:M-1},\bm{u}_{0:M-1}\right]over a (possibly infinite) horizonM𝑀M. A policyπ:𝒳×𝒰→[0,1]:𝜋→𝒳𝒰01\pi:\mathcal{X}\times\mathcal{U}\rightarrow[0,1]specifies a probability distribution over state-action pairs, whereπ​(𝒖|𝒙)𝜋conditional𝒖𝒙\pi\left(\bm{u}|\bm{x}\right)represents the probability of selecting
an action𝒖𝒖\bm{u}in state𝒙𝒙\bm{x}. The goal of RL is to find
an optimal policyπ⋆superscript𝜋⋆\pi^{\star}that minimizes the total expected cost.

Policy search methodshave shown success in solving high-dimensional
problems, such as robotic control(Kober & Peters,2011; Peters & Schaal,2008a; Sutton et al.,2000).
These methods represent the policyπ𝜶​(𝒖|𝒙)subscript𝜋𝜶conditional𝒖𝒙\pi_{\bm{\alpha}}(\bm{u}|\bm{x})using a vector𝜶∈ℝd𝜶superscriptℝ𝑑\bm{\alpha}\in\mathbb{R}^{d}of control parameters.
The optimal policyπ⋆superscript𝜋⋆\pi^{\star}is found by determining the parameters𝜶⋆superscript𝜶⋆\bm{\alpha}^{\star}that minimize the expected average cost:

wheren𝑛nis the total number of trajectories, andp𝜶​(𝝉(k))subscript𝑝𝜶superscript𝝉𝑘p_{\bm{\alpha}}\!\left(\bm{\tau}^{(k)}\right)and𝑪​(𝝉(k))𝑪superscript𝝉𝑘\bm{C}\!\left(\bm{\tau}^{(k)}\right)are the probability and
cost of trajectory𝝉(k)superscript𝝉𝑘\bm{\tau}^{(k)}:

with an initial state distribution𝒫0:𝒳→[0,1]:subscript𝒫0→𝒳01\mathcal{P}_{0}:\mathcal{X}\rightarrow[0,1]. We handle a constrained version of policy search, in which optimality not only corresponds to minimizing the total expected cost, but also to ensuring that the policy satisfies safety constraints. These constraints vary between applications, for example corresponding to maximum joint torque or prohibited physical positions.

In this paper, we employ a special form ofregret minimization games, which we briefly review here. A regret minimization game is a triple⟨𝒦,ℱ,R⟩𝒦ℱ𝑅\langle\mathcal{K},\mathcal{F},R\rangle, where𝒦𝒦\mathcal{K}is a non-empty decision set,ℱℱ\mathcal{F}is the set of moves of the adversary which contains bounded convex functions fromℝnsuperscriptℝ𝑛\mathbb{R}^{n}toℝℝ\mathbb{R}, andR𝑅Ris the total number of rounds. The game proceeds in rounds, where at each roundj=1,…,R𝑗1…𝑅j=1,\dots,R, the agent chooses a prediction𝜽j∈𝒦subscript𝜽𝑗𝒦\bm{\theta}_{j}\in\mathcal{K}and the environment (i.e., the adversary) chooses a loss functionlj∈ℱsubscript𝑙𝑗ℱl_{j}\in\mathcal{F}. At the end of the round, the loss functionljsubscript𝑙𝑗l_{j}is revealed to the agent and the decision𝜽jsubscript𝜽𝑗\bm{\theta}_{j}is revealed to the environment. In this paper, we handle the full-information case, where the agent may observe the entire loss functionljsubscript𝑙𝑗l_{j}as its feedback and can exploit this in making decisions. The goal is to minimize the cumulative regret∑j=1Rlj​(𝜽j)−inf𝒖∈𝒦​[∑j=1Rlj​(𝒖)]superscriptsubscript𝑗1𝑅subscript𝑙𝑗subscript𝜽𝑗subscriptinf𝒖𝒦delimited-[]superscriptsubscript𝑗1𝑅subscript𝑙𝑗𝒖\sum_{j=1}^{R}l_{j}(\bm{\theta}_{j})-\text{inf}_{\bm{u}\in\mathcal{K}}\left[\sum_{j=1}^{R}l_{j}(\bm{u})\right].
When analyzing the regret of our methods, we use a variant of this definition to handle the lifelong RL case:

whereltj​(⋅)subscript𝑙subscript𝑡𝑗⋅l_{t_{j}}(\cdot)denotes the loss of taskt𝑡tat roundj𝑗j.

For our framework, we adopt a variant of regret minimization called “Follow the Regularized Leader,” which minimizes regret in two steps. First, the unconstrained solution𝜽~~𝜽\tilde{\bm{\theta}}is determined (see Sect.4.1) by solving an unconstrained optimization over the accumulated losses observed so far.
Given𝜽~~𝜽\tilde{\bm{\theta}}, the constrained solution is then determined by learning a projection into the constraint set via Bregman projections (seeAbbasi-Yadkori et al. (2013)).

We adopt a lifelong learning framework in which the agent learns multiple
RL tasks consecutively, providing it the opportunity to transfer knowledge
between tasks to improve learning. Let𝒯𝒯\mathcal{T}denote the set
of tasks, each element of which is an MDP. At any time, the learner
may face any previously seen task, and so must strive to maximize
its performance across all tasks. The goal is to learn optimal policiesπ𝜶1⋆⋆,…,π𝜶|𝒯|⋆⋆superscriptsubscript𝜋superscriptsubscript𝜶1⋆⋆…superscriptsubscript𝜋superscriptsubscript𝜶𝒯⋆⋆\pi_{\bm{\alpha}_{1}^{\star}}^{\star},\dots,\pi_{\bm{\alpha}_{{|\mathcal{T}|}}^{\star}}^{\star}for all tasks, where policyπ𝜶t⋆⋆superscriptsubscript𝜋superscriptsubscript𝜶𝑡⋆⋆\pi_{\bm{\alpha}_{t}^{\star}}^{\star}for taskt𝑡tis parameterized by𝜶t⋆∈ℝdsuperscriptsubscript𝜶𝑡⋆superscriptℝ𝑑\bm{\alpha}_{t}^{\star}\in\mathbb{R}^{d}.
In addition, each task is equipped with safety constraints to ensure
acceptable policy behavior:𝑨t​𝜶t≤𝒃tsubscript𝑨𝑡subscript𝜶𝑡subscript𝒃𝑡\bm{A}_{{t}}\bm{\alpha}_{{t}}\leq\bm{b}_{{t}},
with𝑨t∈ℝd×dsubscript𝑨𝑡superscriptℝ𝑑𝑑\bm{A}_{{t}}\in\mathbb{R}^{d\times d}and𝒃t∈ℝdsubscript𝒃𝑡superscriptℝ𝑑\bm{b}_{{t}}\in\mathbb{R}^{d}representing the allowed policy combinations. The precise form of
these constraints depends on the application domain, but this formulation supports constraints on (e.g.) joint torque, acceleration, position,
etc.

At each roundj𝑗j, the learner observes a set ofntjsubscript𝑛subscript𝑡𝑗n_{{t_{j}}}trajectories{𝝉tj(1),…,𝝉tj(ntj)}superscriptsubscript𝝉subscript𝑡𝑗1…superscriptsubscript𝝉subscript𝑡𝑗subscript𝑛subscript𝑡𝑗\left\{\bm{\tau}_{{t_{j}}}^{(1)},\dots,\bm{\tau}_{{t_{j}}}^{(n_{{t_{j}}})}\right\}from a tasktj∈𝒯subscript𝑡𝑗𝒯{t_{j}}\in\mathcal{T}, where each trajectory has
lengthMtjsubscript𝑀subscript𝑡𝑗M_{{t_{j}}}. To support knowledge transfer between tasks,
we assume that each task’s policy parameters𝜶tj∈ℝdsubscript𝜶subscript𝑡𝑗superscriptℝ𝑑\bm{\alpha}_{{t_{j}}}\in\mathbb{R}^{d}at roundj𝑗jcan be written as a linear combination of a shared latent
basis𝑳∈ℝd×k𝑳superscriptℝ𝑑𝑘\bm{L}\in\mathbb{R}^{d\times k}with coefficient vectors𝒔tj∈ℝksubscript𝒔subscript𝑡𝑗superscriptℝ𝑘\bm{s}_{{t_{j}}}\in\mathbb{R}^{k}; therefore,𝜶tj=𝑳​𝒔tjsubscript𝜶subscript𝑡𝑗𝑳subscript𝒔subscript𝑡𝑗\bm{\alpha}_{{t_{j}}}=\bm{L}\bm{s}_{{t_{j}}}.
Each column of𝑳𝑳\bm{L}represents a chunk of transferrable knowledge;
this task construction has been used successfully in previous multi-task
learning work(Kumar & Daumé III,2012; Ruvolo & Eaton,2013; Bou Ammar et al.,2014).
Extending this previous work, we ensure that the shared knowledge
repository is “informative” by incorporating bounding constraints
on the Frobenius norm∥⋅∥𝖥\|\cdot\|_{\mathsf{F}}of𝑳𝑳\bm{L}. Consequently, the optimization
problem after observingr𝑟rrounds is:

wherep𝑝pandq𝑞qare the constraints on‖𝑳‖𝖥subscriptnorm𝑳𝖥\|\bm{L}\|_{\mathsf{F}},ηtj∈ℝsubscript𝜂subscript𝑡𝑗ℝ\eta_{{t_{j}}}\in\mathbb{R}are design weighting parameters111We describe later how to set theη𝜂\eta’s later in Sect.5to obtain regret bounds, and leave them as variables now for generality.,ℐr={t1,…,tr}subscriptℐ𝑟subscript𝑡1…subscript𝑡𝑟\mathcal{I}_{r}=\left\{t_{1},\dots,t_{r}\right\}denotes
the set of all tasks observed so far through roundr𝑟r, and𝑺𝑺\bm{S}is the collection of all coefficients

The loss functionltj​(𝜶tj)subscript𝑙subscript𝑡𝑗subscript𝜶subscript𝑡𝑗l_{{t_{j}}}(\bm{\alpha}_{{t_{j}}})in Eq. (4)
corresponds to a policy gradient learner for tasktjsubscript𝑡𝑗{t_{j}}, as
defined in Eq. (1). Typical policy gradient methods(Kober & Peters,2011; Sutton et al.,2000)maximize a lower bound of the expected costltj​(𝜶tj)subscript𝑙subscript𝑡𝑗subscript𝜶subscript𝑡𝑗l_{{t_{j}}}\!\left(\bm{\alpha}_{{t_{j}}}\right),
which can be derived by taking the logarithm and applying Jensen’s
inequality:

Therefore, our goal is to minimize the following objective:

The optimization problem above can be mapped to the standard online
learning framework by unrolling𝑳𝑳\bm{L}and𝑺𝑺\bm{S}into a vector𝜽=[vec​(𝑳)​vec​(𝑺)]𝖳∈ℝd​k+k​|𝒯|𝜽superscriptdelimited-[]vec𝑳vec𝑺𝖳superscriptℝ𝑑𝑘𝑘𝒯\bm{\theta}=[\text{vec}(\bm{L})\ \text{vec}(\bm{S})]^{\mathsf{T}}\in\mathbb{R}^{dk+k{|\mathcal{T}|}}. Choosing𝛀0​(𝜽)=μ2​∑i=1d​k𝜽i2+μ1​∑i=d​k+1d​k+k​|𝒯|𝜽i2subscript𝛀0𝜽subscript𝜇2superscriptsubscript𝑖1𝑑𝑘superscriptsubscript𝜽𝑖2subscript𝜇1superscriptsubscript𝑖𝑑𝑘1𝑑𝑘𝑘𝒯superscriptsubscript𝜽𝑖2\bm{\Omega}_{0}(\bm{\theta})=\mu_{2}\sum_{i=1}^{dk}\bm{\theta}_{i}^{2}+\mu_{1}\sum_{i=dk+1}^{dk+k{|\mathcal{T}|}}\bm{\theta}_{i}^{2}\enspace, and𝛀j​(𝜽)=𝛀j−1​(𝜽)+ηtj​ltj​(𝜽)subscript𝛀𝑗𝜽subscript𝛀𝑗1𝜽subscript𝜂subscript𝑡𝑗subscript𝑙subscript𝑡𝑗𝜽\bm{\Omega}_{j}(\bm{\theta})=\bm{\Omega}_{j-1}(\bm{\theta})+\eta_{{t_{j}}}l_{{t_{j}}}(\bm{\theta}), we can write the safe lifelong policy search problem (Eq. (6)) as:

where𝒦⊆ℝd​k+k​|𝒯|𝒦superscriptℝ𝑑𝑘𝑘𝒯\mathcal{K}\subseteq\mathbb{R}^{dk+k{|\mathcal{T}|}}is the set of allowable policies under the given safety constraints.
Note that the loss for tasktjsubscript𝑡𝑗{t_{j}}can be written as a bilinear
product in𝜽𝜽\bm{\theta}:

We see that the problem in Eq. (7) is equivalent
to Eq. (6) by noting
that atr𝑟rrounds,𝛀r=∑j=1rηtj​ltj​(𝜽)+𝛀0​(𝜽)subscript𝛀𝑟superscriptsubscript𝑗1𝑟subscript𝜂subscript𝑡𝑗subscript𝑙subscript𝑡𝑗𝜽subscript𝛀0𝜽\bm{\Omega}_{r}=\sum_{j=1}^{r}\eta_{{t_{j}}}l_{{t_{j}}}(\bm{\theta})+\bm{\Omega}_{0}(\bm{\theta}).

We solve Eq. (7)
in two steps. First, we determine the unconstrained solution𝜽~r+1subscript~𝜽𝑟1\tilde{\bm{\theta}}_{r+1}when𝒦=ℝd​k+k​|𝒯|𝒦superscriptℝ𝑑𝑘𝑘𝒯\mathcal{K}=\mathbb{R}^{dk+k{|\mathcal{T}|}}(see Sect.4.1). Given𝜽~r+1subscript~𝜽𝑟1\tilde{\bm{\theta}}_{r+1},
we derive the constrained solution𝜽^r+1subscript^𝜽𝑟1\hat{{\bm{\theta}}}_{r+1}by learning a projectionProj𝛀r,𝒦​(𝜽~r+1)subscriptProjsubscript𝛀𝑟𝒦subscript~𝜽𝑟1\text{Proj}_{\bm{\Omega}_{r},\mathcal{K}}\left(\tilde{\bm{\theta}}_{r+1}\right)to the constraint set𝒦⊆ℝd​k+k​|𝒯|𝒦superscriptℝ𝑑𝑘𝑘𝒯\mathcal{K}\subseteq\mathbb{R}^{dk+k{|\mathcal{T}|}},
which amounts to minimizing the Bregman divergence over𝛀r​(𝜽)subscript𝛀𝑟𝜽\bm{\Omega}_{r}(\bm{\theta})(see Sect.4.2)222In Sect.4.2, we linearize the
loss around the constrained solution of the previous round
to increase stability and ensure convergence. Given the linear losses,
it suffices to solve the Bregman divergence over the regularizer, reducing the
computational cost.. The complete approach is given in Algorithm1and is available as a software implementation on the authors’ websites.

Although Eq. (6)
is not jointly convex in both𝑳𝑳\bm{L}and𝑺𝑺\bm{S}, it is separably convex (for log-concave policy distributions). Consequently,
we follow an alternating optimization approach, first computing𝑳𝑳\bm{L}while holding𝑺𝑺\bm{S}fixed, and then updating𝑺𝑺\bm{S}given the acquired𝑳𝑳\bm{L}. We detail this process for two popular PG learners, eREINFORCE(Williams,1992)and eNAC(Peters & Schaal,2008b). The derivations of the update rules below can be found in AppendixA.

These updates are governed by learning ratesβ𝛽\betaandλ𝜆\lambdathat decay over time;β𝛽\betaandλ𝜆\lambdacan be chosen using line-search methods as discussed byBoyd & Vandenberghe (2004). In our experiments, we adopt a simple yet effective strategy, whereβ=c​j−1𝛽𝑐superscript𝑗1\beta=cj^{-1}andλ=c​j−1𝜆𝑐superscript𝑗1\lambda=cj^{-1}, with0<c<10𝑐10<c<1.

Step 1: UpdatingL𝐿\bm{L}Holding𝑺𝑺\bm{S}fixed, the
latent repository can be updated according to:

with learning rateη𝑳β∈ℝsuperscriptsubscript𝜂𝑳𝛽ℝ\eta_{\bm{L}}^{\beta}\in\mathbb{R},
and𝑮−1​(𝑳,𝑺)superscript𝑮1𝑳𝑺\bm{G}^{-1}(\bm{L},\bm{S})as the inverse of the Fisher
information matrix(Peters & Schaal,2008b).

In the special case of Gaussian policies, the update for𝑳𝑳\bm{L}can be derived in a closed form as𝑳β+1=𝒁𝑳−1​𝒗𝑳subscript𝑳𝛽1superscriptsubscript𝒁𝑳1subscript𝒗𝑳\bm{L}_{\beta+1}=\bm{Z}_{\bm{L}}^{-1}\bm{v}_{\bm{L}},
where

σtj2superscriptsubscript𝜎subscript𝑡𝑗2\sigma_{{t_{j}}}^{2}is the covariance of the Gaussian
policy for a tasktjsubscript𝑡𝑗{t_{j}}, and𝚽=𝚽​(𝒙m(k,tj))𝚽𝚽superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗\bm{\Phi}=\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)denotes the state features.

Step 2: UpdatingS𝑆\bm{S}Given the fixed basis𝑳𝑳\bm{L}, the coefficient
matrix𝑺𝑺\bm{S}is updated column-wise for alltj∈ℐrsubscript𝑡𝑗subscriptℐ𝑟{t_{j}}\in\mathcal{I}_{r}:

with learning rateη𝑺λ∈ℝsuperscriptsubscript𝜂𝑺𝜆ℝ\eta_{\bm{S}}^{\lambda}\in\mathbb{R}. For Gaussian policies, the closed-form of the update
is𝒔tj=𝒁𝒔tj−1​𝒗𝒔tjsubscript𝒔subscript𝑡𝑗superscriptsubscript𝒁subscript𝒔subscript𝑡𝑗1subscript𝒗subscript𝒔subscript𝑡𝑗\bm{s}_{{t_{j}}}=\bm{Z}_{\bm{s}_{{t_{j}}}}^{-1}\bm{v}_{\bm{s}_{{t_{j}}}}, where

Once we have obtained the unconstrained solution𝜽~r+1subscript~𝜽𝑟1\tilde{\bm{\theta}}_{r+1}(which
satisfies Eq. (7), but can lead to policy parameters
in unsafe regions), we then derive the constrained solution to ensure safe policies. We learn a projectionProj𝛀r,𝒦​(𝜽~r+1)subscriptProjsubscript𝛀𝑟𝒦subscript~𝜽𝑟1\text{Proj}_{\bm{\Omega}_{r},\mathcal{K}}\left(\tilde{\bm{\theta}}_{r+1}\right)from𝜽~r+1subscript~𝜽𝑟1\tilde{\bm{\theta}}_{r+1}to the constraint set:

whereℬ𝛀r,𝒦​(𝜽,𝜽~r+1)subscriptℬsubscript𝛀𝑟𝒦𝜽subscript~𝜽𝑟1\mathcal{B}_{\bm{\Omega}_{r},\mathcal{K}}\!\left(\!\bm{\theta},\tilde{\bm{\theta}}_{r+1}\!\right)is the Bregman divergence over𝛀rsubscript𝛀𝑟\bm{\Omega}_{r}:

Solving Eq. (8) is computationally
expensive since𝛀r​(𝜽)subscript𝛀𝑟𝜽\bm{\Omega}_{r}(\bm{\theta})includes
the sum back to the original round. To remedy this problem, ensure the
stability of our approach, and guarantee that the constrained solutions
for all observed tasks lie within a bounded region, we linearize the
current-round loss functionltr​(𝜽)subscript𝑙subscript𝑡𝑟𝜽l_{{t_{r}}}(\bm{\theta})around
theconstrainedsolution of the previous round𝜽^rsubscript^𝜽𝑟\hat{\bm{\theta}}_{r}:

where

Given the above linear form, we can rewrite the optimization problem
in Eq. (8) as:

Consequently, determiningsafe policiesfor lifelong policy
search reinforcement learning amounts to solving:

To solve the optimization problem above, we start by converting the
inequality constraints to equality constraints by introducing slack variables𝒄tj≥0subscript𝒄subscript𝑡𝑗0\bm{c}_{{t_{j}}}\geq 0. We also guarantee that these slack variables
are bounded by incorporating‖𝒄tj‖≤𝒄max,∀tj∈{1,…,|𝒯|}formulae-sequencenormsubscript𝒄subscript𝑡𝑗subscript𝒄maxfor-allsubscript𝑡𝑗1…𝒯\|\bm{c}_{{t_{j}}}\|\leq\bm{c}_{\text{max}},\ \forall{t_{j}}\in\{1,\dots,{|\mathcal{T}|}\}:

With this formulation, learningProj𝛀r,𝒦​(𝜽~r+1)subscriptProjsubscript𝛀𝑟𝒦subscript~𝜽𝑟1\text{Proj}_{\bm{\Omega}_{r},\mathcal{K}}\left(\tilde{\bm{\theta}}_{r+1}\right)amounts to solving second-order cone and semi-definite programs.

This section determines the constrained projection of the shared
basis𝑳𝑳\bm{L}given fixed𝑺𝑺\bm{S}and𝑪𝑪\bm{C}. We show that𝑳𝑳\bm{L}can be acquired efficiently, since this step can be relaxed to solving
a semi-definite program in𝑳​𝑳𝖳𝑳superscript𝑳𝖳\bm{L}\bm{L}^{\mathsf{T}}(Boyd & Vandenberghe,2004).
To formulate the semi-definite program, note that

From the constraint set, we recognize:

Sincespectrum​(𝑳​𝑳𝖳)=spectrum​(𝑳𝖳​𝑳)spectrum𝑳superscript𝑳𝖳spectrumsuperscript𝑳𝖳𝑳\text{spectrum}\left(\bm{L}\bm{L}^{\mathsf{T}}\right)=\text{spectrum}\left(\bm{L}^{\mathsf{T}}\bm{L}\right),
we can write:

Having determined𝑳𝑳\bm{L}, we can acquire𝑺𝑺\bm{S}and
update𝑪𝑪\bm{C}by solving a second-order cone
program(Boyd & Vandenberghe,2004)of the following form:

This section quantifies the performance of our approach by providing
formal analysis of the regret afterR𝑅Rrounds. We
show that the safe lifelong reinforcement learner exhibitssublinearregret in the total number of rounds. Formally, we prove the following
theorem:

AfterR𝑅Rrounds and choosing∀tj∈ℐRηtj=η=1Rformulae-sequencefor-allsubscript𝑡𝑗subscriptℐ𝑅subscript𝜂subscript𝑡𝑗𝜂1𝑅\forall{t_{j}}\in\mathcal{I}_{R}\ \ \eta_{{t_{j}}}\!=\eta=\frac{1}{\sqrt{R}},𝐋|𝛉^1=diag𝐤​(ζ)evaluated-at𝐋subscript^𝛉1subscriptdiag𝐤𝜁\bm{L}\Big{|}_{\hat{\bm{\theta}}_{1}}=\text{diag}_{\bm{k}}(\zeta),
withdiag𝐤​(⋅)subscriptdiag𝐤⋅\text{diag}_{\bm{k}}(\cdot)being a diagonal matrix among
the𝐤𝐤\bm{k}columns of𝐋𝐋\bm{L},p≤ζ2≤q𝑝superscript𝜁2𝑞p\leq\zeta^{2}\leq q, and𝐒|𝛉^1=𝟎k×|𝒯|evaluated-at𝐒subscript^𝛉1subscript0𝑘𝒯\bm{S}\Big{|}_{\hat{\bm{\theta}}_{1}}=\bm{0}_{k\times{|\mathcal{T}|}},
the safe lifelong reinforcement learner exhibits sublinear regret
of the form:

Proof Roadmap:The remainder of this section completes our proof of Theorem1; further details are given in AppendixB.
We assume linear losses for all tasks in the constrained case in accordance
with Sect.4.2. Although linear losses for
policy search RL are too restrictive given a single
operating point, as discussed previously, we remedy this problem by
generalizing to the case of piece-wise linear losses, where the linearization
operating point is a resultant of the optimization problem.
To bound the regret, we need to bound the dual Euclidean norm
(which is the same as the Euclidean norm) of the gradient of the loss function,
then prove Theorem1by bounding: (1) tasktjsubscript𝑡𝑗{t_{j}}’s
gradient loss (Sect.5.1), and (2) linearized
losses with respect to𝑳𝑳\bm{L}and𝑺𝑺\bm{S}(Sect.5.2).

We start by stating essential lemmas for Theorem1; due to space constraints, proofs for all lemmas are available in the supplementary material.
Here, we bound the gradient of a loss functionltj​(𝜽)subscript𝑙subscript𝑡𝑗𝜽l_{{t_{j}}}(\bm{\theta})at roundr𝑟runder Gaussian policies333Please note that derivations for other forms of log-concave policy
distributions could be derived in similar manner. In this work, we
focus on Gaussian policies since they cover a broad spectrum of real-world
applications..

We assume that the policy for a tasktjsubscript𝑡𝑗{t_{j}}is Gaussian, the action set𝒰𝒰\mathcal{U}is bounded by𝐮maxsubscript𝐮\bm{u}_{\max},
and the feature set is upper-bounded by𝚽maxsubscript𝚽\bm{\Phi}_{\max}.

Assume tasktjsubscript𝑡𝑗{t_{j}}’s policy at roundr𝑟ris given byπ𝛂tj(tj)​(𝐮m(k,tj)|𝐱m(k,tj))|𝛉^r=𝒩​(𝛂tj𝖳|𝛉^r​𝚽​(𝐱m(k,tj)),𝛔tj)evaluated-atsuperscriptsubscript𝜋subscript𝛂subscript𝑡𝑗subscript𝑡𝑗conditionalsuperscriptsubscript𝐮𝑚𝑘subscript𝑡𝑗superscriptsubscript𝐱𝑚𝑘subscript𝑡𝑗subscript^𝛉𝑟𝒩evaluated-atsuperscriptsubscript𝛂subscript𝑡𝑗𝖳subscript^𝛉𝑟𝚽superscriptsubscript𝐱𝑚𝑘subscript𝑡𝑗subscript𝛔subscript𝑡𝑗\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\!\left(\!\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\!\right)\!\Big{|}_{\hat{\bm{\theta}}_{r}}\!=\mathcal{N}\!\left(\!\bm{\alpha}_{{t_{j}}}^{\mathsf{T}}\Big{|}_{\hat{\bm{\theta}}_{r}}\bm{\Phi}\left(\!\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\!,\bm{\sigma}_{{t_{j}}}\!\right),
for states𝐱m(k,tj)∈𝒳tjsuperscriptsubscript𝐱𝑚𝑘subscript𝑡𝑗subscript𝒳subscript𝑡𝑗\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{X}_{{t_{j}}}and actions𝐮m(k,tj)∈𝒰tjsuperscriptsubscript𝐮𝑚𝑘subscript𝑡𝑗subscript𝒰subscript𝑡𝑗\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{U}_{{t_{j}}}. Forltj​(𝛂tj)=−1ntj​∑k=1ntj∑m=0Mtj−1log⁡[π𝛂tj(tj)​(𝐮m(k,tj)|𝐱m(k,tj))]subscript𝑙subscript𝑡𝑗subscript𝛂subscript𝑡𝑗1subscript𝑛subscript𝑡𝑗superscriptsubscript𝑘1subscript𝑛subscript𝑡𝑗superscriptsubscript𝑚0subscript𝑀subscript𝑡𝑗1superscriptsubscript𝜋subscript𝛂subscript𝑡𝑗subscript𝑡𝑗conditionalsuperscriptsubscript𝐮𝑚𝑘subscript𝑡𝑗superscriptsubscript𝐱𝑚𝑘subscript𝑡𝑗l_{{t_{j}}}\!\!\left(\bm{\alpha}_{{t_{j}}}\right)=-\frac{1}{n_{{t_{j}}}}\displaystyle\sum_{k=1}^{n_{{t_{j}}}}\!\sum_{m=0}^{M_{{t_{j}}}-1}\!\!\!\log\left[\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\!\left(\!\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\!\right)\!\right], the gradient∇𝛂tjltj​(𝛂tj)|𝛉^revaluated-atsubscript∇subscript𝛂subscript𝑡𝑗subscript𝑙subscript𝑡𝑗subscript𝛂subscript𝑡𝑗subscript^𝛉𝑟\nabla_{\!\bm{\alpha}_{{t_{j}}}}l_{{t_{j}}}\!\!\left(\!\bm{\alpha}_{{t_{j}}}\!\right)\!\Big{|}_{\hat{\bm{\theta}}_{r}}satisfies||∇𝛂tjltj​(𝛂tj)|𝛉^r||2≤evaluated-atsubscriptsubscript∇subscript𝛂subscript𝑡𝑗subscript𝑙subscript𝑡𝑗subscript𝛂subscript𝑡𝑗subscript^𝛉𝑟2absent\left|\left|\nabla_{\!\bm{\alpha}_{{t_{j}}}}l_{{t_{j}}}\!\!\left(\!\bm{\alpha}_{{t_{j}}}\!\right)\!\Big{|}_{\hat{\bm{\theta}}_{r}}\right|\right|_{2}\leq

for all trajectories and all tasks, withumax=maxk,m⁡{|𝐮m(k,tj)|}subscript𝑢subscript𝑘𝑚superscriptsubscript𝐮𝑚𝑘subscript𝑡𝑗u_{\max}\!=\!\displaystyle\max_{k,m}\left\{\left|\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\right|\right\}and𝚽max=maxk,m⁡{‖𝚽​(𝐱m(k,tj))‖2}subscript𝚽subscript𝑘𝑚subscriptnorm𝚽superscriptsubscript𝐱𝑚𝑘subscript𝑡𝑗2\bm{\Phi}_{\max}\!=\!\displaystyle\max_{k,m}\left\{\left|\left|\bm{\Phi}\left(\!\bm{x}_{m}^{\left(k,\ {t_{j}}\!\right)}\right)\right|\right|_{2}\right\}.

As discussed previously, we linearize the
loss of tasktrsubscript𝑡𝑟{t_{r}}around the constraint
solution of the previous round𝜽^rsubscript^𝜽𝑟\hat{\bm{\theta}}_{r}.
To acquire the regret bounds in Theorem1, the next
step is to bound the dual norm,‖𝒇^tr|𝜽^r∥2⋆=‖𝒇^tr|𝜽^r∥2evaluated-atsubscriptdelimited-‖|subscript^𝒇subscript𝑡𝑟subscript^𝜽𝑟2⋆evaluated-atsubscriptdelimited-‖|subscript^𝒇subscript𝑡𝑟subscript^𝜽𝑟2\left\|\hat{\bm{f}}_{{t_{r}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2}^{\star}=\left\|\hat{\bm{f}}_{{t_{r}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2}of Eq. (9). It can be easily seen

Since|ltr(𝜽)|𝜽^r|\left|l_{{t_{r}}}\left(\bm{\theta}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}\right|can be bounded by𝜹ltrsubscript𝜹subscript𝑙subscript𝑡𝑟\bm{\delta}_{l_{{t_{r}}}}(see Sect.2),the next step is to bound‖∇𝜽ltr​(𝜽)|𝜽^r∥2evaluated-atsubscriptdelimited-‖|subscript∇𝜽subscript𝑙subscript𝑡𝑟𝜽subscript^𝜽𝑟2\left\|\nabla_{\bm{\theta}}l_{{t_{r}}}\left(\bm{\theta}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2},
and‖𝜽^r‖2subscriptnormsubscript^𝜽𝑟2\|\hat{\bm{\theta}}_{r}\|_{2}.

The norm of the gradient
of the loss function evaluated at𝛉^rsubscript^𝛉𝑟\hat{\bm{\theta}}_{r}satisfies

To finalize the bound of‖𝒇^tr|𝜽^r∥2evaluated-atsubscriptdelimited-‖|subscript^𝒇subscript𝑡𝑟subscript^𝜽𝑟2\left\|\hat{\bm{f}}_{{t_{r}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2}as needed for deriving the regret, we must derive an upper-bound for‖𝜽^r‖2subscriptnormsubscript^𝜽𝑟2\|\hat{\bm{\theta}}_{r}\|_{2}:

The L2norm of the constraint solution at roundr−1𝑟1r-1,‖𝛉^r‖22superscriptsubscriptnormsubscript^𝛉𝑟22\|\hat{\bm{\theta}}_{r}\|_{2}^{2}is bounded by

where|ℐr−1|subscriptℐ𝑟1\left|\mathcal{I}_{r-1}\right|is the number of
unique tasks observed so far.

Given the previous two lemmas, we can prove the bound
for‖𝒇^tr|𝜽^r∥2evaluated-atsubscriptdelimited-‖|subscript^𝒇subscript𝑡𝑟subscript^𝜽𝑟2\left\|\hat{\bm{f}}_{{t_{r}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2}:

The L2norm of the linearizing
term ofltr​(𝛉)subscript𝑙subscript𝑡𝑟𝛉l_{{t_{r}}}(\bm{\theta})around𝛉^rsubscript^𝛉𝑟\hat{\bm{\theta}}_{r},‖𝐟^tr|𝛉^r∥2evaluated-atsubscriptdelimited-‖|subscript^𝐟subscript𝑡𝑟subscript^𝛉𝑟2\left\|\hat{\bm{f}}_{{t_{r}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2},
is bounded by

where𝛅ltrsubscript𝛅subscript𝑙subscript𝑡𝑟\bm{\delta}_{l_{{t_{r}}}}is the constant upper-bound
on|ltr(𝛉)|𝛉^r|\left|l_{{t_{r}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}\right|,
and

Given the lemmas in the previous section, we now can derive the
sublinear regret bound given in Theorem1.
Using results developed byAbbasi-Yadkori et al. (2013), it is easy to see that

From the convexity of the regularizer, we obtain:

We have:

Therefore, for any𝒖∈𝒦𝒖𝒦\bm{u}\in\mathcal{K}

Assuming that∀tj​ηtj=ηfor-allsubscript𝑡𝑗subscript𝜂subscript𝑡𝑗𝜂\forall{t_{j}}\ \eta_{{t_{j}}}=\eta, we can derive:

The following lemma finalizes the proof of Theorem1:

AfterR𝑅Rrounds with∀tj​ηtj=η=1Rfor-allsubscript𝑡𝑗subscript𝜂subscript𝑡𝑗𝜂1𝑅\forall{t_{j}}\ \eta_{{t_{j}}}=\eta=\frac{1}{\sqrt{R}},
for any𝐮∈𝒦𝐮𝒦\bm{u}\in\mathcal{K}we have that∑j=1Rltj​(𝛉^j)−ltj​(𝐮)≤𝒪​(R)superscriptsubscript𝑗1𝑅subscript𝑙subscript𝑡𝑗subscript^𝛉𝑗subscript𝑙subscript𝑡𝑗𝐮𝒪𝑅\sum_{j=1}^{R}l_{{t_{j}}}(\hat{\bm{\theta}}_{j})-l_{{t_{j}}}(\bm{u})\leq\mathcal{O}\left(\sqrt{R}\right).

From Eq. (12), it follows that

with𝜸3​(R)=4​𝜸12​(R)+2​maxtj∈ℐR−1⁡𝜹tj2subscript𝜸3𝑅4superscriptsubscript𝜸12𝑅2subscriptsubscript𝑡𝑗subscriptℐ𝑅1superscriptsubscript𝜹subscript𝑡𝑗2\bm{\gamma}_{3}(R)=4\bm{\gamma}_{1}^{2}(R)+2\max_{{t_{j}}\in\mathcal{I}_{R-1}}\bm{\delta}_{{t_{j}}}^{2}.
Since|ℐR−1|≤|𝒯|subscriptℐ𝑅1𝒯|\mathcal{I}_{R-1}|\leq{|\mathcal{T}|}, we have that‖𝒇^tj|θ^r∥22≤𝜸5​(R)​|𝒯|evaluated-atsubscriptdelimited-‖|subscript^𝒇subscript𝑡𝑗subscript^𝜃𝑟22subscript𝜸5𝑅𝒯\left\|\hat{\bm{f}}_{{t_{j}}}\Big{|}_{\hat{\theta}_{r}}\right\|_{2}^{2}\leq\bm{\gamma}_{5}(R){|\mathcal{T}|}with𝜸5=8​d/p2​q​𝜸12​(R)​maxtk∈ℐR−1⁡{‖𝑨tk†‖22​(‖𝒃tk‖2+𝒄max)2}subscript𝜸58𝑑superscript𝑝2𝑞superscriptsubscript𝜸12𝑅subscriptsubscript𝑡𝑘subscriptℐ𝑅1superscriptsubscriptnormsuperscriptsubscript𝑨subscript𝑡𝑘†22superscriptsubscriptnormsubscript𝒃subscript𝑡𝑘2subscript𝒄max2\displaystyle\bm{\gamma}_{5}=8\nicefrac{{d}}{{p^{2}}}q\bm{\gamma}_{1}^{2}(R)\max_{{t_{k}}\in\mathcal{I}_{R-1}}\left\{\|\bm{A}_{{t_{k}}}^{\dagger}\|_{2}^{2}\left(\|\bm{b}_{{t_{k}}}\|_{2}+\bm{c}_{\text{max}}\right)^{2}\right\}.

Given that𝛀0​(𝒖)≤q​d+𝜸5​(R)​|𝒯|subscript𝛀0𝒖𝑞𝑑subscript𝜸5𝑅𝒯\bm{\Omega}_{0}(\bm{u})\leq qd+\bm{\gamma}_{5}(R){|\mathcal{T}|},
with𝜸5​(R)subscript𝜸5𝑅\bm{\gamma}_{5}(R)being a constant, we have:

InitializingL𝐿\bm{L}andS𝑆\bm{S}:We initialize𝑳|𝜽^1=diag𝒌​(ζ)evaluated-at𝑳subscript^𝜽1subscriptdiag𝒌𝜁\bm{L}\Big{|}_{\hat{\bm{\theta}}_{1}}\!=\text{diag}_{\bm{k}}(\zeta),
withp≤ζ2≤q𝑝superscript𝜁2𝑞p\leq\zeta^{2}\leq qand𝑺|𝜽^1=𝟎k×|𝒯|evaluated-at𝑺subscript^𝜽1subscript0𝑘𝒯\bm{S}\Big{|}_{\hat{\bm{\theta}}_{1}}\!=\bm{0}_{k\times{|\mathcal{T}|}}to ensure the invertibility of𝑳𝑳\bm{L}and that the constraints are
met. This leads to

Choosing∀tj​ηtj=η=1/Rfor-allsubscript𝑡𝑗subscript𝜂subscript𝑡𝑗𝜂1𝑅\forall{t_{j}}\ \eta_{{t_{j}}}=\eta=\nicefrac{{1}}{{\sqrt{R}}},
we acquire sublinear regret, finalizing the statement of Theorem1:

∎

To validate the empirical performance of our method, we applied our safe online PG algorithm to learn multiple consecutive control tasks on three dynamical systems (Figure1). To generate multiple tasks, we varied the parameterization of each system, yielding a set of control tasks from each domain with varying dynamics. The optimal control policies for these systems vary widely with only minor changes in the system parameters, providing substantial diversity among the tasks within a single domain.

[图片: images\image_1.05798v1]
图片说明: Figure 1:Dynamical systems used in the experiments:a)simple mass system (left),b)cart-pole (middle), andc)quadrotor unmanned aerial vehicle (right).

Simple Mass Spring Damper:The simple mass (SM) system is
characterized by three parameters: the spring constantk𝑘kin N/m,
the damping constantd𝑑din Ns/m and the massm𝑚min kg. The system’s
state is given by the position𝒙𝒙\bm{x}and𝒙˙˙𝒙\dot{\bm{x}}of the
mass, which varies according to a linear force𝑭𝑭\bm{F}. The goal is to train a policy for controlling the mass in a specific state𝒈ref=⟨𝒙ref,𝒙˙ref⟩subscript𝒈refsubscript𝒙refsubscript˙𝒙ref\bm{g}_{\text{ref}}=\langle\bm{x}_{\text{ref}},\dot{\bm{x}}_{\text{ref}}\rangle.

Cart Pole:The cart-pole (CP) has been used extensively
as a benchmark for evaluating RL methods(Busoniu et al.,2010). CP dynamics
are characterized by the cart’s massmcsubscript𝑚𝑐m_{c}in kg, the pole’s massmpsubscript𝑚𝑝m_{p}in kg, the pole’s length in meters, and a damping parameterd𝑑din Ns/m. The state is given by the cart’s position𝒙𝒙\bm{x}and velocity𝒙˙˙𝒙\dot{\bm{x}}, as well as the pole’s angle𝜽𝜽\bm{\theta}and angular velocity𝜽˙˙𝜽\dot{\bm{\theta}}. The goal is to
train a policy that controls the pole in an upright position.

We generated 10 tasks for each domain by varying the system
parameters to ensure a variety of tasks with diverse optimal policies,
including those with highly chaotic dynamics that are difficult to control.
We ran each experiment for a total ofR𝑅Rrounds, varying from150150150for the simple mass to10,0001000010,000for the quadrotor to train𝑳𝑳\bm{L}and𝑺𝑺\bm{S}, as well as for updating the PG-ELLA and PG models. At each roundj𝑗j, the learner observed a tasktjsubscript𝑡𝑗{t_{j}}through 50 trajectories of 150 steps and updated𝑳𝑳\bm{L}and𝒔tjsubscript𝒔subscript𝑡𝑗\bm{s}_{{t_{j}}}. The dimensionalityk𝑘kof the latent space was chosen independently for each domain via cross-validation over 3 tasks, and the learning step size for
each task domain was determined by a line search after gathering 10 trajectories of length 150. We used eNAC, a standard PG algorithm, as the base learner.

We compared our approach to both standard PG (i.e., eNAC) and PG-ELLA(Bou Ammar et al.,2014), examining both the constrained and unconstrained variants of our algorithm. We also varied the number of iterations in our alternating optimization from101010to100100100to evaluate the effect of these inner iterations on the performance, as shown in Figures2and3. For the two MTL algorithms (our approach and PG-ELLA), the policy parameters for each tasktjsubscript𝑡𝑗{t_{j}}were initialized
using the learned basis (i.e.,𝜶tj=𝑳​𝒔tjsubscript𝜶subscript𝑡𝑗𝑳subscript𝒔subscript𝑡𝑗\bm{\alpha}_{{t_{j}}}=\bm{L}\bm{s}_{{t_{j}}}). We configured PG-ELLA as described byBou Ammar et al. (2014), ensuring a fair comparison. For the standard PG learner, we provided additional trajectories in order to ensure a fair comparison, as described below.

For the experiments with policy constraints, we generated a set of constraints(𝑨t,𝒃t)subscript𝑨𝑡subscript𝒃𝑡(\bm{A}_{t},\bm{b}_{t})for each task that restricted the policy parameters to pre-specified “safe” regions, as shown in Figures2(c)and2(d).
We also tested different values for the constraints on𝑳𝑳\bm{L}, varyingp𝑝pandq𝑞qbetween0.10.10.1to101010; our approach showed robustness against this broad range, yielding similar average cost performance.

Figure2reports our results on the benchmark
simple mass and cart-pole systems. Figures2(a)and2(b)depicts the performance of the learned policy in a lifelong learning
setting over consecutive unconstrained tasks, averaged over all 10 systems over 100 different initial conditions. These results demonstrate that our approach is capable of outperforming both standard PG (which was provided with 50additionaltrajectories each iteration to ensure a more fair comparison) and PG-ELLA, both in terms of initial performance and learning speed. These figures also show that the performance of our method increases as it is given more alternating iterations per-round for fitting𝑳𝑳\bm{L}and𝑺𝑺\bm{S}.

[图片: images\image_2.png]
图片说明: (a)Simple Mass

[图片: images\image_3.png]
图片说明: (a)Simple Mass

[图片: images\image_4.png]
图片说明: (a)Simple Mass

[图片: images\image_5.png]
图片说明: (a)Simple Mass

We evaluated the ability of these methods to respect safety constraints, as shown in
Figures2(c)and2(d). The thicker black lines in each figure depict the allowable “safe” region of the policy space. To enable online
learning per-task, the same tasktjsubscript𝑡𝑗{t_{j}}was observed on each
round and the shared basis𝑳𝑳\bm{L}and coefficients𝒔tjsubscript𝒔subscript𝑡𝑗\bm{s}_{{t_{j}}}were updated using alternating optimization. We then plotted the change in the policy parameter
vectors per iterations (i.e.,𝜶tj=𝑳​𝒔tjsubscript𝜶subscript𝑡𝑗𝑳subscript𝒔subscript𝑡𝑗\bm{\alpha}_{{t_{j}}}=\bm{L}\bm{s}_{{t_{j}}}) for each method, demonstrating that our approach abides by the safety constraints,
while standard PG and PG-ELLA can violate them (since they only solve an unconstrained optimization problem). In addition, these figures show that increasing the number of alternating iterations in our method causes it to take a more direct path to the optimal solution.

We also applied our approach to the more challenging domain of quadrotor control. The dynamics of the quadrotor system (Figure1) are influenced by inertial
constants around𝒆1,Bsubscript𝒆1𝐵\bm{e}_{1,B},𝒆2,Bsubscript𝒆2𝐵\bm{e}_{2,B}, and𝒆3,Bsubscript𝒆3𝐵\bm{e}_{3,B},
thrust factors influencing how the rotor’s speed affects the overall
variation of the system’s state, and the lengths of the rods supporting
the rotors. Although the overall state of the system can be described
by a 12-dimensional vector, we focus on stability and so consider
only six of these state-variables. The quadrotor system has a high-dimensional
action space, where the goal is to control the four rotational velocities{wi}i=14superscriptsubscriptsubscript𝑤𝑖𝑖14\{w_{i}\}_{i=1}^{4}of the rotors to stabilize the system. To ensure
realistic dynamics, we used the simulated model described by(Bouabdallah,2007; Voos & Bou Ammar,2010),
which has been verified and used in the control of physical quadrotors.

We generated 10 different quadrotor systems by varying the inertia around the
x, y and z-axes. We used a linear quadratic regulator, as described
byBouabdallah (2007), to initialize the policies in both the learning
and testing phases. We followed a similar experimental procedure to
that discussed above to update the models.

Figure3shows the performance of the unconstrained solution
as compared to standard PG and PG-ELLA. Again, our approach clearly outperforms
standard PG and PG-ELLA in both the initial performance
and learning speed. We also evaluated constrained tasks in a similar manner, again showing that our approach is capable of respecting constraints. Since the policy space is higher dimensional, we cannot visualize it as well as the benchmark systems, and so instead report the number of iterations it takes our approach to project the policy into the safe region. Figure4shows that our approach requires only one observation of the task to
acquire safe policies, which is substantially lower then standard PG or PG-ELLA (e.g., which require 545 and 510 observations, respectively,
in the quadrotor scenario).

[图片: images\image_6.png]
图片说明: Figure 3:Performance on quadrotor control.

[图片: images\image_7.png]
图片说明: Figure 4:Average number of task observations before acquiring policy parameters that abide by the constraints, showing that our approach immediately projects policies to safe regions.

We described the first lifelong PG learner that provides sublinear regret𝒪​(R)𝒪𝑅\mathcal{O}(\sqrt{R})withR𝑅Rtotal rounds. In addition, our approach supports safety constraints on the learned policy, which are essential for robust learning in real applications. Our framework formalizes
lifelong learning as online MTL with limited
resources, and enables safe transfer by sharing policy parameters through
a latent knowledge base that is efficiently updated over time.

In this appendix, we derive the update equations for𝑳𝑳\bm{L}and𝑺𝑺\bm{S}in the special case of Gaussian policies. Please note that these derivations can be easily extended to other policy forms in higher dimensional action spaces.

For a tasktjsubscript𝑡𝑗t_{j}, the policyπ𝜶tj(tj)​(𝒖m(k,tj)|𝒙m(k,tj))subscriptsuperscript𝜋subscript𝑡𝑗subscript𝜶subscript𝑡𝑗conditionalsuperscriptsubscript𝒖𝑚𝑘subscript𝑡𝑗superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗\pi^{(t_{j})}_{\bm{\alpha}_{t_{j}}}\left(\bm{u}_{m}^{(k,t_{j})}|\bm{x}_{m}^{(k,t_{j})}\right)is given by:

Therefore, the safe lifelong reinforcement learning optimization objective can be written as:

To arrive at the update equations, we need to derive Eq. (13) with respect to each𝑳𝑳\bm{L}and𝑺𝑺\bm{S}.

Starting with the derivative of𝒆r​(𝑳,𝑺)subscript𝒆𝑟𝑳𝑺\bm{e}_{r}(\bm{L},\bm{S})with respect to the shared repository𝑳𝑳\bm{L}, we can write:

To acquire the minimum, we set the above to zero:

Noting that𝒔tj𝖳​𝑳𝖳​𝚽​(𝒙m(k,tj))∈ℝsuperscriptsubscript𝒔subscript𝑡𝑗𝖳superscript𝑳𝖳𝚽superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗ℝ\bm{s}_{t_{j}}^{\mathsf{T}}\bm{L}^{\mathsf{T}}\bm{\Phi}\left(\bm{x}_{m}^{(k,t_{j})}\right)\in\mathbb{R}, we can write:

To solve Eq. (14), we introduce the standardvec​(⋅)vec⋅\text{vec}(\cdot)operator leading to:

Knowing that for a given set of matrices𝑨𝑨\bm{A},𝑩𝑩\bm{B}, and𝑿𝑿\bm{X},vec​(𝑨​𝑿​𝑩)=(𝑩𝖳⊗𝑨)​vec​(𝑿)vec𝑨𝑿𝑩tensor-productsuperscript𝑩𝖳𝑨vec𝑿\text{vec}(\bm{A}\bm{X}\bm{B})=\left(\bm{B}^{\mathsf{T}}\otimes\bm{A}\right)\text{vec}(\bm{X}), we can write

By choosing𝒁𝑳=2​μ2​𝑰d​k×d​k+∑j=1rηtjntj​σtj2​∑k=1ntj∑m=0Mtj−1vec​(𝚽​(𝒙m(k,tj))​𝒔tj𝖳)​(𝚽​(𝒙m(k,tj))⊗𝒔tj𝖳)subscript𝒁𝑳2subscript𝜇2subscript𝑰𝑑𝑘𝑑𝑘superscriptsubscript𝑗1𝑟subscript𝜂subscript𝑡𝑗subscript𝑛subscript𝑡𝑗superscriptsubscript𝜎subscript𝑡𝑗2superscriptsubscript𝑘1subscript𝑛subscript𝑡𝑗superscriptsubscript𝑚0subscript𝑀subscript𝑡𝑗1vec𝚽superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗superscriptsubscript𝒔subscript𝑡𝑗𝖳tensor-product𝚽superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗superscriptsubscript𝒔subscript𝑡𝑗𝖳\bm{Z}_{\bm{L}}=2\mu_{2}\bm{I}_{dk\times dk}+\sum_{j=1}^{r}\frac{\eta_{t_{j}}}{n_{t_{j}}\sigma_{t_{j}}^{2}}\sum_{k=1}^{n_{t_{j}}}\sum_{m=0}^{M_{t_{j}}-1}\text{vec}\left(\bm{\Phi}\left(\bm{x}_{m}^{(k,t_{j})}\right)\bm{s}_{t_{j}}^{\mathsf{T}}\right)\left(\bm{\Phi}\left(\bm{x}_{m}^{(k,t_{j})}\right)\otimes\bm{s}_{t_{j}}^{\mathsf{T}}\right), and𝒗𝑳=∑j=1rηtjntj​σtj2​∑k=1ntj∑m=0Mtj−1vec​(𝒖m(k,tj)​𝚽​(𝒙m(k,tj))​𝒔tj𝖳)subscript𝒗𝑳superscriptsubscript𝑗1𝑟subscript𝜂subscript𝑡𝑗subscript𝑛subscript𝑡𝑗superscriptsubscript𝜎subscript𝑡𝑗2superscriptsubscript𝑘1subscript𝑛subscript𝑡𝑗superscriptsubscript𝑚0subscript𝑀subscript𝑡𝑗1vecsuperscriptsubscript𝒖𝑚𝑘subscript𝑡𝑗𝚽superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗superscriptsubscript𝒔subscript𝑡𝑗𝖳\bm{v}_{\bm{L}}=\sum_{j=1}^{r}\frac{\eta_{t_{j}}}{n_{t_{j}}\sigma_{t_{j}}^{2}}\sum_{k=1}^{n_{t_{j}}}\sum_{m=0}^{M_{t_{j}}-1}\text{vec}\left(\bm{u}_{m}^{(k,t_{j})}\bm{\Phi}\left(\bm{x}_{m}^{(k,t_{j})}\right)\bm{s}_{t_{j}}^{\mathsf{T}}\right), we can update𝑳=𝒁𝑳−1​𝒗𝑳𝑳superscriptsubscript𝒁𝑳1subscript𝒗𝑳\bm{L}=\bm{Z}_{\bm{L}}^{-1}\bm{v}_{\bm{L}}.

To derive the update equations with respect to𝑺𝑺\bm{S}, similar approach to that of𝑳𝑳\bm{L}can be followed. The derivative of𝒆r​(𝑳,𝑺)subscript𝒆𝑟𝑳𝑺\bm{e}_{r}(\bm{L},\bm{S})with respect to𝑺𝑺\bm{S}can be computed column-wise for all tasks observed so far:

Using a similar analysis to the previous section, choosing

we can update𝒔tj=𝒁stj−1​𝒗𝒔tjsubscript𝒔subscript𝑡𝑗superscriptsubscript𝒁subscript𝑠subscript𝑡𝑗1subscript𝒗subscript𝒔subscript𝑡𝑗\bm{s}_{t_{j}}=\bm{Z}_{s_{t_{j}}}^{-1}\bm{v}_{\bm{s}_{t_{j}}}.

In this appendix, we prove the claims and lemmas from the main paper, leading to sublinear regret (Theorem 1).

Assume the policy for a tasktjsubscript𝑡𝑗{t_{j}}at a roundr𝑟rto be given byπ𝛂tj(tj)​(𝐮m(k,tj)|𝐱m(k,tj))|𝛉^r=𝒩​(𝛂tj𝖳|𝛉^r​𝚽​(𝐱m(k,tj)),𝛔tj)evaluated-atsuperscriptsubscript𝜋subscript𝛂subscript𝑡𝑗subscript𝑡𝑗conditionalsuperscriptsubscript𝐮𝑚𝑘subscript𝑡𝑗superscriptsubscript𝐱𝑚𝑘subscript𝑡𝑗subscript^𝛉𝑟𝒩evaluated-atsuperscriptsubscript𝛂subscript𝑡𝑗𝖳subscript^𝛉𝑟𝚽superscriptsubscript𝐱𝑚𝑘subscript𝑡𝑗subscript𝛔subscript𝑡𝑗\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\left(\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}=\mathcal{N}\left(\bm{\alpha}_{{t_{j}}}^{\mathsf{T}}\Big{|}_{\hat{\bm{\theta}}_{r}}\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right),\bm{\sigma}_{{t_{j}}}\right),
for𝐱m(k,tj)∈𝒳tjsuperscriptsubscript𝐱𝑚𝑘subscript𝑡𝑗subscript𝒳subscript𝑡𝑗\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{X}_{{t_{j}}}and𝐮m(k,tj)∈𝒰tjsuperscriptsubscript𝐮𝑚𝑘subscript𝑡𝑗subscript𝒰subscript𝑡𝑗\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{U}_{{t_{j}}}with𝒳tjsubscript𝒳subscript𝑡𝑗\mathcal{X}_{{t_{j}}}and𝒰tjsubscript𝒰subscript𝑡𝑗\mathcal{U}_{{t_{j}}}representing
the state and action spaces, respectively. The gradient∇𝛂tjltj​(𝛂tj)|𝛉^revaluated-atsubscript∇subscript𝛂subscript𝑡𝑗subscript𝑙subscript𝑡𝑗subscript𝛂subscript𝑡𝑗subscript^𝛉𝑟\nabla_{\bm{\alpha}_{{t_{j}}}}l_{{t_{j}}}\left(\bm{\alpha}_{{t_{j}}}\right)\Big{|}_{\hat{\bm{\theta}}_{r}},
forltj​(𝛂tj)=−1/ntj​∑k=1ntj∑m=0Mtj−1log⁡[π𝛂tj(tj)​(𝐮m(k,tj)|𝐱m(k,tj))]subscript𝑙subscript𝑡𝑗subscript𝛂subscript𝑡𝑗1subscript𝑛subscript𝑡𝑗superscriptsubscript𝑘1subscript𝑛subscript𝑡𝑗superscriptsubscript𝑚0subscript𝑀subscript𝑡𝑗1superscriptsubscript𝜋subscript𝛂subscript𝑡𝑗subscript𝑡𝑗conditionalsuperscriptsubscript𝐮𝑚𝑘subscript𝑡𝑗superscriptsubscript𝐱𝑚𝑘subscript𝑡𝑗l_{{t_{j}}}\left(\bm{\alpha}_{{t_{j}}}\right)=-\nicefrac{{1}}{{n_{{t_{j}}}}}\sum_{k=1}^{n_{{t_{j}}}}\sum_{m=0}^{M_{{t_{j}}}-1}\log\left[\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\left(\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\right]satisfies

withumax=maxk,m⁡{|𝐮m(k,tj)|}subscript𝑢maxsubscript𝑘𝑚superscriptsubscript𝐮𝑚𝑘subscript𝑡𝑗u_{\text{max}}=\max_{k,m}\left\{\left|\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\right|\right\}and𝚽max=maxk,m⁡{‖𝚽​(𝐱m(k,tj))‖2}subscript𝚽maxsubscript𝑘𝑚subscriptnorm𝚽superscriptsubscript𝐱𝑚𝑘subscript𝑡𝑗2\bm{\Phi}_{\text{max}}=\max_{k,m}\left\{\left|\left|\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\right|\right|_{2}\right\}for all trajectories and all tasks.

The proof of the above lemma will be provided as a
collection of claims. We start with the following:

Claim:Givenπ𝜶tj(tj)​(𝒖m(k)|𝒙m(k))|𝜽^r=𝒩​(𝜶tj𝖳|𝜽^r​𝚽​(𝒙m(k,tj)),𝝈tj)evaluated-atsuperscriptsubscript𝜋subscript𝜶subscript𝑡𝑗subscript𝑡𝑗conditionalsuperscriptsubscript𝒖𝑚𝑘superscriptsubscript𝒙𝑚𝑘subscript^𝜽𝑟𝒩evaluated-atsuperscriptsubscript𝜶subscript𝑡𝑗𝖳subscript^𝜽𝑟𝚽superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗subscript𝝈subscript𝑡𝑗\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\left(\bm{u}_{m}^{(k)}|\bm{x}_{m}^{(k)}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}=\mathcal{N}\left(\bm{\alpha}_{{t_{j}}}^{\mathsf{T}}\Big{|}_{\hat{\bm{\theta}}_{r}}\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right),\bm{\sigma}_{{t_{j}}}\right),
for𝒙m(k,tj)∈𝒳tjsuperscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗subscript𝒳subscript𝑡𝑗\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{X}_{{t_{j}}}and𝒖m(k,tj)∈𝒰tjsuperscriptsubscript𝒖𝑚𝑘subscript𝑡𝑗subscript𝒰subscript𝑡𝑗\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\in\mathcal{U}_{{t_{j}}},
andltj​(𝜶tj)=−1/ntj​∑k=1ntj∑m=0Mtj−1log⁡[π𝜶tj(tj)​(𝒖m(k,tj)|𝒙m(k,tj))]subscript𝑙subscript𝑡𝑗subscript𝜶subscript𝑡𝑗1subscript𝑛subscript𝑡𝑗superscriptsubscript𝑘1subscript𝑛subscript𝑡𝑗superscriptsubscript𝑚0subscript𝑀subscript𝑡𝑗1superscriptsubscript𝜋subscript𝜶subscript𝑡𝑗subscript𝑡𝑗conditionalsuperscriptsubscript𝒖𝑚𝑘subscript𝑡𝑗superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗l_{{t_{j}}}\left(\bm{\alpha}_{{t_{j}}}\right)=-\nicefrac{{1}}{{n_{{t_{j}}}}}\sum_{k=1}^{n_{{t_{j}}}}\sum_{m=0}^{M_{{t_{j}}}-1}\log\left[\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\left(\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\right],||∇𝜶tjltj​(𝜶tj)|𝜽^r||2evaluated-atsubscriptsubscript∇subscript𝜶subscript𝑡𝑗subscript𝑙subscript𝑡𝑗subscript𝜶subscript𝑡𝑗subscript^𝜽𝑟2\left|\left|\nabla_{\bm{\alpha}_{{t_{j}}}}l_{{t_{j}}}\left(\bm{\alpha}_{{t_{j}}}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}\right|\right|_{2}satisfies

Proof:Sinceπ𝜶tj(tj)​(𝒖m(k,tj)|𝒙m(k,tj))|𝜽^r=𝒩​(𝜶tj𝖳|𝜽^r​𝚽​(𝒙m(k,tj)),𝝈tj)evaluated-atsuperscriptsubscript𝜋subscript𝜶subscript𝑡𝑗subscript𝑡𝑗conditionalsuperscriptsubscript𝒖𝑚𝑘subscript𝑡𝑗superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗subscript^𝜽𝑟𝒩evaluated-atsuperscriptsubscript𝜶subscript𝑡𝑗𝖳subscript^𝜽𝑟𝚽superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗subscript𝝈subscript𝑡𝑗\pi_{\bm{\alpha}_{{t_{j}}}}^{\left({t_{j}}\right)}\left(\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}|\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}=\mathcal{N}\left(\bm{\alpha}_{{t_{j}}}^{\mathsf{T}}\Big{|}_{\hat{\bm{\theta}}_{r}}\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right),\bm{\sigma}_{{t_{j}}}\right),
we can write

Therefore:

Denotingmaxk,m⁡{|𝒖m(k,tj)|}=umaxsubscript𝑘𝑚superscriptsubscript𝒖𝑚𝑘subscript𝑡𝑗subscript𝑢max\max_{k,m}\left\{\left|\bm{u}_{m}^{\left(k,\ {t_{j}}\right)}\right|\right\}=u_{\text{max}}andmaxk,m⁡{‖𝚽​(𝒙m(k,tj))‖2}=𝚽maxsubscript𝑘𝑚subscriptnorm𝚽superscriptsubscript𝒙𝑚𝑘subscript𝑡𝑗2subscript𝚽max\max_{k,m}\left\{\left|\left|\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\right|\right|_{2}\right\}=\bm{\Phi}_{\text{max}}for all trajectories and all tasks, we can write

Using the Cauchy-Shwarz inequality(Horn & Mathias,1990), we can upper
boundmaxk,m{|⟨𝜶tj|𝜽^r,𝚽(𝒙m(k,tj))⟩|}\max_{k,m}\left\{\left|\left\langle\bm{\alpha}_{{t_{j}}}\Big{|}_{\hat{\bm{\theta}}_{r}},\bm{\Phi}\left(\bm{x}_{m}^{\left(k,\ {t_{j}}\right)}\right)\right\rangle\right|\right\}as

Finalizing the statement of the claim, the overall bound on the norm
of the gradient ofltj​(𝜶tj)subscript𝑙subscript𝑡𝑗subscript𝜶subscript𝑡𝑗l_{{t_{j}}}(\bm{\alpha}_{{t_{j}}})can be
written as

■■\blacksquare

Claim:The norm of the gradient of the loss function satisfies:

Proof:As mentioned previously, we consider
the linearization of the loss functionltjsubscript𝑙subscript𝑡𝑗l_{{t_{j}}}around the
constraint solution of the previous round,𝜽^rsubscript^𝜽𝑟\hat{\bm{\theta}}_{r}.
Since𝜽^rsubscript^𝜽𝑟\hat{\bm{\theta}}_{r}satisfies𝑨tk​𝜶tk=𝒃tk−𝒄tk,∀tk∈ℐr−1formulae-sequencesubscript𝑨subscript𝑡𝑘subscript𝜶subscript𝑡𝑘subscript𝒃subscript𝑡𝑘subscript𝒄subscript𝑡𝑘for-allsubscript𝑡𝑘subscriptℐ𝑟1\bm{A}_{{t_{k}}}\bm{\alpha}_{{t_{k}}}=\bm{b}_{{t_{k}}}-\bm{c}_{{t_{k}}},\forall{t_{k}}\in\mathcal{I}_{r-1}.
Hence, we can write

Therefore

Combining the above results with those of Eq. (16)
we arrive at

■■\blacksquare

The previous result finalizes the statement of the lemma, bounding
the gradient of the loss function in terms of thesafetyconstraints.

∎

The norm of the gradient
of the loss function evaluated at𝛉^rsubscript^𝛉𝑟\hat{\bm{\theta}}_{r}satisfies

The derivative ofltj​(𝜽)|𝜽^revaluated-atsubscript𝑙subscript𝑡𝑗𝜽subscript^𝜽𝑟l_{{t_{j}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}can be written as

The results of Lemma1bound‖∇𝜶tjltj​(𝜽)|𝜽^r∥22evaluated-atsubscriptdelimited-‖|subscript∇subscript𝜶subscript𝑡𝑗subscript𝑙subscript𝑡𝑗𝜽subscript^𝜽𝑟22\left\|\nabla_{\bm{\alpha}_{{t_{j}}}}l_{{t_{j}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2}^{2}.

Now, we target to bound each of||𝒔tj|𝜽^r||22evaluated-atsubscriptsubscript𝒔subscript𝑡𝑗subscript^𝜽𝑟22\Big{|}\Big{|}\bm{s}_{{t_{j}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{2}^{2}and||𝑳|𝜽^r||𝖥2evaluated-atsubscript𝑳subscript^𝜽𝑟𝖥2\Big{|}\Big{|}\bm{L}\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{\mathsf{F}}^{2}.

Considering the constraint𝑨tj​𝑳​𝒔tj+𝒄tj=𝒃tjsubscript𝑨subscript𝑡𝑗𝑳subscript𝒔subscript𝑡𝑗subscript𝒄subscript𝑡𝑗subscript𝒃subscript𝑡𝑗\bm{A}_{{t_{j}}}\bm{L}\bm{s}_{{t_{j}}}+\bm{c}_{{t_{j}}}=\bm{b}_{{t_{j}}}for a tasktjsubscript𝑡𝑗{t_{j}}, we realize that𝒔tj=𝑳+​(𝑨tj+​(𝒃tj−𝒄tj))subscript𝒔subscript𝑡𝑗superscript𝑳superscriptsubscript𝑨subscript𝑡𝑗subscript𝒃subscript𝑡𝑗subscript𝒄subscript𝑡𝑗\bm{s}_{{{t_{j}}}}=\bm{L}^{+}\left(\bm{A}_{{t_{j}}}^{+}\left(\bm{b}_{{t_{j}}}-\bm{c}_{{t_{j}}}\right)\right).
Therefore,

Noting that

To relate‖𝑳+‖𝟤subscriptnormsuperscript𝑳2\left|\left|\bm{L}^{+}\right|\right|_{\mathsf{2}}to‖𝑳‖𝖥subscriptnorm𝑳𝖥\left|\left|\bm{L}\right|\right|_{\mathsf{F}}, we need to bound‖(𝑳𝖳​𝑳)−1‖2subscriptnormsuperscriptsuperscript𝑳𝖳𝑳12\left|\left|\left(\bm{L}^{\mathsf{T}}\bm{L}\right)^{-1}\right|\right|_{2}in terms of‖𝑳‖𝖥subscriptnorm𝑳𝖥\|\bm{L}\|_{\mathsf{F}}. Denoting the spectrum of𝑳𝖳​𝑳superscript𝑳𝖳𝑳\bm{L}^{\mathsf{T}}\bm{L}asspec​(𝑳𝖳​𝑳)={𝝀1,…,𝝀k}specsuperscript𝑳𝖳𝑳subscript𝝀1…subscript𝝀𝑘\text{spec}\left(\bm{L}^{\mathsf{T}}\bm{L}\right)=\left\{\bm{\lambda}_{1},\dots,\bm{\lambda}_{k}\right\}such that0<𝝀1≤⋯≤𝝀k0subscript𝝀1⋯subscript𝝀𝑘0<\bm{\lambda}_{1}\leq\dots\leq\bm{\lambda}_{k}, thenspect​((𝑳𝖳​𝑳)−1)={1/𝝀1,…,1/𝝀k}spectsuperscriptsuperscript𝑳𝖳𝑳11subscript𝝀1…1subscript𝝀𝑘\text{spect}\left(\left(\bm{L}^{\mathsf{T}}\bm{L}\right)^{-1}\right)=\left\{\nicefrac{{1}}{{\bm{\lambda}_{1}}},\dots,\nicefrac{{1}}{{\bm{\lambda}_{k}}}\right\}such that1/𝝀k≤⋯≤1/𝝀k1subscript𝝀𝑘⋯1subscript𝝀𝑘\nicefrac{{1}}{{\bm{\lambda}_{k}}}\leq\dots\leq\nicefrac{{1}}{{\bm{\lambda}_{k}}}.
Hence,‖(𝑳𝖳​𝑳)−1‖2=max⁡{spec​((𝑳𝖳​𝑳)−1)}=1/𝝀1=1/λmin​(𝑳𝖳​𝑳)subscriptnormsuperscriptsuperscript𝑳𝖳𝑳12specsuperscriptsuperscript𝑳𝖳𝑳11subscript𝝀11subscript𝜆superscript𝑳𝖳𝑳\left|\left|\left(\bm{L}^{\mathsf{T}}\bm{L}\right)^{-1}\right|\right|_{2}=\max\left\{\text{spec}\left(\left(\bm{L}^{\mathsf{T}}\bm{L}\right)^{-1}\right)\right\}=\nicefrac{{1}}{{\bm{\lambda}_{1}}}=\nicefrac{{1}}{{\lambda_{\min}\left(\bm{L}^{\mathsf{T}}\bm{L}\right)}}.
Noticing thatspec​(𝑳𝖳​𝑳)=spec​(𝑳​𝑳𝖳)specsuperscript𝑳𝖳𝑳spec𝑳superscript𝑳𝖳\text{spec}\left(\bm{L}^{\mathsf{T}}\bm{L}\right)=\text{spec}\left(\bm{L}\bm{L}^{\mathsf{T}}\right),
we recognize‖(𝑳𝖳​𝑳)−1‖2=1/𝝀min​(𝑳​𝑳𝖳)≤1/psubscriptnormsuperscriptsuperscript𝑳𝖳𝑳121subscript𝝀min𝑳superscript𝑳𝖳1𝑝\left|\left|\left(\bm{L}^{\mathsf{T}}\bm{L}\right)^{-1}\right|\right|_{2}=\nicefrac{{1}}{{\bm{\lambda}_{\text{min}}\left(\bm{L}\bm{L}^{\mathsf{T}}\right)}}\leq\nicefrac{{1}}{{p}}.
Therefore

Plugging the results of Eq. (18) into Eq. (17),
we arrive at

Finally, since𝜽^rsubscript^𝜽𝑟\hat{\bm{\theta}}_{r}satisfies
the constraints, we note that‖𝑳|𝜽^r∥𝖥2≤q×devaluated-atsubscriptdelimited-‖|𝑳subscript^𝜽𝑟𝖥2𝑞𝑑\left\|\bm{L}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{\mathsf{F}}^{2}\leq q\times d.
Consequently,

∎

The L2norm of the constraint solution at roundr−1𝑟1r-1,‖𝛉^r‖22superscriptsubscriptnormsubscript^𝛉𝑟22\|\hat{\bm{\theta}}_{r}\|_{2}^{2}is bounded by

with|ℐr−1|subscriptℐ𝑟1\left|\mathcal{I}_{r-1}\right|being the cardinality
ofℐr−1subscriptℐ𝑟1\mathcal{I}_{r-1}representing the number of
different tasks observed so-far.

Noting
that𝜽^r=[𝜽1,…,𝜽d​k⏟𝑳|𝜽^r,𝜽d​k+1,…⏟𝒔i1|θ^r,…,…⏟𝒔ir−1|θr^,…,𝜽d​k+k​T⋆⏟𝟎’s: unobserved tasks]𝖳subscript^𝜽𝑟superscriptsubscript⏟subscript𝜽1…subscript𝜽𝑑𝑘evaluated-at𝑳subscript^𝜽𝑟subscript⏟subscript𝜽𝑑𝑘1…evaluated-atsubscript𝒔subscript𝑖1subscript^𝜃𝑟subscript⏟……evaluated-atsubscript𝒔subscript𝑖𝑟1^subscript𝜃𝑟subscript⏟…subscript𝜽𝑑𝑘𝑘superscript𝑇⋆𝟎’s: unobserved tasks𝖳\hat{\bm{\theta}}_{r}=\Big{[}\underbrace{\bm{\theta}_{1},\dots,\bm{\theta}_{dk}}_{\bm{L}\Big{|}_{\hat{\bm{\theta}}_{r}}},\underbrace{\bm{\theta}_{dk+1},\dots}_{\bm{s}_{i_{1}}\Big{|}_{\hat{\theta}_{r}}},\underbrace{\dots,\dots}_{\bm{s}_{i_{r-1}}\Big{|}_{\hat{\theta_{r}}}},\underbrace{\dots,\bm{\theta}_{dk+kT^{\star}}}_{\text{$\bm{0}$'s: unobserved tasks}}\Big{]}^{\mathsf{T}},
it is easy to see

∎

The L2norm of the linearizing term ofltj​(𝛉)subscript𝑙subscript𝑡𝑗𝛉l_{{t_{j}}}(\bm{\theta})around𝛉^rsubscript^𝛉𝑟\hat{\bm{\theta}}_{r},‖𝐟^tj|𝛉^r∥2evaluated-atsubscriptdelimited-‖|subscript^𝐟subscript𝑡𝑗subscript^𝛉𝑟2\left\|\hat{\bm{f}}_{{t_{j}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\right\|_{2},
is bounded by

with𝛅ltjsubscript𝛅subscript𝑙subscript𝑡𝑗\bm{\delta}_{l_{{t_{j}}}}being the constant upper-bound
on|ltj(𝛉)|𝛉^r|\left|l_{{t_{j}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}\right|,
and

We have previously shown that||𝒇^tj|𝜽^r||2≤||∇𝜽ltj​(𝜽)|𝜽^r||2+|ltj​(θ^r)|+||∇𝜽ltj​(𝜽)|𝜽^r||2×‖𝜽^r‖2evaluated-atsubscriptsubscript^𝒇subscript𝑡𝑗subscript^𝜽𝑟2evaluated-atsubscriptsubscript∇𝜽subscript𝑙subscript𝑡𝑗𝜽subscript^𝜽𝑟2subscript𝑙subscript𝑡𝑗subscript^𝜃𝑟evaluated-atsubscriptsubscript∇𝜽subscript𝑙subscript𝑡𝑗𝜽subscript^𝜽𝑟2subscriptnormsubscript^𝜽𝑟2\Big{|}\Big{|}\hat{\bm{f}}_{{t_{j}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{2}\leq\Big{|}\Big{|}\nabla_{\bm{\theta}}l_{{t_{j}}}\left(\bm{\theta}\right)\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{2}+\Big{|}l_{{t_{j}}}\left(\hat{\theta}_{r}\right)\Big{|}+\Big{|}\Big{|}\nabla_{\bm{\theta}}l_{{t_{j}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{2}\times\Big{|}\Big{|}\hat{\bm{\theta}}_{r}\Big{|}\Big{|}_{2}.
Using the previously derived lemmas we can upper-bound||𝒇^tj|𝜽^r||2evaluated-atsubscriptsubscript^𝒇subscript𝑡𝑗subscript^𝜽𝑟2\Big{|}\Big{|}\hat{\bm{f}}_{{t_{j}}}\Big{|}_{\hat{\bm{\theta}}_{r}}\Big{|}\Big{|}_{2}as follows

Further,

Therefore

with𝜹ltjsubscript𝜹subscript𝑙subscript𝑡𝑗\bm{\delta}_{l_{{t_{j}}}}being the constant upper-bound
on|ltj(𝜽)|𝜽^r|\left|l_{{t_{j}}}(\bm{\theta})\Big{|}_{\hat{\bm{\theta}}_{r}}\right|,
and

∎

AfterR𝑅Rrounds and choosingηt1=⋯=ηtj=η=1Rsubscript𝜂subscript𝑡1⋯subscript𝜂subscript𝑡𝑗𝜂1𝑅\eta_{{t_{1}}}=\dots=\eta_{{t_{j}}}=\eta=\frac{1}{\sqrt{R}},𝐋|𝛉^1=diag𝐤​(ζ)evaluated-at𝐋subscript^𝛉1subscriptdiag𝐤𝜁\bm{L}\Big{|}_{\hat{\bm{\theta}}_{1}}=\text{diag}_{\bm{k}}(\zeta),
withdiag𝐤​(⋅)subscriptdiag𝐤⋅\text{diag}_{\bm{k}}(\cdot)being a diagonal matrix among
the𝐤𝐤\bm{k}columns of𝐋𝐋\bm{L},p≤ζ2≤q𝑝superscript𝜁2𝑞p\leq\zeta^{2}\leq q, and𝐒|𝛉^1=𝟎k×|𝒯|evaluated-at𝐒subscript^𝛉1subscript0𝑘𝒯\bm{S}\Big{|}_{\hat{\bm{\theta}}_{1}}=\bm{0}_{k\times{|\mathcal{T}|}},
for any𝐮∈𝒦𝐮𝒦\bm{u}\in\mathcal{K}our algorithm exhibits a sublinear regret of the form

Given the ingredients of the previous section, next
we derive the sublinear regret results which finalize the statement
of the theorem. First, it is easy to see that

Further, from strong convexity of the regularizer we obtain:

It can be seen that

Finally, for any𝒖∈𝒦𝒖𝒦\bm{u}\in\mathcal{K}, we have:

Assumingηt1=⋯=ηtj=ηsubscript𝜂subscript𝑡1⋯subscript𝜂subscript𝑡𝑗𝜂\eta_{{t_{1}}}=\dots=\eta_{{t_{j}}}=\eta, we can derive

The following lemma finalizes the statement of the theorem:

After T rounds and forηt1=⋯=ηtj=η=1Rsubscript𝜂subscript𝑡1⋯subscript𝜂subscript𝑡𝑗𝜂1𝑅\eta_{{t_{1}}}=\dots=\eta_{{t_{j}}}=\eta=\frac{1}{\sqrt{R}},
our algorithm exhibits, for any𝐮∈𝒦𝐮𝒦\bm{u}\in\mathcal{K}, a sublinear
regret of the form

It is then easy to see

Since|ℐR−1|≤|𝒯|subscriptℐ𝑅1𝒯|\mathcal{I}_{R-1}|\leq{|\mathcal{T}|}with|𝒯|𝒯{|\mathcal{T}|}being the total number of tasks available, then we can write

with𝜸5=8​d/p2​q​𝜸12​(R)​maxtk∈ℐR−1⁡{‖𝑨tk†‖22​(‖𝒃tk‖2+𝒄max)2}subscript𝜸58𝑑superscript𝑝2𝑞superscriptsubscript𝜸12𝑅subscriptsubscript𝑡𝑘subscriptℐ𝑅1superscriptsubscriptnormsuperscriptsubscript𝑨subscript𝑡𝑘†22superscriptsubscriptnormsubscript𝒃subscript𝑡𝑘2subscript𝒄max2\bm{\gamma}_{5}=8\nicefrac{{d}}{{p^{2}}}q\bm{\gamma}_{1}^{2}(R)\max_{{t_{k}}\in\mathcal{I}_{R-1}}\left\{\|\bm{A}_{{t_{k}}}^{\dagger}\|_{2}^{2}\left(\|\bm{b}_{{t_{k}}}\|_{2}+\bm{c}_{\text{max}}\right)^{2}\right\}.
Further, it is easy to see that𝛀0​(𝒖)≤q​d+𝜸5​(R)​|𝒯|subscript𝛀0𝒖𝑞𝑑subscript𝜸5𝑅𝒯\bm{\Omega}_{0}(\bm{u})\leq qd+\bm{\gamma}_{5}(R){|\mathcal{T}|}with𝜸5​(R)subscript𝜸5𝑅\bm{\gamma}_{5}(R)being a constant, which
leads to

InitializingL𝐿\bm{L}andS𝑆\bm{S}:We initialize𝑳|𝜽^1=diag𝒌​(ζ)evaluated-at𝑳subscript^𝜽1subscriptdiag𝒌𝜁\bm{L}\Big{|}_{\hat{\bm{\theta}}_{1}}=\text{diag}_{\bm{k}}(\zeta),
withp≤ζ2≤q𝑝superscript𝜁2𝑞p\leq\zeta^{2}\leq qand𝑺|𝜽^1=𝟎k×|𝒯|evaluated-at𝑺subscript^𝜽1subscript0𝑘𝒯\bm{S}\Big{|}_{\hat{\bm{\theta}}_{1}}=\bm{0}_{k\times{|\mathcal{T}|}}ensures the invertability of𝑳𝑳\bm{L}and that the constraints are
met. This leads us to

Choosingηt1=⋯=ηtj=η=1/Rsubscript𝜂subscript𝑡1⋯subscript𝜂subscript𝑡𝑗𝜂1𝑅\eta_{{t_{1}}}=\dots=\eta_{{t_{j}}}=\eta=\nicefrac{{1}}{{\sqrt{R}}},
we acquire sublinear regret, finalizing the statement of the theorem:

with𝜸5​(R)subscript𝜸5𝑅\bm{\gamma}_{5}(R)being a constant. ∎

∎

[图片: images\image_8.png]

[图片: images\image_9.png]

