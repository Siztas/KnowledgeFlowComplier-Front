###基于次线性遗憾的安全终身强化学习方法

该论文提出了一种新颖的终身强化学习（RL）方法，通过引入具有次线性遗憾的安全策略搜索来解决现有技术的局限性。当前终身学习方法通常存在遗憾不收敛的问题，并可能产生不安全的控制策略，这限制了其在实际场景中的应用。

针对这些问题，作者开发了一种在对抗性环境中运行的终身策略梯度学习器，该方法在强制执行策略安全约束的同时，实现了遗憾量级为\(\mathcal{O}(\sqrt{R})\)（R为总回合数）的次线性遗憾。该框架将终身学习形式化为在线多任务学习问题，其中每个任务的策略参数表示为共享潜在基底的线性组合。这种设计不仅实现了跨任务的高效知识迁移，还能确保策略始终保持在约束条件定义的安全区域内。

通过在包括四旋翼飞行器控制应用在内的多个基准动力系统上进行验证，该方法展现出优于标准策略梯度方法和现有终身学习技术的性能。理论分析证明了次线性遗憾界的成立，实证结果则表明该算法能够利用先前获得的知识快速学习出高性能的安全控制策略。这项研究特别针对机器人控制领域的关键限制——不安全策略可能导致物理损坏或系统故障的问题，使其特别适合部署在高维度、安全至上的实际应用场景。
