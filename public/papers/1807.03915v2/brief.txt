
###Seq2Seq2Sentiment：多模态情感分析中的无监督表示学习

本文《Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment Analysis》旨在解决多模态情感分析中的联合表示学习问题，作者提出了两种基于序列到序列（Seq2Seq）模型的无监督学习方法：标准的模态翻译模型和层次化模态翻译模型。情感分析传统上多依赖文本单模态，使用词汇和短语的极性特征进行建模，但这一方式难以捕捉情感的复杂性。为此，近年来研究者尝试结合音频与视频模态，从中提取更丰富的情感信息。在此背景下，作者指出现有多模态学习大多为监督方法，依赖大量人工标注数据，忽视了网络中丰富的无标签数据资源。因此本文聚焦于无监督表示学习，尝试利用多模态Seq2Seq架构，通过模态翻译的方式将一种模态映射为另一种模态，同时捕捉各模态间的顺序结构与相关性，以构建有效的共享表示。相较于传统拼接式融合方法，本文方法采用完整序列的编码表示，即通过函数 \$\widetilde{X\_i} = \text{Seq2Seq\_Encoder}(X\_i)\$ 生成跨模态嵌入，并在CMU-MOSI数据集上验证了其优越性，尤其在双模态设置中F1得分提升显著。此外，为对齐多模态输入，作者基于文本时间步将各模态统一处理，并在实验设计中将原始视频划分为若干短片段，实现多模态信息的细粒度学习。与以往MV-LSTM、Tensor Fusion Network等监督方法不同，本文不仅实现了无监督训练，还利用了注意力机制强化对变长序列中长期依赖关系的建模，进一步提升了模型对情感的表达能力。这项工作在理论上拓展了条件生成模型的研究框架，在实践上为多模态情感识别提供了更具泛化性的解决方案。
