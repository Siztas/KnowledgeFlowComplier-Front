标题：Seq2Seq2Sentiment:Multimodal Sequence to Sequence Models for Sentiment Analysis

Multimodal machine learning is a core research area spanning the language, visual and acoustic modalities. The central challenge in multimodal learning involves learning representations that can process and relate information from multiple modalities. In this paper, we propose two methods for unsupervised learning of joint multimodal representations using sequence to sequence (Seq2Seq) methods: aSeq2Seq Modality Translation Modeland aHierarchical Seq2Seq Modality Translation Model. We also explore multiple different variations on the multimodal inputs and outputs of these seq2seq models. Our experiments on multimodal sentiment analysis using the CMU-MOSI dataset indicate that our methods learn informative multimodal representations that outperform the baselines and achieve improved performance on multimodal sentiment analysis, specifically in the Bimodal case where our model is able to improve F1 Score by twelve points. We also discuss future directions for multimodal Seq2Seq methods.

Sentiment analysis, which involves identifying a speaker’s sentiment, is an open research problem. In this field, the majority of work done focused on unimodal methodologies -
primarily textual analysis - where investigating was limited to identifying usage of words in positive and negative scenarios.
However, unimodal textual sentiment analysis through usage of words, phrases, and their interdependencies were found to be insufficient for extracting affective content from textual opinions(Rosas et al.,2013).111*These authors contributed equally.As a result, there has been a recent push towards using statistical methods to extract additional behavioral cues not present in the language modality from the video and audio modalities. This research field is known as multimodal sentiment analysis and it extends the conventional text-based definition of sentiment analysis to a multimodal setup where different modalities contribute to modeling the sentiment of the speaker. For example,(Kaushik et al.,2013)explores modalities such as audio, while(Wöllmer et al.,2013)explores a multimodal approach to predicting sentiment. This push has been further bolstered by the advent of multimodal social media platforms, such as YouTube, Facebook, and VideoLectures which are used to express personal opinions on a worldwide scale.
As a result, several multimodal datasets, such as CMU-MOSI(Zadeh et al.,2016)and later CMU-MOSEI(Zadeh et al.,2018c), ICT-MMMO(Wöllmer et al.,2013)and YouTube(Morency et al.,2011), take advantage of the abundance of multimodal data on the Internet.
At the same time, neural network based multimodal models have been proposed that are highly effective at learning multimodal representations for multimodal sentiment analysis(Chen et al.,2017; Poria et al.,2017; Zadeh et al.,2018a,b).

Recent progress has been limited to supervised learning using labeled data, and does not take advantage of the abundant unlabeled data on the Internet.
To address this gap, our work is primarily one of unsupervised representation learning.
We attempt to learn a multimodal representation of our data in a structured paradigm and explore whether a joint multimodal representation trained via unsupervised learning can improve the performance for multimodal sentiment analysis.
While representation learning has been an area of rapid research in the past years, there has been limited work that explores multimodal setting.
To this end, we propose two methods: aSeq2Seq Modality Translation Modeland aHierarchical Seq2Seq Modality Translation Modelfor unsupervised learning of multimodal representations. Our results show that using multimodal representations learned from our Seq2Seq modality translation method outperforms the baselines and achieves improved performance on multimodal sentiment analysis.

In the past, approaches to text-based emotion and sentiment recognition rely mainly on rule-based techniques, bag of words (BoW) modeling or SNoW architecture(Chaumartin,2007)using a large sentiment or emotion lexicon(Mishne et al.,2005), or statistical approaches that assume the availability of a large dataset annotated with polarity or emotion labels.

Multimodal sentiment analysis has gained a lot of research interests over the last few years(Baltrusaitis et al.,2017). Probably the most challenging task in multimodal sentiment analysis is to find a joint representation of multiple modalities. This problem is has been approached in a number of ways. Earlier works such as(Ngiam et al.,2011; Lazaridou et al.,2015; Kiros et al.,2014)have pushed some progress towards this direction.

Recently, more advanced neural network models were proposed to learn multimodal representations. The Multi-View LSTM (MV-LSTM)(Rajagopalan et al.,2016)was suggested to exploit fusion and temporal relationships. MV-LSTM partitions memory cells and gates into multiple regions corresponding to different views. Tensor Fusion Network(Zadeh et al.,2017)presented an efficient method based on Cartesian-product to take into consideration intramodal and intermodal relations between video, audio and text of the reviews to create a novel feature representation for each utterance. The Gated Multimodal Embedding modelChen et al. (2017)created an algorithm using reinforcement learning to train an on-off switch that decided what values the video and audio components would have. Noisy modalities are turned off and clean modalities are allowed to pass through.Zadeh et al. (2018a)utilizes external multimodal memory mechanisms to store multimodal information and create multimodal representations through time.Zadeh et al. (2018b)proposed using multiple attention coefficient assignments to represent multiple cross-modal interactions. However, all these methods discussed so far are purely supervised approaches to multimodal sentiment analysis and do not leverage the power of unsupervised data and generative approaches towards learning multimodal representations.

Besides supervised approaches, generative methods based on generative adversarial networks (GAN)(Goodfellow et al.,2014)have attracted significant interest in learning joint distribution between two or more modalities(Donahue et al.,2016; Li et al.,2017; Gan et al.,2017). Another method to deal with multimodal problems is to view them as conditional problems which learn to map a modality to the other(Mirza and Osindero,2014; Kingma et al.,2014; Pandey and Dukkipati,2017). Our work can be viewed as an extension of the conditional approach, as both utilize unsupervised learning. However, our work differs from those in that it takes into account the sequential dependency within each modality.

Finally, attention based layers have also proved themselves to be effective tools to boost performance of neural network models, such as in neural machine translation(Klein et al.,; Bahdanau et al.,2014; Luong et al.,2015), speech recognition(Sriram et al.,2017)and in image captioning(Xu et al.,2015). Our work also employs this mechanism in an attempt to better handle long-term dependencies of variable-length sequences.

Given a dataset with dataX=(Xt​e​x​t,Xa​u​d​i​o,Xv​i​d​e​o)𝑋superscript𝑋𝑡𝑒𝑥𝑡superscript𝑋𝑎𝑢𝑑𝑖𝑜superscript𝑋𝑣𝑖𝑑𝑒𝑜X=(X^{text},X^{audio},X^{video})whereXt​e​x​tsuperscript𝑋𝑡𝑒𝑥𝑡X^{text},Xa​u​d​i​osuperscript𝑋𝑎𝑢𝑑𝑖𝑜X^{audio},Xv​i​d​e​osuperscript𝑋𝑣𝑖𝑑𝑒𝑜X^{video}stand for text, audio and video modality inputs, respectively. Typically a dataset is indexed by videos. This means that if we haven𝑛nvideos, thenX=(X1,X2,…,Xn)𝑋subscript𝑋1subscript𝑋2…subscript𝑋𝑛X=(X_{1},X_{2},...,X_{n})whereXi=(Xit​e​x​t,Xia​u​d​i​o,Xiv​i​d​e​o),1≤i≤nformulae-sequencesubscript𝑋𝑖superscriptsubscript𝑋𝑖𝑡𝑒𝑥𝑡superscriptsubscript𝑋𝑖𝑎𝑢𝑑𝑖𝑜superscriptsubscript𝑋𝑖𝑣𝑖𝑑𝑒𝑜1𝑖𝑛X_{i}=({X_{i}}^{text},{X_{i}}^{audio},{X_{i}}^{video}),\,1\leq i\leq n. The corresponding labels for thesen𝑛nvideos areY=(Y1,Y2,…,Yn),Yi∈ℝformulae-sequence𝑌subscript𝑌1subscript𝑌2…subscript𝑌𝑛subscript𝑌𝑖ℝY=(Y_{1},Y_{2},...,Y_{n}),\,Y_{i}\in\mathbb{R}.

To simplify the problem, we align the input based on words. Typically, researchers often segment each video into a smaller set in which each segmented video will last a couple of seconds, instead of minutes as done inChen et al. (2017). After such alignment and segmentation, we have the equal-length inputs of each modality per video. For example, at theit​hsuperscript𝑖𝑡ℎi^{th}video, we haveXit​e​x​t=(wi(1),wi(2),…,wi(Ti))superscriptsubscript𝑋𝑖𝑡𝑒𝑥𝑡superscriptsubscript𝑤𝑖1superscriptsubscript𝑤𝑖2…superscriptsubscript𝑤𝑖subscript𝑇𝑖{X_{i}}^{text}=({w_{i}}^{(1)},{w_{i}}^{(2)},...,{w_{i}}^{(T_{i})})wherewi(t)superscriptsubscript𝑤𝑖𝑡{w_{i}}^{(t)}stands for thett​hsuperscript𝑡𝑡ℎt^{th}word andTisubscript𝑇𝑖T_{i}is the length of theit​hsuperscript𝑖𝑡ℎi^{th}video’s text input,a.k.atime steps. Note that different videos will have different time steps. Similarly for this video, we have a sequence of audio inputXia​u​d​i​o=(ai(1),ai(2),…,ai(Ti))superscriptsubscript𝑋𝑖𝑎𝑢𝑑𝑖𝑜superscriptsubscript𝑎𝑖1superscriptsubscript𝑎𝑖2…superscriptsubscript𝑎𝑖subscript𝑇𝑖{X_{i}}^{audio}=({a_{i}}^{(1)},{a_{i}}^{(2)},...,{a_{i}}^{(T_{i})})and video inputXiv​i​d​e​o=(vi(1),vi(2),…,vi(Ti))superscriptsubscript𝑋𝑖𝑣𝑖𝑑𝑒𝑜superscriptsubscript𝑣𝑖1superscriptsubscript𝑣𝑖2…superscriptsubscript𝑣𝑖subscript𝑇𝑖{X_{i}}^{video}=({v_{i}}^{(1)},{v_{i}}^{(2)},...,{v_{i}}^{(T_{i})}).

In this work we are tackling the input learning problem where we want to learn the embedding representation for all text, audio, and video modalities:Xi~=f​(Xi)=f​((Xit​e​x​t,Xia​u​d​i​o,Xiv​i​d​e​o))~subscript𝑋𝑖𝑓subscript𝑋𝑖𝑓superscriptsubscript𝑋𝑖𝑡𝑒𝑥𝑡superscriptsubscript𝑋𝑖𝑎𝑢𝑑𝑖𝑜superscriptsubscript𝑋𝑖𝑣𝑖𝑑𝑒𝑜\widetilde{X_{i}}=f(X_{i})=f(({X_{i}}^{text},{X_{i}}^{audio},{X_{i}}^{video})).
In our baseline model, the functionf𝑓fis simply the concatenation at time step level:xi~t=[wit;ait;vit]superscript~subscript𝑥𝑖𝑡superscriptsubscript𝑤𝑖𝑡superscriptsubscript𝑎𝑖𝑡superscriptsubscript𝑣𝑖𝑡\widetilde{x_{i}}^{t}=[{w_{i}}^{t};{a_{i}}^{t};{v_{i}}^{t}]

In our proposed method, we learnXi~~subscript𝑋𝑖\widetilde{X_{i}}by using a Seq2Seq model. We do not calculate each embedding representation for each time step, but for the whole sequence. Formally,Xi~=f​(Xi)=S​e​q​2​S​e​q​_​E​n​c​o​d​e​r​(Xi)~subscript𝑋𝑖𝑓subscript𝑋𝑖𝑆𝑒𝑞2𝑆𝑒𝑞_𝐸𝑛𝑐𝑜𝑑𝑒𝑟subscript𝑋𝑖\widetilde{X_{i}}=f(X_{i})=Seq2Seq\_Encoder(X_{i})whereS​e​q​2​S​e​q​_​E​n​c​o​d​e​r𝑆𝑒𝑞2𝑆𝑒𝑞_𝐸𝑛𝑐𝑜𝑑𝑒𝑟Seq2Seq\_Encoderis the encoder part of our Seq2Seq model.

Now, we have the transformed inputsX~=(X1~,X2~,…,Xn~)~𝑋~subscript𝑋1~subscript𝑋2…~subscript𝑋𝑛\widetilde{X}=(\widetilde{X_{1}},\widetilde{X_{2}},...,\widetilde{X_{n}})and outputsY=(Y1,Y2,…,Yn)𝑌subscript𝑌1subscript𝑌2…subscript𝑌𝑛Y=(Y_{1},Y_{2},...,Y_{n})forn𝑛nvideos, whereXi~=(xi~1,xi~2,…,xi~Ti)~subscript𝑋𝑖superscript~subscript𝑥𝑖1superscript~subscript𝑥𝑖2…superscript~subscript𝑥𝑖subscript𝑇𝑖\widetilde{X_{i}}=(\widetilde{x_{i}}^{1},\widetilde{x_{i}}^{2},...,\widetilde{x_{i}}^{T_{i}}). For simplicity, in the next formula, we omit the index of video segmenti𝑖i, and so the input becomesX~=(x~1,x~2,…,x~T)~𝑋superscript~𝑥1superscript~𝑥2…superscript~𝑥𝑇\widetilde{X}=(\widetilde{x}^{1},\widetilde{x}^{2},...,\widetilde{x}^{T}), and the labels becomeY=(y1,y2,…,yT)𝑌superscript𝑦1superscript𝑦2…superscript𝑦𝑇{Y}=({y}^{1},{y}^{2},...,{y}^{T}).

We will be using a Recurrent Neural Network (RNN) such as LSTM(Hochreiter and Schmidhuber,1997)or GRU(Chung et al.,2015)to model this sequence. In detail, this RNN has a stack ofK𝐾Khidden layersh=(h1,h2,…,hK)ℎsuperscriptℎ1superscriptℎ2…superscriptℎ𝐾h=({h^{1},h^{2},...,h^{K}}), each containsD𝐷Dhidden neurons:hk=(h1k,h2k,…,hDk),k∈[1,K]formulae-sequencesuperscriptℎ𝑘superscriptsubscriptℎ1𝑘superscriptsubscriptℎ2𝑘…superscriptsubscriptℎ𝐷𝑘𝑘1𝐾{h}^{k}=({h_{1}^{k},h_{2}^{k},...,h_{D}^{k}}),\,k\in[1,K]. We denoteW𝑊{W}andb𝑏{b}to be weight and bias, then for the first layer which contacts directly with input:

whereH𝐻His the RNN cell function. For example of LSTM, it containsinput, forget, outputandcell state. At hidden layerk∈[2,K]𝑘2𝐾k\in[2,K]:

Optionally, we apply a soft attention mechanismon topof the last hidden layerhKsuperscriptℎ𝐾{h^{K}}, with shared weightWαsubscript𝑊𝛼W_{\alpha}overT𝑇Ttime steps, then we can obtain the attention outputα𝛼\alpha:

The last hidden layer’s output now becomes:

And the last output layer with regression score is:

Finally, we calculate the loss with respect to the labels. As inChen et al. (2017), we choose Mean Absolute Error (MAE) as our loss and later train with stochastic gradient descent:

[图片: images\image_1.png]
图片说明: Figure 1:Seq2Seq Modality Translation Model
with input(X1,…,XN)subscript𝑋1…subscript𝑋𝑁(X_{1},...,X_{N})and output is(Y1,…,YT)subscript𝑌1…subscript𝑌𝑇(Y_{1},...,Y_{T}).
Seq2Seq makes use of the whole input sequence in the decoding phase for every tokenYisubscript𝑌𝑖Y_{i}.
If attention model (yellow color) is used, for eachYisubscript𝑌𝑖Y_{i}, it learns a separate weight vectorw.r.teach token of inputX𝑋Xto see which token should the decoder “attend” more.

[图片: images\image_2.png]
图片说明: Figure 2:Hierarchical Seq2Seq Modality Translation Model: first we train with 2 modalities, then we add one more on the second phase, from which the results will be fed into RNN for sentiment prediction.
The green boxes denote the joint representation learned by Seq2Seq models: the joint representation of modalities A and B will be fed into another Seq2Seq model which in turn learns the joint representation of AB and another modality C. Finally the joint representation of ABC will be fed into a RNN to predict sentiment.

In this section we describe the different approaches that we plan to take to improve affect recognition through learning multimodal representations.

TheSeq2Seq Modality Translation Modelaims to learn multimodal representations that can be used for discriminative tasks. While Seq2Seq models have been predominantly used for machine translation(Bahdanau et al.,2014; Luong et al.,2015), we extend its usage to the realm of multimodal machine learning where we use it to translate one modality to another, or translate a joint representation to another single or joint representation. To do so, we propose a Seq2Seq modality translation model with attention mechanism, as shown in Figure1. ModalityX𝑋Xis translated into modalityY𝑌Y. Our hypothesis is that the intermediate representation of this model, i.e. the output of Seq2Seq’s encoder, or the input of its decoder, is close to the joint representation(X,Y)𝑋𝑌(X,Y)of the two modalities involved. As a result, this representation can be used for tasks that involve learning joint representation across multiple modalities. The detail is in Algorithm1.

Formally, the Seq2Seq Modality Translation Model consists of 2 separate steps: encoding and decoding, each phase typically consists of a single RNN or a stack of them. This model accepts variable-length inputs ofX𝑋XandY𝑌Y, and the network should be trained to maximize the translational condition probabilityp​(Y|X)𝑝conditional𝑌𝑋p(Y|X). For encoding, it encodes the whole input sequence X into an embedded representation. The hidden state output of each time step is based on the previous hidden state along with the input sequence (refer to Figure1):

The encoder’s output is the final hidden state’s output of the encoding RNN:

where N is the length of the input sequence X. The decoder tries to decode each tokenYisubscript𝑌𝑖Y_{i}at a time based onℰℰ\mathcal{E}and all previous decoded tokens, which is formulated as:

The Seq2Seq training target is to find the best translation sequence which is as close to the ground truth Y as possible, or formally:

And while there are some other search algorithms such as random sampling or greedy search to decode each tokenNeubig (2017), we use the traditional beam search approachSutskever et al. (2014).

The Seq2Seq Modality Translation Model only learns joint representation between 2 modalitiesX𝑋XandY𝑌Y. While this might be a strong starting point, we believe an approach that captures the joint interactions between all different modalitiesX,Y,Z𝑋𝑌𝑍X,Y,Zis more effective in modeling the full distribution of the multimodal data and therefore more useful for regression or classification. In response, we propose theHierarchical Seq2Seq Modality Translation Modelthat learns a joint multimodal representation. Once the Seq2Seq Modality Translation Model is trained for 2 modalitiesX𝑋XandY𝑌Y, we obtain the intermediate representationℰX​Ysubscriptℰ𝑋𝑌\mathcal{E}_{XY}which is the joint representation of(X,Y)𝑋𝑌(X,Y).ℰX​Ysubscriptℰ𝑋𝑌\mathcal{E}_{XY}is in turn treated as input sequence for the next Seq2Seq Modality Translation Model to decode the third modalityZ𝑍Z. The final multimodal representationℰX​Y​Zsubscriptℰ𝑋𝑌𝑍\mathcal{E}_{XYZ}represents the joint representation of(X,Y,Z)𝑋𝑌𝑍(X,Y,Z). The Hierarchical Seq2Seq Modality Translation Model is described as in Algorithm2.

This strategy is also illustrated in Figure2. The output of the second Seq2Seq model is the input of the last RNN model where we will train to predict regression sentiment scores. This last Seq2Seq model will be trained using MAE loss function and it perform subsequent regression process.

We explored the applications of this model to the CMU-MOSI dataset(Zadeh et al.,2016). We implemented a baseline LSTM model based off the work done in(Chen et al.,2017). Our implementation uses 66.67% of the data for training from which we take a 15.15% held-out set for validation, and the remaining 33.33% is used for testing.
Finally, we evaluated our proposed model against the baseline results generated by the implementation of(Chen et al.,2017). Here we compared our results against the various multimodal configurations evaluating our performance using precision, recall, and F1 scores.

The dataset that we use to explore applications of our model is the CMU Multimodal Opinion-level Sentiment Intensity dataset (CMU-MOSI). The dataset contains video, audio, and transcriptions of 89 different speakers in 93 different videos divided into 2199 separate opinion sentiments. Each video has an associated sentiment label in the range from -3 to 3. The low end of the spectrum (-3) indicates strongly negative sentiment, where as the high end of the spectrum indicates strongly positive sentiment (+3), and ratings of 0 indicate neutral sentiment. The CMU-MOSI dataset is currently subject to much research(Poria et al.,2017; Chen et al.,2017; Zadeh et al.,2018a,b)and the current state of the art is achieved by(Poria et al.,2017)with an F1 score of 80.3 using a context aware model across entire videos. The state of the art using only individual segments is achieved by(Zadeh et al.,2018a)with an F1 score of 77.3.

With respect to raw features that are being given as inputs to our model, we perform feature extraction in the same manner as described in(Chen et al.,2017). In the text domain, pretrained 300 dimensional GLoVe embeddings(Pennington et al.,2014)were used to represent the textual tokens. In the audio domain, low level acoustic features including 12 Mel-frequency cepstral coefficients (MFCCs), pitch tracking and voiced/unvoiced segmenting features(Drugman and Alwan,2011), glottal source parameters(Childers and Lee,1991; Drugman et al.,2012; Alku,1992; Alku et al.,1997,2002), peak slope parameters and maxima dispersion quotients(Kane and Gobl,2013)were extracted automatically using COVAREPDegottex et al. (2014). Finally, in the video domain, FacetiMotions (2017)is used to extract per-frame basic and advanced emotions and facial action units as indicators of facial muscle movement(Ekman,1992; Ekman et al.,1980).

In situations where the same time alignment between different modalities are required, we choose the granularity of the input to be at the level of words. The words are aligned with audio using P2FAYuan and Liberman (2008)to get their exact utterance times. The visual and acoustic modalities are aligned to words using these utterance times.

We use a LSTM model implemented in 3 different ways (one for each different grouping of the modalities).
First in the unimodal domain, we run sentiment regression based solely on one modality, second in the bimodal domain we change the input to the concatenation of any pair of modality, and finally in the trimodal domain we concatenate all three modalities.
This baseline not only serves to act as a benchmark for comparing our results but also acts as a starting point for our code development. As such, any improvements in our metrics are strictly as a result of the representations that we have learned and not structural changes in our model.

Throughout our experimentation, we apply the algorithms in Section4with several intuitive variations of how to translate modalities.
Below are all approaches that we try to
maximize our chances of learning a strong representation.

For bimodal, we translate one modality into another one. For example, A→→\rightarrowV stands for translating from Audio to Video, and take the embedding state, which we refer to asembed(A+V), to predict sentiment. Here we employ the Seq2Seq Modality Translation Model mentioned in Algorithm1.

For trimodal, there are a lot more variations as follows. First, since we have 3 different modality and Seq2Seq is only capable of translating one modality to another, we use the Hierarchical Seq2Seq Modality Translation Model which is mentioned in Algorithm2, e.g. we translate fromTtoAto have the joint representationembed(T+A), and then continue the translation fromembed(T+A) to the rest modality which isV, which in turn yields the joint representationembed(T+A+V) to make sentiment prediction.

Second, we reuse the previous Seq2Seq Modality Translation Model to translate a concatenation of 2 modality to the rest, e.g.concat(T+V) to A, and vice versa, e.g. translating from A back toconcat(T+V).

Finally, we still use the Seq2Seq Modality Translation Model to translate from a concatenation of 2 modality to another concatenation of other 2. With this setting, at least one modality is repeated, and base on many previous works and our experience, we tend to favor text modality (T) over the other two and make it repeated.

We see that with the baseline model, as shown Table1, the text modality is by far the most discriminative when it comes to detecting emotion. This implies that users rely heavily on their word choice and language to convey meaning and emotion. While this may be true, we know that other works such as(Zadeh et al.,2018a; Poria et al.,2017)have achieved higher scores by combining all these different modalities. This implies that with some careful thinking and pointed model construction, we should be able to improve upon our baseline unimodal results through the integration of additional modalities into our model.

The results of our different baseline multimodal approaches is shown in Table2for bimodal and Table3for trimodal. We see that of the multimodal baselines the model which combines the 3 modalities of text, speech, and video performed the best. The baseline model which combined text and audio arrived in second place followed closely by the combined text and video model. The model which combines video and audio arrived in last place by a significant margin. This corroborates our results from our unimodal baselines which implied that the text modality is the most discriminative modality in this dataset.

On the whole we can see that when all three modalities are working in concert we get the best result in a multimodal context, however, it is worth noting that we were not able to match out unimodal baseline with our multimodal models. This implies that there is still more to be drawn from our data when constructing our model and there is generally more work to be done. We believe that incorporating a stronger more robust representation of our data will be beneficial to our later attempts at classification. Though we view this to be out of scope of this work as the focus of this work is on learning informative representations.

The common trend that we see among all of those baseline models is the consistent failure to identify extreme cases of either positive or negative emotions. We believe that this phenomenon is due to two possibilities. First we see that there are very few cases of highly positive (+33+3) and highly negative (−33-3) examples in the training data. As a result the models that are trained are highly biased towards not selecting+33+3or−33-3ratings. Secondly, our baseline models are performing categorical classification as opposed to regression or ordinal classification. We plan to solve by training the model to perform this type of prediction as a regression task as opposed to a categorical classification task.

Our bimodal models require the exploration of two modalities, one for the encoding step and another for the decoding step. We explored several different different encoder/decoder frameworks for these models. The first model that we explored were representations generated from encoding exactly one modality and then decoding exactly one different modality. The results of this approach are included below in Table2. Here we can see that the Seq2Seq Modality Translation Model outperforms the baseline method in terms of F1 consistently and outperforms in terms of precision and recall in several cases, but not all.

We try all variations mentioned in Section5.3and
the full breakdown of these results can be found in the Table3.
According to that, while the Hierarchical Seq2Seq Modality Translation Model is a natural extension to the normal Seq2Seq Modality Translation model, it does not perform well on the CMU-MOSI dataset.
Otherwise, using the normal non-hierarchical model with concatenation variations does improve the performance, and particularly beats the baseline (for only F1 score) on the model which translates fromconcat(T,V) toconcat(T+A) for the 7-class case.
As mentioned in Section5.3, we favor the text (T) modality and make it repeated in this setting because it typically contributes more significantly to sentiment prediction. Indeed, we have tried to repeat video or audio modality but the result shrinks dramatically.

One possible reason for this behavior is the scarcity of training data. Given that at every phase of Seq2Seq translation, we only have 1289 train samples, 230 validation and 269 test samples, Seq2Seq, which typically requires more data for training a good model, does not work efficiently. This affects even more in the hierarchical Seq2Seq cases where we train two phases of Seq2Seq. We project the performance will improve if we work on other dataset which is bigger, or if we pretrain our model on other dataset first before applying it to MOSI.

The language modality is the most discriminative as well as the most important towards learning multimodal representations. While we outperform the baseline multimodal approach we were unable to outperform the baseline unimodal text approach. Clearly from these results we know that that the text modality is the most discriminative of all of these modalities. However, it appears that these models which we have described are not able to truly separate the importance of the text modality. The fact that we are merging these modalities into a shared representation space is likely decreasing the resolution of the text domain and thus decreasing the modeling power of the domain. This is why we believe that the top performing multimodal model is one that incorporates the text domain so much (see Tables2and3).

It is worth noting that some of the learned representations were quite poor when it came to their use in prediction. For example, representations that were learned using only audio and video generally performed poorly. This is to be expected given the already known information that these modalities are not as discriminative as the language modality. At the same time, some of the worse performing representations were learned in the methodology of learning a representation based on an existing embedding. We believe this to be due to the representation losing the resolution of the original two domains from which the original source embedding was learned and instead being focused on learning the best representation to predict the final modality.

This research opens up a promising direction in joint unsupervised learning of multimodal representations and supervised learning of multimodal temporal data. We propose the following extensions that could improve performance:

Firstly, using an Variational Autoencoder (VAE)(Kingma and Welling,2013)in conjunction with LSTM Encoder/Decoder model (as in the case of VAE Seq2Seq model) would be an interesting avenue to explore. This is because VAEs have been shown to learn better representations as compared to vanilla autoencoders(Kingma and Welling,2013; Pu et al.,2016).

Secondly, since our method for multimodal representation learning is unsupervised, we could take advantage of larger external datasets to pre-train the multimodal representations before fine-tuning further with CMU-MOSI. We believe this will boost performance because we have limited data in CMU-MOSI for training (CMU-MOSI has 2199 training segments). Some datasets that come to mind include the Persuasion Opinion Multimodal (POM) datasetPark et al. (2014)with 1000 total videos (longer than segments) and the IEMOCAP dataset with 10000 total segment. Since these datasets also consist of monologue speaker videos, we expect the learnt multimodal representations to generalize.

Thirdly, our method does not train our combined model end to end: the representations that we use to generated during on training run and the sentiment classification model are trained separately. Exploring an end-to-end version of this model end to end could possibly result in better performance where we could additionally fine tune the learned multimodal representation for sentiment analysis.

To conclude, this paper investigate the problem of multimodal representation learning to leverage the abundance of unlabeled multimedia data available on the internet. We presente two methods for unsupervised learning of joint multimodal representations using multimodal Seq2Seq models: theSeq2Seq Modality Translation Modeland theHierarchical Seq2Seq Modality Translation Model. We found that these intermediate multimodal representations can then be used for multimodal downstream tasks. Our experiments indicate that the multimodal representations learned from our Seq2Seq modality translation method are highly informative and achieves improved performance on multimodal sentiment analysis.

The authors are thankful to the many student peers who commented on and critiqued this work. Specific thanks to Louis-Phillipe Morency and Amir Zadeh for their helpful discussions and thoughtful critiques. We are grateful to our peers who helped us evaluate our methodology, in particular Stephen Tsou and Kshitij Khode. Finally, we also thank the anonymous reviewers for helpful and constructive feedback.

[图片: images\image_3.png]

[图片: images\image_4.png]

