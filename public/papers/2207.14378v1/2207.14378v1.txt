标题：Latent Properties of Lifelong Learning Systems

Creating artificial intelligence (AI) systems capable of demonstrating lifelong learning is a fundamental challenge, and many approaches and metrics have been proposed to analyze algorithmic properties. However, for existing lifelong learning metrics, algorithmic contributions are confounded by task and scenario structure. To mitigate this issue, we introduce an algorithm-agnostic explainable surrogate-modeling approach to estimate latent properties of lifelong learning algorithms. We validate the approach for estimating these properties via experiments on synthetic data. To validate the structure of the surrogate model, we analyze real performance data from a collection of popular lifelong learning approaches and baselines adapted for lifelong classification and lifelong reinforcement learning.

Inspired by the way that humans acquire new skills sequentially and improve over time, lifelong or continual learning  (Chen & Liu (2018); Silver et al. (2013)) describes the goal of enabling AI systems to learn tasks sequentially over time while improving performance on both previous and future tasks. Lifelong learning has received much attention in the AI community, and many algorithms have been proposed for both supervised (Delange et al. (2021)) and reinforcement learning (Khetarpal et al. (2020)). We include additional review of lifelong learning approaches in AppendixA.2. A key challenge, apart from the learning algorithm itself, is the assessment of the learner: how well it works, where it fails, and what factors influence its success or failure. Many existing approaches focus on defining continual learning metrics, such asforgetting,backward transfer, andforward transfer(Díaz-Rodríguez et al. (2018); Powers et al. (2021)); recently a suite of metrics for continual learning has been proposedNew et al. (2022). However, a metric such as forward transfer confounds three factors: the relationship between the tasks to be learned, the particular sequencing of tasks (curriculum), and the capabilities of the learner (Farquhar & Gal (2018)). Use of systematic benchmarks (e.g.,Powers et al. (2021); Lomonaco & Maltoni (2017)) can address part of this concern by fixing the tasks and curricula, but does not address how the learner might do on other (non-benchmark) tasks.

In this work, we present a novel approach we refer to as Continual Learning Analysis via a Model of Performance, orCLAMP, to estimate the separate contributions of task structure and algorithm capabilities from performance data. We refer to these contributions aslatent propertiesby analogy to latent variable modeling, where the goal is to identify a lower-dimensional space that effectively captures the probability distribution of high-dimensional data. In our case, we treat the performance curves (the time series of the learner’s performance over the curriculum) as the high-dimensional data, and consider how to identify underlying properties of the learner and tasks that can best explain the observed performance curves.
Our strategy is to (a) define a generative surrogate performance model of lifelong learning that has a small set of explainable parameters, and (b) estimate these parameters of the surrogate model from a set of performance curves. The explainable parameters specify algorithm properties (transfer efficiency, skill retention, expertise translation) and task properties (the “similarity” between the tasks in the curriculum). Our approach works equally well with classification learners and reinforcement learners, and relies solely on the performance curves (i.e., it does not need access to the implementation of the learner or the original task environment, similar to other post-hoc ML model assessment approaches (Martin et al. (2021))). Additional review of explainable surrogate models literature is included in AppendixA.1.

Here we introduce Continual Learning Analysis via a Model of Performance (CLAMP) and define the latent properties being estimated by the approach. This work represents what we believe is the first explainable surrogate performance model for lifelong learning system analysis. We further believe this is the first model-agnostic attempt to quantitatively characterize both algorithm capabilities and inter-task relationships using only performance data available from multiple continual learning algorithms and multiple tasks.

A lifelong learning system (called the agent) experiencesn𝑛ntaskst1,t2,…,tnsubscript𝑡1subscript𝑡2…subscript𝑡𝑛{t_{1},t_{2},\ldots,t_{n}}from an set𝒯𝒯\mathcal{T}. Formally, task experiences are tuples(x,y)∈𝒳×𝒴𝑥𝑦𝒳𝒴(x,y)\in\mathcal{X}\times\mathcal{Y}for an input set𝒳𝒳\mathcal{X}and output set𝒴𝒴\mathcal{Y}.
We assume task experiences are ordered and known. Agents experience a sequence ofm𝑚mexperience tuples(x1,y1),…,(xm,ym)subscript𝑥1subscript𝑦1…subscript𝑥𝑚subscript𝑦𝑚(x_{1},y_{1}),\ldots,(x_{m},y_{m})drawn from a paired sequence of tasks𝒞=[c1,c2,…,cm]𝒞subscript𝑐1subscript𝑐2…subscript𝑐𝑚\mathcal{C}=[c_{1},c_{2},\ldots,c_{m}], called a curriculum, where eachcj∈𝒯subscript𝑐𝑗𝒯c_{j}\in\mathcal{T}. While single-task learning problems suppose that(xj,yj)∼ℙ𝒳×𝒴similar-tosubscript𝑥𝑗subscript𝑦𝑗subscriptℙ𝒳𝒴(x_{j},y_{j})\sim\mathbb{P}_{\mathcal{X}\times\mathcal{Y}}, sampling with curriculum supposes, by contrast, that(xj,yj)∼ℙ𝒳×𝒴cjsimilar-tosubscript𝑥𝑗subscript𝑦𝑗subscriptsuperscriptℙsubscript𝑐𝑗𝒳𝒴(x_{j},y_{j})\sim\mathbb{P}^{c_{j}}_{\mathcal{X}\times\mathcal{Y}}forcj∈𝒯subscript𝑐𝑗𝒯c_{j}\in\mathcal{T}.111Formally, letS:={(x1,y1),…,(xm,ym)}⊂𝒳×𝒴assign𝑆subscript𝑥1subscript𝑦1…subscript𝑥𝑚subscript𝑦𝑚𝒳𝒴S:=\{(x_{1},y_{1}),\ldots,(x_{m},y_{m})\}\subset\mathcal{X}\times\mathcal{Y}be a labeled data set, andSj={(xi,yi)∈S:ci=cj}superscript𝑆𝑗conditional-setsubscript𝑥𝑖subscript𝑦𝑖𝑆subscript𝑐𝑖subscript𝑐𝑗S^{j}=\{(x_{i},y_{i})\in S:\,c_{i}=c_{j}\}; then we suppose(x,y)∼ℙ𝒳×𝒴cjsimilar-to𝑥𝑦subscriptsuperscriptℙsubscript𝑐𝑗𝒳𝒴(x,y)\sim\mathbb{P}^{c_{j}}_{\mathcal{X}\times\mathcal{Y}}for(x,y)∈Sj𝑥𝑦superscript𝑆𝑗(x,y)\in S^{j}.As noted in AppendixA.3, task incremental learning includes taskcjsubscript𝑐𝑗c_{j}along with labeled data(xj,yj)subscript𝑥𝑗subscript𝑦𝑗(x_{j},y_{j})for the learning algorithm, while domain incremental learning does not.

We consider two categories of properties. Intrinsic-task properties characterize a task to be encountered and are assumed to be invariant to changes in the lifelong learning algorithm encountering that task. Likewise, intrinsic-algorithm properties characterize properties of a given lifelong learning algorithm and are assumed to be invariant to what tasks that algorithm encounters. These strong assumptions enable us to ensure thatCLAMPis explainable.

Thetask transfer matrixA∈ℝn×nAsuperscriptℝ𝑛𝑛\textbf{A}\in\mathbb{R}^{n\times n}(recall that|𝒯|=n𝒯𝑛|\mathcal{T}|=n) with entries𝐀i,j∈[−1,1]subscript𝐀𝑖𝑗11\mathbf{A}_{i,j}\in[-1,1]indicating how gaining experience from taski𝑖iaffects performance on taskj𝑗jindependent of the algorithm selected. We assume a linear model of task transfer.

Thetask difficulty scored𝑑din range[0,∞)0[0,\infty)as a task associated parameter that represents the intrinsic difficulty in increasing performance on a task given experience.

Thetransfer efficiency scoreγ𝛾\gammais an intrinsic property of lifelong learning algorithms in the range[0,∞)0[0,\infty)that indicates how efficiently the algorithm translates experience from one task to experience on another.

Theexperience retention scorehℎhas a property of lifelong learning algorithms that conveys how well the algorithm retains prior experiences. Values are in the range[0,1]01[0,1]with00indicating complete forgetting of prior task knowledge and111indicating complete retention of prior task performance.

Theexpertise translation scoreλ𝜆\lambdato be an algorithm property reflecting the ability of the algorithm to translate performance from one task into experience on another task, for any fixed task transfer matrix. Values are in the range[0,∞)0[0,\infty).

We assume a functional form for experience accumulation. For an algorithma𝑎aand curriculum𝒞𝒞\mathcal{C}, accumulated experience for positioni𝑖iin curriculum𝒞𝒞\mathcal{C}, we define experienceℰja​(cl)subscriptsuperscriptℰ𝑎𝑗subscript𝑐𝑙\mathcal{E}^{a}_{j}(c_{l})of tasktjsubscript𝑡𝑗t_{j}at all pointsl𝑙lin the curriculum as follows. For the base case, we assume for alltj∈𝒯subscript𝑡𝑗𝒯t_{j}\in\mathcal{T}that the initial experience for all tasks is zero.ℰja​(c0)=0subscriptsuperscriptℰ𝑎𝑗subscript𝑐00\mathcal{E}^{a}_{j}(c_{0})=0For curriculum stepsj>0𝑗0j>0we define experience accumulation by the following expression:

where𝐀,γ,h,λ𝐀𝛾ℎ𝜆\mathbf{A},\gamma,h,\lambdaare task and algorithm properties defined in Section2.1. Experience in tasktjsubscript𝑡𝑗t_{j}is mapped to performance𝒫​(cj)𝒫subscript𝑐𝑗\mathcal{P}(c_{j})in tasktjsubscript𝑡𝑗t_{j}through a sigmoid functionS​(x)=(1+e−x)−1𝑆𝑥superscript1superscript𝑒𝑥1S(x)=(1+e^{-x})^{-1}, which is then shifted and scaled so that the inflection passes through zero. If curriculum stepclsubscript𝑐𝑙c_{l}experiences tasktjsubscript𝑡𝑗t_{j}, then its performance𝒫i​(cl)subscript𝒫𝑖subscript𝑐𝑙\mathcal{P}_{i}(c_{l})is given by:

Suppose we have a curriculum𝒞𝒞\mathcal{C}containingm𝑚mencounters ofn𝑛ndifferent tasks, and, for a set of algorithmsa=1,…,p𝑎1…𝑝a=1,\ldots,p, a time-series of performance valuesPa∈ℝn×msuperscript𝑃𝑎superscriptℝ𝑛𝑚P^{a}\in\mathbb{R}^{n\times m}, where the entriesPj​lasubscriptsuperscript𝑃𝑎𝑗𝑙P^{a}_{jl}are the performance of thej𝑗jth task at thel𝑙lth curriculum entry. We desire to estimate the task transfer matrix𝐀𝐀\mathbf{A}and task difficulty scored𝑑d, as well as, for each algorithma𝑎a, the transfer efficiencyγasubscript𝛾𝑎\gamma_{a}, experience retentionhasubscriptℎ𝑎h_{a}, and expertise translationλasubscript𝜆𝑎\lambda_{a}. We can group these parameters together asΘ={𝐀,d}∪⋃a{γa,ha,λa}Θ𝐀𝑑subscript𝑎subscript𝛾𝑎subscriptℎ𝑎subscript𝜆𝑎\Theta=\{\mathbf{A},d\}\cup\bigcup_{a}\{\gamma_{a},h_{a},\lambda_{a}\}. ThenCLAMPuses𝐀,d,γa,ha𝐀𝑑subscript𝛾𝑎subscriptℎ𝑎\mathbf{A},d,\gamma_{a},h_{a}, andλasubscript𝜆𝑎\lambda_{a}to predict a time-series of performance valuesP^a∈ℝn×msuperscript^𝑃𝑎superscriptℝ𝑛𝑚\hat{P}^{a}\in\mathbb{R}^{n\times m}, whereP^j​lasubscriptsuperscript^𝑃𝑎𝑗𝑙\hat{P}^{a}_{jl}is the predicted performance of thej𝑗jth task at thel𝑙lth curriculum entry. Our parametersΘΘ\Thetacan be estimated by solving the following minimization problem:

This minimization problem is differentiable in its parameters and has linear constraints, so it may be solved with techniques like projected subgradient descent. In practice, we implement our approach in PyTorchPaszke et al. (2019)and use 1,000 steps of gradient descent with an Adam optimizerKingma & Ba (2014)with default parameters and learning rate. After each step, the parameters are projected back onto their feasible set.

We designed experiments to (i) validate the approach used to estimate latent properties of lifelong learning, (ii) validate the functional form of the surrogate performance model with data coming from multiple baseline algorithms and multiple baseline datasets; including MNIST, CIFAR100 and Atari. Due to space constraints, several sections are included in the appendices including: quantitative validation ofCLAMPbased on synthetic lifelong learning performance data in AppendixB, analysis of domain adaptation with MNIST in AppendixC.1, and task adaptation with CIFAR100 in AppendixC.2.

To illustrate the adaptability ofCLAMP, we analysed the performance of various lifelong learning reinforcement learning approaches from an experiment consisting of a set of Atari tasks from AGI-Labs222https://github.com/AGI-Labs/continual_rl/blob/develop/docs/ATARI_RESULTS.md. In particular, the algorithms we consider include Continual Learning with Experience And Replay (CLEAR,Rolnick et al. (2019)), online EWC, Progress & Compress (P&C,Schwarz et al. (2018)), and Importance Weighted Actor-Learner Architecture (IMPALA,Espeholt et al. (2018)).

[图片: images\image_1.png]
图片说明: Figure 1:Performance of several common lifelong reinforcement learning approaches and baselines applied to a curriculum of 6 Atari tasks. Each phase of the curriculum was trained for 5 million steps. Green vertical bars indicate the regions where the current task is being trained. Dashed lines indicate the predicted performance curves estimates byCLAMP.

Before runningCLAMP, we downsampled the performance data for each continual learning algorithm to capture task performance at task transition boundaries. We then normalized reward results across tasks using the procedure detailed inNew et al. (2022). The resulting data is shown in Figure1. We fit aCLAMPmodel to the data as described in Section2.3resulting in the estimated algorithm performance data shown in1(0.0070.0070.007MSE).

The property estimates fromCLAMPshown in Tables1a,1b, and7qualitatively match well with our observations about the data shown in Figure1. Of the approaches considered in this experiment, CLEAR was estimated to have the best transfer efficiency0.120.120.12and experience retention0.90.90.9due to the experience replay. The data in Figure1show that Clear frequently had the highest task performance and the highest task performance for tasks not actively being trained. We observed that EWC and online EWC had relatively lower task performance, but they retained task performance while other tasks were training. The CLAMP analysis explains the relatively poor performance of EWC and online EWC as being a result of relatively low transfer efficiency. This makes sense as the mechanism of EWC and online EWC are based on regularization which adds additional loss terms that are simultaneously being optimized to slow parameter changes to parts of the network identified as useful for other tasks. The algorithm parameter estimates from from CLAMP qualitatively match our expectations from the given performance data and a mechanistic understanding of the lifelong learning approaches.

The analysis of synthetic data in AppendixBdemonstrates that the optimization approach inCLAMPwas able to recover underlying parameters of the model from synthetic lifelong learning performance data with low error. From Section3and AppendixC.1andC.2, we found thatCLAMPhad low mean squared error to real performance data from several benchmarks. Qualitative assessment of the latent property estimates from analysis of the Atari experiment indicated broad agreement with expectations given the performance data and the underlying mechanisms of the lifelong learning approaches.

Existing metrics for evaluating lifelong learning conflate task structure with algorithm performance. We have introducedCLAMP- Continual Learning Analysis via a Model of Performance - as the first attempt to estimate the separate contributions of task structure from lifelong learning algorithm performance.CLAMPis model and task agnostic, and it can estimate several important properties of lifelong learning systems, including how well they learn from new experiences and how well they retain prior experience. We have also demonstrated the applicability of this approach to continual learning in both the classification and reinforcement learning settings.

Explainable surrogate models are a common approach in the explainable AI (XAI,Angelov et al. (2021); Adadi & Berrada (2018); Arrieta et al. (2020); Gunning & Aha (2019)) literature. These are a form of explanation by simplification (Tritscher et al. (2020)). By fitting a simplified model to a more complex model, by comparison, it may be possible to better interpret salient factors for prediction. For example, a linear model or decision tree might be fit to the original model to reduce the model complexity compared to the original model.Doran et al. (2017)refer to this category of models as interpretable in that it is possible to mathematically analyze the algorithmic properties.

Another characteristic of explainable models are the assumptions that they make about knowledge of the original model structure. Some approaches assume knowledge of the original model composition (white box assumption) while other approaches only assume that access to inputs and output are available from the full model (black box assumption) (Adadi & Berrada (2018)). Approaches that make black-box assumptions are also referred to as model-agnostic, in the sense that they can be used without detailed knowledge of the original models structure. For example, Local Interpretable Model-agnostic Explanations (Ribeiro et al. (2016)) (LIME) is a recent example of a model agnostic approach that creates locally-optimized explanations by training a surrogate model.

Furthermore, there is a rich history of latent parameter inference in the Bayesian (Lindley (1972); von Toussaint (2011)) and generative model (Zou & Adams (2012)) literature. Bayesian belief networks (Holzinger (2018)) have been explored for use in XAI.

While lifelong learning attempts to address various shortcomings of modern AI, numerous lifelong learning algorithms pay special attention tocatastrophic forgettingFrench (1999); McCloskey & Cohen (1989)). Catastrophic forgetting is the behavior of an algorithm or model where, after training the model on one task, training on a new task adversely effects model’s performance on the first task; often to the point that the model performs worse than a model behaving randomly. Nuance between how to calculate relevance and minimization of adverse effects differentiates algorithms such aselastic weight consolidation(EWC,Kirkpatrick et al. (2017)),continual learning through synaptic intelligence(SI,Zenke et al. (2017)), andmemory aware synapses(MAS,Aljundi et al. (2018)). SI, for example, holds weights relatively fixed according to the sensitivity of loss function with respect to them, whereas MAS considers sensitivity of weights in the predictor itself. EWC, on the other hand, uses Fisher information for determining how to update weights. Gradient episodic memory (GEM,Lopez-Paz & Ranzato (2017)) like SI stores a subset of previous task data and seeks to minimize cost function on current labeled data point while constraining that loss on the episodic memory does not increase.

We make use of two different continual learning experimental categories in this study which we briefly describe below. They are referred to as domain incremental learning and task incremental learning.

Domain incremental learningas defined byHsu et al. (2018)andvan de Ven & Tolias (2018)implies that task identifiers are not given to the lifelong learning algorithm. The algorithm must infer from the given data which task is being performed.

Task incremental learningis defined byHsu et al. (2018)as prediction where the task label is given to the algorithm. Lifelong learning approaches for task adaptation often have an output prediction head for each task. During inference, only the predictions from the current task head are considered.

We aimed to verify that our surrogate model functional form yields problems that are identifiable – i.e., that a performance curve can be mapped back to a unique set of parameters. We designed an experiment using synthetic data where we could be certain of the underlying parameters governing the model. We sampled ground truth parameters (𝐀,d,γ,h,𝐀𝑑𝛾ℎ\mathbf{A},d,\gamma,h,andλ𝜆\lambda) uniformly from[−1,1]11[-1,1]for𝐀𝐀\mathbf{A}and[0,1]01[0,1]for the other parameters, and we generated a random curriculum of length999from a set of five tasks. Then we usedEja​(cl)subscriptsuperscript𝐸𝑎𝑗subscript𝑐𝑙{E}^{a}_{j}(c_{l})and𝒫j​(cl)subscript𝒫𝑗subscript𝑐𝑙\mathcal{P}_{j}(c_{l})to generate synthetic lifelong learning performance data for 3 algorithms over a curriculum composed of555tasks.

[图片: images\image_2.png]
图片说明: Figure 2:Synthetic performance data and the fit of the surrogate model to the data. Green bars on thex𝑥x-axis for a given task indicate periods in the curriculum where that task was trained. Our curriculum was randomly generated, so Task 5 was never trained on. In all cases, the estimated performance curves (dashed lines) overlap very closely to the synthetic performance data (solid lines).

To validate the model fitting approach to recover underlying model parameters, we fit a randomly initialized surrogate performance model to the synthetic performance data for 1000 epochs using an Adam optimizer (Kingma & Ba (2014)) with mean squared error as the loss function. The estimated curves match the synthetic data with low error as expected.
We then computed the mean squared error between the ground truth parameters and the parameters in the fitted model. The results are shown in Table2. We can conclude that the approach used to fit the explainable performance surrogate model can be used to recover accurate estimates of the true underlying parameter values with small error.

Here we evaluate whether the strong assumptions we used to develop our model of learning can describe real lifelong learning performance data for classification algorithms. We designed experiments including the MNIST and CIFAR100 datasets to quantitatively evaluate how well our model fits real lifelong performance data. To generate performance data, we followed the experimental protocol described byHsu et al. (2018).

Performance curves from four classification algorithms were considered. The first (NormalNN) makes use of a neural network where weights are transferred from one task to the next. A second (L2) includedℓ2superscriptℓ2\ell^{2}regularization in its loss function to minimize the change in weights from those that were previously learned. Naive rehearsal (Naive_Rehearsal_[k]) was a third algorithm that made use of an experience replay memory to store and retrain on a subset of sizek𝑘kof previously experienced data at random. Each training batch was composed of equal parts from the current task and the experience replay memory. The number of stored experiences over all tasks is a parameter of the approach.

We also included curves from four state-of-the-art lifelong learning algorithms: elastic weight consolidation (EWC,Kirkpatrick et al. (2017)), online elastic weight consolidation (EWC_online,Schwarz et al. (2018)), synaptic intelligence (SI,Zenke et al. (2017)), memory-aware synapses (MAS,Aljundi et al. (2018)), and gradient episodic memory (GEM,Lopez-Paz & Ranzato (2017)).

For MNIST, we considered the problem of domain-incremental learning. We followed the experimental protocol described byHsu et al. (2018), which we briefly describe below.

[图片: images\image_3.png]
图片说明: Figure 3:Continual learning performance for incremental domain learning on Split-MNIST along with performance curves estimated withCLAMP. Naive rehearsal was considered with experience storage limits of110011001100and440044004400.

We used the dataset splitting procedure to create multiple binary classification tasks (e.g., distinguish between images of 0 and images of 1) from the MNIST dataset with 60k training images (∼similar-to\sim6,000 per digit) and 10,000 test images. The 32x32 greyscale images were normalized to have a mean of zero and variance of one.

The results of our MNIST analysis are given in Fig.3(estimated and true performance curves) and Tables3(estimated properties) and4(estimated transfer scores). Averaged over each classification algorithma𝑎a, the surrogate models achieved a squared errormeana​‖P^a−Pa‖F2subscriptmean𝑎superscriptsubscriptnormsuperscript^𝑃𝑎superscript𝑃𝑎𝐹2\text{mean}_{a}||\hat{P}^{a}-P^{a}||_{F}^{2}of0.0050.0050.005. The ability of the surrogate model to fit the performance data with low error is a partial validation of the formulation of the surrogate performance model. Table4is the estimated task transfer matrix. The estimated task transfer diagonal was strongly positive. The highest off-diagonal transfer estimate was that training with the6,7676,7task is beneficial for the task8,9898,9task. We believe this result follows intuition because of the character similarity between666and999.

The estimated algorithm properties are shown in Table3. At first glance it might seem surprising that a normal neural network (Normal NN) had one of the highest estimated transfer efficienciesγ𝛾\gammafor the MNIST set of tasks, given that we know that normal neural networks are ill-suited for continual learning. One intuition for the result is that normal neural networks are able to transfer well because their weight updates are unconstrained, while the regularization in other methods prevents such rapid transfer. However, because the normal neural network updates are unconstrained, performance on past tasks is not maintained, while the regularized methods are able to maintain performance on various tasks throughout the curricula.

For analysis on CIFAR100, we used results fromtask-incremental learning. We used the protocol byHsu et al. (2018)for preparing the CIFAR100 dataset and running the continual learning approaches.

Like the MNIST-task set, we used the dataset splitting procedure to create 5 binary classification tasks based on the CIFAR100 dataset. The 100 CIFAR classes were subdivided at random into 10 classes. Each binary classification task consisted of two of the 10 distinct classes. Data preprocessing resulted in 32x32 images that were normalized with a mean of zero and variance of one. To ensure a fair comparison among lifelong learning approaches, all lifelong learning approaches used the same WideResNet (Zagoruyko & Komodakis (2016)) architecture.

The solid lines in Figure4show the performance (i.e. 1-error) for each of the tasks across the curriculum. The highlighted green regions in the figure illustrate when the task is actively being trained. As expected, we broadly observe that performance of a task increases sharply when actively being trained.

The fit of the surrogate performance model is shown as dashed lines in Figure4, and illustrates broad general agreement between predicted performance and the underlying data. We observed a MSE of0.010.010.01between the predicted and observed performance curves.

[图片: images\image_4.png]
图片说明: Figure 4:Continual learning performance for incremental task learning on Split-CIFAR100 tasks of several lifelong learning algorithms along with the performance estimated withCLAMP. Naive rehearsal was considered with experience storage limits of140014001400and560056005600.

In Table8, we compared the estimated transfer efficiency of multiple algorithms between the MNIST domain and CIFAR-100 domain. Each column was colored from green to red relative to the values in the column. Between the MNIST and CIFAR100 experiment there is broad agreement about the rank ordering of algorithms with respect to transfer efficiency. This indicates that latent property analysis might be useful to estimate the relative performance of a lifelong learning algorithm from one dataset to another.

Our contributions include: (i) the introduction of the first explainable algorithm-agnostic surrogate performance model of lifelong learning, (ii) quantitative validation of the optimization procedure used to estimate the metrics, (iii) qualitative validation of the lifelong learning surrogate performance model using data produced by several popular lifelong learning approaches on benchmark datasets including CIFAR100, MNIST, and Atari.

Our approach to task transfer score recovery is dataset and algorithm-agnostic, and therefore generally applicable to all domains of lifelong learning. The algorithm latent properties seem to be predictive of how the algorithm may perform on other datasets.

A limitation ofCLAMPis that it makes a number of assumption about lifelong learning with the goal of explaining in part the performance of lifelong learning algorithms over diverse tasks. For example, we assume linear experience transfer between tasks. In future work, it would be interesting to explore relaxing the linear task transfer assumptions and consider non-linear transfer.

[图片: images\image_5.png]

[图片: images\image_6.png]

