æ ‡é¢˜ï¼šLatent Properties of Lifelong Learning Systems

Creating artificial intelligence (AI) systems capable of demonstrating lifelong learning is a fundamental challenge, and many approaches and metrics have been proposed to analyze algorithmic properties. However, for existing lifelong learning metrics, algorithmic contributions are confounded by task and scenario structure. To mitigate this issue, we introduce an algorithm-agnostic explainable surrogate-modeling approach to estimate latent properties of lifelong learning algorithms. We validate the approach for estimating these properties via experiments on synthetic data. To validate the structure of the surrogate model, we analyze real performance data from a collection of popular lifelong learning approaches and baselines adapted for lifelong classification and lifelong reinforcement learning.

Inspired by the way that humans acquire new skills sequentially and improve over time, lifelong or continual learning Â (Chen & Liu (2018); Silver etÂ al. (2013)) describes the goal of enabling AI systems to learn tasks sequentially over time while improving performance on both previous and future tasks. Lifelong learning has received much attention in the AI community, and many algorithms have been proposed for both supervisedÂ (Delange etÂ al. (2021)) and reinforcement learningÂ (Khetarpal etÂ al. (2020)). We include additional review of lifelong learning approaches in AppendixA.2. A key challenge, apart from the learning algorithm itself, is the assessment of the learner: how well it works, where it fails, and what factors influence its success or failure. Many existing approaches focus on defining continual learning metrics, such asforgetting,backward transfer, andforward transfer(DÃ­az-RodrÃ­guez etÂ al. (2018); Powers etÂ al. (2021)); recently a suite of metrics for continual learning has been proposedNew etÂ al. (2022). However, a metric such as forward transfer confounds three factors: the relationship between the tasks to be learned, the particular sequencing of tasks (curriculum), and the capabilities of the learnerÂ (Farquhar & Gal (2018)). Use of systematic benchmarks (e.g.,Powers etÂ al. (2021); Lomonaco & Maltoni (2017)) can address part of this concern by fixing the tasks and curricula, but does not address how the learner might do on other (non-benchmark) tasks.

In this work, we present a novel approach we refer to as Continual Learning Analysis via a Model of Performance, orCLAMP, to estimate the separate contributions of task structure and algorithm capabilities from performance data. We refer to these contributions aslatent propertiesby analogy to latent variable modeling, where the goal is to identify a lower-dimensional space that effectively captures the probability distribution of high-dimensional data. In our case, we treat the performance curves (the time series of the learnerâ€™s performance over the curriculum) as the high-dimensional data, and consider how to identify underlying properties of the learner and tasks that can best explain the observed performance curves.
Our strategy is to (a) define a generative surrogate performance model of lifelong learning that has a small set of explainable parameters, and (b) estimate these parameters of the surrogate model from a set of performance curves. The explainable parameters specify algorithm properties (transfer efficiency, skill retention, expertise translation) and task properties (the â€œsimilarityâ€ between the tasks in the curriculum). Our approach works equally well with classification learners and reinforcement learners, and relies solely on the performance curves (i.e., it does not need access to the implementation of the learner or the original task environment, similar to other post-hoc ML model assessment approachesÂ (Martin etÂ al. (2021))). Additional review of explainable surrogate models literature is included in AppendixA.1.

Here we introduce Continual Learning Analysis via a Model of Performance (CLAMP) and define the latent properties being estimated by the approach. This work represents what we believe is the first explainable surrogate performance model for lifelong learning system analysis. We further believe this is the first model-agnostic attempt to quantitatively characterize both algorithm capabilities and inter-task relationships using only performance data available from multiple continual learning algorithms and multiple tasks.

A lifelong learning system (called the agent) experiencesnğ‘›ntaskst1,t2,â€¦,tnsubscriptğ‘¡1subscriptğ‘¡2â€¦subscriptğ‘¡ğ‘›{t_{1},t_{2},\ldots,t_{n}}from an setğ’¯ğ’¯\mathcal{T}. Formally, task experiences are tuples(x,y)âˆˆğ’³Ã—ğ’´ğ‘¥ğ‘¦ğ’³ğ’´(x,y)\in\mathcal{X}\times\mathcal{Y}for an input setğ’³ğ’³\mathcal{X}and output setğ’´ğ’´\mathcal{Y}.
We assume task experiences are ordered and known. Agents experience a sequence ofmğ‘šmexperience tuples(x1,y1),â€¦,(xm,ym)subscriptğ‘¥1subscriptğ‘¦1â€¦subscriptğ‘¥ğ‘šsubscriptğ‘¦ğ‘š(x_{1},y_{1}),\ldots,(x_{m},y_{m})drawn from a paired sequence of tasksğ’=[c1,c2,â€¦,cm]ğ’subscriptğ‘1subscriptğ‘2â€¦subscriptğ‘ğ‘š\mathcal{C}=[c_{1},c_{2},\ldots,c_{m}], called a curriculum, where eachcjâˆˆğ’¯subscriptğ‘ğ‘—ğ’¯c_{j}\in\mathcal{T}. While single-task learning problems suppose that(xj,yj)âˆ¼â„™ğ’³Ã—ğ’´similar-tosubscriptğ‘¥ğ‘—subscriptğ‘¦ğ‘—subscriptâ„™ğ’³ğ’´(x_{j},y_{j})\sim\mathbb{P}_{\mathcal{X}\times\mathcal{Y}}, sampling with curriculum supposes, by contrast, that(xj,yj)âˆ¼â„™ğ’³Ã—ğ’´cjsimilar-tosubscriptğ‘¥ğ‘—subscriptğ‘¦ğ‘—subscriptsuperscriptâ„™subscriptğ‘ğ‘—ğ’³ğ’´(x_{j},y_{j})\sim\mathbb{P}^{c_{j}}_{\mathcal{X}\times\mathcal{Y}}forcjâˆˆğ’¯subscriptğ‘ğ‘—ğ’¯c_{j}\in\mathcal{T}.111Formally, letS:={(x1,y1),â€¦,(xm,ym)}âŠ‚ğ’³Ã—ğ’´assignğ‘†subscriptğ‘¥1subscriptğ‘¦1â€¦subscriptğ‘¥ğ‘šsubscriptğ‘¦ğ‘šğ’³ğ’´S:=\{(x_{1},y_{1}),\ldots,(x_{m},y_{m})\}\subset\mathcal{X}\times\mathcal{Y}be a labeled data set, andSj={(xi,yi)âˆˆS:ci=cj}superscriptğ‘†ğ‘—conditional-setsubscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–ğ‘†subscriptğ‘ğ‘–subscriptğ‘ğ‘—S^{j}=\{(x_{i},y_{i})\in S:\,c_{i}=c_{j}\}; then we suppose(x,y)âˆ¼â„™ğ’³Ã—ğ’´cjsimilar-toğ‘¥ğ‘¦subscriptsuperscriptâ„™subscriptğ‘ğ‘—ğ’³ğ’´(x,y)\sim\mathbb{P}^{c_{j}}_{\mathcal{X}\times\mathcal{Y}}for(x,y)âˆˆSjğ‘¥ğ‘¦superscriptğ‘†ğ‘—(x,y)\in S^{j}.As noted in AppendixA.3, task incremental learning includes taskcjsubscriptğ‘ğ‘—c_{j}along with labeled data(xj,yj)subscriptğ‘¥ğ‘—subscriptğ‘¦ğ‘—(x_{j},y_{j})for the learning algorithm, while domain incremental learning does not.

We consider two categories of properties. Intrinsic-task properties characterize a task to be encountered and are assumed to be invariant to changes in the lifelong learning algorithm encountering that task. Likewise, intrinsic-algorithm properties characterize properties of a given lifelong learning algorithm and are assumed to be invariant to what tasks that algorithm encounters. These strong assumptions enable us to ensure thatCLAMPis explainable.

Thetask transfer matrixAâˆˆâ„nÃ—nAsuperscriptâ„ğ‘›ğ‘›\textbf{A}\in\mathbb{R}^{n\times n}(recall that|ğ’¯|=nğ’¯ğ‘›|\mathcal{T}|=n) with entriesğ€i,jâˆˆ[âˆ’1,1]subscriptğ€ğ‘–ğ‘—11\mathbf{A}_{i,j}\in[-1,1]indicating how gaining experience from taskiğ‘–iaffects performance on taskjğ‘—jindependent of the algorithm selected. We assume a linear model of task transfer.

Thetask difficulty scoredğ‘‘din range[0,âˆ)0[0,\infty)as a task associated parameter that represents the intrinsic difficulty in increasing performance on a task given experience.

Thetransfer efficiency scoreÎ³ğ›¾\gammais an intrinsic property of lifelong learning algorithms in the range[0,âˆ)0[0,\infty)that indicates how efficiently the algorithm translates experience from one task to experience on another.

Theexperience retention scorehâ„has a property of lifelong learning algorithms that conveys how well the algorithm retains prior experiences. Values are in the range[0,1]01[0,1]with00indicating complete forgetting of prior task knowledge and111indicating complete retention of prior task performance.

Theexpertise translation scoreÎ»ğœ†\lambdato be an algorithm property reflecting the ability of the algorithm to translate performance from one task into experience on another task, for any fixed task transfer matrix. Values are in the range[0,âˆ)0[0,\infty).

We assume a functional form for experience accumulation. For an algorithmağ‘aand curriculumğ’ğ’\mathcal{C}, accumulated experience for positioniğ‘–iin curriculumğ’ğ’\mathcal{C}, we define experienceâ„°jaâ€‹(cl)subscriptsuperscriptâ„°ğ‘ğ‘—subscriptğ‘ğ‘™\mathcal{E}^{a}_{j}(c_{l})of tasktjsubscriptğ‘¡ğ‘—t_{j}at all pointslğ‘™lin the curriculum as follows. For the base case, we assume for alltjâˆˆğ’¯subscriptğ‘¡ğ‘—ğ’¯t_{j}\in\mathcal{T}that the initial experience for all tasks is zero.â„°jaâ€‹(c0)=0subscriptsuperscriptâ„°ğ‘ğ‘—subscriptğ‘00\mathcal{E}^{a}_{j}(c_{0})=0For curriculum stepsj>0ğ‘—0j>0we define experience accumulation by the following expression:

whereğ€,Î³,h,Î»ğ€ğ›¾â„ğœ†\mathbf{A},\gamma,h,\lambdaare task and algorithm properties defined in Section2.1. Experience in tasktjsubscriptğ‘¡ğ‘—t_{j}is mapped to performanceğ’«â€‹(cj)ğ’«subscriptğ‘ğ‘—\mathcal{P}(c_{j})in tasktjsubscriptğ‘¡ğ‘—t_{j}through a sigmoid functionSâ€‹(x)=(1+eâˆ’x)âˆ’1ğ‘†ğ‘¥superscript1superscriptğ‘’ğ‘¥1S(x)=(1+e^{-x})^{-1}, which is then shifted and scaled so that the inflection passes through zero. If curriculum stepclsubscriptğ‘ğ‘™c_{l}experiences tasktjsubscriptğ‘¡ğ‘—t_{j}, then its performanceğ’«iâ€‹(cl)subscriptğ’«ğ‘–subscriptğ‘ğ‘™\mathcal{P}_{i}(c_{l})is given by:

Suppose we have a curriculumğ’ğ’\mathcal{C}containingmğ‘šmencounters ofnğ‘›ndifferent tasks, and, for a set of algorithmsa=1,â€¦,pğ‘1â€¦ğ‘a=1,\ldots,p, a time-series of performance valuesPaâˆˆâ„nÃ—msuperscriptğ‘ƒğ‘superscriptâ„ğ‘›ğ‘šP^{a}\in\mathbb{R}^{n\times m}, where the entriesPjâ€‹lasubscriptsuperscriptğ‘ƒğ‘ğ‘—ğ‘™P^{a}_{jl}are the performance of thejğ‘—jth task at thelğ‘™lth curriculum entry. We desire to estimate the task transfer matrixğ€ğ€\mathbf{A}and task difficulty scoredğ‘‘d, as well as, for each algorithmağ‘a, the transfer efficiencyÎ³asubscriptğ›¾ğ‘\gamma_{a}, experience retentionhasubscriptâ„ğ‘h_{a}, and expertise translationÎ»asubscriptğœ†ğ‘\lambda_{a}. We can group these parameters together asÎ˜={ğ€,d}âˆªâ‹ƒa{Î³a,ha,Î»a}Î˜ğ€ğ‘‘subscriptğ‘subscriptğ›¾ğ‘subscriptâ„ğ‘subscriptğœ†ğ‘\Theta=\{\mathbf{A},d\}\cup\bigcup_{a}\{\gamma_{a},h_{a},\lambda_{a}\}. ThenCLAMPusesğ€,d,Î³a,hağ€ğ‘‘subscriptğ›¾ğ‘subscriptâ„ğ‘\mathbf{A},d,\gamma_{a},h_{a}, andÎ»asubscriptğœ†ğ‘\lambda_{a}to predict a time-series of performance valuesP^aâˆˆâ„nÃ—msuperscript^ğ‘ƒğ‘superscriptâ„ğ‘›ğ‘š\hat{P}^{a}\in\mathbb{R}^{n\times m}, whereP^jâ€‹lasubscriptsuperscript^ğ‘ƒğ‘ğ‘—ğ‘™\hat{P}^{a}_{jl}is the predicted performance of thejğ‘—jth task at thelğ‘™lth curriculum entry. Our parametersÎ˜Î˜\Thetacan be estimated by solving the following minimization problem:

This minimization problem is differentiable in its parameters and has linear constraints, so it may be solved with techniques like projected subgradient descent. In practice, we implement our approach in PyTorchPaszke etÂ al. (2019)and use 1,000 steps of gradient descent with an Adam optimizerKingma & Ba (2014)with default parameters and learning rate. After each step, the parameters are projected back onto their feasible set.

We designed experiments to (i) validate the approach used to estimate latent properties of lifelong learning, (ii) validate the functional form of the surrogate performance model with data coming from multiple baseline algorithms and multiple baseline datasets; including MNIST, CIFAR100 and Atari. Due to space constraints, several sections are included in the appendices including: quantitative validation ofCLAMPbased on synthetic lifelong learning performance data in AppendixB, analysis of domain adaptation with MNIST in AppendixC.1, and task adaptation with CIFAR100 in AppendixC.2.

To illustrate the adaptability ofCLAMP, we analysed the performance of various lifelong learning reinforcement learning approaches from an experiment consisting of a set of Atari tasks from AGI-Labs222https://github.com/AGI-Labs/continual_rl/blob/develop/docs/ATARI_RESULTS.md. In particular, the algorithms we consider include Continual Learning with Experience And Replay (CLEAR,Rolnick etÂ al. (2019)), online EWC, Progress & Compress (P&C,Schwarz etÂ al. (2018)), and Importance Weighted Actor-Learner Architecture (IMPALA,Espeholt etÂ al. (2018)).

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1:Performance of several common lifelong reinforcement learning approaches and baselines applied to a curriculum of 6 Atari tasks. Each phase of the curriculum was trained for 5 million steps. Green vertical bars indicate the regions where the current task is being trained. Dashed lines indicate the predicted performance curves estimates byCLAMP.

Before runningCLAMP, we downsampled the performance data for each continual learning algorithm to capture task performance at task transition boundaries. We then normalized reward results across tasks using the procedure detailed inNew etÂ al. (2022). The resulting data is shown in Figure1. We fit aCLAMPmodel to the data as described in Section2.3resulting in the estimated algorithm performance data shown in1(0.0070.0070.007MSE).

The property estimates fromCLAMPshown in Tables1a,1b, and7qualitatively match well with our observations about the data shown in Figure1. Of the approaches considered in this experiment, CLEAR was estimated to have the best transfer efficiency0.120.120.12and experience retention0.90.90.9due to the experience replay. The data in Figure1show that Clear frequently had the highest task performance and the highest task performance for tasks not actively being trained. We observed that EWC and online EWC had relatively lower task performance, but they retained task performance while other tasks were training. The CLAMP analysis explains the relatively poor performance of EWC and online EWC as being a result of relatively low transfer efficiency. This makes sense as the mechanism of EWC and online EWC are based on regularization which adds additional loss terms that are simultaneously being optimized to slow parameter changes to parts of the network identified as useful for other tasks. The algorithm parameter estimates from from CLAMP qualitatively match our expectations from the given performance data and a mechanistic understanding of the lifelong learning approaches.

The analysis of synthetic data in AppendixBdemonstrates that the optimization approach inCLAMPwas able to recover underlying parameters of the model from synthetic lifelong learning performance data with low error. From Section3and AppendixC.1andC.2, we found thatCLAMPhad low mean squared error to real performance data from several benchmarks. Qualitative assessment of the latent property estimates from analysis of the Atari experiment indicated broad agreement with expectations given the performance data and the underlying mechanisms of the lifelong learning approaches.

Existing metrics for evaluating lifelong learning conflate task structure with algorithm performance. We have introducedCLAMP- Continual Learning Analysis via a Model of Performance - as the first attempt to estimate the separate contributions of task structure from lifelong learning algorithm performance.CLAMPis model and task agnostic, and it can estimate several important properties of lifelong learning systems, including how well they learn from new experiences and how well they retain prior experience. We have also demonstrated the applicability of this approach to continual learning in both the classification and reinforcement learning settings.

Explainable surrogate models are a common approach in the explainable AI (XAI,Angelov etÂ al. (2021); Adadi & Berrada (2018); Arrieta etÂ al. (2020); Gunning & Aha (2019)) literature. These are a form of explanation by simplificationÂ (Tritscher etÂ al. (2020)). By fitting a simplified model to a more complex model, by comparison, it may be possible to better interpret salient factors for prediction. For example, a linear model or decision tree might be fit to the original model to reduce the model complexity compared to the original model.Doran etÂ al. (2017)refer to this category of models as interpretable in that it is possible to mathematically analyze the algorithmic properties.

Another characteristic of explainable models are the assumptions that they make about knowledge of the original model structure. Some approaches assume knowledge of the original model composition (white box assumption) while other approaches only assume that access to inputs and output are available from the full model (black box assumption)Â (Adadi & Berrada (2018)). Approaches that make black-box assumptions are also referred to as model-agnostic, in the sense that they can be used without detailed knowledge of the original models structure. For example, Local Interpretable Model-agnostic ExplanationsÂ (Ribeiro etÂ al. (2016)) (LIME) is a recent example of a model agnostic approach that creates locally-optimized explanations by training a surrogate model.

Furthermore, there is a rich history of latent parameter inference in the BayesianÂ (Lindley (1972); von Toussaint (2011)) and generative modelÂ (Zou & Adams (2012)) literature. Bayesian belief networksÂ (Holzinger (2018)) have been explored for use in XAI.

While lifelong learning attempts to address various shortcomings of modern AI, numerous lifelong learning algorithms pay special attention tocatastrophic forgettingFrench (1999); McCloskey & Cohen (1989)). Catastrophic forgetting is the behavior of an algorithm or model where, after training the model on one task, training on a new task adversely effects modelâ€™s performance on the first task; often to the point that the model performs worse than a model behaving randomly. Nuance between how to calculate relevance and minimization of adverse effects differentiates algorithms such aselastic weight consolidation(EWC,Kirkpatrick etÂ al. (2017)),continual learning through synaptic intelligence(SI,Zenke etÂ al. (2017)), andmemory aware synapses(MAS,Aljundi etÂ al. (2018)). SI, for example, holds weights relatively fixed according to the sensitivity of loss function with respect to them, whereas MAS considers sensitivity of weights in the predictor itself. EWC, on the other hand, uses Fisher information for determining how to update weights. Gradient episodic memory (GEM,Lopez-Paz & Ranzato (2017)) like SI stores a subset of previous task data and seeks to minimize cost function on current labeled data point while constraining that loss on the episodic memory does not increase.

We make use of two different continual learning experimental categories in this study which we briefly describe below. They are referred to as domain incremental learning and task incremental learning.

Domain incremental learningas defined byHsu etÂ al. (2018)andvanÂ de Ven & Tolias (2018)implies that task identifiers are not given to the lifelong learning algorithm. The algorithm must infer from the given data which task is being performed.

Task incremental learningis defined byHsu etÂ al. (2018)as prediction where the task label is given to the algorithm. Lifelong learning approaches for task adaptation often have an output prediction head for each task. During inference, only the predictions from the current task head are considered.

We aimed to verify that our surrogate model functional form yields problems that are identifiable â€“ i.e., that a performance curve can be mapped back to a unique set of parameters. We designed an experiment using synthetic data where we could be certain of the underlying parameters governing the model. We sampled ground truth parameters (ğ€,d,Î³,h,ğ€ğ‘‘ğ›¾â„\mathbf{A},d,\gamma,h,andÎ»ğœ†\lambda) uniformly from[âˆ’1,1]11[-1,1]forğ€ğ€\mathbf{A}and[0,1]01[0,1]for the other parameters, and we generated a random curriculum of length999from a set of five tasks. Then we usedEjaâ€‹(cl)subscriptsuperscriptğ¸ğ‘ğ‘—subscriptğ‘ğ‘™{E}^{a}_{j}(c_{l})andğ’«jâ€‹(cl)subscriptğ’«ğ‘—subscriptğ‘ğ‘™\mathcal{P}_{j}(c_{l})to generate synthetic lifelong learning performance data for 3 algorithms over a curriculum composed of555tasks.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2:Synthetic performance data and the fit of the surrogate model to the data. Green bars on thexğ‘¥x-axis for a given task indicate periods in the curriculum where that task was trained. Our curriculum was randomly generated, so Task 5 was never trained on. In all cases, the estimated performance curves (dashed lines) overlap very closely to the synthetic performance data (solid lines).

To validate the model fitting approach to recover underlying model parameters, we fit a randomly initialized surrogate performance model to the synthetic performance data for 1000 epochs using an Adam optimizerÂ (Kingma & Ba (2014)) with mean squared error as the loss function. The estimated curves match the synthetic data with low error as expected.
We then computed the mean squared error between the ground truth parameters and the parameters in the fitted model. The results are shown in Table2. We can conclude that the approach used to fit the explainable performance surrogate model can be used to recover accurate estimates of the true underlying parameter values with small error.

Here we evaluate whether the strong assumptions we used to develop our model of learning can describe real lifelong learning performance data for classification algorithms. We designed experiments including the MNIST and CIFAR100 datasets to quantitatively evaluate how well our model fits real lifelong performance data. To generate performance data, we followed the experimental protocol described byHsu etÂ al. (2018).

Performance curves from four classification algorithms were considered. The first (NormalNN) makes use of a neural network where weights are transferred from one task to the next. A second (L2) includedâ„“2superscriptâ„“2\ell^{2}regularization in its loss function to minimize the change in weights from those that were previously learned. Naive rehearsal (Naive_Rehearsal_[k]) was a third algorithm that made use of an experience replay memory to store and retrain on a subset of sizekğ‘˜kof previously experienced data at random. Each training batch was composed of equal parts from the current task and the experience replay memory. The number of stored experiences over all tasks is a parameter of the approach.

We also included curves from four state-of-the-art lifelong learning algorithms: elastic weight consolidation (EWC,Kirkpatrick etÂ al. (2017)), online elastic weight consolidation (EWC_online,Schwarz etÂ al. (2018)), synaptic intelligence (SI,Zenke etÂ al. (2017)), memory-aware synapses (MAS,Aljundi etÂ al. (2018)), and gradient episodic memory (GEM,Lopez-Paz & Ranzato (2017)).

For MNIST, we considered the problem of domain-incremental learning. We followed the experimental protocol described byHsu etÂ al. (2018), which we briefly describe below.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3:Continual learning performance for incremental domain learning on Split-MNIST along with performance curves estimated withCLAMP. Naive rehearsal was considered with experience storage limits of110011001100and440044004400.

We used the dataset splitting procedure to create multiple binary classification tasks (e.g., distinguish between images of 0 and images of 1) from the MNIST dataset with 60k training images (âˆ¼similar-to\sim6,000 per digit) and 10,000 test images. The 32x32 greyscale images were normalized to have a mean of zero and variance of one.

The results of our MNIST analysis are given in Fig.3(estimated and true performance curves) and Tables3(estimated properties) and4(estimated transfer scores). Averaged over each classification algorithmağ‘a, the surrogate models achieved a squared errormeanaâ€‹â€–P^aâˆ’Paâ€–F2subscriptmeanğ‘superscriptsubscriptnormsuperscript^ğ‘ƒğ‘superscriptğ‘ƒğ‘ğ¹2\text{mean}_{a}||\hat{P}^{a}-P^{a}||_{F}^{2}of0.0050.0050.005. The ability of the surrogate model to fit the performance data with low error is a partial validation of the formulation of the surrogate performance model. Table4is the estimated task transfer matrix. The estimated task transfer diagonal was strongly positive. The highest off-diagonal transfer estimate was that training with the6,7676,7task is beneficial for the task8,9898,9task. We believe this result follows intuition because of the character similarity between666and999.

The estimated algorithm properties are shown in Table3. At first glance it might seem surprising that a normal neural network (Normal NN) had one of the highest estimated transfer efficienciesÎ³ğ›¾\gammafor the MNIST set of tasks, given that we know that normal neural networks are ill-suited for continual learning. One intuition for the result is that normal neural networks are able to transfer well because their weight updates are unconstrained, while the regularization in other methods prevents such rapid transfer. However, because the normal neural network updates are unconstrained, performance on past tasks is not maintained, while the regularized methods are able to maintain performance on various tasks throughout the curricula.

For analysis on CIFAR100, we used results fromtask-incremental learning. We used the protocol byHsu etÂ al. (2018)for preparing the CIFAR100 dataset and running the continual learning approaches.

Like the MNIST-task set, we used the dataset splitting procedure to create 5 binary classification tasks based on the CIFAR100 dataset. The 100 CIFAR classes were subdivided at random into 10 classes. Each binary classification task consisted of two of the 10 distinct classes. Data preprocessing resulted in 32x32 images that were normalized with a mean of zero and variance of one. To ensure a fair comparison among lifelong learning approaches, all lifelong learning approaches used the same WideResNetÂ (Zagoruyko & Komodakis (2016)) architecture.

The solid lines in Figure4show the performance (i.e. 1-error) for each of the tasks across the curriculum. The highlighted green regions in the figure illustrate when the task is actively being trained. As expected, we broadly observe that performance of a task increases sharply when actively being trained.

The fit of the surrogate performance model is shown as dashed lines in Figure4, and illustrates broad general agreement between predicted performance and the underlying data. We observed a MSE of0.010.010.01between the predicted and observed performance curves.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: Figure 4:Continual learning performance for incremental task learning on Split-CIFAR100 tasks of several lifelong learning algorithms along with the performance estimated withCLAMP. Naive rehearsal was considered with experience storage limits of140014001400and560056005600.

In Table8, we compared the estimated transfer efficiency of multiple algorithms between the MNIST domain and CIFAR-100 domain. Each column was colored from green to red relative to the values in the column. Between the MNIST and CIFAR100 experiment there is broad agreement about the rank ordering of algorithms with respect to transfer efficiency. This indicates that latent property analysis might be useful to estimate the relative performance of a lifelong learning algorithm from one dataset to another.

Our contributions include: (i) the introduction of the first explainable algorithm-agnostic surrogate performance model of lifelong learning, (ii) quantitative validation of the optimization procedure used to estimate the metrics, (iii) qualitative validation of the lifelong learning surrogate performance model using data produced by several popular lifelong learning approaches on benchmark datasets including CIFAR100, MNIST, and Atari.

Our approach to task transfer score recovery is dataset and algorithm-agnostic, and therefore generally applicable to all domains of lifelong learning. The algorithm latent properties seem to be predictive of how the algorithm may perform on other datasets.

A limitation ofCLAMPis that it makes a number of assumption about lifelong learning with the goal of explaining in part the performance of lifelong learning algorithms over diverse tasks. For example, we assume linear experience transfer between tasks. In future work, it would be interesting to explore relaxing the linear task transfer assumptions and consider non-linear transfer.

[å›¾ç‰‡: images\image_5.png]

[å›¾ç‰‡: images\image_6.png]

