æ ‡é¢˜ï¼šAn Introduction to Lifelong Supervised Learning

Shagun SodhaniFAIR, Meta AIand Mojtaba FaramarziUniversite de Montrealand Sanket Vaibhav MehtaCarnegie Mellon Universityand Pranshu MalviyaPolytechnique MontrÃ©aland Mohamed AbdelsalamUniversite de Montrealand Janarthanan RajendranUniversite de Montrealand Sarath ChandarPolytechnique MontrÃ©alCanada CIFAR AI ChairQuebec Artificial Intelligence Institute (Mila)\addbibresourcemain.bib

Artificial Intelligence (AI) systems can be defined as systems that think and act rationally like humans[bellman1978introduction,kurzweil1990age,schalkoff1991artificial,rich1992artificial,winston1992artificial,haugeland1997mind,russell2005ai]. While the term was formally coined at the famous Dartmouth conference in195619561956[mccarthy2006proposal,Woo.2014], philosophers dating back to Aristotle and Plato contemplated formulating the law governing the rational part of the mind. The idea of creatingintelligentsystems inspired myths like the story of Talos, a giant bronze robot created by gods that carried within it a mysterious life source and guarded the island of Crete[Shashkevich.2019]. Since then, psychologists, behaviorists, cognitive scientists, linguists, and computer scientists have championed various approaches for understanding intelligence and developing AI systems.

Early AI systems were oftenrule-based: given a collection ofrules of the world, they would use approaches like search and symbol manipulation to solve a given task. These systems focused on (and generally performed well) on reasoning-related problems, like proving theorems (e.g., Logic Theorist[gugerty2006newell]and General Problem Solver[newell1959variety]) or focused on setups with few entities to interact with[minsky1972artificial]. Systems relying on these classical approaches enabled significant breakthroughs like IBMâ€™s Deep Blue system defeating the then world champion of Chess in 1997. However, these systems were often limited by how fast they couldprocessthe rules. As a result, these systems do not work well when the number of combinations of rules becomes large. Another significant limitation of the rule-based systems is that they need a clean and well-curated collection of rules to start with. It is possible that one can define and describe these rules explicitly for a game like Chess, but this is often not feasible in real-life scenarios.

The over-reliance of early AI systems on hard-coded knowledge limited their scope and use for complex setups and real-world applications. Machine Learning (ML) is a sub-field of AI that aims to address this limitation by inferring knowledge from raw data using techniques like pattern mining, association rule mining, representation learning, classification, regression, etc. Machine Learning systems can be broadly categorized into two groups:

Parametric modelsare models that â€œsummarizeâ€ (or encode) the knowledge in the given dataset/task111For simplicity, we use the terms dataset and task interchangeably in the introductionusing a set of parameters. These models generally assume that a function exists that explains the knowledge in the data and infer the parameters of that function. Once the parameters have been learned, the original data is no longer needed. Common examples of parametric models include logistic regression, linear discriminant analysis, and neural networks.

Non-parametric modelsdo not infer any parameters from the given data, though they may infer some summary statistics, like mean, to speed up inference. Common examples of parametric models include k-Nearest Neighbors and Support Vector Machines.

In general, a machine learning system may or may not have tolearnfeature representations for a given dataset. For example, consider an email spam classifier where the input to the system is a set of features like â€œis the email from an unknown userâ€ or if certain keywords are present or not. In this case, these features could be fed as input to a logistic regression classifier, and only the classifier needs to be trained. In general, we can not assume access to high-quality, informative features, and the machine learning system has to infer these features. For example, in the email spam classifier example, the system may only have access to the blob of email text. It would need to learn a good feature representation that can be used as input to the classifier. In this case, the system could use a non-parametric approach liketerm frequency-inverse document frequency(TF-IDF)[ramos2003using]or use a parametric representation learning model like a recurrent neural network[hochreiter1997long,cho2014learning]

Deep Learning is a sub-field within machine learning that focuses on representation learning (learning representation from the given data), usually using parametric models. The high-level idea behind deep learning is as follows: There are some base computational units called layers, like the convolutional neural network layer[lecun1989backpropagation], which can be stacked over each other (or, in general, composed arbitrarily) to create powerful architectures. For example, the ResNet architecture is composed using a stack of convolutional layers, along with other layers like max-pooling layers.

As the feature representation passes through the subsequent layers, it is transformed into more complex features. The resulting feature could be used as input to a classifier system. The entire system, i.e., the representation learning system, and the classifier system, can be trained together end-to-end. Today, machine learning is one of the most popular AI paradigms, and deep learning is the most popular representation learning approach. It is worth noting that the current AI systems are often a combination of techniques from different sub-fields. For example, AlphaGo[silver2016mastering], which defeated the world champion of Go, uses convolution networks, a deep learning approach, to learn feature representation, and Monte-Carlo Tree Search, a traditional AI approach, to search for the next action.

Machine Learning Systems have come a long way since the McCulloch-Pitts Neuron, the first computational model of a neuron[mcculloch1943logical]. ML systems have shown impressive results in a number of problem settings where the previous AI approaches struggled: fundamental sciences[gemp2021eigengame,PhysRevResearch.2.033429,bapst2020unveiling], bio-medicine[cirecsan2013mitosis,litjens2016deep], life-sciences[senior2020improved,yim2020predicting,tomavsev2019clinically,leibo2018psychlab], hardware design and manufacturing[schmidt2019recent,bhuvaneswari2021deep,mirhoseini2021graph], graph analysis[10.1145/2736277.2741093,kipf2016semi,NIPS2017_5dd9db5e], neuroscience[mathis2018deeplabcut,mathis2020deep]etc.

Even for domains where traditional AI systems were used earlier, the current generation of ML systems have led to significant improvements. This includes areas such as image understanding[Krizhevsky2012-imagenet-classification-with-deep-convolutional-neural-networks,xie2017aggregated], semantic segmentation[girshick2014rich,ren2015faster], video processing[fan2021multiscale], machine translation[bahdanau2014neural,cho2014learning], question answering[lan2019albert,zhang2020retrospective], text summarization[raffel2019exploring,lewis2019bart], text generation[radford2019language_models_are_unsupervised_multitask_learners,kaplan2020scaling], speech recognition[schneider2019wav2vec,baevski2020wav2vec], textless NLP[lakhotia2021generative,kharitonov2021text,Polyak2021SpeechRF],

robotics[hadsell2008deep,koutnik2013evolving,chen2015deepdriving], social network analysis[sodhani2019attending,tang2021graphbased],

etc. ML systems have reached super-human performance on several tasks[hochreiter1997long,bahdanau2014neural,graves2014neural,mnih2015human,he2016deep,miller2016key,vaswani2017attention,krizhevsky2017imagenet,silver2017mastering,silver2018general,devlin2018bert,vinyals2019grandmaster,zhang2020resnest,brown2020language,schrittwieser2020mastering,badia2020agent57]. These ML systems were already used in the digital world[Lewis-Kraus.2016,zhai2017visual,naumov2019deep]but are now being actively deployed in the physical world as well[Satariano.2020,Vincent.2021,Davies.2021].

These advances are bringing the current generation of AI systems closer to the long-standing goal of AI practitioners - designing systems that canimitatethe behavior of humans or can demonstrate human-like general intelligence[10.1093/mind/LIX.236.433]. However, despite all the success and promising results, there are still significant gaps in the capabilities of even the most powerful AI systems when compared to humans.

A key criticism of the current machine learning systems is that they tend to bedata-hungry[marcus2018deep,ford2018architects]. Take the example of theGPT-3model[brown2020language], a large scale language model that is trained with300300300B tokens from text data sources like Common Crawl corpus[raffel2019exploring](570570570GB of data after filtering and cleaning), WebText[radford2019language_models_are_unsupervised_multitask_learners], two internet-based book corpora and Wikipedia pages. The datasets had to be curated and processed to provide meaningful learning signals to the training models. While recent advances in self-supervised learning have reduced the dependence on large-scale, clean and well-labeled datasets, we still need to account for the time and cost of pre-training large-scale models. For instance, theGPT-3model used compute equivalent to3.14â€‹e233.14superscriptğ‘’233.14e^{23}flops222floating point operationsand it would take355355355years to train GPT-3 on a single NVIDIA Tesla V100 GPU. The sample efficiency of ML systems significantly lags behind that of humans, making them expensive to develop and deploy.

A second key challenge is that standard AI paradigms are not good at transferring (or leveraging) knowledge across tasks. While it is possible to train systems that provide excellent performance on a specific task (or related distribution of tasks, in the case of multi-task learning), it is much harder to train general-purpose AI systems that can perform a diverse set of tasks. When AI systems are trained over a sequence of tasks, they tend toforgetsthe crucial knowledge they acquired from the previous tasks. This phenomenon is often referred to as catastrophic forgetting[mccloskey1989catastrophic,ratcliff1990connectionist]and affects all parametric AI systems. Sometimes, knowledge transfer even hurts the performance on the current task due tonegative interference(a common challenge for multi-task learning) of knowledge across tasks[standley2020tasks,yu2020gradient,mansilla2021domain,chen2018gradnorm]. Even in the case of paradigms like transfer learning (which specifically emphasizes the transfer of knowledge across tasks), the knowledge transfer is often uni-directional, i.e., the knowledge from the previous tasks is used to improve the performance on the current task (and not all the tasks). The emphasis is on improving the performance of the current task, even if that hurts the performance of the previous tasks. In an ideal world, we would want the learning systems to perform bothforward(training on the current task improves the performance on the future tasks) as well asbackwardtransfer of knowledge (training on the current task improves the performance on the previous tasks).

These two challenges are related. AI systems need a lot of data to train on because they start training on every task fromscratch. Imagine a system that has to learn the alphabet every time it reads a book. Such a system would have a poor sample complexity because it cannot transfer knowledge across tasks (of learning alphabets and reading books). In terms of learning strategy, the current AI systems are closer to this hypothetical system than humans. As the new data becomes available, the AI systems can notincrementallyacquire new knowledge (without forgetting the prior knowledge). These challenges also make the AI systems harder to adapt to new tasks/datasets. Since these systems do not effectively transfer knowledge across tasks, they need a lot of data to adapt to the new task when they encounter a new task. These behaviors are in sharp contrast to how humans learn and behave. Humans do not need totrainover a stationary data distribution for multiple epochs. While they do not have perfect memory, they can incrementally acquire and update knowledge over their lifetime without catastrophically forgetting the knowledge relevant for the previous tasks. Moreover, humans can efficiently leverage experience across tasks and exhibit knowledge transfer to improve performance on new (forward transfer) and previous (backward transfer) tasks. Over time, humans learn how to quickly adapt to novel situations without learning everything from scratch.

TheLifelong Learningparadigm is the branch of AI that focuses on developing lifelong learning systems - systems that keep accumulating new knowledge throughout their lifetime without forgetting the prior knowledge and use this accumulated knowledge to improve their performance on the different tasks. We highlight that the lifelong learning paradigm is not unique to the multi-task setup and applies to the single-task setup as well. Lifelong learning is a general setup since it makes fewer assumptions about the task (or tasks). Consider a standard single-task supervised learning setup where the learner can access the entire dataset before starting the training. In this case, the learner can perform multiple epochs over the dataset, shuffling the data in each epoch to keep the data distribution, i.i.d (independent and identically distributed). However, there are many implicit assumptions in this setup - since we have access to the dataset beforehand, we know how many unique classes exist in the dataset. We also have access to the class distribution and can weigh the classes differently. We can also over/under-sample the data. While these assumptions make the setup amenable for training, they also take the setup away from the more general open-ended learning setup. If we were not to assume access to the dataset (or even the number of unique classes), the AI system would have to address challenges like modifying the network architecture as it sees new classes, not forgetting the old data points as it trains on new data points and potentially increasing the capacity of the system as new data keeps coming in. All these challenges are studied under the paradigm of lifelong learning.

This primer is an attempt to provide a detailed summary of the different facets of lifelong learning. We start withChapter2which provides a high-level overview of lifelong learning systems. In this chapter, we discuss prominent scenarios in lifelong learning (Section2.4), provide a high-level organization of different lifelong learning approaches (Section2.5), enumerate the desiderata for an ideal lifelong learning system (Section2.6), discuss how lifelong learning is related to other learning paradigms (Section2.7), describe common metrics used to evaluate lifelong learning systems (Section2.8). This chapter is more useful for readers who are new to lifelong learning and want to get introduced to the field without focusing on specific approaches or benchmarks.

The remaining chapters focus on specific aspects (either learning algorithms or benchmarks) and are more useful for readers who are looking for specific approaches or benchmarks.Chapter3focuses on regularization-based approaches that do not assume access to any data from previous tasks.Chapter4discusses memory-based approaches that typically use areplay bufferor anepisodic memoryto save subset of data across different tasks.Chapter5focuses on different architecture families (and their instantiations) that have been proposed for training lifelong learning systems. Following these different classes of learning algorithms, we discuss the commonly used evaluation benchmarks and metrics for lifelong learning (Chapter6) and wrap up with a discussion of future challenges and important research directions inChapter7.

The primer is designed to serve as an introduction to lifelong learning paradigm and address questions like â€œwhat is lifelong learningâ€, â€œwhy is it a relevant problem to work onâ€, â€œwhat are some key desiderata of a lifelong learning systemâ€, â€œwhat are some common design decisions when developing lifelong learning systemâ€, â€œwhat are commonly used benchmarks in lifelong learningâ€ etc. While we include (and describe) several lifelong learning approaches and benchmarks and intend to keep the document updated over time, the primer is not an exhaustive literature survey by any means. The selection of work is based on the diversity of approaches and pedagogical reasons. We note that we are focusing on lifelong learning approaches in the context of supervised learning and do not cover work-related to lifelong reinforcement learning, which is an important and interesting topic on its own. We recommend the readers to referkhetarpal2020towardsfor a survey on lifelong reinforcement learning.

The target audience for this primer is both newcomers (people who are new to the field of lifelong learning or are just curious about lifelong learning) and practitioners (who are working on lifelong learning or related areas like meta-learning, transfer learning, multi-task learning, etc.). It should be useful for people across the spectrum - from researchers working on the fundamental problems to practitioners working on applications of ML.Chapter2is particularly useful for readers who are new to the area of lifelong learning. Readers already familiar with lifelong learning may benefit more fromChapter3,Chapter4andChapter5that focus on different classes of lifelong learning algorithms. Readers looking to evaluate their lifelong learning systems or create new evaluation benchmarks would benefit from a discussion on benchmarks and metricsÂ (Chapter6).

Consider a setup where a machine learning model is trained over a sequence of tasks. Let us assume that the model has trained on the firstkğ‘˜ktasks and is starting to train on thek+1tâ€‹hğ‘˜superscript1ğ‘¡â„k+1^{th}task. As the model trains on thek+1tâ€‹hğ‘˜superscript1ğ‘¡â„k+1^{th}task, a couple of scenarios are possible: (i) the model learns to solve the current task at the expense of performance on the previous tasks, (ii) the model fails to learn the new tasks though it retains its performance on the previous tasks, (iii) the model learns the new tasks while retaining its performance on the previous tasks, or (iv) the model does not learn the new task while forgetting its knowledge on the previous task. While the ideal outcome is the one where the model learns the new tasks while retaining its performance on the previous tasks, in practice, the model would likely forget some of the previous knowledge and may not be able to learn the new task.

This setup can be viewed from the lens ofstability-plasticity dilemma[mermillod2013stability]. Here,plasticityrefers to the ability to integrate new knowledge, andstabilityrefers to the ability to retain previous knowledge[mirzadeh2020dropout]. Too much plasticity will likely lead to forgetting previous knowledge, while too much stability will hurt learning on the current task. Any learning system, biological or artificial, needs to balance plasticity with stability to ensure continued learning without catastrophic forgetting.

Much work in machine learning looks at thestability-plasticity dilemmaas two separate problems and puts more emphasis on one of the two aspects. For example, transfer learning approaches focus exclusively on the plasticity aspect, while approaches to alleviate catastrophic forgetting focus more on the stability aspect. TheLifelong Learningparadigm focuses on both the challenges at once, with the goal of developing lifelong learning systems - systems that keep accumulating new knowledge throughout their lifetime (plasticity) without catastrophically forgetting the prior knowledge (stability) and use this accumulated knowledge to improve their performance on the different tasks.

As discussed inSection1.5, in this primer, we focus on lifelong learning paradigm in context of supervised learning. We briefly recap the supervised learning setupÂ (Section2.2), describe the lifelong supervised learning paradigmÂ (Section2.3) and discuss three prominent scenarios in lifelong supervised learningÂ (Section2.4). For the sake of simplicity, we drop the termsupervisedwhen referring to lifelong learning and make it explicit when we are referring to lifelong reinforcement learning.

In supervised learning, we want to learn a functionf:ğ’³â†’ğ’´:ğ‘“â†’ğ’³ğ’´f:\mathcal{X}\rightarrow\mathcal{Y}that is able to predict a target vectoryâˆˆğ’´ğ‘¦ğ’´y\in\mathcal{Y}, when given an input samplexâˆˆğ’³ğ‘¥ğ’³x\in\mathcal{X}(wherexğ‘¥xcan be in raw form, or in the form of a curated set of features for the raw input). To do so, we have access to some training dataD={(xi,yi)i=1n}ğ·superscriptsubscriptsubscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–ğ‘–1ğ‘›D=\{(x_{i},y_{i})_{i=1}^{n}\}, which consists ofnğ‘›npairs of input samplesxiâˆˆğ’³subscriptğ‘¥ğ‘–ğ’³x_{i}\in\mathcal{X}and their corresponding target vectorsyiâˆˆğ’´subscriptğ‘¦ğ‘–ğ’´y_{i}\in\mathcal{Y}. We assume data is drawn i.i.d. from a fixed distributionPâ€‹(x,y)ğ‘ƒğ‘¥ğ‘¦P(x,y).

In order to train this functionfğ‘“f, we use some loss functionLğ¿Lthat captures how much the functionâ€™s predictiony^=fâ€‹(x)^ğ‘¦ğ‘“ğ‘¥\hat{y}=f(x)is different from the ground truthyğ‘¦ygiven a samplexğ‘¥x. The risk associated with this function becomes:

and hence the optimal functionfâˆ—superscriptğ‘“f^{*}is the function that minimizes this risk:

However, since the distributionPğ‘ƒPis unknown, the riskRğ‘…Rcannot be computed. As an alternative, the Empirical Risk Minimization (ERM) principle[risk_minimization]is usually used, which seeks to obtain the optimal functionf^^ğ‘“\hat{f}that minimizes the empirical riskR^^ğ‘…\hat{R}

In the lifelong learning setup, there exists a sequence of tasks, where each tasktğ‘¡trepresents a set of unique classesğ’(t)superscriptğ’ğ‘¡\mathcal{C}^{(t)}, whereğ’(t)âŠ†ğ’´superscriptğ’ğ‘¡ğ’´\mathcal{C}^{(t)}\subseteq\mathcal{Y}(the set of all possible classes). The tasks come in a sequence one by one and each tasktğ‘¡tcomes with its set of dataD(t)={(xi,yi)i=ss+nt}superscriptğ·ğ‘¡superscriptsubscriptsubscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–ğ‘–ğ‘ ğ‘ subscriptğ‘›ğ‘¡D^{(t)}=\{(x_{i},y_{i})_{i=s}^{s+n_{t}}\}, wherexiâˆˆğ’³subscriptğ‘¥ğ‘–ğ’³x_{i}\in\mathcal{X}andyiâŠ†ğ’(t)subscriptğ‘¦ğ‘–superscriptğ’ğ‘¡y_{i}\subseteq\mathcal{C}^{(t)}.

The output spaceğ’´(ğ’¯)superscriptğ’´ğ’¯\mathcal{Y}^{(\operatorname{\mathcal{T}})}keeps expanding whenever a new taskğ’¯ğ’¯\operatorname{\mathcal{T}}is introducedğ’´(ğ’¯)=â‹ƒt=1ğ’¯ğ’(t)superscriptğ’´ğ’¯superscriptsubscriptğ‘¡1ğ’¯superscriptğ’ğ‘¡\mathcal{Y}^{(\operatorname{\mathcal{T}})}=\bigcup_{t=1}^{\operatorname{\mathcal{T}}}\mathcal{C}^{(t)}, the goal is still to learn the function that maps the input to output space across all seen tasksfğ’¯:ğ’³â†’ğ’´(t):subscriptğ‘“ğ’¯â†’ğ’³superscriptğ’´ğ‘¡f_{\operatorname{\mathcal{T}}}:\mathcal{X}\rightarrow\mathcal{Y}^{(t)}. Applying the ERM principle as is would lead us to the following equation:

However, as the data from older taskst<ğ’¯ğ‘¡ğ’¯t<\operatorname{\mathcal{T}}is not available anymore, calculating the risk this way becomes infeasible. On the other hand, minimizing the risk on only the currently available data will lead to good performance on the current task and potential catastrophic forgetting of the previous tasks. We shall explain in the next chapter how the different existing methods try to deal with this issue.

There are three prominent scenarios in lifelong learning: Domain-incremental Learning, Task-incremental Learning, and Class-incremental Learning. These scenarios assume that during training, there are clear and well-defined boundaries between the tasks to be learned[vandeven2019scenarios](though the learning system may not have access to these task boundaries). These scenarios are distinguished by whether task identitytğ‘¡tis provided during evaluation and, if it is not, whether task identity must be inferred.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: (a)Domain-Incremental

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: (a)Domain-Incremental

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: (a)Domain-Incremental

In the domain-incremental learning scenarioÂ (Figure2.1(a)), the system does not need (and does not have) access to the task identitytğ‘¡tduring evaluation. In this setup, the input distributions are different, while the output distribution is the same, i.e.,Pâ€‹(x(a))â‰ Pâ€‹(x(b))ğ‘ƒsuperscriptğ‘¥ğ‘ğ‘ƒsuperscriptğ‘¥ğ‘P(x^{(a)})\neq P(x^{(b)})andğ’(a)=ğ’(b)â€‹âˆ€a,bâˆˆWâ€‹ifâ€‹aâ‰ bformulae-sequencesuperscriptğ’ğ‘superscriptğ’ğ‘for-allğ‘ğ‘ğ‘Šifğ‘ğ‘\mathcal{C}^{(a)}=\mathcal{C}^{(b)}\leavevmode\nobreak\ \forall a,b\in W\text{ if }a\neq bwhereWğ‘ŠWis the set of whole numbers.
In this setup, the models have a single-headed output layer, and each class has the same semantic meaning across all the tasks. Since the system does not have to choose an output head, it does not need to infer the task identity.

In the task-incremental learning scenario, the model is trained on a sequence of tasks with known task identities. Since task identity is always provided, it is possible to train models with task-specific components using amulti-headedoutput layer (for deep neural networks)[ewc,chaudhry2019tiny,mirzadeh2020understanding].
The output classes are disjoint between tasks,Pâ€‹(x(a))â‰ Pâ€‹(x(b))ğ‘ƒsuperscriptğ‘¥ğ‘ğ‘ƒsuperscriptğ‘¥ğ‘P(x^{(a)})\neq P(x^{(b)}),Pâ€‹(y(a))â‰ Pâ€‹(y(b)),ğ’(a)âˆ©ğ’(b)=Î¦âˆ€a,bâˆˆWâ€‹ifâ€‹aâ‰ bformulae-sequenceğ‘ƒsuperscriptğ‘¦ğ‘ğ‘ƒsuperscriptğ‘¦ğ‘formulae-sequencesuperscriptğ’ğ‘superscriptğ’ğ‘Î¦for-allğ‘ğ‘ğ‘Šifğ‘ğ‘P(y^{(a)})\neq P(y^{(b)}),\leavevmode\nobreak\ \mathcal{C}^{(a)}\cap\mathcal{C}^{(b)}=\Phi\quad\forall a,b\in W\text{ if }a\neq bin the task-incremental scenario and models are evaluated by their average final performance across all tasks after being trained on all tasks sequentially (see Figure2.1(b)). Here, when evaluating on a given task, the modelâ€™s predictions for only the classes corresponding to the given task are considered[vandeven2019scenarios].

In the class-incremental learning scenario, the model must infer the task identity and solve the tasks seen so far. It is, by far, the most challenging setting in lifelong learning, and many existing methods fail in this setting[Rebuffi_2017,aljundi2019online]. This scenario employs a single-head architecture where the output space is the same for all distributions, and the model needs to classify all labels without a task-ID (Figure2.1(c)). Here,Pâ€‹(x(a))â‰ Pâ€‹(x(b))ğ‘ƒsuperscriptğ‘¥ğ‘ğ‘ƒsuperscriptğ‘¥ğ‘P(x^{(a)})\neq P(x^{(b)})andPâ€‹(y(a))â‰ Pâ€‹(y(b))â€‹âˆ€a,bâˆˆWâ€‹ifâ€‹aâ‰ bformulae-sequenceğ‘ƒsuperscriptğ‘¦ğ‘ğ‘ƒsuperscriptğ‘¦ğ‘for-allğ‘ğ‘ğ‘Šifğ‘ğ‘P(y^{(a)})\neq P(y^{(b)})\leavevmode\nobreak\ \forall a,b\in W\text{ if }a\neq b. For instance, considering a classification task using deep neural networks, the units of all the classes seen so far are active in this scenario.

A wide range of methods have been proposed in the past years to tackle the challenges in lifelong learning. However, each method makes assumptions that are not consistent due to the presence of different settings defined above. In particular, a few methods require fewer supervisory signals during both training and inference times and hence generalize better to different lifelong learning settings. Such signals can be a natural number for task identity, a natural language descriptor, a vector representation of data describing a task, etc. However, there is a clear trend in recent works to simultaneously apply multiple techniques to tackle this problem.

Methods proposed in lifelong learning are broadly categorized into the following three categories: Regularization-based, Memory-based, and Architecture-based methods[de2019continual,masana2020class].

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: Figure 2.2:Common strategies in Lifelong Learning

Regularization-based methods prevent a drastic change in the network parameters as the new task arrives to mitigate forgetting. These methods are further classified as importance-based, Bayesian-based, distillation-based, and optimization trajectory-based. Here, the importance-based methods regularize the loss function to minimize changes in the parameters important for previous tasks. Distillation-based methods transfers knowledge from the model trained on the previous task to the model being trained on the new data. On the other hand, optimization trajectory-based methods exploit the geometric nature of the local minima to prevent catastrophic forgetting. These methods are shown to be vulnerable to domain shift between tasks[2017_expert_gate_lifelong_learning_with_a_network_of_experts]. We discuss regularization-based methods in Chapter3.

Memory-based methods maintain an â€˜episodic memoryâ€™, containing a few examples from past tasks that are revisited while learning a new task. These methods apply gradient-based updates that facilitate a high-level transfer across different tasks through the examples from the past tasks that are simultaneously available while training on the current new task. For instance, Averaged Gradient Episodic Memory (A-GEM)[chaudhry2019efficient]uses the episodic memory to project the gradients based on hard constraints defined using the episodic memory and the current mini-batch. Experience Replay (ER)[chaudhry2019tiny]uses both replay memory and input mini-batches in the optimization step by averaging their gradients to mitigate forgetting. However, selecting which examples to store is also a significant challenge that has been the focus of various research works. Instead of storing raw samples, Generative Replay trains a deep generative model such as GAN[goodfellow2020generative]to generate data that mimic past data for replay. However, it takes a long time to train such generative models and hence is not a viable option for complex datasets in terms of computational cost. We discuss memory-based methods in Chapter4.

Architecture-based methods either freeze or add a set of parameters with the idea that different tasks should have their own set of isolated parameters. These methods alleviate catastrophic forgetting in general, but they rely on a strong base network and work on a small number of tasks. For example,2017_expert_gate_lifelong_learning_with_a_network_of_expertsassigns a model copy to every new task that arrives. Similarly, there are expansion-based methods that handle the lifelong learning problem by expanding the model capacity in order to adapt to new tasks[2018_on_training_recurrent_neural_networks_for_lifelong_learning,rao2019continual]. We discuss architecture-based methods in Chapter5.

Several works have outlined useful properties and open challenges for lifelong learning systems, both in the context of supervised learning[sodhani2021multi,veniat2021efficient,cl_dnn_hadsell]and reinforcement learning[schaul2018barbados]. We compile these properties into a list of desired properties of a model suitable for lifelong learning settings:

Knowledge Retention- As the model trains over the new tasks, it should not forget the knowledge from the previous tasks. Learning new tasks should not happen at the expense of the knowledge from the previous tasks. Much of the existing literature focuses on this problem (under the name of Catastrophic Forgetting[mccloskey1989catastrophic,french1999catastrophic]). Due to this problem, conventional deep learning tends to focus on offline training, with i.i.d. sampling of mini-batches with multiple epochs over the training data. Therefore, the model requires a significant amount of previous tasks data to learn and accumulate explicit knowledge. Some works refer to this knowledge retention property asplasticityorstability.

Knowledge Transfer- The model should be able to reuse the knowledge across tasks. This includes bothforwardtransfer of knowledge where the knowledge acquired during previous tasks is used to solve the subsequent tasks, andbackwardtransfer where the knowledge acquired in the current/future tasks is used to improve performance on the previous tasks. The underlying premise is, if the tasks are related, this knowledge transfer could lead to faster learning and better generalization. Most current approaches for knowledge transfer focus on the forward transfer of knowledge.

Model Expansion- As the model trains over a sequence of tasks, the model should be able toexpanditself or increase its learningcapacity. This could mean that the model can introduce new trainable parameters in practice. Also, training a separate model entirely for each task discounts the possibility of transferring knowledge forward and backward directions when the tasks are related. This further discounts better generalization or faster learning. InSection5.2.3, we discuss expanding networks that aim to tackle these problems by increasing the model capacity and reusing learned representations.

Parameter Efficiency- While increasing the modelâ€™s capacity, we would also want the computational and memory costs of the model to increase only sub-linearly (or to be bounded) as the model trains on new tasks to avoid computational performance degradation. The model expansion property comes with additional constraints: In the true lifelong learning setting, the model would experience a continual stream of training data that can not be stored. Hence the model would, at best, have access to only a small sample of the historical data. We can not rely on past examples to train the expanded model from scratch in such a setting, and a zero-shot knowledge transfer is desired.

Lifelong learning is referred by different names in the literature: incremental learning[solomonoff1989system], continual learning[de2019continual], explanation-based learning[thrun1996explanation,thrun2012explanation], never-ending learning[carlson2010toward], etc. The underlying idea in all these works is that lifelong learning systems would be more effective at learning and retaining knowledge across different tasks. In principle, the ability to generalize is one of the most important characteristics of a machine learning model. If tasks are related, then knowledge transfer between tasks should lead to a better generalization, and faster learning[Biesialska_2020].

Lifelong learning also bears some resemblance to other dominant research areas. It is closely related to areas like Multitask Learning[caruana1997multitask_learning], Meta Learning[schmidhuber1987evolutionary,Thrun1998], Transfer Learning[pan2009survey], Online Learning[Shalev-shwartz07onlinelearning:,MAL-018], and Curriculum Learning[bengio2009curriculum].

The paradigm of multitask learning focuses on improving the performance of a single model on multiple tasks by sharing knowledge across tasks[caruana1997multitask_learning,zhang2014facial_landmark_detection_by_deep_multitask_learning,ruder2017overview,radford2019language_models_are_unsupervised_multitask_learners,sodhani2021multi]. This goal is quite similar to the goal of lifelong learning systems, with one major difference - multitask learning approaches generally assume that information about all the tasks is known when the training starts. In practice, this means that the learning system has access to all the tasks, and in some cases, the system can even choose the ordering of the tasks[bengio2009curriculum,pentina2015curriculum]as done in curriculum learning. This assumption is generally not valid for lifelong learning setups where neither the number of tasks nor the nature of tasks is assumed to be known when starting the training.

Since the learning system has upfront knowledge about all the tasks, catastrophic forgetting is not usually studied in multitask learning. However, a closely related challenge that is well-studied in the context of multitask learning is the problem of negative interference[adapting_auxiliary_losses_using_gradient_similarity,regularizing_deep_multi_task_networks_using_orthogonal_gradients,yu2020gradient]where the gradients corresponding to the different tasks interfere negatively with each other, thus slowing down (or completely inhibiting) training on multiple tasks. Negative interference is related to catastrophic forgetting as it can cause the learning system to forget (orunlearn) knowledge from one or more tasks.

In some cases of multitask learning, referred to as sequential multitask learning[zhang2017survey,xiong2018guided], the different tasks may be introduced sequentially, over a period of time. This setup is closer to the lifelong learning setup (as compared to the general multitask learning setup) but even in the case of sequential multitask learning, the information about all the tasks is generally assumed to be known upfront.

Multitask Learning also shares several similarities with lifelong learning in terms of inductive biases and architecture choices. For example, modular networks are a common design choice for both multitask learning[end_to_end_multi_task_learning_with_attention,learning_modular_neural_network_policies_for_multi_task_and_multi_robot_transfer,chang2018automatically,sodhani2021multi]and lifelong learning (Section5.1). In both the cases, the inductive bias of compositionality and learning expertknowledge(orskills) is seen as a useful property for the learning model.

Meta Learning, also known aslearning to learn[Thrun1998,bengio2013optimization], is the machine learning paradigm that focuses on enabling the training system to learn aspects of the learning process itself. This paradigm can be seen as a natural extension from learning features and models to learningalgorithms. Meta Learning comprises of three broad family of approaches: (i) Metric-based[Koch2015SiameseNN,Vinyals2016MatchingNF,Sung2018LearningTC,Snell2017PrototypicalNF], (ii) Model-based[Santoro2016MetaLearningWM,Munkhdalai2017MetaN], and (iii) Gradient-based[mishra2018a,Ravi2017OptimizationAA,Finn2017ModelAgnosticMF].

Meta-learning and lifelong learning approaches have similar motivation - train on the distribution of tasks to improve performance on new, potentially unseen tasks. Similar to lifelong learning, several meta-learning approaches generally do not assume access to all the training tasks at the start of the training. Several works have started focusing on the intersection of lifelong learning and meta-learning[AlShedivat2018ContinuousAV,Ritter2018BeenTD,Nagabandi2019LearningTA,Javed2019MetaLearningRF,wang2020efficientML]. However, the two paradigms also have some differences. Unlike lifelong learning methods, Meta-learning approaches generally do not focus on challenges like catastrophic forgetting or capacity saturation. On the other hand, meta-learning approaches use an explicit objective function that incentives faster training on the new tasks while lifelong learning implicitly optimizes for accelerating training.

The paradigm of transfer learning[dai2009eigentransfer,pan2009survey,torrey2010transfer,bengio2012deep,weiss2016survey,ying2018transfer,tan2018survey,zamir2018taskonomy,zhuang2020comprehensive]focuses on transferring knowledge from one or moresourcetasks to one or moretargettasks. It is related to the lifelong learning paradigm as the learning system is trained over multiple tasks with the hope of doing a forward knowledge transfer to the subsequent tasks.

Transfer learning faces several challenges similar to lifelong learning when considering to transfer a trained model to a new task:Â (i) Should the modelâ€™s architecture be changed (for example by adding more parameters[2016_progressive_neural_networks]or modules[1999_modular_neural_networks_a_survey])?Â (ii) Should some parts of the model be frozen (if yes, which parts?) or should the entire network be finetuned?Â (iii) How should we set the learning rate on the new tasks?Â (iv) How toinfertherelatednessbetween the tasks. Interestingly, some of the architecture choices and inductive biases (like modular architectures) that are useful for lifelong learning[veniat2021efficient]is useful for transfer learning as well[houlsby2019parameter,stickland2019bert].

However, transfer learning is also different from lifelong learning in several ways:Â (i) Transfer learning generally focuses onone-waytransfer of knowledge, from the older to the newer task. In contrast, lifelong learning focuses ontwo-waytransfer of knowledge, from both old tasks to new tasks and vice-versa.Â (ii) Transfer learning focuses primarily on the performance of the current task. Catastrophic forgetting is not seen as a problem and is often not even measured. On the other hand, lifelong learning aims to improve performance over all the tasks.Â (iii) Transfer learning is often used toinitializea model such that it can perform well on the target task. Hence, many approaches for transfer learning involve pre-training on a large corpus. This is not the case with Lifelong learning.

Standard machine learning paradigms (especially in the context of supervised learning and unsupervised learning) use thebatch(oroffline) learning approach where any given data point can be used for training any number of times. While this approach often works well in practice, it may be infeasible in certain setups. For example, there may be privacy-related restrictions for storing the data, making it infeasible to use it for offline training. In other cases, the size of the training data could be unbounded (as in the case of click-stream data). In such a case, storing (and training over) all the historical data is not practical.Online learning[Shalev-shwartz07onlinelearning:,MAL-018,10.5555/3041838.3041955]is a paradigm in machine learning that aims to address some of these limitations.

Online learning techniques have been used in conjunction with other machine learning paradigms like multi-task learning[dekel2006online,agarwal2008matrix,li2013collaborative,wang2016large], metric learning[shalev2004online,jain2008online,7244184], transfer learning[ZHAO201476,10.5555/1953048.2021051,10.1145/2505515.2505603,6467144]etc.
The connection between lifelong learning and online learning is less obvious as the majority of works in lifelong learning focus on thebatch(samples within a task) setup. However, several recent lifelong learning works are starting to focus on the online setup and have argued in favor of online lifelong learning setup to be closer to real-life learning as compared to the offline counterpart[pmlr-v119-chrysakis20a,sodhani2020toward,2020arXiv200309114P,2021arXiv210110423M,NEURIPS2019_15825aee,Aljundi_2019_CVPR,pham2021contextual,Liu_2020,kruszewski2021evaluating,malviya2021tag].

Humans and animals learn more efficiently when starting with simplerconcepts(or tasks) and progressively learning more complex concepts (or tasks)[skinner1958reinforcement,peterson2004day]. For example, the human education system is designed as a curriculum where new concepts build on (and leverage) previous concepts. Curriculum learning[elman1993learning,bengio2009curriculum,pmlr-v97-hacohen19a,wang2020survey]is the machine learning paradigm that aims to leverage insights about the importance of curriculum and use these insights to improve the training of machine learning models. Given the generic nature of curriculum learning, it has been used in conjunction with other machine learning paradigms like multi-task learning[pentina2015curriculum,sarafianos2017curriculum,murugesan2017self], reinforcement learning[narvekar2017curriculum,narvekar2018learning,narvekar2020curriculum,portelas2020automatic], transfer learning[dong2017multi,weinshall2018curriculum], etc.

Curriculum learning and lifelong learning have several commonalities. Both the paradigms can be motivated from the perspective of human cognition and involve a notion ofcontinuouslearning - over a sequence of datasets in lifelong learning and over a sequence of splits of one (or more) datasets in curriculum learning. However, there are notable differences as well. In the general lifelong learning setup, the learning system has no control over the sequence of data points. In contrast, curriculum learning focuses on the most optimal sequence of data points (optimal to learning). In curriculum learning, information about the different datasets/data points is available beforehand, while in the lifelong learning setup, this information becomes available during training. Despite these differences, curriculum learning can be a helpful technique in the context of lifelong learning, and the general idea of selecting data points, based on their estimatedhardness, has been used in several approaches for experience replay[andrychowicz2017hindsight,li2021parallel].

Lifelong learning differs from supervised learning in terms of how the systems are trained and evaluated. These differences imply that the use of the traditional,single-taskperformance metrics like top-111or top-555error rates is not suitable for lifelong learning systems. As discussed in the previous sections, alleviating catastrophic forgetting and knowledge transfer are the crucial challenges that the methods should focus on in lifelong learning. Therefore, we need metrics to measure the modelsâ€™ performance appropriately in a lifelong learning setup. The metrics should evaluate lifelong learning methods to assess their performance through time, including how much the model forgets or gains on the previously learned knowledge. In this section, we explain some of the most popular metrics in lifelong learning, including the average accuracy for overall performance[chaudhry2019efficient], the average forgetting[chaudhry2018riemannian], the forward and the backward knowledge transfer that assesses the ability of the models to transfer knowledge[lopezpaz2017gradient,lesort2019continual].

In lifelong learning settings, a system learns from the dataset(x1,y1),â€¦,(xğ’¯,yğ’¯),subscriptğ‘¥1subscriptğ‘¦1â€¦subscriptğ‘¥ğ’¯subscriptğ‘¦ğ’¯(x_{1},\,y_{1}),\ldots,(x_{\operatorname{\mathcal{T}}},\,y_{\operatorname{\mathcal{T}}}),at each episode wherexisubscriptğ‘¥ğ‘–x_{i}denotes input variable andyisubscriptğ‘¦ğ‘–y_{i}denotes target/output variable belonging to training set of a taskiğ‘–i. However, the systemâ€™s performance is reported based on{x1tâ€‹eâ€‹sâ€‹t,y1tâ€‹eâ€‹sâ€‹t},â€¦,{x1:ğ’¯tâ€‹eâ€‹sâ€‹t,y1:ğ’¯tâ€‹eâ€‹sâ€‹t}subscriptsuperscriptğ‘¥ğ‘¡ğ‘’ğ‘ ğ‘¡1subscriptsuperscriptğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡1â€¦subscriptsuperscriptğ‘¥ğ‘¡ğ‘’ğ‘ ğ‘¡:1ğ’¯subscriptsuperscriptğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡:1ğ’¯\{x^{test}_{1},y^{test}_{1}\},\ldots,\{x^{test}_{1:\operatorname{\mathcal{T}}},y^{test}_{1:\operatorname{\mathcal{T}}}\}.

In the class incremental learning setting, the system incrementally learns a set of classes. The model also incrementally learns a new task at each time in task incremental learning. The model aims to have less forgetting through time and better performance. Theaverage accuracy[chaudhry2018riemannian]is computed as follows:

whereAâˆˆ[0,1]ğ´01A\in[0,1],ğ’¯ğ’¯\mathcal{T}is the total number of tasks seen so far, andan,isubscriptğ‘ğ‘›ğ‘–a_{n,i}is the test classification accuracy on taskiğ‘–iafter sequentially learning thenthsuperscriptğ‘›thn^{\text{th }}task. Forgetting Measure is the another metric that is very crucial in the lifelong learning model performance report.chaudhry2018riemannianintroduce theforgettingmeasure, formally defined as follows:

whereFâˆˆ[âˆ’1,1]ğ¹11F\in[-1,1],fj,isubscriptğ‘“ğ‘—ğ‘–f_{j,i}is a measure of forgetting on taskiğ‘–iafter training up to taskjğ‘—j.fj,isubscriptğ‘“ğ‘—ğ‘–f_{j,i}is defined as the difference between best accuracy achieved on taskiğ‘–iin the past and the final accuracy of taskiğ‘–iafter training on taskjğ‘—j:

Theaverage forgetting ratiois another metric introduced by2018_overcoming_catastrophic_forgetting_with_hard_attention_to_the_task. It measures the amount of forgetting over time and studies the effectiveness of the lifelong learning method in multiple datasets relatively. After training on tasktğ‘¡t, it computes the accuracy on all testing sets of tasksÏ„â‰¤tğœğ‘¡\tau\leq t. This process is repeated multiple times using different seeds for uniformly randomized task-order. Then, the forgetting ratio is defined as follows:

whereAÏ„â‰¤tsuperscriptğ´ğœğ‘¡A^{\tau\leq t}is the accuracy measured on taskÏ„ğœ\tauafter sequentially learning taskt,ARÏ„ğ‘¡superscriptsubscriptğ´Rğœt,A_{\mathrm{R}}^{\tau}is the accuracy of a random multi-layer linear classifier using the class information of taskÏ„ğœ\tauandAJÏ„â‰¤tsuperscriptsubscriptğ´Jğœğ‘¡A_{\mathrm{J}}^{\tau\leq t}is the accuracy measured on taskÏ„ğœ\tauafter jointly learningtğ‘¡ttasks in a multitask learning manner[2018_overcoming_catastrophic_forgetting_with_hard_attention_to_the_task].
To compute the average ratio, we can simply compute the average as follows:

The Positive Backward Transfer and Forward Transfer metrics are two more important metrics in lifelong learning.2021arXiv210110423Mvisually shows how we can compute these metrics and what they measure. Figure2.3illustrates their explanation.

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: Figure 2.3:tâ€‹riğ‘¡subscriptğ‘Ÿğ‘–tr_{i}andtâ€‹eiğ‘¡subscriptğ‘’ğ‘–te_{i}denote training and test set of task i.Bâ€‹Wâ€‹T+ğµğ‘Šsuperscriptğ‘‡BWT^{+}is the average of the difference between accuracies in purple and accuracies in the diagonal.Fâ€‹Wâ€‹Tğ¹ğ‘Šğ‘‡FWTis the average of accuracies in green.ATsubscriptğ´ğ‘‡A_{T}is the average of accuracies in the box in the last row[2021arXiv210110423M].

The Positive Backward Transfer metric measures the positive influence of learning a new task on preceding tasksâ€™ performance. Positive Backward Transfer metric is denoted asBâ€‹Wâ€‹T+ğµğ‘Šsuperscriptğ‘‡BWT^{+}and computed as follows[2021arXiv210110423M]:

where Backward Transfer and Positive Backward Transfer are denoted asBâ€‹Wâ€‹Tğµğ‘Šğ‘‡BWTandBâ€‹Wâ€‹T+ğµğ‘Šsuperscriptğ‘‡BWT^{+}respectively.
As Figure2.3shows, the purple area corresponds to the area used to compute Positive Backward Transfer.Bâ€‹Wâ€‹T<0ğµğ‘Šğ‘‡0BWT<0indicates catastrophic forgetting, andBâ€‹Wâ€‹T>0ğµğ‘Šğ‘‡0BWT>0indicates that learning new tasks has helped with the preceding tasks[ebrahimi2020adversarial].
The Forward Transfer metric denoted asFâ€‹Wâ€‹Tğ¹ğ‘Šğ‘‡FWTmeasures the positive influence of learning a task on future tasksâ€™ performance. We can computeFâ€‹Wâ€‹Tğ¹ğ‘Šğ‘‡FWTas follows:

In Figure2.3,Fâ€‹Wâ€‹Tğ¹ğ‘Šğ‘‡FWTis the average of accuracies in green.

Learning Curve Area (LCAâˆˆ[0,1])\in[0,1])is another performance metric proposed bychaudhry2019efficient. To explain the LCA metric, we need to define an averagebğ‘b-shot performance after the model has been trained for all theTğ‘‡Ttasks as:

wherebğ‘bis the number of mini-batches.LCALCA\mathrm{LCA}atÎ²ğ›½\betais defined as the area of the convergence curveZbsubscriptğ‘ğ‘Z_{b}as a function ofbâˆˆ[0,Î²]ğ‘0ğ›½b\in[0,\beta]:

It is worth mentioning thatLCA0subscriptLCA0\mathrm{LCA}_{0}is the average zero-shot performance and is considered the same as the forward transfer performance.LCAÎ²subscriptLCAğ›½\mathrm{LCA}_{\beta}and the area under theZbsubscriptğ‘ğ‘Z_{b}curve will be high when the zero-shot performance is good, and it shows how quickly the model learns new tasks. This metric is valuable when two models have the sameZÎ²subscriptğ‘ğ›½Z_{\beta}orATsubscriptğ´ğ‘‡A_{T}, but very differentLCAÎ²subscriptLCAğ›½\mathrm{LCA}_{\beta}where one learns much faster than the other with same final accuracy[chaudhry2019efficient].

Aside from the metrics discussed here, there are other useful metrics that can reveal the potential weakness or strength of the methods. Following is a list of some of the traditional performance metrics that can be used in the lifelong learning domain.

Throughput (images/sec) at train and test time.

Mean and standard deviation of top-1 and top-5 error rates for each individual task in task incremental learning.

Comparison of the methodâ€™s performance considering the replay buffer size or the memory overhead.

Confusion matrix comparison. Since it is tough to observe the differences between
the reported confusion matrix from different approaches, we have to find a nice and
cheap way to compare two very similar confusion matrix[wu2019large,Hou_2019_CVPR,Abdelsalam_2021_CVPR].

Similarity Measurement of the classes that the model should learn continually. In this case, the order of the tasks will be more meaningful in the experiment result. Since the forgetting of the model correlates with the order of the task, considering the tasks sequence and measuring their similarity will be essential to have a fair comparison with other methods.

The time-related metrics that can be reported either task by task or as a historical average are taskâ€™s training time comparison, testing time comparison, and validation time comparison.

Replay-based methods are the most popular methods in lifelong learning. These methods do not assume any limitations to access to data from the previous tasks. In practice, samples of data, from the previous tasks, are retained into a memory bank called thereplay buffer. The performance of these methods depends on the size of the replay buffer and the strategy of selecting, editing, and removing samples from the replay buffer. Indeed, the size of the memory or replay buffer is the most critical parameter that should be reported and included in the model performance evaluation process. Following is a list of simple metrics and parameters that we should consider when comparing methods.

Replay buffer size and the strategy for constructing and updating the replay buffer.

Fixed memory size should be reported if a fixed window is used to keep samples in the replay.

Some methods use a replay such that a fixed window per class is reserved to keep track of replayed samples. In this case, there is no fixed memory size for the lifelong learning method, but instead, a fixed window is reserved for each class. Therefore as the model learns new classes or tasks, we expand the memory size for new tasks or set of classes. Reporting the strategy that is used to construct the memory and the size of the memory is important.

A metric to show how much the memory consumption grows as the model learns a new concept. This is different from the size of the replay that is explained above. Some approaches need more memory to alleviate the catastrophic forgetting and use the parameter isolation method (discussed inSection5.2). In such cases one needs to keep track of the memory consumption overhead as models learn new concepts through time.

A metric to evaluate the memory update rates when we train a lifelong learning model in a large-scale memory-based distributed computing cluster. The memory update rates are important in such cases because of the network communication overhead that might decrease the computation speed up either at training or testing time.

The number of memory components also should be reported if the proposed method uses a different type of extra memory component for different purposes such as keeping previous task models or hidden representation or extracted features of some samples from the previous task.

In this chapter, we shall explore how the different methods try to overcome the challenges of lifelong learning without having access to any data from previous tasks nor having the ability to expand the network to learn new tasks. It should be noted that this chapter and the following two chapters are, for the most part, orthogonal to each other in their approaches, which means that the different approaches can, in practice, be combined as done in previous works likesodhani2020toward.

When a neural network is trained sequentially on a series of tasks, the network parameters try to minimize the objective on the current task irrespective of what happens to the performance of the previous tasks. In other words, the network does not see except what we allow it to see, and it does not solve except what we ask it to solve. When the network is trained on task 1, we ask it to minimize the objective on task 1. However, when it is trained on task 2, if we only ask it to minimize the objective on task 2 (as we do not have access to the data of task 1 anymore), this can lead to the deterioration of the network performance on task 1. The reason behind this is that task 1 is not incorporated in the objective function when training on task 2. This kind of behavior is known as catastrophic forgetting (see Figure3.1), which takes place when the parameters of the network change sufficiently across the tasks, such that their performance on previous tasks degrades significantly.

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: Figure 3.1:After learning the taskğ’¯âˆ’1ğ’¯1\operatorname{\mathcal{T}}-1, the parameters are atÎ¸ğ’¯âˆ’1âˆ—subscriptsuperscriptğœƒğ’¯1\theta^{*}_{\operatorname{\mathcal{T}}-1}. While learning the taskğ’¯ğ’¯\operatorname{\mathcal{T}}, if the model follows the solid line to reachÎ¸ğ’¯âˆ—subscriptsuperscriptğœƒğ’¯\theta^{*}_{\operatorname{\mathcal{T}}}, it may incur a significant loss on taskğ’¯âˆ’1ğ’¯1\operatorname{\mathcal{T}}-1, i.e., it suffers from catastrophic forgetting. On the other hand, the goal is to follow the dashed line to reach an optimal parameter setting to incur a minimal loss on both tasks.

Let us consider a setup where we receive a continuum of data from different tasks in a sequential manner:(x1,d1,y1),â‹¯,(xt,dt,yt),â‹¯,(xğ’¯,dğ’¯,yğ’¯)subscriptğ‘¥1subscriptğ‘‘1subscriptğ‘¦1â‹¯subscriptğ‘¥ğ‘¡subscriptğ‘‘ğ‘¡subscriptğ‘¦ğ‘¡â‹¯subscriptğ‘¥ğ’¯subscriptğ‘‘ğ’¯subscriptğ‘¦ğ’¯(x_{1},d_{1},y_{1}),\cdots,(x_{t},d_{t},y_{t}),\cdots,(x_{\operatorname{\mathcal{T}}},d_{\operatorname{\mathcal{T}}},y_{\operatorname{\mathcal{T}}})wherextsubscriptğ‘¥ğ‘¡x_{t}is the input data,dtsubscriptğ‘‘ğ‘¡d_{t}is the task descriptor andytsubscriptğ‘¦ğ‘¡y_{t}is the target variable of tasktğ‘¡t. Let the current task beğ’¯ğ’¯\operatorname{\mathcal{T}}, and the set of weights after training on taskğ’¯ğ’¯\operatorname{\mathcal{T}}beÎ¸ğ’¯subscriptğœƒğ’¯\theta_{\operatorname{\mathcal{T}}}.Î¸ğœƒ\thetaconsists ofNğ‘NparametersÎ¸âˆˆRNğœƒsuperscriptğ‘…ğ‘\theta\in R^{N},
withÎ¸tsubscriptğœƒğ‘¡\theta_{t}denoting the parameters after training on tasktğ‘¡t. The training objectiveL~~ğ¿\tilde{L}would normally be our original objectiveLğ’¯subscriptğ¿ğ’¯L_{\operatorname{\mathcal{T}}}(for example, cross entropy, in the case of classification, on the data that belongs to taskğ’¯ğ’¯\operatorname{\mathcal{T}}).

wherelğ‘™lis the per sample loss, andDğ’¯subscriptğ·ğ’¯D_{\operatorname{\mathcal{T}}}is the set of samples that belong to taskğ’¯ğ’¯\operatorname{\mathcal{T}}.

Regularization-based approaches constrain the update of neural networks to prevent catastrophic forgetting by adding a penalty term (Rğ’¯subscriptğ‘…ğ’¯R_{\operatorname{\mathcal{T}}}) such that the new objective function looks like:

Regularization-based approaches can be roughly categorized into four main types: (i) importance-based regularization, (ii) Bayesian-based regularization, (iii) distillation-based regularization, and (iv) optimization trajectory-based regularization. Out of these four categories, the first three categories explicitly defineRğ’¯subscriptğ‘…ğ’¯R_{\operatorname{\mathcal{T}}}using old parameters, training examples/outputs or parameter distribution, etc. On the other hand, optimization trajectory-based regularization exploits the geometric nature of the local minima, and the corresponding trajectories followed to prevent catastrophic forgetting. We discuss each of these categories in detail in this chapter.

Importance-based Regularization tries to introduce solutions that do not require access to samples from previous tasks to alleviate catastrophic forgetting. The first such solution is to try to make the network parameters after training on task 2 (let us call themÎ¸2subscriptğœƒ2\theta_{2}) as close as possible to the parameters of the network trained on task 1 (Î¸1subscriptğœƒ1\theta_{1}). One way to do so would be by applying a quadratic constraint between each pair of parameters inÎ¸1subscriptğœƒ1\theta_{1}andÎ¸2subscriptğœƒ2\theta_{2}. Although this solution might seem to significantly limit the capacity of the model to solve the different tasks, it can be supported by the over-parametrization of neural networks, where many configurations ofÎ¸ğœƒ\thetacan still lead to the same performance[hecht1992theory,sussmann1992uniqueness]. However, doing the regularization this way can still significantly reduce the networkâ€™s capacity, especially when training takes place sequentially. It is not guaranteed that there exists another solution in the vicinity ofÎ¸1subscriptğœƒ1\theta_{1}that can still perform well on both tasks.

More sophisticated solutions under the importance-based regularization umbrella try to solve this problem by making the regularization more selective, giving the network some freedom to change some parameters while limiting this freedom for other parameters, based on the importance of each parameter with respect to the previous tasks. Here the central question becomes how to measure the importance of each parameter in an efficient and tractable way.

Let us make the above ideas more concrete. The objective function defined in Eq.3.1has no guarantees on the performance over the data of the previous tasks1:ğ’¯âˆ’1:1ğ’¯11:\operatorname{\mathcal{T}}-1. Hence, a naive solution would be to add a quadratic constraint so that the parameters when training on taskğ’¯ğ’¯\operatorname{\mathcal{T}}do not deviate from the parameters of taskğ’¯âˆ’1ğ’¯1\operatorname{\mathcal{T}}-1, i.e.,

whereÎ±ğ›¼\alphais a regularization weight. The idea here is to find a new set of weightsÎ¸ğ’¯âˆ—subscriptsuperscriptğœƒğ’¯\theta^{*}_{\operatorname{\mathcal{T}}}that can perform well on taskğ’¯ğ’¯\operatorname{\mathcal{T}}, while at the same time is close enough toÎ¸ğ’¯âˆ’1âˆ—subscriptsuperscriptğœƒğ’¯1\theta^{*}_{\operatorname{\mathcal{T}}-1}so that the performance on taskğ’¯âˆ’1ğ’¯1\operatorname{\mathcal{T}}-1is preserved. However, adding this constraint this way might be too limiting to the networkâ€™s capacity. Given that not all the parametersÎ¸isubscriptğœƒğ‘–\theta_{i}contribute equally to the performance, a more selective approach can be used so that the parameters that are more important for previous tasks are regularized more than the other parameters:

whereÎ©iğ’¯âˆ’1superscriptsubscriptÎ©ğ‘–ğ’¯1\Omega_{i}^{\operatorname{\mathcal{T}}-1}represents the importance of each parameterÎ¸isubscriptğœƒğ‘–\theta_{i}after training on taskğ’¯âˆ’1ğ’¯1\operatorname{\mathcal{T}}-1. Given thatÎ©iğ’¯âˆ’1superscriptsubscriptÎ©ğ‘–ğ’¯1\Omega_{i}^{\operatorname{\mathcal{T}}-1}is the only measure of importance used, it should also be a function ofÎ©itâ€‹âˆ€t<ğ’¯âˆ’1superscriptsubscriptÎ©ğ‘–ğ‘¡for-allğ‘¡ğ’¯1\Omega_{i}^{t}\ \forall\ t<\operatorname{\mathcal{T}}-1, so that the performance is preserved on all the previous taskst<ğ’¯ğ‘¡ğ’¯t<\operatorname{\mathcal{T}}. In the next section, we shall explore how Elastic Weight Consolidation (EWC)[ewc]tries to estimate these importance parametersÎ©isubscriptÎ©ğ‘–\Omega_{i}.

EWC[ewc]uses the diagonal terms of the Fisher information matrix as a proxy for the importance of the parameters (Î©isubscriptÎ©ğ‘–\Omega_{i}). However, it motivates it from a probabilistic perspective. Given a set of labelled datağ’Ÿğ’Ÿ\mathcal{D}, the optimum set of weightsÎ¸ğœƒ\thetawould be the weights that maximize the posterior probabilitypâ€‹(Î¸|ğ’Ÿ)ğ‘conditionalğœƒğ’Ÿp(\theta|\mathcal{D}), wherepâ€‹(Î¸|ğ’Ÿ)ğ‘conditionalğœƒğ’Ÿp(\theta|\mathcal{D})is:

Assuming that the datasetğ’Ÿğ’Ÿ\mathcal{D}is divided into two parts,DAsubscriptğ·ğ´D_{A}andDBsubscriptğ·ğµD_{B}, where both are independent, then:

By applyinglâ€‹oâ€‹gğ‘™ğ‘œğ‘”logto the previous function we get:

This means that maximizing the posterior on the datasetğ’Ÿğ’Ÿ\operatorname{\mathcal{D}}is equivalent to maximizing the posterior on the subsetDAsubscriptğ·ğ´D_{A}while minimizing the Negative Log-Likelihood (NLL) ofDBsubscriptğ·ğµD_{B}(âˆ’logâ¡pâ€‹(DB|Î¸)ğ‘conditionalsubscriptğ·ğµğœƒ-\log{p(D_{B}|\theta)}).

Since calculating the posteriorpâ€‹(Î¸|DA)ğ‘conditionalğœƒsubscriptğ·ğ´p(\theta|D_{A})is intractable,EWCapproximates the posterior as a Gaussian distribution with the mean given byÎ¸Aâˆ—subscriptsuperscriptğœƒğ´\theta^{*}_{A}(the parameters that minimize the NLL onDAsubscriptğ·ğ´D_{A}), and the diagonal precision given by the diagonal of the Fisher information matrixFğ¹F, which is known as the Laplace approximation[mackay1992practical]. Hence, optimizing the parametersÎ¸ğœƒ\thetafor the posterior in Eq. (3.7) would be equivalent to minimizing the loss:

whereNğ‘Nis the total number of parameters (|Î¸|ğœƒ|\theta|) andLBâ€‹(Î¸)subscriptğ¿ğµğœƒL_{B}(\theta)is the loss on taskBğµBdata.EWCuses the diagonal terms of the Fisher information matrix, which is equivalent to the positive semi-definite second-order derivative of the loss near a minimum, as a proxy for the importance of each of the weight. The Fisher information matrix is:

From Eq. (3.4), we have:

where the Fisher information matrixFğ¹Fis calculated as a point estimate at the end of each task.

When training on subsequent tasks,EWCtries to minimize the distance from previous optimal parameters that correspond to each taskt<ğ’¯ğ‘¡ğ’¯t<\operatorname{\mathcal{T}}, where the objective function becomes:

ThereforeEWCrequires keeping all the Fisher matrices from previous tasksFtsuperscriptğ¹ğ‘¡F^{t}, as well as the optimal parameters for these tasksÎ¸tâˆ—superscriptsubscriptğœƒğ‘¡\theta_{t}^{*}, which gives it a memory complexity that increases linearly with the number of tasksğ’¯ğ’¯\operatorname{\mathcal{T}}(ğ’ªâ€‹(Nâ€‹ğ’¯)ğ’ªğ‘ğ’¯\mathcal{O}(N\operatorname{\mathcal{T}})). Algorithm1shows the pseudo code for EWC.

The derivation ofEWCprovided inewcuses a two task case.huszar2017quadraticextends the Laplace approximation inEWCto the multiple tasks leading to the following objective function:

This objective function depends only on the last set of optimal parametersÎ¸ğ’¯âˆ’1,iâˆ—superscriptsubscriptğœƒğ’¯1ğ‘–\theta_{\operatorname{\mathcal{T}}-1,i}^{*}and the sum of the previous Fisher matricesâˆ‘t<ğ’¯[Î»(t)â€‹Fit]subscriptğ‘¡ğ’¯delimited-[]superscriptğœ†ğ‘¡superscriptsubscriptğ¹ğ‘–ğ‘¡\sum_{t<\operatorname{\mathcal{T}}}[\lambda^{(t)}F_{i}^{t}]. If the above modification is made, its memory complexity is constant in the number of tasks.

Approximating the Fisher information matrix using only the diagonal terms means that the interactions between the parameters are ignored.ritter2018onlinetries to obtain a better approximation by using the Kronecker product approximation of the Fisher information matrix, and it uses a block diagonal Fisher information matrix rather than a diagonal one. This translates to ignoring the interactions between the parameters across the layers but taking into account the interactions within the same layer at the expense of having a higher computational complexity.

chaudhry2018riemannianintroduceEWC++, which is a more efficient version ofEWC.EWC++uses the KL-divergence between the conditional probabilitiespÎ¸â€‹(y|x)subscriptğ‘ğœƒconditionalğ‘¦ğ‘¥p_{\theta}(y|x)andpÎ¸+Î”â€‹Î¸â€‹(y|x)subscriptğ‘ğœƒÎ”ğœƒconditionalğ‘¦ğ‘¥p_{\theta+\Delta\theta}(y|x)as a regularization loss. The conditional probability is represented by the neural network functionfâ€‹(x;Î¸):=pÎ¸â€‹(y|x)assignğ‘“ğ‘¥ğœƒsubscriptğ‘ğœƒconditionalğ‘¦ğ‘¥f(x;\theta):=p_{\theta}(y|x).

Here we follow the derivation for theDKâ€‹L(pÎ¸||pÎ¸+Î”â€‹Î¸)D_{KL}(p_{\theta}||p_{\theta+\Delta\theta})provided inchaudhry2018riemannianthat shows the relationship between the KL divergence and the Fisher information matrix.

Letâ€™s first start by defining the shorthand notationspÎ¸â€‹(z)=pÎ¸â€‹(y|x)subscriptğ‘ğœƒğ‘§subscriptğ‘ğœƒconditionalğ‘¦ğ‘¥p_{\theta}(z)=p_{\theta}(y|x)andğ”¼z[.]=ğ”¼xâˆ¼ğ’Ÿ,yâˆ¼pÎ¸â€‹(y|x)[.]\mathbb{E}_{z}[.]=\mathbb{E}_{x\sim\mathcal{D},y\sim p_{\theta}(y|x)}[.], then we have:

Using the second order Taylor expansion (zğ‘§zis omitted for brevity), we have:

By substituting this in Eq. (3.13), we get:

Here, the first term cancels out:

This means that Eq. (3.16) simplifies to:

whereFğ¹Fis the empirical Fisher information matrix andF=âˆ’Hğ¹ğ»F=-Hat the maximum likelihood estimate. AsFğ¹Fis prohibitively expensive, the diagonal Fisher is used instead (which assumes that the parameters are independent), which gives:

chaudhry2018riemannianproposesEWC++which uses theDKâ€‹Lsubscriptğ·ğ¾ğ¿D_{KL}as the regularization term. After taskğ’¯ğ’¯\operatorname{\mathcal{T}}is introduced, we have:

It has to be noted that many of the approximations that were performed were based on the assumption thatÎ”â€‹Î¸Î”ğœƒ\Delta\thetais small. If the newÎ¸ğœƒ\thetadiverges away from the oldÎ¸ğœƒ\theta, these approximations become very imprecise, but that should not happen given thatÎ¸ğœƒ\thetais already constrained in the objective function to stay as close to the olderÎ¸ğœƒ\thetaas possible.

We can see from Eq. (3.20) thatEWC++is equivalent toEWC(Eq. (3.8)) whenğ’¯ğ’¯\operatorname{\mathcal{T}}represents the second task, while they start to diverge afterwards (Eq. (3.11)).EWC++is more efficient thanEWCas it only needs to keep a single Fisher matrix and a single set of parametersÎ¸ğ’¯âˆ’1subscriptğœƒğ’¯1\theta_{\operatorname{\mathcal{T}}-1}, which gives it a constant memory complexity.

Another difference betweenEWCandEWC++is that inEWC++, the Fisher matrix is updated in an online fashion, and hence no extra pass over the dataset is needed after the taskğ’¯ğ’¯\operatorname{\mathcal{T}}is done. After each minibatch, the diagonal Fisher matrixFğ¹Fis updated as follows:

The pseudo code forEWC++is given in Algorithm2.

Synaptic Intelligence (SI)[pmlr-v70-zenke17a]tries to measure the importance of the parameters by estimating how much each parameter affects the loss trajectory. In more concrete terms, the contribution of each parameterÎ¸isubscriptğœƒğ‘–\theta_{i}to the loss during the current taskğ’¯ğ’¯\operatorname{\mathcal{T}}is defined as:

wheretâ€²superscriptğ‘¡â€²t^{\prime}is the time step andÎ¸iâ€²â€‹(tâ€²)â€‹dâ€‹tâ€²superscriptsubscriptğœƒğ‘–â€²superscriptğ‘¡â€²ğ‘‘superscriptğ‘¡â€²\theta_{i}^{\prime}(t^{\prime})dt^{\prime}contributes to the the parameter change from the initial point (at timetğ’¯âˆ’1â€²subscriptsuperscriptğ‘¡â€²ğ’¯1t^{\prime}_{\operatorname{\mathcal{T}}-1}) to the final point (at timetğ’¯â€²subscriptsuperscriptğ‘¡â€²ğ’¯t^{\prime}_{\operatorname{\mathcal{T}}}). Since gradient descent uses discrete time steps to perform the updates, the effect of each parameter during taskğ’¯ğ’¯\operatorname{\mathcal{T}}(Ï‰i(ğ’¯)superscriptsubscriptğœ”ğ‘–ğ’¯\omega_{i}^{(\operatorname{\mathcal{T}})}) is, in practice, calculated as an online running sum of the product of the gradient with the parameter updateÎ¸ğ’¯,iâ€²â€‹(tâ€²)superscriptsubscriptğœƒğ’¯ğ‘–â€²superscriptğ‘¡â€²\theta_{\operatorname{\mathcal{T}},i}^{\prime}(t^{\prime}), whereÏ‰i(ğ’¯)superscriptsubscriptğœ”ğ‘–ğ’¯\omega_{i}^{(\operatorname{\mathcal{T}})}is initialized to zero for each new taskğ’¯ğ’¯\operatorname{\mathcal{T}}.

The parameter importanceÎ©i(ğ’¯)superscriptsubscriptÎ©ğ‘–ğ’¯\Omega_{i}^{(\operatorname{\mathcal{T}})}fromEq.3.4is calculated to be directly proportional to the contributionÏ‰i(ğ’¯)superscriptsubscriptğœ”ğ‘–ğ’¯\omega_{i}^{(\operatorname{\mathcal{T}})}of each parameter to the loss during the taskğ’¯ğ’¯\operatorname{\mathcal{T}}, normalized with the square of final changeÎ¸isubscriptğœƒğ‘–\theta_{i}needed to make duringğ’¯ğ’¯\operatorname{\mathcal{T}}to avoid large changes to important parameters (similar to Eq. (3.10)):

whereÏµitalic-Ïµ\epsilonis the damping parameter used to bound the expression in case (Î¸t,iâˆ—âˆ’Î¸tâˆ’1,iâˆ—)â†’0\theta^{*}_{t,i}-\theta^{*}_{t-1,i})\xrightarrow{}0.

We have seen in Section3.2.1(Elastic Weight Consolidation) that the importance of the parameter is inversely related to its uncertainty, for which the Fisher information matrix acts as a proxy. Hence a parameter with a higher precision is a more important parameter (from a probabilistic point of view as in Eq. (3.7)).mas, on the other hand, try to estimate the importance of a parameter based on how sensitive the learned functionFğ’¯âˆ’1subscriptğ¹ğ’¯1F_{\operatorname{\mathcal{T}}-1}is to a change in that parameter. This removes the dependence on a labeled set to estimate the importance, and hence an unlabeled set can be used to estimate the parameter importance. Hence for a specific output classcğ‘c, and a set of unlabeled dataset (or just the new task set)Dğ·D, the sensitivity would be measured using the following equation:

As an alternative to computing the importance per output/class,maspropose using theL2subscriptğ¿2L_{2}norm of all the output nodesâ€–Fâ€‹(x;Î¸)â€–22superscriptsubscriptnormğ¹ğ‘¥ğœƒ22||F(x;\theta)||_{2}^{2}as a representative for all the outputs. Hence, the alternative equation would be (similar to Eq. (3.10)):

Calculating the importanceÎ©isubscriptÎ©ğ‘–\Omega_{i}this way allows for decoupling the updates of the importance parameterÎ©Î©\Omega, and the training on the task itself, since there is no dependence ofÎ©Î©\Omegaon the task data. As mentioned earlier,Dğ·Dcan be an unlabeled set of samples that are fixed across the tasks, but it also allows the use of an online stream of samples.

benzing2021unifyingshows that althoughSIandMASare motivated differently thanEWC, as elaborated earlier, they both approximate the square root of the Fisher Information Matrix, which gives a unified picture for the three importance-based methods.

There have been some other recent advances in importance-based regularization techniques in lifelong learning. For example, building upon MAS[mas], Importance Driven Continual Learning approach[ozgun2020importance]defines a parameter-specific learning rate such that the learning rate becomes a function of the parameterâ€™s importance.

jung2020continualproposed Adaptive Group Sparsity based lifelong learning that introduced a loss function based on group-sparsity norms for parameter-wise importance regularization in neural networks.

Bayesian-based regularization can be considered a special type of importance-based regularization. some scenarios, we donâ€™t have access or are not allowed to store previously seen data due to privacy or security restrictions. In such scenarios, the lifelong learning algorithmâ€™s goal is to train a model at the current task using training data related to the current task without revisiting the training data from the previous task and reducing catastrophic forgetting through time. In this section, we revisit this scenario from a Bayesian inference perspective. The goal of the model for the first task is to predict a set of targets denoted asY(t1)superscriptğ‘Œsubscriptğ‘¡1Y^{(t_{1})}, wheret1subscriptğ‘¡1t_{1}is the task ID, in a supervised manner using parametersÎ¸ğœƒ\theta, with a group of hyperparameters that the model uses to reach its highest performance. For a set ofnğ‘›ni.i.d. samples used for training in the first task, the joint probability distribution ofY(t1)superscriptğ‘Œsubscriptğ‘¡1Y^{(t_{1})}and modelâ€™s parameters, given the hyperparameters used in the training procedure, can be formalized as follows:

whereXğ‘‹Xis the set of observation for taskt1subscriptğ‘¡1t_{1}andÎ±ğ›¼\alpharepresents the hyperparameters. Computing the integral overÎ¸ğœƒ\thetagives the desired marginal distributionâˆ«pâ€‹(Y(t1),Î¸âˆ£Î±,X)â€‹dÎ¸=pâ€‹(Y(t1)âˆ£Î±,X)ğ‘superscriptYsubscriptğ‘¡1conditionalğœƒğ›¼ğ‘‹differential-dğœƒğ‘conditionalsuperscriptYsubscriptğ‘¡1ğ›¼ğ‘‹\int p\left(\mathrm{Y}^{(t_{1})},\theta\mid\alpha,X\right)\mathrm{d}\theta=p\left(\mathrm{Y}^{(t_{1})}\mid\alpha,X\right). By dividing and normalizing the joint distribution, we can also get the posterior distribution aspâ€‹(Î¸âˆ£Y(t1),Î±,X)ğ‘conditionalğœƒsuperscriptğ‘Œsubscriptğ‘¡1ğ›¼ğ‘‹p\left(\theta\mid Y^{(t_{1})},\alpha,X\right). The model can be trained to predict both the posterior distribution and marginal distribution: (i) predicting the targets given by the best hyperparameters and (ii) having the best distribution of the model parameters given by targets and the set of observations. For lifelong learning, the posterior distribution can be obtained by multiplying the previous posterior by the likelihood of the dataset belonging to the new task and the regularization term can be interpreted as a prior.

We cannot compute the posterior distribution directly since we do not have any knowledge of the model parameters. However, we can expand the probability distribution using Bayesian inference and, from that, try to find the best distribution for model parameters. So, according to the Bayesian inference fornğ‘›ni.i.d. training samples:

where(xi,yi)subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–(x_{i},y_{i})is a input and and target pair from the observation set.

In Eq. (3.27), calculatingpâ€‹(Î¸âˆ£Î±,X)ğ‘conditionalğœƒğ›¼ğ‘‹p(\theta\mid\alpha,X)is intractable in most cases i.e., it is not computationally possible to perform exact inference when the dimension ofÎ¸ğœƒ\thetais high. This motivates us to approximate the exact posterior distribution by another distribution that is computationally easier to handle.
We can approximatepâ€‹(Î¸âˆ£Î±,X)ğ‘conditionalğœƒğ›¼ğ‘‹p(\theta\mid\alpha,X)with a posterior distribution using prior knowledge that we have overÎ¸ğœƒ\thetaby assuming that it comes from a Gaussian distribution. With this assumption, we can getqâˆ—â€‹(Î¸âˆ£Î±,X)superscriptğ‘conditionalğœƒğ›¼ğ‘‹q^{*}(\theta\mid\alpha,X)as a posterior of thepâ€‹(Î¸âˆ£Î±,X)ğ‘conditionalğœƒğ›¼ğ‘‹p(\theta\mid\alpha,X)using the Hidden Markov Model (HMM) or Gaussian processes approaches. Therefore, we can define the joint probability distribution for the second task as follows:

wherenğ‘›nandmğ‘šmrefer to number of samples in taskst1subscriptğ‘¡1t_{1}andt2subscriptğ‘¡2t_{2}respectively. Substitutingpâ€‹(Y(t1),Î¸âˆ£Î±,X)ğ‘superscriptYsubscriptğ‘¡1conditionalğœƒğ›¼ğ‘‹p\left(\mathrm{Y}^{(t_{1})},\theta\mid\alpha,X\right)according to Eq. (3.27) we have:

The approximating distribution is usually chosen to be a product of several independent distributions, one for each parameter or a set of similar parameters. Such methods have been used for solving various inference problems in machine learning.

There are several approaches for approximate inference, including Moment matching[li2015generative], variational KL minimization[hershey2007variational], Taylor expansion, Importance Sampling[tokdar2010importance]and Laplaceâ€™s approximation[friston2007variational].

Approximate inference can be used to alleviate catastrophic forgetting in the lifelong learning setup. Proposed lifelong learning methods using approximate inference can be categorized as prior-focused methods. To this end, the main focus is on approximating the modelâ€™s parameters distribution. In a lifelong learning setup, we can approximate the modelâ€™s parameters recursively using the optimal parameters at previous tasks. This intuition and approach to find the best model parameters can show the importance of Approximate Inference to propose new methods in this setup. Both Taylor expansion and variational KL minimization can be used to alleviate catastrophic forgetting in lifelong learning methods. Next, we describe the proposed lifelong learning methods that employ these concepts in detail.

As we discussed in the EWC method in Section3.2.1, the goal of regularization techniques is to tune the model parameters such that the parameters do not deviate from model parameters in the previous task. EWC alleviates the catastrophic forgetting by forcing model parameters to move around the last task parameters space considering the parametersâ€™ importance using Fisher Information Matrix. To reach the same goal, Variational Continual Learning (VCL) method proposed an alternative way by using Approximate inference. To not deviate too much from previously learned parametersâ€™ space, we can use the variational KL minimization to reduce the parametersâ€™ distribution distance from what the model learned in the previous task[nguyen2017variational].

Before showing the advantage of using variational KL minimization to approximate the parametersâ€™ posterior distribution, we review some basic divergence measures such as the Kullback-Leibler (KL) divergence that is used to measure the closeness of the two distributions. It is defined as:

whereKL(q||p)KL(q||p)indicatespğ‘pâ€™s divergence fromqğ‘q. Intuitively, whenpâ€‹(Î¸)ğ‘ğœƒp(\theta)is large, butqâ€‹(Î¸)ğ‘ğœƒq(\theta)is small, there is a large divergence. Whenpâ€‹(Î¸)ğ‘ğœƒp(\theta)is small andqâ€‹(Î¸)ğ‘ğœƒq(\theta)is large, there will a again be a large divergence, but not as large as the previous case.

VCL, as an influential method in prior-focused approaches, computes the posterior distribution of the parameters given the previous examples and keeps changing it over time. Computing the posterior distribution can be simplified using the mean-field approximation. VCL finds the new posterior for tasktğ‘¡tby that minimizes the KL-divergence with the old posterior at time steptâˆ’1ğ‘¡1t-1as follows:

whereDtsubscriptğ·ğ‘¡{D}_{t}is the training data at timetğ‘¡t,Ztsubscriptğ‘ğ‘¡Z_{t}represents the intractable normalizing constant[nguyen2017variational]. VCL predicts the targets for the test set inputs denoted asxâˆ—superscriptğ‘¥x^{*}as follow:

whereğ’Ÿ1:tsubscriptğ’Ÿ:1ğ‘¡\mathcal{D}_{1:t}represents the data from the beginning to the end of timetğ‘¡t.

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: Figure 3.2:The summary of the Variational Continual Learning (VCL) algorithm.

To get the advantage of keeping some samples from the previous task in the replay, VCL proposes using a replay buffer called Coreset in the VCL-Coreset version. Figure3.2illustrates the VCL-Coreset algorithm. The VCL-Coreset observes the current task data denoted asDtsubscriptğ·ğ‘¡D_{t}. It updates the coreset combining the information currently existing in the Coreset andDtsubscriptğ·ğ‘¡D_{t}denoted asCtsubscriptğ¶ğ‘¡C_{t}. Then VCL updates the variational distribution for all samples inğ’ŸtâˆªCtâˆ’1âˆ–Ctsubscriptğ’Ÿğ‘¡subscriptğ¶ğ‘¡1subscriptğ¶ğ‘¡\mathcal{D}_{t}\cup C_{t-1}\setminus C_{t}as marked111in the Figure3.2. VCL uses the sample in theCtsubscriptğ¶ğ‘¡C_{t}to compute the final variational distribution, which is used only for prediction and not propagation as marked by222in the figure3.2. VCL uses the Eq. (3.32) to perform prediction at test time.

To create a new CoresetCtsubscriptğ¶ğ‘¡C_{t}at timetğ‘¡t, VCL selects new data points from the current task and a selection from the old coresetCtâˆ’1subscriptğ¶ğ‘¡1C_{t-1}as shown in figure3.2. To select samples, any heuristic, including greedy approaches or simple random selection, can be used to selectKğ¾Kdata points fromğ’Ÿtsubscriptğ’Ÿğ‘¡\mathcal{D}_{t}and added toCtâˆ’1subscriptğ¶ğ‘¡1C_{t-1}. It helps to have an unbiased coreset for computing the model parameters for the current task.

Following the VCL, Functional Regularisation for Continual Learning with Gaussian Processes (FRCL)[titsias2019functional]uses Gaussian process for a function familyf=(f1,â€¦,fn)ğ‘“subscriptğ‘“1â€¦subscriptğ‘“ğ‘›{f}=\left(f_{1},\ldots,f_{n}\right)such that functions sampled fromğ’©â€‹(Î¼,Î£)ğ’©ğœ‡Î£\mathcal{N}({\mu},\;\Sigma)whereÎ¼ğœ‡{\mu}andÎ£Î£\Sigmais defined by a mean functionmean(x)mean(x)\operatorname{\textit{mean({x})}}and covariance functionKâ€‹(x,xâ€²)Kxsuperscriptxâ€²\mathrm{K}\left(\mathrm{x},\mathrm{x}^{\prime}\right)such thatfâ€‹(x)âˆ¼ğ’¢â€‹ğ’«â€‹(meanâ¡(x),Kâ€‹(x,xâ€²))similar-toğ‘“ğ‘¥ğ’¢ğ’«meanğ‘¥Kğ‘¥superscriptğ‘¥â€²f({x})\sim\mathcal{G}\mathcal{P}\left(\operatorname{mean}({x}),\;\mathrm{K}\left({x},\;{x}^{\prime}\right)\right). In FRCL each functionfisubscriptğ‘“ğ‘–f_{i}defined as follow:

whereÎ©isubscriptÎ©ğ‘–\Omega_{i}is the task-specific weights andÏ•â€‹(x;Î¸)italic-Ï•ğ‘¥ğœƒ\phi(x;\;\theta)represents the shared feature vector. FRCL should maximizeâ„±â€‹(Î¸,qâ€‹(wk))â„±ğœƒğ‘subscriptğ‘¤ğ‘˜\mathcal{F}\left(\theta,\;q\left(w_{k}\right)\right)as a learning objective that is computed as follow:

where,

Theâˆ’âˆ‘i=1kâˆ’1KLâ¡(qâ€‹(ui)âˆ¥pÎ¸â€‹(ui))superscriptsubscriptğ‘–1ğ‘˜1KLconditionalğ‘subscriptğ‘¢ğ‘–subscriptğ‘ğœƒsubscriptğ‘¢ğ‘–-\sum_{i=1}^{k-1}\operatorname{KL}\left(q\left({u}_{i}\right)\|\;p_{\theta}\left({u}_{i}\right)\right)term is also considered as the regularisation term (Rğ’¯subscriptğ‘…ğ’¯R_{\operatorname{\mathcal{T}}}in Eq. (3.2)) that is computed for the previous tasks[titsias2019functional]. FRCL uses theKâ€‹Lğ¾ğ¿KLterm to distinguish the task boundaries such that ifKLâ‰«0much-greater-thanKL0\mathrm{KL}\gg 0shows the task shift andKLâ‰ˆ0KL0\mathrm{KL}\approx 0shows that model is still in the same task.

Recently, Uncertainty-regularized Continual Learning proposed by[ahn2019uncertainty]builds on a Bayesian learning framework with variational inference with the notion of node-wise uncertainty. The authors perform an interpretation of the closed-form of the KL-divergence term for the Gaussian mean-field approximation and the Bayesian neural network pruning that reduces the number of additional parameters for implementing per-weight regularization. On the other hand, Uncertainty- guided Continual Bayesian Neural Networks[ebrahimi2019uncertainty]introduced a learning rate that adapts according to the uncertainty defined in the probability distribution of the weights in networks and retains task performance after pruning weights by saving binary masks per task.kumar2021bayesianapplies variational Bayesian-based regularization for both discriminative and generative settings by learning priors from previous tasks.

Distillation-based regularization is mainly based on the following premises: if the network has access to the samples of task 1, and was forced to produce the same output on these samples while training on task 2, then the performance on task 1 would be preserved and no catastrophic forgetting would take place. However, the samples of task 1, evidently, are not there anymore, but since images lie on a low dimensional manifold[pless2009survey], images of the new task may provide some sort of sampling of the images of task 1, and hence, preserving the network output on these images for the classes of task 1 may keep the performance on task 1 from degrading, depending on how similar the images are between the two tasks. As this mechanism is similar to knowledge distillation[gou2021knowledge], where the teacher network is the network trained on task 1 and the student network is the network being trained on task 2, we shall call it distillation-based regularization.

The distillation based regularization in the context of lifelong learning was introduced by Learning Without Forgetting (LWF)[2017_learning_without_forgetting]. A copy of the model that was trained on the last task (fğ’¯âˆ’1subscriptğ‘“ğ’¯1f_{\operatorname{\mathcal{T}}-1}) is saved. When a new taskğ’¯ğ’¯\operatorname{\mathcal{T}}is introduced, the output offğ’¯âˆ’1subscriptğ‘“ğ’¯1f_{\operatorname{\mathcal{T}}-1}is used as a soft target for the new modelfğ’¯subscriptğ‘“ğ’¯f_{\operatorname{\mathcal{T}}}to imitate. This happens for the classes that are shared between the two models (the classes that belong to the previous tasks). This idea is inspired from knowledge distillation[hinton2015distilling], where the model trained on the previous taskfğ’¯âˆ’1subscriptğ‘“ğ’¯1f_{\operatorname{\mathcal{T}}-1}is the teacher model, and the model being trained on the current taskfğ’¯subscriptğ‘“ğ’¯f_{\operatorname{\mathcal{T}}}is the student model. Since the student model is only trained on the current task data, LWF is making the assumption that the samples of the current task might provide a poor sampling for the older tasks. Hence, if there is visual similarity between the previous tasks and the new tasks, this mechanism can help in alleviating catastrophic forgetting. iCaRL[Rebuffi_2017]extends LWF to the case when there is a replay buffer that provides samples for the older tasks (Chapter4).

In more concrete terms, the objective function that LWF tries to minimize when training on taskğ’¯ğ’¯\operatorname{\mathcal{T}}is:

Along with the cross-entropy loss for the new task (first part), the knowledge distillation loss (second part) is incorporated to impose output stability of old tasks with new data. A temperature term might be multiplied to the distillation loss in order to control how sensitive is the distillation loss to the difference betweenfcâ€‹(x;Î¸ğ’¯âˆ’1âˆ—)subscriptğ‘“ğ‘ğ‘¥subscriptsuperscriptğœƒğ’¯1f_{c}(x;\theta^{*}_{\operatorname{\mathcal{T}}-1})andfcâ€‹(x;Î¸)subscriptğ‘“ğ‘ğ‘¥ğœƒf_{c}(x;\theta).

Learning without Memorization (LWM)[dhar2019learning]claims that imitating the output of the previous model is not enough, but also the model has to remember where to look at; what are the regions in the image that it used to look at before. The intuition is that the new modelfğ’¯subscriptğ‘“ğ’¯f_{\operatorname{\mathcal{T}}}has no extra information about the older classes than the old modelfğ’¯âˆ’1subscriptğ‘“ğ’¯1f_{\operatorname{\mathcal{T}}-1}, and hence it makes no sense to look elsewhere when evaluating the older classes. In addition to the distillation on the output of the teacher model, it applies a distillation loss on the attention maps of the teacher model for the classes that belong to the old tasks as following:

whereÎ²ğ›½\betaandÎ³ğ›¾\gammaare regularization parameters,LDâ€‹(Î¸)subscriptğ¿ğ·ğœƒL_{D}(\theta)is the distillation loss as used in LWF (3.35) andLAâ€‹Dâ€‹(Î¸)subscriptğ¿ğ´ğ·ğœƒL_{AD}(\theta)is the attention distillation loss that is defined as the sum of element wiseL1subscriptğ¿1L_{1}difference of the normalized, vectorized attention map (generated using Grad-CAM[selvaraju2017grad]). Therefore, the attention maps represent the regions in the image which resemble the base classes.

Deep Model Consolidation (DMC)[zhang2020class]leverages the unlabeled auxiliary data instead of old training data to ensure the student model absorbs the knowledge in an unbiased way. Essentially, it learns a new network for each new task and then trains a new model on the outputs of the old and new networks on some unlabeled data to promote symmetric knowledge transfer. The premise is that if natural images lie on a low-dimensional manifold, then the unlabeled data from a similar domain will provide some representation for the datasets of the learned tasks.

Recent knowledge distillation-based regularization methods in lifelong learning includes Batch-level Distillation (BLD)[fini2020online]that adopts a dynamic weighting strategy while minimizing the memory overhead. Similarly,zhong2021discriminativeproposed a discriminative distillation approach by adding an expert classifier whose knowledge is distilled to the new classifier to discriminate features between confusing classes in lifelong learning.douillard2021plopproposed a multi-scale pooling distillation approach to preserve long and short-range spatial relationships at the feature level. To avoid catastrophic forgetting of the old classes, they perform an entropy-based pseudo-labeling of the background for the classes predicted by the old model.

Another direction in the lifelong learning research is to look at the problem from the perspective of the optimization.mirzadeh2020understandinghas shown that the geometric nature of the local minima reached by the learning algorithm affects how much the model is affected by catastrophic forgetting. Letwiâˆ—superscriptsubscriptğ‘¤ğ‘–w_{i}^{*}be the minima obtained after sequential training on theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}task andLjâ€‹(wi)subscriptğ¿ğ‘—subscriptğ‘¤ğ‘–L_{j}(w_{i})as the loss ofjtâ€‹hsuperscriptğ‘—ğ‘¡â„j^{th}task with parameterswisubscriptğ‘¤ğ‘–w_{i}. Then forgetting of the first taskF1subscriptğ¹1F_{1}after training on the second task is defined as:

According to the second-order Taylor expansion ofL1â€‹(w2âˆ—)subscriptğ¿1superscriptsubscriptğ‘¤2L_{1}(w_{2}^{*})aroundw1âˆ—superscriptsubscriptğ‘¤1w_{1}^{*}:

By using the above approximation toL1â€‹(w2âˆ—)subscriptğ¿1superscriptsubscriptğ‘¤2L_{1}(w_{2}^{*}),mirzadeh2020understandingderived an upper bound toF1subscriptğ¹1F_{1}in terms of the maximum eigen value (Î»1mâ€‹aâ€‹xsuperscriptsubscriptğœ†1ğ‘šğ‘ğ‘¥\lambda_{1}^{max}) ofâˆ‡2L1â€‹(w1âˆ—)superscriptâˆ‡2subscriptğ¿1superscriptsubscriptğ‘¤1\nabla^{2}L_{1}(w_{1}^{*}):

.

According to the bound inEq.3.39, smallerÎ»1mâ€‹aâ€‹xsuperscriptsubscriptğœ†1ğ‘šğ‘ğ‘¥\lambda_{1}^{max}means lesser forgetting.Î»1mâ€‹aâ€‹xsuperscriptsubscriptğœ†1ğ‘šğ‘ğ‘¥\lambda_{1}^{max}has been used to characterize the width of the local minima â€“ small values correspond to flat minima, and large values correspond to sharp/ narrow minima[hochreiter1997flat]. Based upon the above analysis and empirical results on existing benchmarks,mirzadeh2020understandingconclude that flatter minima lead to lesser forgetting. Building on this result,mehta2021empiricalhas shown that models initialized with pre-trained weights undergo lesser forgetting than random weights as pre-trained models have an inductive bias towards flat task minima.

To promote flat minima during lifelong learning,mirzadeh2020understandingsuggests modifying training dynamics by varying hyper-parameters like batch size, learning rate, and dropout regularization. It is well known that these hyper-parameters influence the optimization trajectory and loss curvature around minima[xie2020diffusion], also the variance of the gradients[jastrzebski2019break], implicitly leading to flatter minima for certain values. However, searching for appropriate hyper-parameters for lifelong learning is ill-defined as one does not know task sequence apriori. To address these limitations,mehta2021empiricalproposes to explicitly optimize for the loss sharpness (alternatively flatter minima) during lifelong learning. Specifically, they employ a Sharpness-Aware Minimization (SAM) procedure[foret2020sharpness]to simultaneously minimize task loss value and loss sharpness.

SAM searches for parameters that lie in neighborhoods with uniformly low loss regions by minimizing the following loss sharpness (for modelfğ‘“fwith parameterswğ‘¤w):

where the maximization region is defined to beâ„“2superscriptâ„“2\ell^{2}ball with radiusÏğœŒ\rhoaroundwğ‘¤w. Formally, the SAM procedure comprises solving the following minimax optimization problem:

LetÏµ^â€‹(w)^italic-Ïµğ‘¤\hat{\epsilon}(w)denotes the solution to the inner maximization problem inEq.3.41. By using first-order Taylor expansion offâ€‹(w+Ïµ)ğ‘“ğ‘¤italic-Ïµf(w+\epsilon)w.r.tÏµitalic-Ïµ\epsilonaround00and solving for dual norm problem,foret2020sharpnessderivesÏµ^â€‹(w)^italic-Ïµğ‘¤\hat{\epsilon}(w)to be:

By using the value ofÏµ^â€‹(w)^italic-Ïµğ‘¤\hat{\epsilon}(w)fromEq.3.42, the gradient for the minimax problem inEq.3.41is approximated as:

OGD proposed byfarajtabar2020orthogonalworks on the optimization trajectory by projecting the gradients of the new taskğ’¯ğ’¯\operatorname{\mathcal{T}}in a direction that is still useful for learning the taskğ’¯ğ’¯\operatorname{\mathcal{T}}, but that does not change the predictions on the older taskst<ğ’¯ğ‘¡ğ’¯t<\operatorname{\mathcal{T}}. In other words, it tries to maintain a space of the gradient directions of the predictions in the previous tasks, and project the gradients in the new task in a direction perpendicular to that space (Figure3.3).

In more concrete terms, given a taskAğ´Awhich containscğ‘cclasses and a datasetDAsubscriptğ·ğ´D_{A}containingnAsubscriptğ‘›ğ´n_{A}samples, we would havenAÃ—csubscriptğ‘›ğ´ğ‘n_{A}\times cgradient directionsâˆ‡Î¸fjâ€‹(x;Î¸)â€‹âˆ€jâ‰¤csubscriptâˆ‡ğœƒsubscriptğ‘“ğ‘—ğ‘¥ğœƒfor-allğ‘—ğ‘\nabla_{\theta}f_{j}(x;\theta)\forall j\leq c, wherefjsubscriptğ‘“ğ‘—f_{j}is the model output for classjğ‘—j. During training on taskBğµBwhen obtaining the gradientgğ‘”g,OGDtries to find the gradient directiong~~ğ‘”\tilde{g}such that:

In practice, instead of keepingâˆ‡Î¸fjsubscriptâˆ‡ğœƒsubscriptğ‘“ğ‘—\nabla_{\theta}f_{j}for all classes, the average of the labels can be kept, or only the ground truth class can be kept as well (which is what is mainly adopted infarajtabar2020orthogonal). Moreover, since obtaining the exactâˆ‡Î¸fâ€‹(x;Î¸)subscriptâˆ‡ğœƒğ‘“ğ‘¥ğœƒ\nabla_{\theta}f(x;\theta)for anyÎ¸ğœƒ\thetarequires having the whole datasetDAsubscriptğ·ğ´D_{A}, only the gradients at the optimumÎ¸Aâˆ—superscriptsubscriptğœƒğ´\theta_{A}^{*}are kept, assuming that the optimization of the subsequent tasks will let the parametersÎ¸ğœƒ\thetastay in the vicinity ofÎ¸Aâˆ—superscriptsubscriptğœƒğ´\theta_{A}^{*}. Finally, not all the gradients are kept for the wholeDAsubscriptğ·ğ´D_{A}, but rather a subset of these gradients that is chosen randomly.

As mentioned before, when training on taskğ’¯ğ’¯\operatorname{\mathcal{T}}, the gradientg~~ğ‘”\tilde{g}should be perpendicular to the space of the gradients of the ground truth logits for the previous taskst<ğ’¯ğ‘¡ğ’¯t<\operatorname{\mathcal{T}}. This space is defined as:

The orthogonal basis forSğ‘†Sare obtained using the Gram-Schmidt procedure, where they are computed iteratively as follows:

Wherent=|Dt|subscriptğ‘›ğ‘¡subscriptğ·ğ‘¡n_{t}=|D_{t}|,n=âˆ‘t<Tntğ‘›subscriptğ‘¡ğ‘‡subscriptğ‘›ğ‘¡n=\sum_{t<T}n_{t}.kt,lsubscriptğ‘˜ğ‘¡ğ‘™k_{t,l}represents the ground truth index for samplelğ‘™linDtsubscriptğ·ğ‘¡D_{t}such thatyt,lkt,l=1subscriptğ‘¦ğ‘¡subscriptğ‘™subscriptğ‘˜ğ‘¡ğ‘™1y_{t,l_{k_{t,l}}}=1. Finally,pâ€‹râ€‹oâ€‹jvâ€‹(u)=âŸ¨u,vâŸ©âŸ¨v,vâŸ©â€‹vğ‘ğ‘Ÿğ‘œsubscriptğ‘—ğ‘£ğ‘¢ğ‘¢ğ‘£ğ‘£ğ‘£ğ‘£proj_{v}(u)=\frac{\langle u,v\rangle}{\langle v,v\rangle}v. After obtaining the spaceSğ‘†S, the gradientg~~ğ‘”\tilde{g}is obtained using:

where it is proven infarajtabar2020orthogonalthatg~~ğ‘”\tilde{g}is still a descent direction for taskğ’¯ğ’¯\operatorname{\mathcal{T}}, where there exists a non zero learning rateÎ·ğœ‚\eta, such that taking a step in the directionÎ·â€‹g~ğœ‚~ğ‘”\eta\tilde{g}will reduce the loss on taskğ’¯ğ’¯\operatorname{\mathcal{T}}.

[å›¾ç‰‡: images\image_8.png]
å›¾ç‰‡è¯´æ˜: Figure 3.3:Orthogonal Gradient Descent

TAG proposed bymalviya2021taguses an adaptive learning rate based on the relatedness between tasks in the incremental task setup.TAGis applied mainly to theRMSPropoptimization method, butmalviya2021tagoffers other versions for howTAGcan be applied to other adaptive optimization methods as well.

RMSPropupdate rule works as follows:

Wherenğ‘›nrepresents the update step,Vnsubscriptğ‘‰ğ‘›V_{n}is the moving average of the square of the gradients,Î·ğœ‚\etais the original learning rate, andÏµitalic-Ïµ\epsilonis a very small value to avoid degenerate cases. Although adaptive optimization methods tend to perform well in the normal supervised learning setup, they tend to perform poorly in the lifelong learning setup[mirzadeh2020understanding].

TAGtries to adaptRMSPropin a way that is more aligned with the lifelong learning setup. First, it captures the first moment of the gradient for each tasktğ‘¡t:

This moment acts as a proxy for the adaptation trajectory that the model followed for tasktğ‘¡t, and hence the correlationÎ±nâ€‹(ğ’¯,t)subscriptğ›¼ğ‘›ğ’¯ğ‘¡\alpha_{n}(\operatorname{\mathcal{T}},t)between the different taskstğ‘¡tandğ’¯ğ’¯\operatorname{\mathcal{T}}can be measured by using this moment as follows:

whereMn(ğ’¯)superscriptsubscriptğ‘€ğ‘›ğ’¯M_{n}^{(\operatorname{\mathcal{T}})}is the moment for the current taskTğ‘‡Tafter thenğ‘›n-th step, andMN(t)superscriptsubscriptğ‘€ğ‘ğ‘¡M_{N}^{(t)}is the final moment computed on a previous tasktğ‘¡t.bğ‘bis a hyperparameter.

They argue that if a tasktğ‘¡tis correlated with a previous taskğ’¯ğ’¯\operatorname{\mathcal{T}}, the learning rate in the parameter update step would be higher to encourage the transfer of knowledge between tasktğ‘¡tandğ’¯ğ’¯\operatorname{\mathcal{T}}. Whereas if the current tasktğ‘¡tis uncorrelated or negatively correlated to a previous taskÏ„ğœ\tau, the new updates over parameters may cause catastrophic forgetting and hence the learning rate should adapt to lessen the effects of the new updates. By usingÎ±nâ€‹(ğ’¯,t)subscriptğ›¼ğ‘›ğ’¯ğ‘¡\alpha_{n}(\operatorname{\mathcal{T}},t)as a proxy for the correlation between tasks,TAGmodifies Eq. (3.48) to take this correlation into account for any current taskğ’¯>1ğ’¯1\operatorname{\mathcal{T}}>1as follows:

Using the update rule this way ensures that the second moment of the gradients from the previous tasks is taken into account when adapting the learning rate, and that the more similar tasks have a stronger effect than the less similar tasks. With the exponential term,Î±nâ€‹(t,Ï„)subscriptğ›¼ğ‘›ğ‘¡ğœ\alpha_{n}(t,\tau)will attain a higher value for uncorrelated tasks and will minimize the new updates (hence prevent forgetting).

To overcome catastrophic forgetting, the regularization-based methods presented in this chapter add a penalty term in the objective function, to constrain the drastic changes in the model parameters when data from a new task arrives. Based on the motivation for defining the penalty terms and storing the past knowledge, the regularization-based methods are further categorized.

The first type of approach involves quantifying the importance of each parameter with respect to the previous tasks and using this knowledge to control the new changes in the parameters. These approaches require storing the model parameters before it started learning the new task. We present several proposed methods that compute the importance-term in different ways.

Another closely related methods apply concepts of Bayesian inference in lifelong learning. These methods approximate the modelâ€™s parameters distribution using the optimal parameters at previous tasks. In particular, we expand upon the two prominent approximate inference techniques employed to alleviate catastrophic forgetting: 1) Taylor expansion and 2) Variational KL minimization.

The importance-based approach and the Bayesian-based approach rely on the prior knowledge based on model parameters. The third type of approach presented in this chapter is data-focused since it involves knowledge distillation to preserve knowledge by imposing output stability of past tasks with new data.

The final type of approach utilizes the optimization trajectories to impose hard constraints and apply learning rates that adapt for each task in lifelong learning. These approaches essentially require storing knowledge in the form of gradients computed during the parameter updates.

Since the regularization-based methods generally require storing model parameters from previous tasks, they are computationally expensive and often depend on the choice of prior. Moreover, when the model needs to adapt to a large number of tasks, the interference between task-based knowledge is inevitable with fixed model capacity. In the next chapter, we explore memory-based methods that do not require storing the model and comprise several state-of-the-art techniques in lifelong learning with fixed model capacity. We shall discuss methods with dynamic model capacity in the chapter after that.

The vanilla approach for lifelong learning is to fine-tune the model parameters on a new task (tğ‘¡t) starting from the previous task parameters (Î¸tâˆ’1subscriptğœƒğ‘¡1\theta_{t-1}). Chapter3focuses on parameter-based regularization approaches, which prevent the model parameters from deviating too far from their initialization while optimizing the current task loss. On the other hand, in this chapter, we discuss data-based regularization approaches that hope to induce a similar behavior as parameter-based approaches. Specifically, we focus on memory-based approaches for lifelong learning, and the main feature of such approaches is anepisodic memory,â„³tsubscriptâ„³ğ‘¡\mathcal{M}_{t}. Episodic memory retains a subset of the observed examples from tasktğ‘¡t, and memory-based approaches use it to regularize the learning of the future tasks to alleviate forgetting of previous tasks.

Classical cognitive science studies view human memory as a single system, i.e., â€œmemory is memory." However, recent studies show that human memory consists of several components, each performing varied functionalities and operating under different principles. In one such study,tulving1985manyproposes a ternary classification scheme of memory constituting procedural, semantic, and episodic memories. These three memories form a hierarchical arrangement - procedural memory at the lowest level, followed by specialized semantic memory and episodic memory at the top level. Procedural memory helps deal stimulus patterns with response chains; semantic memory enables humans to construct mental models of the world (based on the capability of internally representing states of the world); episodic memory handles acquisition and retention of individual experiences and allows visiting (replaying) them again. In another work,mcclelland1995thereproposes a theory of Complementary Learning Systems (CLS) which advocates that humans rely on episodic memory to store past experiences and conduct experience rehearsal to retain previously learned knowledge. Specifically, there are two complementary systems: one that allows for the gradual accumulation of knowledge and another that allows rapid adaptation to individual experiences.

Motivated by the above studies, a plethora of works[mitchell2018never,chen2015lifelong,lopez2017gradient,chaudhry2019efficient,sprechmann2018memorybased,de2019episodic,wang2020efficientML,riemer2019learning,guo2020improved]employ memory modules for lifelong learning, particularly to alleviate the catastrophic forgetting phenomena. These works differ along multiple dimensions:

What type of memory system is used?Episodic memoryretains a subset of the observed examples for replay[lopez2017gradient,de2019episodic],Semantic memoryretains the structured knowledge[mitchell2018never,chen2015lifelong,schwarz2018progress],Generative memorylearns the parametric model of the data and reconstructs past task examples for replay[shin2017continual,sun2020lamol].

How is memory employed?Explicitconstraints on the current task gradients[lopez2017gradient,chaudhry2019efficient,guo2020improved],Implicitconstraints on the current task gradients[chaudhry2019tiny,riemer2019learning,sprechmann2018memorybased].

How is memory populated?Randomexamples[chaudhry2019tiny],Uncertainexamples[aljundi2019gradient],Forgettableexamples[wang2020efficientML].

Memory-based approaches use episodic memory (â„³=âˆªk<tâ„³kâ„³subscriptğ‘˜ğ‘¡subscriptâ„³ğ‘˜\mathcal{M}=\cup_{k<t}\mathcal{M}_{k}) to replay the examples from previous tasks while updating the model with the current tasktğ‘¡t. While several methods have been developed, here we abstract away from their specific implementations and instead focus on a unified view of episodic memory-based approaches for lifelong learning.

We consider a setup with continuum of task data:

(x1,t1,y1),â‹¯,(xi,di,yi),â‹¯,(xn,tn,yn)subscriptğ‘¥1subscriptğ‘¡1subscriptğ‘¦1â‹¯subscriptğ‘¥ğ‘–subscriptğ‘‘ğ‘–subscriptğ‘¦ğ‘–â‹¯subscriptğ‘¥ğ‘›subscriptğ‘¡ğ‘›subscriptğ‘¦ğ‘›(x_{1},t_{1},y_{1}),\cdots,(x_{i},d_{i},y_{i}),\cdots,(x_{n},t_{n},y_{n}). Each triplet(xi,di,yi)subscriptğ‘¥ğ‘–subscriptğ‘‘ğ‘–subscriptğ‘¦ğ‘–(x_{i},d_{i},y_{i})consists of a task descriptordiâˆˆğ’¯subscriptğ‘‘ğ‘–ğ’¯d_{i}\in\mathcal{T}, input dataxiâˆˆğ’Ÿdisubscriptğ‘¥ğ‘–subscriptğ’Ÿsubscriptğ‘‘ğ‘–x_{i}\in\mathcal{D}_{d_{i}}and target labelsyiâˆˆğ’´disubscriptğ‘¦ğ‘–subscriptğ’´subscriptğ‘‘ğ‘–y_{i}\in\mathcal{Y}_{d_{i}}. Here we assume that an explicit task descriptordisubscriptğ‘‘ğ‘–d_{i}is available. Further, we assume that the continuum is locally i.i.d., i.e., triplet(xi,di,yi)subscriptğ‘¥ğ‘–subscriptğ‘‘ğ‘–subscriptğ‘¦ğ‘–(x_{i},d_{i},y_{i})satisfies(xi,yi)âˆ¼iâ€‹iâ€‹dğ’«diâ€‹(X,Y)superscriptsimilar-toğ‘–ğ‘–ğ‘‘subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–subscriptğ’«subscriptğ‘‘ğ‘–ğ‘‹ğ‘Œ(x_{i},y_{i})\stackrel{{\scriptstyle iid}}{{\sim}}\mathcal{P}_{d_{i}}(X,Y). Overall, the goal is to learn a predictorfÎ¸:ğ’³Ã—ğ’¯â†’ğ’´:subscriptğ‘“ğœƒâ†’ğ’³ğ’¯ğ’´f_{\theta}:\mathcal{X}\times\mathcal{T}\rightarrow\mathcal{Y}such as a neural network, parameterized byÎ¸âˆˆâ„Pğœƒsuperscriptâ„ğ‘ƒ\theta\in\mathbb{R}^{P}, to minimize the average expected risk of allTğ‘‡Ttasks:

withâ„“â€‹(â‹…,â‹…)â„“â‹…â‹…\ell(\cdot,\cdot)being the specific task loss. While the average risk is commonly evaluated after the model has seen all tasks, one can also evaluate test pairs(x,t)ğ‘¥ğ‘¡(x,t)from previously observed tasks at different stages to demonstrate the modelâ€™s training behavior, and evaluate its robustness against catastrophic forgetting in terms of backward and forward transfer. While different methods have been developed to optimizeEq.4.1, in this chapter we focus on memory based approaches for lifelong learning. The main feature of these approaches is anepisodic memory,â„³tsubscriptâ„³ğ‘¡\mathcal{M}_{t}, which retains a subset of the observed examples from each tasktğ‘¡t.

Formally, given a tasktğ‘¡t,btsubscriptğ‘ğ‘¡b_{t}denotes a mini-batch sampled fromğ’Ÿtsubscriptğ’Ÿğ‘¡\mathcal{D}_{t},Eq.4.2defines the task loss onbtsubscriptğ‘ğ‘¡b_{t}andEq.4.3defines the replay loss onâ„³tsubscriptâ„³ğ‘¡\mathcal{M}_{t}.

Algorithm4outlines the overall training procedure corresponding to a unified approach. There are three main routines in Algorithm4-MEMREAD,UPDATEandMEMWRITEand most of the memory-based approaches differ in terms of a specific implementation of these. MEMREAD routine implements a strategy to sample examples from the memory while updating the model with current task gradients. MEMWRITE routine implements a strategy to select a subset of the observed examples to write to the memory for replay. UPDATE routine implements how two objectives - task lossEq.4.2and replay lossEq.4.3are combined to alleviate forgetting and enable backward/ forward transfer. In this section, we discuss different realizations of the UPDATE routine, and Section4.3discusses the rest of the two routines in detail.

LetÎ¸tksuperscriptsubscriptğœƒğ‘¡ğ‘˜\theta_{t}^{k}denote the model parameters when training onkğ‘˜k-th mini-batch of the tasktğ‘¡t. Under the unified view, the joint optimization problem covering the task loss and replay loss is defined as follows:

Using stochastic gradient descent method to solveEq.4.4, one-step gradient descent update for model parameters starting withÎ¸tksuperscriptsubscriptğœƒğ‘¡ğ‘˜\theta_{t}^{k}is defined as follows:

whereÎ±1â€‹(Î¸),Î±2â€‹(Î¸)subscriptğ›¼1ğœƒsubscriptğ›¼2ğœƒ\alpha_{1}(\theta),\alpha_{2}(\theta)are real-valued functions controlling the relative importance ofLTâ€‹Aâ€‹Sâ€‹Kâ€‹(Î¸)subscriptğ¿ğ‘‡ğ´ğ‘†ğ¾ğœƒL_{TASK}(\theta)andLRâ€‹Eâ€‹Pâ€‹(Î¸)subscriptğ¿ğ‘…ğ¸ğ‘ƒğœƒL_{REP}(\theta)in each mini-batch. Now we deep-dive into existing replay-based methods and see how they all are different manifestations of the updateEq.4.5.

The most basic type of update strategy is to replay examples,Mksubscriptğ‘€ğ‘˜M_{k}, for each taskkğ‘˜klearned so far while learning the current tasktğ‘¡t. Under the unified view, for the updateEq.4.5we haveÎ±1â€‹(Î¸tk)=Î±2â€‹(Î¸tk)=1subscriptğ›¼1superscriptsubscriptğœƒğ‘¡ğ‘˜subscriptğ›¼2superscriptsubscriptğœƒğ‘¡ğ‘˜1\alpha_{1}(\theta_{t}^{k})=\alpha_{2}(\theta_{t}^{k})=1inEq.4.5.chaudhry2019tinyshows that as simple as this strategy seems, it performs exceedingly well compared to other more sophisticated algorithms.

Gradient Episodic Memory (GEM)[lopez2017gradient]has a constrained optimization-based update strategy. While updating the current task lossEq.4.2, GEM ensures that losses on the episodic memory (ofk<tğ‘˜ğ‘¡k<ttasks)Eq.4.3does not increase in comparison to the previous task model (Î¸tâˆ’1subscriptğœƒğ‘¡1\theta_{t-1}). Formally, the constrained objective is defined as follows:

To inspect the episodic memory loss increase, GEM computes the angle between the loss gradient vectors of previous tasksgksubscriptğ‘”ğ‘˜g_{k}and the proposed gradient update on the current taskgğ‘”g. When the angle betweengğ‘”gand any of thegksubscriptğ‘”ğ‘˜g_{k}â€™s is greater than90âˆ˜superscript9090^{\circ},gğ‘”gis projected to the closest inâ„“â€‹2âˆ’limit-fromâ„“2\ell 2-norm gradientg~~ğ‘”\tilde{g}such that it avoids the increase in losses but allows their decrease. Formally, the modified objective is defined as follows:

GEM solves the above optimization problemEq.4.7via quadratic programming in the dual space withtâˆ’1ğ‘¡1t-1variables (vâˆˆâ„(tâˆ’1)Ã—1ğ‘£superscriptâ„ğ‘¡11v\in\mathbb{R}^{(t-1)\times 1}):

whereG=âˆ’(g1,â‹¯,gtâˆ’1)âˆˆâ„(tâˆ’1)Ã—Pğºsubscriptğ‘”1â‹¯subscriptğ‘”ğ‘¡1superscriptâ„ğ‘¡1ğ‘ƒG=-(g_{1},\cdots,g_{t-1})\in\mathbb{R}^{(t-1)\times P},gâˆˆâ„PÃ—1ğ‘”superscriptâ„ğ‘ƒ1g\in\mathbb{R}^{P\times 1},Pğ‘ƒPis the number of model parameters. Notice thatGğºGis computed at each gradient step of training. Letvâˆ—superscriptğ‘£v^{*}denote the solution ofEq.4.8, then the projected gradient used for updating the model is computed asg~=GâŠ¤â€‹vâˆ—+g~ğ‘”superscriptğºtopsuperscriptğ‘£ğ‘”\tilde{g}=G^{\top}v^{*}+g.
Under the unified frameworkEq.4.4, GEM algorithm reduces to setting the relative importance weightsÎ±1â€‹(Î¸tk)subscriptğ›¼1superscriptsubscriptğœƒğ‘¡ğ‘˜\alpha_{1}(\theta_{t}^{k})andÎ±2â€‹(Î¸tk)subscriptğ›¼2superscriptsubscriptğœƒğ‘¡ğ‘˜\alpha_{2}(\theta_{t}^{k})as follows:

GEM constraints the current task gradient such that the episodic loss on each of the previous tasksk<tğ‘˜ğ‘¡k<tEq.4.3does not increase. To enforce these constraints, it requires computing the gradient using the whole replay bufferâ„³â„³\mathcal{M}, as well as solving a quadratic programming problemEq.4.7. However, the expensive nature of these computations limits the scalability of the GEM to a large number of tasks.

Averaged GEM (A-GEM) provides a more efficient version of GEM by relaxing the constraints as it only requires that theaverageepisodic memory loss over the previous tasks does not increase, which reduce the constraints fromtâˆ’1ğ‘¡1t-1constraints to a single constraint:

The optimization problem corresponding toEq.4.10is defined as:

wheregRâ€‹Eâ€‹Psubscriptğ‘”ğ‘…ğ¸ğ‘ƒg_{REP}is a gradient computed using batch of replay examples, sampled randomly over all previously seen tasks from the episodic memory.Eq.4.11can be solved using just an inner product between the gradients ofLTâ€‹Aâ€‹Sâ€‹Ksubscriptğ¿ğ‘‡ğ´ğ‘†ğ¾L_{TASK}(gğ‘”g) andLRâ€‹Eâ€‹Psubscriptğ¿ğ‘…ğ¸ğ‘ƒL_{REP}(gRâ€‹Eâ€‹Psubscriptğ‘”ğ‘…ğ¸ğ‘ƒg_{REP}) instead of a quadratic program. When the current task gradientgğ‘”gviolates the constraint, the project gradientg~~ğ‘”\tilde{g}is computed as:

For the composite objectiveEq.4.4, A-GEM algorithm reduces to setting the importance weightsÎ±1â€‹(Î¸kt)subscriptğ›¼1superscriptsubscriptğœƒğ‘˜ğ‘¡\alpha_{1}(\theta_{k}^{t})andÎ±2â€‹(Î¸kt)subscriptğ›¼2superscriptsubscriptğœƒğ‘˜ğ‘¡\alpha_{2}(\theta_{k}^{t})as follows:

whereğˆAsubscriptğˆğ´\mathbf{I}_{A}is the indicator function which evaluates to111if A holds and otherwise to 0.

Under the unified framework, one can see that GEM and A-GEM put the same weight on the current task loss regardless of how the loss changes over time (Î±1â€‹(Î¸kt)=1subscriptğ›¼1superscriptsubscriptğœƒğ‘˜ğ‘¡1\alpha_{1}(\theta_{k}^{t})=1inEq.4.9,Eq.4.13).guo2020improvedargues that such a strategy does not capture a good balance between current task lossEq.4.2and replay lossEq.4.3. For example, if the current task loss is small (LTâ€‹Aâ€‹Sâ€‹K<Ïµsubscriptğ¿ğ‘‡ğ´ğ‘†ğ¾italic-ÏµL_{TASK}<\epsilon), then the model performs well on the current task, and the model should focus on previous tasks in the episodic memory. On the other hand, if the current task loss is larger (LTâ€‹Aâ€‹Sâ€‹K>Ïµsubscriptğ¿ğ‘‡ğ´ğ‘†ğ¾italic-ÏµL_{TASK}>\epsilon), then the algorithm should weigh the current task loss relatively higher compared to the replay loss. Based upon this intuition,guo2020improvedproposes Mixed Stochastic Gradient (MEGA-I), which adaptively balances two losses by leveraging the loss information available during training. For the unified updateEq.4.5, MEGA-I sets

whereÏµitalic-Ïµ\epsilonis a pre-defined threshold parameter.

riemer2019learningproposed learning to learn technique using gradient alignment (similar to GEM[lopez2017gradient]) to reduce backward interference with a possibility of future transfer. MER also maintains an experience replay style memory with reservoir sampling. They optimize for the following objective to maximize transfer and minimize interference in lifelong learning:

whereBtsubscriptğµğ‘¡B_{t}is the batch of data points belonging to a tasktğ‘¡t,Lğ¿Lis the loss function,â„³â„³\mathcal{M}is the memory andTğ‘‡Tis the total number of tasks. In other words, the authors integrate the Reptile algorithm[nichol2018first](that was defined in meta-learning context) with an experience replay module to help in lifelong learning by discovering notions of tasks without supervision.

Few recent papers that use experience replay in the lifelong learning domain include Look-ahead MAML (La-MAML)[gupta2020maml], another meta-learning method that modulates per-parameter learning rates to pace the learning of a model across tasks and time. Batch-level Experience Replay[mai2020batch]modifies ER mainly by performing a review step before the final testing to remind the model of the knowledge it has learned during the whole training. Apart from that,buzzega2021rethinkingmodify ER by applying several tricks like performing augmentation, adding a bias correction layer in the model, decaying the learning rate exponentially, and sampling examples greedily using the training loss value.

Section4.1focuses on approaches that use episodic memory during training. However, lifelong learning is a continuous process, and there might not be a clear delimitation between training and evaluation. So one can assume access to the episodic memory even during evaluation. Following this assumption, several works propose to use episodic memory during evaluation[Rebuffi_2017,sprechmann2018memorybased,de2019episodic,wang2020efficientML]and we will review canonical methods in detail here.

iCaRL[Rebuffi_2017]is among the first methods to use episodic memory during test-time. iCaRL introduces three components to alleviate the catastrophic forgetting in the class incremental learning setup: (i) representation learning using knowledge distillation[hinton2015distilling]and experience replay (section4.1.1), (ii) herding based example selection for MEMWRITE, and (iii) test-time classification using nearest mean-of-features from episodic memory.

The idea of using knowledge distillation is similar to the Learning without Forgetting (LwF) approach we discussed in Section3.4.1. Basically, after introducing a new set of classes, the distillation loss is evaluated on the older classes to ensure that the outputs of the current model are close to the output of the previous model, and the classification loss is evaluated only on the new classes. Next, the iCaRL uses a herding-based example selection strategy to write examples to the episodic memory. Instead of choosing the examples randomly, herding chooses them such that their mean approximates the class mean in the feature space. Formal discussion about the herding approach is deferred to Section4.3.

Let us denotefeâ€‹mâ€‹bsubscriptğ‘“ğ‘’ğ‘šğ‘f_{emb}to be a feature extractor andgwsubscriptğ‘”ğ‘¤g_{w}to be an output layer (e.g., linear layer followed by a Softmax). Under the parametric model, the prediction rule is given asyâˆ—=argâ€‹maxyâˆˆ1â€‹â‹¯â€‹Câ¡gwâ€‹(feâ€‹mâ€‹bâ€‹(x))=argâ€‹maxyâˆˆ1â€‹â‹¯â€‹Câ¡wyâŠ¤â€‹feâ€‹mâ€‹bâ€‹(x)superscriptğ‘¦subscriptargmaxğ‘¦1â‹¯ğ¶subscriptğ‘”ğ‘¤subscriptğ‘“ğ‘’ğ‘šğ‘ğ‘¥subscriptargmaxğ‘¦1â‹¯ğ¶subscriptsuperscriptğ‘¤topğ‘¦subscriptğ‘“ğ‘’ğ‘šğ‘ğ‘¥y^{*}=\operatorname*{arg\,max}_{y\in 1\cdots C}g_{w}(f_{emb}(x))=\operatorname*{arg\,max}_{y\in 1\cdots C}w^{\top}_{y}f_{emb}(x). This prediction rule can be viewed as a linear classifier (with weightsw1,â‹¯,wCsubscriptğ‘¤1â‹¯subscriptğ‘¤ğ¶w_{1},\cdots,w_{C}) on top of non-linear features (fromfeâ€‹mâ€‹bsubscriptğ‘“ğ‘’ğ‘šğ‘f_{emb}). Now in the context of class-incremental learning setup, our weightswğ‘¤ware decoupled from the underlying feature extractorfeâ€‹mâ€‹bsubscriptğ‘“ğ‘’ğ‘šğ‘f_{emb}. As a result of this, wheneverfeâ€‹mâ€‹bsubscriptğ‘“ğ‘’ğ‘šğ‘f_{emb}changes, then corresponding predictions can go unchecked, leading to a severe performance drop (interpreted as forgetting). To overcome this issue, iCaRL suggests using episodic memory for classification. Specifically, they use the nearest mean-of-exemplars classification strategy to predict a labelyâˆ—superscriptğ‘¦y^{*}for an examplexğ‘¥x. First, a prototype vector for each class is computed, and then it is used to predict the class label with the most similar prototype withfeâ€‹mâ€‹bâ€‹(x)subscriptğ‘“ğ‘’ğ‘šğ‘ğ‘¥f_{emb}(x). The prototype for classyğ‘¦yis defined to be the average feature vector of all examples with class labelyğ‘¦yin the episodic memory. Formally, the prediction rule is as follows:

whereM(y)superscriptğ‘€ğ‘¦M^{(y)}denotes all examples with class labelyğ‘¦yin the episodic memory. As thefeâ€‹mâ€‹bsubscriptğ‘“ğ‘’ğ‘šğ‘f_{emb}changes, the class prototypes update accordingly, therefore, the prediction ruleEq.4.16does not suffer from decoupled weights issue.

Inspired from CLS theory,sprechmann2018memorybasedintroduce a method that consists of two components: a parametric component (neural network) that learns slowly and a non-parametric component (episodic memory with instances from previous tasks) that rapidly adapts to the parametric component. Particularly, episodic memory is used for instance-based (local) adaptation of the parametric network at inference time. Hence,sprechmann2018memorybasedterm their approach as Memory-based Parameter Adaptation (MbPA). iCaRL proposes a nearest-mean-of-exemplars classifier to overcome issues relating to decoupled weights. MbPA can be viewed as an alternate way of updating the classifier at the test time to address the same issue.

The parametric component consists of an embedding networkfeâ€‹mâ€‹bsubscriptğ‘“ğ‘’ğ‘šğ‘f_{emb}and a task networkgwsubscriptğ‘”ğ‘¤g_{w}. The embedding network is used to encode the instances and the task network is used to predict the output class distribution,pâ€‹(y|x,eâ€‹mâ€‹b,w)=gwâ€‹(feâ€‹mâ€‹bâ€‹(x))ğ‘conditionalğ‘¦ğ‘¥ğ‘’ğ‘šğ‘ğ‘¤subscriptğ‘”ğ‘¤subscriptğ‘“ğ‘’ğ‘šğ‘ğ‘¥p(y|x,emb,w)=g_{w}(f_{emb}(x)). Unlike the previous memory-based approaches,sprechmann2018memorybasedstores instances in the form of key and value pairs, i.e.,Mt={(hi,vi)}subscriptğ‘€ğ‘¡subscriptâ„ğ‘–subscriptğ‘£ğ‘–M_{t}=\{(h_{i},v_{i})\}, wherehi=feâ€‹mâ€‹bâ€‹(xi)subscriptâ„ğ‘–subscriptğ‘“ğ‘’ğ‘šğ‘subscriptğ‘¥ğ‘–h_{i}=f_{emb}(x_{i})andvi=yisubscriptğ‘£ğ‘–subscriptğ‘¦ğ‘–v_{i}=y_{i}. During training, the usual maximum likelihood estimation is used to estimate the parameters{w,eâ€‹mâ€‹b}ğ‘¤ğ‘’ğ‘šğ‘\{w,emb\}. Apart from populating the episodic memory with the observed examples, it is not used during training.

Similar to the iCaRL, MbPA uses episodic memory for classification. Particularly, the encoding of the current inputfeâ€‹mâ€‹bâ€‹(x)subscriptğ‘“ğ‘’ğ‘šğ‘ğ‘¥f_{emb}(x)is used to retrievekğ‘˜knearest neighbors from the episodic memoryC=(hi,vi,wi)i=1kğ¶superscriptsubscriptsubscriptâ„ğ‘–subscriptğ‘£ğ‘–subscriptğ‘¤ğ‘–ğ‘–1ğ‘˜C={(h_{i},v_{i},w_{i})}_{i=1}^{k}. The weightwisubscriptğ‘¤ğ‘–w_{i}measures the closeness of the example to thefeâ€‹mâ€‹bâ€‹(x)subscriptğ‘“ğ‘’ğ‘šğ‘ğ‘¥f_{emb}(x)and is defined using the kernel function:

The local adaptation component corresponds to adapting the output parameterswğ‘¤wto minimize the weighted average negative likelihood over the retrievedkğ‘˜kneighbors. Formally, the update rule is defined as:

Notice that the episodic memory contains keys from the embedding network at different points during the training.de2019episodicargues that this results in the embedding network from drifting over time, and the key of the test examples is closer to that of the recently seen examples. To circumvent this issue, they suggest freezing the embedding network. Given the recent surge of generic pre-trained models,de2019episodicinitializes their embedding network with pre-trained transformer-based BERT model[devlin2018bert]for lifelong language learning.

As MbPA based approaches locally adapt the model at test-time,wang2020efficientMLargue that this results in train and test-time discrepancy as the model is never locally updated during train time. This discrepancy results in negative transfer when locally updated models are evaluated on test examples from the last task. To address this problem,wang2020efficientMLproposes an efficient meta-lifelong learning framework, Meta-MbPA, by recasting the local adaptation problem as learning to â€œquickly" remember using the episodic memory.

In this section, we discuss several strategies for selecting which examples to store in the episodic memory. Some of these strategies are inspired by recent neuroscience research, while others are based on statistical insights.gupta2010hippocampalsuggest that humans replay infrequent events more often than frequent ones. Particularly, infrequent events deemed to be surprising[cheng2008new,mcnamara2014dopaminergic]or rewarding[atherton2015memory,olafsdottir2015hippocampal]. On the other hand, statistical strategies promote matching the data distribution of all tasks with that of the episodic memory[bickel2008transfer,Rebuffi_2017]. Some suggest[aljundi2019gradient,wang2020efficientML]to maximize the coverage of the episodic memory by selecting diverse examples. Alternatively,wang2020efficientMLprovide active learning inspired view of sample selection strategies: a diversity-based method that picks the most representative examples and an uncertainty-based method that picks the most unsure examples (surprise[ramalho2018adaptive], forgettable[toneva2018an]). We will discuss a subset of these strategies in this section.

Rebuffi_2017propose to selectmğ‘šmexamples per class by iteratively selecting the examples that best approximate the average feature vector over allnğ‘›ntraining examples. This iterative example selection is called herding[welling2009herding]and works in an offline manner, i.e., after training the model for the given class c. Due to the offline iterative selection strategy, resulting examples constitute a representative set of samples from a distribution. Thus, this strategy strives to match the distribution of examples with that of the episodic memory at the individual class level. Letfeâ€‹mâ€‹b:ğ’³â†’Rd:subscriptğ‘“ğ‘’ğ‘šğ‘â†’ğ’³superscriptğ‘…ğ‘‘f_{emb}:\mathcal{X}\rightarrow R^{d}be a feature extractor,Dc={x1,â‹¯,xn}subscriptğ·ğ‘subscriptğ‘¥1â‹¯subscriptğ‘¥ğ‘›D_{c}=\{x_{1},\cdots,x_{n}\}be thenğ‘›nexamples corresponding toctâ€‹hsuperscriptğ‘ğ‘¡â„c^{th}class.

cheng2008new,mcnamara2014dopaminergicdiscuss that replay in rodents is connected to unexpected events and based upon this inspiration,isele2018selectivepropose a surprise criterion for sampling transitions in incremental reinforcement learning. On the other hand,ramalho2018adaptivepropose an algorithm to approximate the task distribution based upon the surprising examples encountered during training. Formally, surprise for an example is computed using the modelâ€™s prediction asS=âˆ’logâ¡(yt)ğ‘†subscriptğ‘¦ğ‘¡S=-\log(y_{t}). Intuitively, the higher the probability that the model assigns to the true labelytsubscriptğ‘¦ğ‘¡y_{t}, the less surprising that example is. Further,wang2020efficientMLinvestigate this approach in the context of lifelong language learning by viewing it as one of the uncertainty-based sample selection strategies.

Similar to surprise, some other neuroscience studies[atherton2015memory,olafsdottir2015hippocampal]suggest that rewarding events are often associated with the replay. Therefore, in the context of incremental reinforcement learning,isele2018selectivepropose a reward-based sample selection strategy. Concretely, the absolute value of the future discounted return is used to select the rewarding experiences,â„›â€‹(ei)=|Riâ€‹(ei)|â„›subscriptğ‘’ğ‘–subscriptğ‘…ğ‘–subscriptğ‘’ğ‘–\mathcal{R}(e_{i})=|R_{i}(e_{i})|.

isele2018selective,wang2020efficientMLargue that when the memory buffer is limited in size, it is helpful to sample diverse examples to maximize coverage of the underlying data distribution.isele2018selectivepropose to sample by ranking experiences based upon the number of neighbors in the episodic memory. The experience with the most number of neighbors is selected for replacements. Similarly,wang2020efficientMLleverage a pre-trained feature extractor for estimating the diversity of the sampled examples. Intuitively, for a given example, if there are nearest neighbors in the episodic buffer, that particular example is less diverse and sampled rarely (low probability). Concretely, given a feature extractorfeâ€‹mâ€‹bsubscriptğ‘“ğ‘’ğ‘šğ‘f_{emb}, episodic memory moduleâ„³â„³\mathcal{M}, the probability for selecting examplexâ€²superscriptğ‘¥â€²x^{\prime}is defined as follows:

isele2018selectiveargue that the best strategy for sampling is the one that matches the distribution of the episodic buffer with that of the global train/test distribution over all tasks. In lifelong learning, we see online streams of data, and the global distribution is not known in advance. Therefore, most of the recent works resort to reservoir sampling[vitter1985random]. Given an input stream with unknown length,mğ‘šmto be the maximum capacity of the buffer, andnğ‘›nto be the number of examples observed so far, reservoir sampling picks examples with the probabilitym/nğ‘šğ‘›m/n.

wang2020efficientMLcompares a representative set of the above-discussed strategies and finds that diversity-based sample selection strategies outperform uncertainty-based selection strategies. Notice that reservoir sampling can be viewed as a diversity-based method since it picks examples representing the true data distribution.

All of the above sample selection methods make an implicit or explicit assumption about the availability of task boundary. However, we might not have access to the information in some scenarios when a particular task changes. Motivated by this scenario,aljundi2019gradientdevelop a gradient-based sample selection strategy to populate the replay buffer without any knowledge about the underlying task identity. Specifically, sample selection is formulated as a constraint reduction problem based on a constrained optimization view of the continual learning (see Section4.1.2for original formulation,Eq.4.6andEq.4.7). From the original constraints in the gradient space (Eq.4.7),aljundi2019gradientpropose selecting examples so that the feasible region formed by the constraints corresponding to the selected subset of examples is close to that of the original region. Given previoustğ‘¡ttasks ([0,â‹¯,tâˆ’1]0â‹¯ğ‘¡1[0,\cdots,t-1]), the original feasible region (Cğ¶C) and the reduced feasible region (C~~ğ¶\tilde{C}) corresponding to the memoryâ„³â„³\mathcal{M}are defined as follows:

Asâ„³â„³\mathcal{M}is a subset of the previous tasks examples, the reduced feasible regionC~~ğ¶\tilde{C}is infact larger than the original regionCğ¶C(the number of examples corresponds to the number of constraints defining the feasible region). Therefore, finding the smallestC~~ğ¶\tilde{C}suffices the criterion thatC~~ğ¶\tilde{C}is close toCğ¶C. To define the notion of closeness, the size of the feasible region (convex cone) is defined in terms of the solid angle between the cone and the unit sphere. Moreover, the number of constraints (gradients) is smaller than the dimension of the gradient. Therefore, the feasible region and the solid angle can be defined inMğ‘€M-dimensional spacesâ€‹pâ€‹aâ€‹nâ€‹(â„³)ğ‘ ğ‘ğ‘ğ‘›â„³span(\mathcal{M}). Thus, the sample selection objective is defined as follows:

whereM=|â„³|ğ‘€â„³M=|\mathcal{M}|,SMâˆ’1sâ€‹pâ€‹aâ€‹nâ€‹(â„³)superscriptsubscriptğ‘†ğ‘€1ğ‘ ğ‘ğ‘ğ‘›â„³S_{M-1}^{span(\mathcal{M})}denotes a unit sphere inMâˆ’1ğ‘€1M-1dimensional space, andÎ»Mâˆ’1subscriptğœ†ğ‘€1\lambda_{M-1}is Lebesgue measure. As the above optimization problem is hard to minimize,aljundi2019gradientpropose a surrogate toEq.4.24. Based on the observation that one can reduce the feasible region by increasing the angle between each pair of gradients, the surrogate objective for sample selection is defined as follows:

Interestingly,aljundi2019gradientshow that minimizing the above objective corresponds to maximizing the variance of the gradient direction,Vâ€‹aâ€‹râ€‹[g^]ğ‘‰ğ‘ğ‘Ÿdelimited-[]^ğ‘”Var[\hat{g}], whereg^^ğ‘”\hat{g}is a unit vector.

Previously, we looked at a diversity-based sample selection strategy where diversity is defined in terms of the hidden representations as features. Gradient-based sample selection surrogates can be interpreted as selecting diverse examples based on gradients as features.

Instead of analyzing the model performance under distributional shift,toneva2018aninvestigate the learning dynamics of neural networks on a single task. Specifically,toneva2018andefine the occurrence of a forgetting event when the model transitions from correct to incorrect classification for individual training examples. They report that different examples are forgotten at different frequencies, and removing a significant fraction of least forgettable examples from training data still results in competitive performance. On the other hand, forgettable examples have uncommon features and are difficult to classify. Inspired by these findings,wang2020efficientMLstudies the effectiveness of forgettable examples for replay by considering it as one of the uncertainty-based sample selection strategies.

The forgettable examples are sampled from the actual observations in the above method. On the other hand,chaudhry2021usingpropose to explicitly construct pseudo examples/anchors such that the anchors undergo maximum forgetting after training on future tasks. Formally, given current tasktğ‘¡t, the desirable anchoratsubscriptğ‘ğ‘¡a_{t}with labelytsubscriptğ‘¦ğ‘¡y_{t}can be obtained by maximizing the following loss:

whereÎ¸Tsubscriptğœƒğ‘‡\theta_{T}is the model after training on the future taskT(>t)annotatedğ‘‡absentğ‘¡T(>t). However, for the above optimization problem, one requires access to the entire distributionPtsubscriptğ‘ƒğ‘¡P_{t}and future tasks. To avoid storing the entire dataset for estimatingPtsubscriptğ‘ƒğ‘¡P_{t},chaudhry2021usingmaintain the running average of the mean feature embeddingfeâ€‹mâ€‹btsuperscriptsubscriptğ‘“ğ‘’ğ‘šğ‘ğ‘¡f_{emb}^{t}as follows:

As we do not have access to future tasks,chaudhry2021usingsuggest approximating the future by simulating the past, i.e., evaluate forgetting of the current task after fine-tuning on the past tasks. Hence, the modified objective to learn maximal forgettable anchor is defined as:

As the above method evaluates forgetting in hindsight, the method is called hindsight anchor learning.

As mentioned earlier, the CLS theory[mcclelland1995there,o2002hippocampal]proposes that human memory consists of dual complementary systems: one for gradual accumulation of the structured knowledge (neocortex) and another one for rapid encoding of the current inputs (hippocampus). Moreover, the hippocampal system reactivates the memory trace during sleep[stickgold2007sleep]for the long-term memory consolidation in the neocortex with the help of multiple replays of the encoded experiences. In line with this mechanism, the memory-based approaches retain examples from past tasks for replaying them to alleviate forgetting. Further, there are pieces of evidence[stickgold2007sleep,ramirez2013creating]that the hippocampal system also generates false memory experiences while replaying, thus, performing more than a naive replay. Based upon these studies,shin2017continualargue that generative models are better conceptualizations of the hippocampal system than the replay buffer.
Further, one of the issues with simply replaying of examples from past tasks is that it requires a large memory, which is often unrealistic in real-world applications where access to the past tasksâ€™ data is limited (privacy concerns). By considering the generative models of the data, one can generate pseudo-data for experience replay, thus, relaxing the need to retain the actual examples. In this primer, we discuss two canonical works along this line, (1)shin2017continualpropose a deep generative replay framework with a generative adversarial network (GAN) to mimic the past data and studies the problem on image classification tasks, and (2)sun2020lamolintroduce a language model that simultaneously learns to solve the task and generate pseudo-samples of previous NLP tasks.

Generative models learn to generate realistic samples by maximizing the likelihood of generated samples being in a given data distribution. GAN is one such kind of generative model that defines a zero-sum game between a generator network (G) and a discriminator network (D). The discriminator learns to distinguish between the real and the generated samples, while the generator learns to mimic the given data distribution so that it can fool the discriminator. Formally, given the real data distributionpdâ€‹aâ€‹tâ€‹asubscriptğ‘ğ‘‘ğ‘ğ‘¡ğ‘p_{data}, the overall objective for both the networks is defined as follows:

Based upon the above GAN framework,shin2017continualpropose a scholar model H consisting of a generator G and a solver S. Given a new tasktğ‘¡t, a scholar modelHtsubscriptğ»ğ‘¡H_{t}is trained in two stages using the tasktğ‘¡tâ€™s data and a previous scholar modelHtâˆ’1subscriptğ»ğ‘¡1H_{t-1}. During the first stage, a generatorGtsubscriptğºğ‘¡G_{t}learns to reconstruct the current data (xâˆ¼ptsimilar-toğ‘¥subscriptğ‘ğ‘¡x\sim p_{t}) and the past data (pseudo-samples fromGtâˆ’1subscriptğºğ‘¡1G_{t-1}). In the next stage, a solverStsubscriptğ‘†ğ‘¡S_{t}learns to solve the given tasktğ‘¡twhile remembering the previous tasks (pseudo-labelsy^=Stâˆ’1â€‹(x)^ğ‘¦subscriptğ‘†ğ‘¡1ğ‘¥\hat{y}=S_{t-1}(x)). The overall objective for the scholar modelHtsubscriptğ»ğ‘¡H_{t}is as follows:

whererğ‘Ÿris the mixing coefficient for the two objectives.

Another approach[van2020brain], motivated by anatomy, modifies standard generative replay by merging the generator into the main model. This allows replaying the hidden representations that are generated by the modelâ€™s context-modulated feedback connections.

mccann2018naturalshow that multiple NLP tasks can be cast to a unified question-answering task, thereby enabling the use of a single language model (LM) to solve multiple tasks, i.e., given the context and question, the language model generates an answer. Based upon this observation,sun2020lamolexplore language modeling for lifelong language learning (LAMOL). Fundamentally, LM is inherently a text generator and can learn to generate samples from previous tasks. Taking inspiration from the deep generative replay,sun2020lamolpropose to continuously train a pre-trained language model that simultaneously answers the questions and generates pseudo-samples of the previous tasks.

Although the generative replay-based approaches learn a single model without retaining old task examples, their performance is strictly upper bounded by the replay-based approaches that retain at least a few examples in the buffer[sun2020lamol]. There are open questions around the scalability of the generator with the number of tasks and potential conflicts between the generator and downstream tasks due to fixed shared model capacity.

In this chapter, we presented the unified view of memory-based methods and algorithms in lifelong learning. These methods maintain an episodic memory, containing a few examples from past tasks, and revisit it while learning a new task. We saw in regularization-based methods that different ways to penalize drastic changes in the model parameters were employed and incorporated into the overall objective. Along similar lines, memory-based methods are realizations of three primary strategies combined: 1) how to sample examples from memory, 2) how to update the model with current task loss along with the replay memory loss, and 3) how to select examples to write to the memory.

Most memory-based methods involve defining specific model update strategies using episodic memory for training the model on each new task. In addition, we also presented approaches that use episodic memory during test time for evaluation to prevent catastrophic forgetting.

Next, we presented numerous read/write sampling strategies employed by lifelong learning researchers. Several statistical sample selection strategies are inspired by areas like CLS theory, reinforcement learning, neuroscience and can be classified as diversity-based and uncertainty-based.

While memory-based methods retain examples from past tasks for replaying, generative replay methods avoid storing the examples. Instead, these methods take inspiration from neuroscience and the anatomy of the human brain and generate pseudo-data for experience replay. These methods learn a single model to generate replay data that mimic the actual examples.

So far, we have presented lifelong learning methods that assumed a fixed capacity of the ML model. In the next chapter, we will discuss methods based on isolating task-specific parts of the model and even modifying its architecture to avoid interference for training on diverse tasks.

In this chapter, we will study the different architecture families (and their instantiations) that have been proposed for training lifelong learning systems. Here,architecturerefers to the overall structure of a training system, andarchitecture familyrefers to a family of related architectures. For example, ResNet18, ResNet32, ResNet50, ResNet152[he2016deep], and WideResNet32[xie2017aggregated]are architectures that belong to the family of residual networks. We organize this chapter in terms of the different architectural families.

Some of these architectures are general-purpose and can be used with different settings (for example, class-incremental or task-incremental), modalities (for example, computer vision or natural language), tasks (for example, classification or regression), and datasets. Other architectures are more specialized and useful in specific setups and scenarios. Essentially, the less general approaches make additional assumptions about the setup. If the assumptions hold for the given setup, these approaches are expected to perform better than the general architectures. The choice between general-purpose and specialized architectures is often driven by how much information we have about the setup; the more information we have, the more specialized architecture can be used. We will start the discussion with the general-purpose architectures and introduce the specialized architectures along the way.

Architecture-based approaches can be seen as a mechanism to provide useful inductive biases to the learning system. For example, when the system is trained on a sequence of closely related tasks, it may be helpful to infer the changes across the tasks as new tasks are encountered. For example, the first task could be to â€œpick up a ball" and the second task could be to â€œpick up a cube". Knowing â€œwhat changes across tasks" enables the use of knowledge (from previous tasks) to train the system for the incoming task. The underlying idea is that since we have some additional information about the problem setup (for example, the tasks are closely related), we can design an architecture that can leverage the extra information. We note that while we are focusing on architectures in this chapter, in practice, these architectures are often used in conjunction with other approaches such as regularization-based methods like elastic weight consolidation (as discussed inChapter3) or memory-based methods like experience replay (as discussed inChapter4). Despite the complementary nature of architecture-based approaches, we study them in a separate chapter to understand the common principles and motivations behind these architectures while abstracting away some details like the different replay mechanisms available. The rest of the chapter is organized as follows: We start with Modular Networks, the motivation behind their use, general architecture design, some common manifestations, and limitations. Next, we discuss the parameter isolation systems, which includes both fixed-capacity approaches like masking and pruning and dynamics-capacity approaches[yoon2018lifelong]. We conclude with a discussion on some recently proposed approaches for lifelong learning on graphs. We include these approaches in this chapter since these approaches rely on the inductive biases specific to learning problems in the context of graphs.

Much of the recent success of the machine learning models are limited to thesingle-tasksetups where the data distribution is well-defined and known beforehand. We know that as the system is trained on new tasks, its performance on the previous tasks deteriorates (due to catastrophic forgetting). However, there are several challenges even before a new task is encountered. If the data distribution changes between training and evaluation, the learning systemâ€™s performance often diminishes drastically[zhang2018natural,zhang2020learning,cobbe2020leveraging]suggesting that the system is not able to adapt to even small variations in the data distribution. This behavior is in stark contrast to how humans learn and operate. Not only are humans morerobustlearners, they are much moresample efficientand can quickly adapt to new tasks/data distributions.

Several hypotheses have been proposed to explain the discrepancy between the learning behavior of humans vs. machine learning systems. One of the more popular hypotheses is that the world is inherently compositional, i.e., the representation of thewholeis composed of the representation of thepartsand the humans exploit this compositionality to understand and operate in the world[pmlr-v80-parascandolo18a]. This compositionality also implies that anoveltask can be broken down intopartsthat we have already encountered in the previous tasks (in a different manifestation). For example, when reading a sentence, we break it down into phrases and words and derive meaning from it (even if we have never seen the same sentence before). Essentially, we exploit the compositionality of the world by learning modular, reusable, and general-purpose mechanisms[goyal2021recurrent](or skills). These mechanisms can be shared across different tasks, thus making learning in humans more efficient than learning in neural networks. Since leveraging compositionality is useful for humans, it may be a useful inductive bias for developing lifelong learning systems that operate in the real world and make decisions over extended periods.

Another example for such a case is given inFig.5.1. To reach the goal (the gift box) in (e), the robot will have to learn to solve the first four tasks that involve walls (b), locked doors (c) and their composition (d). When the robot solves these relatively easier tasks, it can exploit the gained knowledge and reuse it to solve the final much harder task quickly.

[å›¾ç‰‡: images\image_9.png]
å›¾ç‰‡è¯´æ˜: Figure 5.1:Example of compositionality

Modular Networksare proposed as one of the promising direction to learn systems that can effectively leverage compositionality to learn more efficiently[happel1994design-and-evolution-of-modular-neural-network,1996_on_combining_artificial_neural_nets,sharkey1997modularity-combining-and-artificial-neural-networks,1998_modular_neural_network_classifiers_a_comparative_study,1999_modularity_in_neural_computing,1999_modular_neural_networks_a_survey,andreas2016-neural-module-networks,andreas2016learning-to-compose-neural-networks-for-question-answering,johnson2017-clevr-a-diagnostic-dataset-for-compositional-language-and-elementary-visual-reasoning,santoro2017simple-neural-network-module-for-relational-reasoning,yu2018mattnet-modula-attention-network-for-referring-expression-comprehension,alet2018modular-meta-learning-in-abstract-graph-neural-networks-for-combanitorial-reasoning,alet2018modular-meta-learning]. Modular Networks incorporatemodularityas the primary inductive bias. Modularity is the property of a system that it can be broken down into several relatively independent, replicable, and composablemodules(or smaller networks)[2019_a_review_of_modularization_techniques_in_artificial_neural_networks]. Each module can be thought of as learning to solve asubtask(or part of a given task). In the context of compositional world hypothesis, a modular network can solve a given task byÂ (i) breaking it into subtasks,Â (ii) using modules to solve the subtasks, andÂ (iii) using the solutions of the subtasks to solve the given task. Thus modular networks can also be interpreted as factorizing knowledge into different modules. Some of the underlying subtasks may change when the task/data distribution changes, even though the high-level task may remain the same. In such a case, if the knowledge is appropriately factorized, only some modules will need to adapt/change (to account for the change in some subtasks), and the other modules can be used as-is. In practice, this would result in faster adaptation to the new distribution[bengio2019meta]. For example, consider an system that is trained to â€œpick up a cup from a table". The system could learn two modules - one forreaching the tableand the other forpicking the cup. If the task changes such that the height of the table is increased, then the system only needs to update the module corresponding topicking the cupas the subtask of reaching the table is not changed.

The benefits of modularity can be easily extended to lifelong learning. As an system trains over a distribution of tasks, it could decompose the tasks into subtasks that are shared across the task distribution. When the system encounters a new task, it could break the task into a combination of novel and previously seen tasks. In that case, the system only needs to learn the novel subtasks, instead of learning the new task from scratch. Building upon the previous example of â€œpick up a cup from a table", the next task could be â€œplace a knife on the shelf". Both these tasks require the ability to â€œmove around". If the system has learned a module for â€œmoving around" (as part of the first task), it can use that module in the second task as well, thus enabling the positive forward transfer of knowledge. Moreover, since the module for â€œmoving around" may also be improved while training on the second task, it can potentially lead to a positive backward transfer of knowledge where training on the second task improves the performance on the first task.

We note that similar to modular networks, several others areas like Out-of-Distribution (OOD) generalization[wang2021generalizing], zero-shot generalization[purushwalkam2019task], few-shot generalization, etc., also focus on narrowing the gap in the performance of humans and machine learning systems.

The use of modular networks for lifelong learning can be motivated from various other perspectives as well.

Cognitive Science Perspective:spelke1990principles,pinker1994language,pinker2005so,spelke2007core,xu2009inductionhypothesized several theories to explain how humans learn coherent, abstract, and highly structured representations of the world from the fragmented but concrete instances ofexperiences. The â€œtheory theoryâ€[carey1985conceptual,gopnik1988conceptual,wellman1992cognitive]states that as humans interact in the world, they construct intuitive theories of the world. These theories have three key aspects: (i) They involve coherent, abstract, causal representations of the world. (ii) They have distinct cognitive functions. For example, theories enable both prediction of the future as well as counterfactual inferences. (iii) They have distinctive dynamic features. For example, the theories can be updated as humans undergo novel experiences and discover new knowledge. These different theories aim to explain the interplay between abstract knowledge and concrete knowledge. In the modular networks, the network topology can be seen as a manifestation of the abstract knowledge, and the modules can be seen as encapsulating the concrete knowledge.

Evolutionary Perspective:2005_spontaneous_evolution_of_modularity_and_network_motifs,2007_varying_environments_can_speed_up_evolutionhypothesized that environments with Modularly Varying Goals (MVGs), i.e., environment consisting of varying goals with common subgoals, leads to modular networks.2013_the_evolutionary_origins_of_modularityhypothesized that modularity evolves as a byproduct from selection to reduce connection costs (like creating, sustaining connections) in a network. This modularity is then sustained by the Modularly Varying Goals (MVGs). A lifelong learning system is expected to learn (and retain the knowledge of) a series of tasks over its lifetime. Modularity can be a useful inductive bias for the neural network if these tasks share some common substructure.

Empirical Perspective: From a practical perspective, a modular neural network can be interpreted as a system of modules where each module is designed to solve one specific task, and the controller learns the mapping between the tasks and the modules. If multiple tasks share a common subtask, the modules corresponding to these tasks can share knowledge with each other. When new tasks are encountered, the network will eventually run out of capacity, a problem referred to ascapacity saturation[sodhani2020toward]. Modular neural networks provide a workaround for that by enabling the addition of new modules that can be added to the system, without disrupting the existing modules (and the knowledge encoded by them). Modularity also helps with catastrophic forgetting by localizing the forgetting effect, i.e., forgetting knowledge about a task should only affect the modules related to that task and not the other modules.

The high-level architecture of modular neural network can be described in terms of the following two components:

A system ofnğ‘›nmodules denoted asM={miâ€‹âˆ€iâˆˆ{1,â‹¯â€‹n}}ğ‘€subscriptğ‘šğ‘–for-allğ‘–1â‹¯ğ‘›M=\{m_{i}\forall i\in\{1,\cdots n\}\}.

Acontrollermechanism that is used to decide the topology of connection between the modules.

We note that different works use different terminology for describing the architecture of neural modular networks. For example, some works denote the modules asexperts[2017_encoder_based_lifelong_learning,2017_expert_gate_lifelong_learning_with_a_network_of_experts]or asprimitives[MLSH,Goyal2020Reinforcement]etc. Similarly, some works refer to thecontrolleras thegating mechanism[2017_encoder_based_lifelong_learning,2017_expert_gate_lifelong_learning_with_a_network_of_experts]or asrouter[rosenbaum2017routing]. While these different works have subtle differences in terms of how modules/controller are instantiated (or interact), the terminology we use here suffices to study them from the perspective of lifelong learning.

The first key component of modular networks is the system ofnğ‘›nmodulesMğ‘€M. Each modulemiâ€‹(âˆ€iâˆˆ{1â€‹â‹¯â€‹n})subscriptğ‘šğ‘–for-allğ‘–1â‹¯ğ‘›m_{i}(\forall i\in\{1\cdots n\})is a neural network with parameters denoted asÎ¸isubscriptğœƒğ‘–\theta_{i}. In terms of the network architecture, the modules may be identical (for example, a collection of ResNet50 models) or similar (for example, a collection of models from the ResNet family) or different (for example, some modules from the ResNet family and some modules from the VGG family). In the case of modules belonging to different architectures, the modules may also operate on different modalities. In terms of functional representation, the modules may be explicitly trained to encode different aspects of the input (maybe by providing direct supervision), or the modules may learn different representations on their own. Some modules may also share parameters with other modules (we denote the shared parameters of theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}module asÎ¸isâ€‹hâ€‹aâ€‹râ€‹eâ€‹dsuperscriptsubscriptğœƒğ‘–ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘\theta_{i}^{shared}). An example of a system of modules is shown inFig.5.2.

[å›¾ç‰‡: images\image_10.png]
å›¾ç‰‡è¯´æ˜: Figure 5.2:Different kind of modules in Modular Architectures

The second key component of modular networks is the controller mechanism. A controller is any functionCğ¶Cthat defines a network topology using the system of modules. The input to the controller mechanism could include: input data points, metadata in the form of the task description, output from the system of modules, or a combination of these. Note that this description of the controller includes both implicit and explicit controllers (parameterized as well as non-parameterized). The output of the controller is a network topologyzğ‘§z. This topology defines thearrangementof (or interaction between) modules, thus instantiating a functionfzsubscriptğ‘“ğ‘§f_{z}as shown inEq.5.2. Note that all the modules are not required to be part of the network topology. Moreover, the controller may not have to learn the entire topology as some parts of the topology may be pre-determined. The controller may generate the entire network topology at once[2017_expert_gate_lifelong_learning_with_a_network_of_experts]or step by step (as done ingoyal2021recurrent).

Several training mechanisms have been proposed for training Neural Modular Networks. These mechanisms vary in terms of the following dimensions:

Should the modules and the controller be trained jointly, in an end-to-end manner (as done in for examplechang2018automatically) or have separate losses for the two components. Should the controller be meta-trained?

Is the network layoutzğ‘§za discrete variable, like a hard-attention mask (or graph with 0-1 edges), or is the network layout represented as a continuous variable, like a soft-attention mask (or graph with soft-edges).

Is the controller mechanism explicit or implicit?

Is a per-module loss available?

The standard, monolithic, deep learning architectures (like ResNets) can be seen as a special form of modular networks. The layers (of the monolithic network) can be seen as the modules, and the controller mechanism is the pre-determined and hardcoded topology where the output of one layer feeds into the next layer. In this sense, the process of learning modules is similar to the process of training layers in a neural network.

Modular Networks represent a very general and flexible family of neural network architectures. They provide several benefits like the ability to use different model architectures as constituent modules and ability to add new modules throughout training. Modular neural networks are also biologically inspired[2015_the_modular_and_integrative_functional_architecture_of_the_human_brain]and the brain has been shown to be modular at different spatial scales, from the micro level of synapses to the macro level of brain regions[2002_revealing_modular_organization_in_the_yeast_transcriptional_network,2006_modularity_and_community_structure_in_networks,2008revealing_modular_architecture_of_human_brain_structural_networks_by_using_cortical_thickness_from_MRI,2007_the_road_to_modularity,2009complex_brain_networks_graph_theoretical_analysis_of_structural_and_functional_systems,2010efficient_physical_embedding_of_topologically_complex_information_processing_networks_in_brains_and_computer_circuits,2010modular_and_hierarchically_modular_organization_of_brain_networks]. In the context of deep learning, Modular Network initially focused on visual questions answering[andreas2016-neural-module-networks]. Since then, they have been extended to several settings like multitask learning[zhang2020multi,sodhani2021multi], compositional generalization[Goyal2020Reinforcement,goyal2021recurrent,chang2018automatically], etc. Different applications of modular networks, in context of lifelong learning, differs in terms of what the modules learn (or should learn).

Several works[MLSH,chang2018automatically,feudal_networks_for_hierarchical_reinforcement_learning,Goyal2020Reinforcement]use the modules to learn composableskills. The general idea is the following: When a model is trained on a given task, the modellearnsskills that it composes to solve the given task. The benefit of learning skills is that skills can be selectively transferred across tasks. In this case, the controller mechanism should help to ensure that modules learn a diverse set of skills and the model learns to solve a task by choosing and composing a subset of modules (skills).

Using modules for learning skills has an additional benefit: A new task may require learning new skills. In that case, new modules can be instantiated and trained on the given task (and used for the subsequent tasks). Some common challenges in this setup are:Â (i) learning a diverse set of skills,Â (ii) not forgetting the previously acquired skilled (in principle, this can be easily achieved by not finetuning a module on subsequent tasks, but this may force the model to learn very similar skills),Â (iii) controlling the growth of the number of modules (if the tasks share the skills, the number of modules should grow sub-linearly with the number of tasks).

We explain the general design and implementation of works in this category usingveniat2021efficientthat proposed a neural modular network architecture for lifelong learning where each modulemisubscriptğ‘šğ‘–m_{i}represents a composable, atomic skill. The model solves a task by transforming the given task to a new task: searching through an exponentially large search space (of the composition of modules) and inferring a composition that enables the model to solve the given task. While modules are shared across tasks (to enable knowledge transfer), modules corresponding to the older tasks are not updated for the newer tasks, thus avoiding catastrophic forgetting. New modules are added only when new skills are required. If tasks share skills, the module grows sub-linearly with the number of tasks.

At the start of training, the model is initialized as a collection oflğ‘™lmodules, arranged inlğ‘™llayers with one module per layer. The modules can be different across the layers. During training, new modules could be added across the layers, with the constraint that all the modules in a layer are of the same architecture (but can learn different weights). The setup can be explained with an example where the model has been trained ontâˆ’1ğ‘¡1t-1tasks. Now, when thettâ€‹hsuperscriptğ‘¡ğ‘¡â„t^{th}task arrives, a new randomly initialized module is added to each layer, and a search space is defined over all the possible ways to combine the modules (both old and new).

The resulting search space is exponentially large, and several constraints are imposed to keep the search tractable:

Modules within the same layer do not connect with each other, so only one module can be selected from each layer.

A newly added module, say at theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}layer, can only connect to another newly added module at thei+1tâ€‹hğ‘–superscript1ğ‘¡â„i+1^{th}.

The second restriction is motivated by the need to reduce the size of the otherwise exponentially large search space. In practice, this restriction can be justified as follows: as new tasks are added, changes are expected in the output distribution and not the input distribution. If the tasks are related, initial layers can be shared across the tasks.

The training objective is to minimize the loss corresponding to thettâ€‹hsuperscriptğ‘¡ğ‘¡â„t^{th}task, as a function of the parameters (of all the modules) and connection between the modules. This connection is essentially a path in the grid of modules. The connection of thettâ€‹hsuperscriptğ‘¡ğ‘¡â„t^{th}task is denoted asÏ€tsubscriptğœ‹ğ‘¡\pi_{t}and the parameters of the modules, that are part of this path, are denoted asÎ¸â€‹(Ï€t)ğœƒsubscriptğœ‹ğ‘¡\theta(\pi_{t}). The model optimizes the loss function:

Hereğ’®ğ’®\mathcal{S}is the set of tasks model has seen so far,x,yğ‘¥ğ‘¦x,ydenote the input and the target label for thettâ€‹hsuperscriptğ‘¡ğ‘¡â„t^{th}task,fâ€‹(x,t|ğ’®,Î¸â€‹(Ï€j))ğ‘“ğ‘¥conditionalğ‘¡ğ’®ğœƒsubscriptğœ‹ğ‘—f(x,t|\mathcal{S},\theta(\pi_{j}))is an instance of modular network using the pathÏ€jsubscriptğœ‹ğ‘—\pi_{j},â„’â„’\mathcal{L}is the loss function andÎ“Î“\Gammais a distribution over the set of possible paths. Other works have considered different approaches for selecting a path. For example2019_an_adaptive_random_path_selection_approach_for_incremental_learningrandomly select a path connecting the modules and keeps using that path until it saturates (i.e., the parameters have been fully used for learning on the previous tasks), while2017_pathnet_evolution_channels_gradient_descent_in_super_neural_networksuse genetic algorithms to select a path connecting the modules.

Note that only the newly added modules can be trained, and the parameters of the previously added modules remain unchanged. At the end of training on thettâ€‹hsuperscriptğ‘¡ğ‘¡â„t^{th}task, any new module, that does not appear on the optimal path, is not retained for the subsequent tasks, thus keeping the modelâ€™s growth sublinear in the number of tasks. The resulting path is saved for the given task and can be retrieved during testing.

There are two approaches for optimizing the loss inEq.5.3. In the first approach, called the stochastic variant, the model alternates between optimizing the path and optimizing the parameters of a given path. In terms of general modular network architecture, this corresponds to alternatively optimizing the controller mechanism and the modules. Specifically, the distributionÎ“Î“\Gammais modeled by a product of multinomial distributions, one for each layer of the model. The modules are selectedone-layer-at-a-time. An entropy regularizer is used to encourage the model to explore different paths. The second approach, called the deterministic variant, is more straightforward - an exhaustive search is performed over the set of paths.

veniat2021efficientuses adata-driven priorwhich works as follows: First,kğ‘˜ktasks, which are most similar to the given task, are selected from the set of previous tasks. The search space for the current task is limited to the perturbations of the paths corresponding to these previous tasks. The most similar previous tasks are chosen by computing the predictions on the current task data, using paths from all the previous tasks (specifically, using the feature from the penultimate layer). The paths that yield the best nearest neighbor classification accuracy are selected. Several other mechanisms are also used to restrict the search space over paths further. For exampleandreas2016-neural-module-networksselects only one module at a time (instead of selecting an arbitrary number of modules at any time), whilegoyal2021recurrentattends to all the modules.

There are two key limitations to existing approaches for training modular neural networks which limit their usability in lifelong learning applications:

Selecting the right level ofmodularity: One big open problem is how to enable the models to learn modularity at the right level of abstraction. If the modules learn very high-level skills, then the modules will become task-specific, while if the modules learn very low-level skills, a large number of modules need to be trained.

Learning a diverse set of modules: Another big challenge is how to train the modules so that they learndiverseskills and do notcollapseto the same skill.

Due to these limitations, several works use hand-crafted modules, i.e., they train the modules to learn specific skills. While this approach may work reasonably well for the single-task setup, scaling it to lifelong learning (or even multi-task) setups has been difficult.

Parameter Isolationbased approaches work on the idea that different tasks should have their own set of â€œisolated" parameters. If no two tasks share any parameters, training on any task can not cause catastrophic forgetting on the other task. The idea of â€œisolating parameters" is in direct contrast to the idea of â€œlearning composable modules" as the parameter isolation approaches focus more on avoiding catastrophic forgetting. In contrast, the modular network-based approaches focus more on knowledge transfer (both forward and backward). Even though these approaches are designed to avoid catastrophic forgetting by not sharing parameters, catastrophic forgetting happens because parameters are updated using loss from multiple tasks (and not because parameters are used to make predictions on several tasks). In practice, this means that parameters of different tasks can be used together in the forward pass but not in the backward pass, thus enabling forward transfer of knowledge, but not a backward transfer of knowledge.

However, in practice, many parameter isolation-based approaches can be described as modular neural networks with slightly different inductive biases. For example, acontroller-likemechanism can be used to select which parameters belong to which task. Despite their similarity (and close relationship with) modular architectures, we consider them as a different category because:

Unlike the modular systems, which explicitly share modules across many tasks (thereby enabling both forward and backward knowledge transfer), parameter isolation-based approaches use different parameters for each task.

Parameter isolation approaches focus on avoiding catastrophic forgetting (at the expense of backward transfer).

Often, these approaches add more parameters as new tasks are encountered. Hence the number of parameters grows asğ’ªâ€‹(n)ğ’ªğ‘›\mathcal{O}(n)wherenğ‘›nis the number of tasks. Thus one frequently encountered challenge is to reduce the memory footprint when adding new tasks. On the other hand, the common challenge for modular networks is to ensure that the knowledge is decomposed into different skills.

Parameter Isolation Systems are generally studied across two dimensions: Â (i) fixed capacity networks andÂ (ii) increasing capacity networks.

The first sub-category of parameter isolation networks consists of models with a fixed capacity (i.e., the modelâ€™s capacity does not change as it trains over subsequent tasks). The isolated set of parameters is instantiated by learning task-specificmaskswhich are used to determine which parameters will be used in the forward/backward pass for which task. These approaches trade-off catastrophic forgetting with model capacity by controlling the gradient flow through the network. Different mask-based approaches differ in terms ofÂ (i) what is masked andÂ (ii) how are the mask values computed.

In parameter isolation systems, it is not required that the parameters for a given task will always form a contiguous block of parameters (like a feedforward layer). The masking function can operate at the fine level of parameters, i.e., the masking function can generate a mask per parameter. The general idea of masking is based on previous works like2015_binaryconnect_training_deep_neural_networks_with_binary_weights_during_propagations,2016_binarized_neural_networksthat train neural networks with binary-valued weights. The basic idea is to use a masking function to binarize real-valued weights during the forward pass and update the real-valued weights using the gradients corresponding to the binarized weights. Other works[2016_dynamic_network_surgery_for_efficient_DNNs]have used implicit masking functions that use the magnitude of the weight as a criterion for masking.

2018_piggyback_adapting_a_single_network_to_multiple_tasks_by_learning_to_mask_weightsproposed to learn bit-wise binary masks for each task. These masks are used to activate/deactivate the weights of a fixed and shared backbone network (on which the masks are applied) by element-wise multiplication with the binary masks. During training, the first step is to pre-train the backbone network.2018_piggyback_adapting_a_single_network_to_multiple_tasks_by_learning_to_mask_weightsused the ImageNet dataset[deng2009imagenet]and reported that the pretrained backbone network works well across multiple tasks. When training on a given task, a mask network is used to generate real-valued weights. These weights are mapped to binary masks using a deterministic thresholding function. These masks are multiplied (elementwise) with the weights of the backbone network, to generate the task-specific weights. The entire setup is trained end to end based on ideas from network binarization[2015_binaryconnect_training_deep_neural_networks_with_binary_weights_during_propagations,2016_binarized_neural_networks_training_deep_neural_networks_with_weights_and_activations_constrained_to_plus_1_or_minus_1]and pruning[2016_dynamic_network_surgery_for_efficient_DNNs]. Once the model is trained on the given task, the masking network is discarded, and only the task-specific bitwise mask is retained. The learned masks â€œpiggyback" on the backbone network to solve a given task. One limitation of this approach is the dependence on pretraining the backbone network as the randomly initialized backbone networks perform quite poorly in practice.

Learning binary bit-masks may appear too restrictive in practice, and some works have explored the possibility of learning real-valued masks as well, although learning real-valued masks can be difficult in practice due to the possibility of forgetting the knowledge useful for the previous tasks. As such, learning real-valued masks requires designing the system carefully.

2018_overcoming_catastrophic_forgetting_with_hard_attention_to_the_taskextend the idea of learning task-specific binary masks over weights to task-specificalmost-binaryattention masks over features (or activations). The proposed method, HAT (Hard Attention to Tasks), uses attention vectors of the previous tasks to define a mask for the current task and constrain the updates to the modelâ€™s weights. The paper motivates this approach as follows:When training on a sequence of tasks, different tasks could reuse the same intermediate features, but in different ways. For example, given a dataset of birds and dogs, the first task may require the model to differentiate between birds and dogs, and the second task may require the model to differentiate between black and brown animals. In such cases, the task identifier could be a useful feature for the model to perform well on both tasks, by conditioning the network layers on the task identifier.The per-layer attention weight is computed as follows:

whereğ’‚ğ’ğ’•superscriptsubscriptğ’‚ğ’ğ’•\bm{a_{l}^{t}}is the attention vector for theltâ€‹hsuperscriptğ‘™ğ‘¡â„l^{th}layer of the model andğ’†ğ’ğ’•superscriptsubscriptğ’†ğ’ğ’•\bm{e_{l}^{t}}is the single-layer task embedding for theltâ€‹hsuperscriptğ‘™ğ‘¡â„l^{th}layer of the model, when training on thettâ€‹hsuperscriptğ‘¡ğ‘¡â„t^{th}task.Ïƒğœ\sigmais a gating function which squashes any input to the range[0,1]01[0,1]and acts as apseudo-stepfunction. A hard step function is not used to enable flow of gradients to the layers of the model.sğ‘ sdenotes a positive scaling parameter that controls thehardnessof thepseudo-stepfunction. In the case of the final layer, the attention vector is binary-hardcoded (since the final layer is a task-conditioned multi-headed output layer). After training the model onttâ€‹hsuperscriptğ‘¡ğ‘¡â„t^{th}task, a cumulative attention mask is computed by taking an element-wise max on all the previous attention vectors. i.e.,ğ’‚ğ’â‰¤ğ’•=mâ€‹aâ€‹xâ€‹(ğ’‚ğ’ğ’•,ğ’‚ğ’â‰¤ğ’•âˆ’ğŸ)superscriptsubscriptğ’‚ğ’absentğ’•ğ‘šğ‘ğ‘¥superscriptsubscriptğ’‚ğ’ğ’•superscriptsubscriptğ’‚ğ’absentğ’•1\bm{a_{l}^{\leq t}}=max(\bm{a_{l}^{t}},\bm{a_{l}^{\leq t-1}}). Since max operation is used, any feature (that was important for any of the previous tasks) gets a high attention score. When computing the gradient on thet+1tâ€‹hğ‘¡superscript1ğ‘¡â„{t+1}^{th}task, the gradientgl,iâ€‹jsubscriptğ‘”ğ‘™ğ‘–ğ‘—g_{l,ij}, corresponding to theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}output andjtâ€‹hsuperscriptğ‘—ğ‘¡â„j^{th}input units in theltâ€‹hsuperscriptğ‘™ğ‘¡â„l^{th}layer is masked using[1âˆ’mâ€‹iâ€‹nâ€‹(al,iâ‰¤t,alâˆ’1,jâ‰¤t)]delimited-[]1ğ‘šğ‘–ğ‘›superscriptsubscriptğ‘ğ‘™ğ‘–absentğ‘¡superscriptsubscriptğ‘ğ‘™1ğ‘—absentğ‘¡[1-min(a_{l,i}^{\leq t},a_{l-1,j}^{\leq t})]. This mask prevents large updates to weights that are important for the previous tasks. When computing the attention weights,sğ‘ sis annealed in an epoch as follows:

wheresmâ€‹aâ€‹xsubscriptğ‘ ğ‘šğ‘ğ‘¥s_{max}is a large positive constant (>>1much-greater-thanabsent1>>1),BğµBis the total number of batches in an epoch andbğ‘bis the number of batches seen so far in the epoch. So at the start of the epoch, all features are approximately equally likely to be activated, and as training progresses, the selection becomes more binarized. However, the gradient annealing scheme introduces some optimization challenges. Specifically, the embeddingsğ’†ğ’ğ’•superscriptsubscriptğ’†ğ’ğ’•\bm{e_{l}^{t}}do not change much during training, and gradient magnitude is weak. Hence an additional gradient compensation term is introduced to compensate the effects of the annealed sigmoid, and some intermediate values (like|sâ€‹ğ’†ğ’ğ’•|ğ‘ superscriptsubscriptğ’†ğ’ğ’•|s\bm{e_{l}^{t}}|) are clamped to obtain well-behaved gradients. For thettâ€‹hsuperscriptğ‘¡ğ‘¡â„t^{th}task, activations with a hard attention value are dedicated to that task. A sparsity constraint is introduced to reserve some model capacity for subsequent tasks by constraining the capacity spent on each task. During inference,sğ‘ sis set tosmâ€‹aâ€‹xsubscriptğ‘ ğ‘šğ‘ğ‘¥s_{max}so that the gating function behaves like a step function.

Generally,masking approachesuses the same set of masks for both forward and backward transfer. While this makes sense in practice (weights that were not used during the forward pass can not have gradients during the backward pass), it also reduces the modelâ€™s flexibility, in terms of transferring knowledge across the tasks.

2020_ternary_feature_masks_continual_learning_without_any_forgettingproposed using ternary, instead of binary, masks for each task. Similar to2018_overcoming_catastrophic_forgetting_with_hard_attention_to_the_task, these masks are applied to the features (or activations) of each layer and not to the weights. Since the number of activations tends to be smaller than the number of weight parameters, the memory overhead of2020_ternary_feature_masks_continual_learning_without_any_forgettingis lower in practice, compensating for the need to store 2x bits per feature instead of111binary bit per weight. However, unlike2018_overcoming_catastrophic_forgetting_with_hard_attention_to_the_task, the masks are binary (and not real-valued) as only binary masks can guarantee to avoidforgettingof previous tasks. Two sets of masks are learned - the masks for features that will beused(forward pass) and another set for the features that will belearned(backward pass). This is related to how freezing layers work, features used in the forward pass do not necessarily have to be used in the backward pass. Specifically, the features can be in one of the three states:used(forward pass only),learned(forward and backward pass), andunused(neither forward nor backward pass).

Generally,masking approachesdo not allow any change to the features corresponding to the previous tasks, so2020_ternary_feature_masks_continual_learning_without_any_forgettingadd task-specific feature normalization that makes the (previously learned) features more optimal use for the later tasks. However, feature normalization comes at the price of storing two extra floating point numbers per activation per task.

An alternate design choice would be to iteratively free-up parameters (via, say, network pruning) so that the subsequent tasks can use the parameters freed by the previous tasks, without having to add new parameters. PackNet[2018_packnet_adding_multiple_tasks_to_a_single_network_by_iterative_pruning]uses network pruning approach from previous works like2015_learning_both_weights_and_connections_for_efficient_neural_networks,2016_dsd_dense_sparse_dense_training_for_deep_neural_networksand proposes to sequentiallypackmultiple tasks into a single network by performing iterative pruning and network re-training. One major difference between PackNet and previous masking-based works is, all the unmasked parameters (that is, parameters that have not already been masked in the previous tasks) are used during both forward and backward passes. The mask for theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}task is computed after training is finished on theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}task. The training setup is quite straightforward. The model starts by training on the first task. After convergence, a certain percentage of weights areprunedi.e., set to00. The network is retrained on the current task (without using the pruned weights) to account for the effect of pruning. When the model is trained on the second task, the unpruned weights (used in the first task) are kept fixed, and the pruned weights (not used in the second task) are trained. After convergence, some of the weights (trained in the second task) are pruned, and the model is trained on the unpruned weights (from the second task). This process is repeated every time a new task is added. In each round of pruning, the weights (in all the convolutional and fully connected layers) are sorted by their absolute magnitude, and the lowest50%percent5050\%(or75%percent7575\%) weights are pruned. The paper reports that subsequent retraining uses half as many epochs as original training.

One important thing to note is that PackNet is evaluated in setups where up to three new tasks are added. While some of the previous works have used fewer tasks[ewc,2017_learning_without_forgetting], scaling PackNet to a large number of tasks may require some changes. For example, it is expected that at some point, the networkâ€™s capacity would have to be increased, probably using approaches like Net2Net[2015_net2net_accelerating_learning_via_knowledge_transfer,sodhani2020toward].

2017_pathnet_evolution_channels_gradient_descent_in_super_neural_networksproposed PathNet that uses evolutionary algorithms to discoverpaths(subparts of the network) to re-use for new tasks. A PathNet is a deep neural network havingLğ¿Llayers, with each layer consisting ofMğ‘€Mmodules. A module is said to beactiveif it is on the currently selected path. At most,Nğ‘N(3/4343/4in practice) modules can be active in any layer at any time. A pathway is represented by a matrix of atmostNÃ—Lğ‘ğ¿N\times Lintegers. The integers in theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}column refer to the active modules of theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}layer. The output of each layer is summed up before passing to the subsequent layers. Pathways can evolve in two ways - serially or parallelly. In theSerial Pathway EvolutionPğ‘ƒP, pathways are initialized randomly and are represented by a matrix of atmostNÃ—Lğ‘ğ¿N\times Lintegers. A binary tournament selection algorithm is used where two pathways are selected randomly and trained forTğ‘‡Tepochs. Thefitnessof a pathway is measured in terms of classification error during training. The winning path is mutated by randomly selecting some modules (on the path) and swapping them with nearby modules in the layer. In theParallel Pathway Evolution, multiple paths are trained in parallel, and as some paths are trained, they are compared with the other trained paths. The winning path overrides the losing paths, followed by a round of mutations.

After the model has learned a task, the best pathway is fixed, and its parameters are no longer updated. Modules that are not part of the best path are reinitialized. As the model trains on the next task, the same procedure (sampling random paths, comparing paths, and mutating the winner) is repeated. The paper reports that the previous best paths are active (during forward pass) in the RL experiments but not in the supervised learning experiments. The previous best paths are never used in the backward pass.

One limitation of the work is that it is evaluated on a sequence of only two tasks, which could limit its usefulness in practice.

Related to previous works on selecting a task-specificpaththrough the network,2019_random_path_selection_for_continual_learningproposed RPS-Net (Random Path Selection Network) that starts with some random candidate paths and discovers the optimal path for a given task. The network consists ofLğ¿Ldistinct layers, where each layer has a set ofMğ‘€Mmodules, stacked in parallel, along with a skip connection. During training, path selection is performed for everyJğ½Jtask. During path selection,Nğ‘Npaths are randomly chosen and followed by the training process. The best path is then used for the nextJğ½Jtasks. Since the previously selected paths are fixed, the computation remains bounded, as at most one module for each of theLğ¿Llayers is being trained. In practice, the work also leverages techniques from regularization (Chapter3) in the form of knowledge distillation and experience replay (Chapter4), thus making it a hybrid approach. Additionally, a controller is used to balance between the current task loss and the knowledge distillation loss. The controller increases the weight of the knowledge distillation loss as the training progresses.

One important strength of RPS-Net is that, during inference, it does not need to know the task to which the given data point belongs as a common inference path is used.

An extreme version of Parameter Isolation Systems is the idea of adding a new network/model per task (which may or may not use predictions from the previously trained models). A very popular instantiation of this approach is Progressive Neural Networks[2016_progressive_neural_networks]where a newcolumn(network) is added every time a new task is encountered. The newly added column has lateral connections to the previous tasks that enable the forward transfer of knowledge from the previously trained models to the newly added model. During training, only the newly added model is trained, and the old weights are kept fixed, thus protecting from catastrophic forgetting. The obvious downside of this approach is that the number of parameters increases linearly with the number of tasks. Progressive Neural Networks also notes that the newly added models are not used to their full capacity, thus leaving scope for improvement.

schwarz2018progresspropose a related, but much more memory efficient idea where two networks are maintained. One network (referred to as theactive column) is used for training on the current task, and the second network (referred to as theknowledge base) stores the knowledge for solving the previous tasks. Training happens in two phases. In the first phase (calledprogressphase), theactive columnis trained on the current task. Once theactive columnconverges, the second phase starts where theactive columnis distilled into theknowledge base. This phase is referred to as thecompressphase. Thus training over multiple tasks proceeds as a sequence ofprogressandcompresssteps, and the approach is known as Progress and Compress. During theprogressphase, lateral connections between the knowledge base and the active column as used to transfer knowledge from the previous tasks to the current task. Only the active column is trained during the progress phase. This is similar to how2016_progressive_neural_networksworks. However, unlike2016_progressive_neural_networks, extra care needs to be taken to avoid catastrophic forgetting duringcompressstage. To that end, the paper uses a modified version of Elastic Weight Consolidation[2017_overcoming_catastrophic_forgetting_by_incremental_moment_matching].
Specifically, whencompressingthe knowledge of thektâ€‹hsuperscriptğ‘˜ğ‘¡â„k^{th}task into the knowledge base, the following loss is optimized, with respect to the parametersÎ¸Kâ€‹Bsuperscriptğœƒğ¾ğµ\theta^{KB}of the knowledge base (while keeping the parameters of the active column fixed):

whereÏ€k(â‹…|x)\pi_{k}(\cdot|x)andÏ€Kâ€‹B(â‹…|x)\pi^{KB}(\cdot|x)are the outputs of the active column (after learning onktâ€‹hsuperscriptğ‘˜ğ‘¡â„k^{th}task) and knowledge base respectively.xğ‘¥xrepresents the input,ğ”¼ğ”¼\mathbb{E}denotes the expectation over either the data under the active column,Î¸kâˆ’1Kâ€‹Bsubscriptsuperscriptğœƒğ¾ğµğ‘˜1\theta^{KB}_{k-1}andFkâˆ’1âˆ—subscriptsuperscriptğ¹ğ‘˜1F^{*}_{k-1}represent the mean and the diagonal Fisher of the online EWC Gaussian approximation resulting from previous tasks, andÎ³ğ›¾\gammais a hyperparameter.

The general architecture of adding task-specific experts is similar to the work on modular neural networks. In fact, the high-level motivation is also very similar: when doing lifelong learning, different tasks share a common substructure or common sub-problems. We hope to capture some of these shared structures/knowledge via the experts, i.e., different experts will learn to solve different tasks, and this knowledge can be shared across tasks. In practice, mixture-of-expert systems generally do notcomposemodules for a given data point or use simplistic aggregation operations likeaverage(similar to how ensembles work). The controller mechanism either selectskğ‘˜kexperts (or assigns soft-attention scores to all the experts). The application setup is closer tosystem identification or task identification[zadeh1956identification,e0da8f25-d850-4a80-af21-e151cc28c4f4,swevers1997optimal,bhat2002computing,gevers2006system,LJUNG20101,van2012subspace,chiuso2019system,ajay2019combining,yu2017osi,zhu2017fast]setup, and the goal is to identify the correct expert to use for a given task. In the mixture-of-experts-based setup, the controller mechanism is often non-parameterized, as we explain in the examples below.

2017_expert_gate_lifelong_learning_with_a_network_of_expertsproposed using a mixture of experts for the lifelong learning setup by training task-specific experts as follows: The model is initialized with one expert which is trained on the first task. As the model trains on subsequent tasks, new experts are added to the model and trained on the newly added tasks. This training setup does not require access to the previously seen data. Moreover, as new experts are added with new tasks, the model does not have the challenge ofcapacity saturation. There are two challenges that need to be addressed:Â (i) how to select thecorrectexpert during inference andÂ (ii) adding one new expert per task leads to linear growth in the number of parameters as new tasks are encountered.

The first challenge is addressed as follows: a set of gating auto-encoders whose job is to decide which expert should be used for a given sample. Specifically, each expert uses a shallow auto-encoder (which is trained with the expert). During inference, all the auto-encoders encode the input sample, and the expert, corresponding to the auto-encoder with the smallest reconstruction error, is selected. The use of auto-encoders also provides a mechanism to measure therelatednessbetween the different tasks, by comparing the reconstruction error between the different encoders. Specifically, given theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}and thejtâ€‹hsuperscriptğ‘—ğ‘¡â„j^{th}tasks, therelatednessis given as:

whereEâ€‹riğ¸subscriptğ‘Ÿğ‘–Er_{i}is the reconstruction error for theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}task. This task relatedness is used to select the most related task (for a new task) to be used as a prior model for learning the new task. Note that in the general neural modular networks, this problem is solved by the controller module and the mechanism used by2017_expert_gate_lifelong_learning_with_a_network_of_experts(or in general mixture-of-expert based approaches) can be seen as a non-parametric controller.

The second challenge, of the linear growth of the number of model parameters with new tasks, violates one of the desiderata of lifelong learning systems2.6: the number of parameters should increase sub-linearly with the number of tasks.2017_expert_gate_lifelong_learning_with_a_network_of_expertsleaves this limitation for the future work.

2017_encoder_based_lifelong_learninguse mixture-of-experts based architecture by training task-specific auto-encoders for mitigating catastrophic forgetting. Their proposed solution works as follows: Let us say that the model has a shared feature extractor, a shared model trunk, and some task-specific layers (that use the activations from the trunk as the input). After training the model on the first task, an auto-encoder is trained on the representations from the first task, in order to capture the most important features from the first task. Specifically, the auto-encoder is trained with two losses:Â (i) the reconstruction loss andÂ (ii) the supervised learning loss using the data from the first task (i.e., the features learned by the auto-encoder should be informative enough to solve the first task). When the model is trained on the second task, two constraints are added, along with the supervised learning loss on the second task. The first constraint is in the form of distillation loss used in2017_learning_without_forgetting. As described inSection3.4, the distillation loss aims to mitigate the influence of the use of different data distributions. The second constraint ensures that the features learned by the auto-encoder (for the first task) are still good for performance on the first task. This procedure can be applied to a sequence of tasks. Like2017_expert_gate_lifelong_learning_with_a_network_of_experts, this method also leads to linear growth in the number of parameters.2017_encoder_based_lifelong_learningjustify the trade-off by arguing that the memory footprint of the encoders is much smaller than the memory footprint of the overall model.

An important aspect of lifelong learning that does not get as much focus is Capacity Saturation. Only a few selective works have focused on that problem[yoon2018lifelong,sodhani2020toward]. Specifically,yoon2018lifelongproposed Dynamically Expandable Network (DEN), that can increase the network capacity dynamically as it trains on a sequence of tasks. There are three key steps involved in training DEN: Selective retraining, Dynamic network expansion, and Network split/duplication.

Selective retraining.At the start, the network is trained with L1 regularization to induce sparsity. As new tasks are encountered, a sparse linear model is trained to solve the task, using the topmost hidden units of the network. The nodes/weights, that changed in the top layer, provide the starting positions for breadth-first search, to find the nodes (in the layers below) that have paths to the nodes in the topmost layer. Only the weights of the selected sub-network are trained.

Dynamic Network Expansion.If selective retraining is not sufficient to train the model on a given task, its capacity is increased by expansion. This step is made efficient by using group sparse regularization. Specifically, the capacity of each layer is increased bykğ‘˜kneurons. Group sparsity
regularization removes the hidden units that are not necessary for training, thus preventing the wasteful addition of neurons as new tasks are encountered. This is one significant improvement over previous approaches like Progressive Neural Networks[2016_progressive_neural_networks].

The last key step isNetwork Split. As the model trains through multiple tasks, the semantic drift of the neurons is tracked, and if the semantic drift becomes too high, the neuron is split into two copies. After the split/duplication step, the network is trained again (to ensure task knowledge is not lost).

Some recent works have looked into hand-designing experts based on the task/domain setup.2020_compositional_language_continual_learningproposed to leverage compositionality to develop a new approach for lifelong learning in sequence-to-sequence tasks. They build on the idea of decomposing syntactic and semantic representations with compositionality by learning separate representations for the semantic and syntactic knowledge[2019_compositional_generalization_for_primitive_substitutions]and the networks encoding these two representations are analogous to experts in the mixture-of-expert based systems. There is no explicit gating mechanism, and the representation from the modules is used for making the final prediction. The network encoding the syntax is trained only on the first task and not updated afterward (with the assumption that the syntax does not change across the tasks).

2015_net2net_accelerating_learning_via_knowledge_transferproposed network expansion techniques that cangrowa smaller network into a larger network using function-preserving transformations. The paper presents two variants: (i) Net2WiderNet (for expanding the width of a given network) and (ii) Net2DeeperNet (for expanding the depth of a given network).

Consider a neural network where theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}and thei+1tâ€‹hğ‘–superscript1ğ‘¡â„{i+1}^{th}layers are fully connected layers and layeriğ‘–iuses a point-wise non-linearity. If theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}layer hasmğ‘šminputs andnğ‘›noutputs and thei+1tâ€‹hğ‘–superscript1ğ‘¡â„{i+1}^{th}layer haspğ‘poutputs, then the Net2WiderNet operation can be used to widenitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}layer by replacing it with a layer that hasNğ‘Noutputs, whereN>nğ‘ğ‘›N>n. First, a random mapping functionrğ‘Ÿr, from{1,2,â‹¯,N}â†’{1,2,â‹¯,n}â†’12â‹¯ğ‘12â‹¯ğ‘›\{1,2,\cdots,N\}\rightarrow\{1,2,\cdots,n\}, is defined as follows:

The weight matricesWisuperscriptğ‘Šğ‘–W^{i}andWi+1superscriptğ‘Šğ‘–1W^{i+1}are replaced byUisuperscriptğ‘ˆğ‘–U^{i}andUi+1superscriptğ‘ˆğ‘–1U^{i+1}such that

The firstnğ‘›ncolumns ofWisuperscriptğ‘Šğ‘–W^{i}are copied as it is intoUisuperscriptğ‘ˆğ‘–U^{i}. Columnsn+1ğ‘›1n+1throughNğ‘NofU(i)superscriptğ‘ˆğ‘–U^{{(i)}}are created by randomly selecting columns ofWğ‘ŠW(with replacement), followed by normalization with a replication factor (computed using the frequency of sampled columns).

Similarly, the Net2DeeperNet operation can be used to replace theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}layer (representingÏ•â€‹(h(iâˆ’1)âŠ¤â€‹W(i))italic-Ï•superscriptâ„limit-fromğ‘–1topsuperscriptğ‘Šğ‘–\phi(h^{(i-1)\top}W^{(i)})) by a deeper layer (representingÏ•(U(i)âŠ¤Ï•(W(i)âŠ¤h(iâˆ’1)\phi(U^{(i)\top}\phi(W^{(i)\top}h^{(i-1)}). The newly addedUğ‘ˆUmatrix is initialized as indentity matrix and updated during finetuning. One limitation of this approach is thatÏ•italic-Ï•\phi(the activation function) must satisfyÏ•â€‹(Iâ€‹Ï•â€‹(v))=Ï•â€‹(v)italic-Ï•ğ¼italic-Ï•ğ‘£italic-Ï•ğ‘£\phi(I\phi(v))=\phi(v)for all vectorsvğ‘£vwhereIğ¼Iis an identity matrix. So while this operation holds for activations like ReLU, using this operation with maxout units requires some modifications to theUğ‘ˆUmatrix and the operation does not hold for the sigmoid activation.

The technique is proposed in the context of accelerating the training of a large neural network by first training a smaller network and then expanding it into a larger network. However, the paper mentions lifelong learning as an application area for their approach. Indeed, subsequent works have applied the technique for alleviating capacity saturation. For example,2018_on_training_recurrent_neural_networks_for_lifelong_learningcombined Net2Net (specifically Net2WiderNet) with Gradient Episodic Memory to create a hybrid model. When training on a given task, the model uses Gradient Episodic Memory updates to retain the knowledge of the previous task. When the modelâ€™s capacity is saturated (and the model is unable to learn new tasks), it is expanded using the Net2Net approach, and the enlarged model continues to train on the subsequent tasks.2018_on_training_recurrent_neural_networks_for_lifelong_learningshows that the combined approach works better in practice than the individual components.

In this chapter, we introduced the architecture-based methods proposed for lifelong learning that assign a model copy to every new task that arrives. We started by discussing the idea of using Modular networks, their motivation from multiple perspectives, their architectures, and finally, their benefits to lifelong learning. Understanding a more general biologically-inspired type of method, called modular networks, is necessary because they provide the ability to use different model architectures as constituent modules, add new modules throughout training, and also help in representing composable skills.

Parameter isolation methods are the types of architecture-based methods which are similar to modular networks. The main difference is that parameter isolation methods have a different inductive bias such that the tasks have their own set of isolated parameters. They are further divided into fixed capacity networks and increasing capacity networks. Fixed capacity networks can be masking-based, pruning-based, and involve discovering paths in the network as different modules. There are expert-based methods that require adding a new network for each task. This is similar to modular networks in the sense that the task-specific experts are continuously added to the main network.

Finally, we presented the capacity saturation aspect of lifelong learning by describing the DEN method. This method avoids capacity saturation and ensures stable learning for future tasks by following three important steps: selective retraining, dynamic network expansion, and network split/duplication.

Lifelong learning methods today are designed and implemented with different assumptions and setups. In earlier chapters, we described how the lifelong learning methods can be categorized from an algorithmic point of view. But benchmarking the proposed methods in different datasets with different settings makes comparing these approaches very complicated. Such untrustworthy benchmarks and comparisons raise the importance of introducing some systematic benchmarks in lifelong learning. With the availability of huge datasets in the machine learning domain, there is an abundance of benchmarks to evaluate lifelong learning methods. In this chapter, we go over some of the most commonly used benchmarks by dividing them based on their application: vision-based and NLP-based.

Many methods proposed in lifelong learning are evaluated on image-based benchmarks. These benchmarks are typically adapted from fields such as image classification, reinforcement learning (Atari games, robot manipulation, imitation), and generative models[lesort2019continual]. To evaluate a lifelong learning method, these benchmarks are modified, augmented, and concatenated together to create sequences of tasks. In this section, we describe such benchmarks in detail and also describe the benchmarks that are designed specifically for lifelong learning settings.

Early lifelong learning methods, with the focus on vision tasks, started with benchmarking methods on some variation of the MNIST dataset[lecun2010mnist]. Permuted MNIST[goodfellow2015empirical]and Split MNIST[lopez2017gradient]were some of the early benchmarks for lifelong learning.
Figure6.1illustrates a simple training protocol on the Split MNIST benchmark. It shows both class and task incremental learning cycle. MNIST dataset contains training samples for supervised learning of classification of handwritten digits zero to nine.
In Split MNIST benchmark, the tasks are basically the disjoint sets of classes from the MNIST dataset. The model has to learn from the training samples that arrive sequentially from these tasks at each training step. Following the supervised learning, the model iterates over given training samples for several epochs and is evaluated on all classes seen so far. In the Split MNIST benchmark[pmlr-v70-zenke17a,farquhar2019robust,aljundi2019online,vandeven2019generative,swaroop2019improving,vandeven2019scenarios], the difference between task and class incremental learning is the awareness of the model to the task shift. To this end, in the task incremental learning, trained on Split MNIST, the model knows the tasksâ€™ boundaries.

[å›¾ç‰‡: images\image_11.png]
å›¾ç‰‡è¯´æ˜: Figure 6.1:Class and task incremental learning on the Split MNIST benchmark.

Permuted MNIST is another benchmark that was introduced after the Split MNIST benchmark. The approach of creating the Permuted MNIST benchmark is straightforward. In this case, the model receives all training samples of ten digits at each training time. The model learns from a regular MNIST dataset as the first task. Then, the model receives the permuted version of regular MNIST as the second task. So the model should learn from the permuted image sample and also should not forget what it learned at the previous task. Similarly, in the next steps, the model will receive samples that have different permutations and will have to adapt without forgetting catastrophically. Figure6.2shows the flow of training samples that the model should learn overtime in the Permuted MNIST benchmark.

[å›¾ç‰‡: images\image_12.png]
å›¾ç‰‡è¯´æ˜: Figure 6.2:Permuted MNIST that have been used as a benchmark for assessing the lifelong learning methods.

lopez2017gradientintroduced and used the Rotated MNIST benchmark to evaluate their Gradient of Episodic Memory (GEM) method. Rotated MNIST is a more practical and meaningful benchmark than the Permuted MNIST. In this benchmark, the goal of the model is to have a robust behavior through time by learning from a sequence of tasks that differ by rotation transformation. In other words, the images in each task are MNIST images with some fixed degree rotation transformation. Figure6.3gives an example of using Rotated MNIST in a lifelong learning setup.

[å›¾ç‰‡: images\image_13.png]
å›¾ç‰‡è¯´æ˜: Figure 6.3:Rotated MNIST benchmark. Each block shows few samples that the model should learn at each training step.

KMNIST is a dataset, adapted from Kuzushiji Dataset that consists of Kuzushiji-MNIST, Kuzushiji-49, and Kuzushiji-Kanji datasets[clanuwat2018deep].
KMNIST Dataset is a drop-in replacement for the MNIST dataset.
KMNIST chooses one character to represent each of the 10 rows of Hiragana. KMNIST adds some complexity to the Split MNIST benchmark since the shape of characters is more complex than the shape of simple digits.

[å›¾ç‰‡: images\image_14.png]
å›¾ç‰‡è¯´æ˜: Figure 6.4:Split KMNIST sample that can be used in a lifelong learning task.

Figure6.5shows the combination of three datasets: MNIST, FashionMNIST (dataset of the articles of clothing at low resolution[xiao2017_online]), and KMNIST datasets in form of a sequence of three tasks also known as the MNIST Fellowship benchmark. In this benchmark, each task is a variation of the MNIST dataset that arrives in a sequence. MNIST Fellowship has 30 classes in total and each instance is in the size of 28x28 images.

[å›¾ç‰‡: images\image_15.png]
å›¾ç‰‡è¯´æ˜: Figure 6.5:MNIST Fellowship benchmark. Each task could be created using a subset of either MNIST, Fashion MNIST, or KMNIST datasets.

As explained above Split MNIST, Permuted MNIST, KMNIST, and MNIST Fellowship benchmarks are the simple benchmarks that have been used in early research in lifelong learning. As the field grew, having a more complex benchmark became crucial for evaluating lifelong learning methods. To create more complex and difficult benchmarks split versions of other popular datasets like CIFAR and ILSVRC2012 have been used.
The performance of the models trained on lifelong learning settings are mostly reported for MNIST[lecun2010mnist], Permuted MNIST, rotated MNIST, CIFAR-10, CIFAR-100[krizhevsky2009learning], ImageNet[deng2009imagenet]where data is split into sequences of classes or tasks. MNIST, Permuted MNIST, and rotated MNIST are usually split into two and five consecutive sets of classes[pmlr-v70-zenke17a,ewc,lopez2017gradient,pmlr-v70-zenke17a,nguyen2017variational,lesort2018generative]. CIFAR-100 is split into two, five, ten, or twenty sets of classes such that the model should learn each set of classes at each time consecutively[lopez2017gradient,pmlr-v70-zenke17a,Rebuffi_2017,Hou_2019_CVPR,castro2018endtoend]. Recently, most of the approaches report the performance of models on split ImageNet[Rebuffi_2017,Hou_2019_CVPR,castro2018endtoend,wu2019large]and Celeb-10000[wu2019large]. Some approaches benchmark on datasets such as ImageNet, CIFAR-100, SVHN, UCF101, Omniglot, GTSR, DPed, Flower, Aircraft, and DTD[li2019learn].

:
Recently proposed methods benchmark using several existing datasets as a sequence of individual tasks. For instance,5-datasetsis a sequence of five different datasets as five 10-way classification tasks[2018_overcoming_catastrophic_forgetting_with_hard_attention_to_the_task,saha2021gradient,ebrahimi2020adversarial]. These datasets are:CIFAR-10,MNIST,SVHN[netzer2011reading],notMNIST[notmnist], andFashion-MNIST[xiao2017_online].

lomonaco2017core50introduced CORe50 that is constructed specifically to evaluate lifelong learning methods. Continual object Recognition benchmark (known as CORe50) consists of 50 domestic objects belonging to 10 categories of simple objects including plug adapters, mobile phones, scissors, light bulbs, cans, glasses, balls, markers, cups, and remote controls. Since the lifelong learning setup pose significant challenges for deep learning models, having clean object-centered instances may reduce learning complexity at each training step. With this idea,lomonaco2017core50attempted to collect clean images centered by the main object and avoid having other objects in the same image like in ImageNet.

Figure6.6shows some examples of CORe50 training samples. As shown in Figure6.6objects are presented in each instance such that the camera point-of-view mimics the operatorâ€™s eyes point of view. In this way, models can learn a training sample that is simple and provided to the model clearly with minimum complexity.
The original benchmark was designed for studying lifelong learning in the robotic domain and it contained short videos instead of images. In order to collect samples, the operator smoothly moves his/her arm to present objects from different angles. It is worth mentioning that the operator changes hands throughout the sessions.

This provides a chance to produce more samples for relevant objects.

[å›¾ç‰‡: images\image_16.png]
å›¾ç‰‡è¯´æ˜: Figure 6.6:Some examples of CORe50 training samples. The grabbing hand (left or right) changes while collecting images of different objects.

lomonaco2017core50collected data in 11 distinct sessions that included samples from eight indoor and three outdoor views designated by different backgrounds and lighting. Each object in the corresponding session presented by a 15 seconds video (at 20 fps) has been recorded with a Kinect 2.0 sensor delivering 300 RGB-D frames.

For low-data streams on lifelong learning setup,antoniou2020definingintroduced a benchmark that defines a systematic approach for the setting of continual few-shot learning on various datasets such that the performance of the model is reported on each dataset individually. The learning agent has access to a very limited set of training samples in most real-world scenarios in each task as described byantoniou2020defining. However, most proposed approaches in lifelong learning need to revisit training samples for several epochs at each training step. That could be considered as the first limitation of the above benchmarks. To this end, benchmarking the lifelong learning methods by using the introduced benchmarks in this chapter such that visiting the training samples are allowed once or for a few numbers of epochs[hayes2020remind,laleh2020chaotic]are interesting and challenging approaches to evaluating lifelong learning methods.

The second limitation of the current approaches is that they alleviate catastrophic forgetting through time for incrementally learned classes or tasks that arise from the same distribution and same dataset.
To this end, current approaches have been focused on a homogeneous lifelong learning problem. To make the benchmark more realistic and closer to real-life scenarios some other benchmarks have been proposed recently to include more challenges for assessing lifelong learning methods. In the next section, we introduce some of the recently proposed benchmarks that try to overcome one or both limitations explained above.

Stojanov_2019_CVPRintroduced CRIB benchmark that is motivated by infant learning concepts in psychology. Infant learning is characterized based on five pillars: Incremental learning, Repetitive exposure to objects, Temporally contiguous visual experience, self-supervised learning (since labeling events are sparse and noisy), and Object Instance learning that precedes categorization. The infant object learning process consists of repetitive exposure to the object, starting with toys in their environment, which is an inherently incremental procedure. The visual experience that infants can see is temporary continuous and poorly smooth, particularly for the first two years. Their supervision is fairly sparse and they do not have constant object naming supervision by their parent.
Infants tend to pick up different toys while playing, explore them for a few minutes, and then put them down. This is a continuous pattern observed in infants.

Therefore, they may pick up a bunch of different objects that only get naming supervision from their parents for a small subset of them.Stojanov_2019_CVPRtried to connect incremental learning with infant learning by having a simple model of object interaction by picking up and putting down objects during episodes. They called picking an object up and examining it for a while, then putting it down and picking another as one learning exposure.

The object can be presented to the learner after a while again. But if it has been seen before, the label will not be provided to the learner. That can mimic the sparse supervision situation. To model such a procedure, they used Toys-200 3D that contains object samples in the dataset with a toy-like appearance. They generate a small smooth video of the picked-up object and rotate it in front of a background in various lighting conditions and then put it down and select another toy object. The selected object in each training sample will not appear in the background. Figure6.7illustrates the CRIB benchmark idea and the exposure learning process in the lifelong learning approach.

[å›¾ç‰‡: images\image_17.png]
å›¾ç‰‡è¯´æ˜: Figure 6.7:CRIB benchmark. Toys-200 dataset of 200 unique toy objects that model infants learning in a lifelong learning approach.

OpenLORIS is another well-known benchmark that simulates closely a real-life scenario for a lifelong learning agent[she2019openlorisobject]. (L)ifel(O)ng (R)obotic V(IS)ion (OpenLORIS) is an Object Recognition benchmark that is designed for facilitating lifelong learning research primarily for the robotic domain and extended to other application domains as well. This benchmark examines the capability of learning the common objects in the home scenario in some limited conditions. Since fully retraining robotic agents at each time for a new task or same task but with different lighting in the environment or different point of view is infeasible, lifelong learning methods can help to overcome such challenges but alleviate catastrophic forgetting and evaluating proposed methods in such conditions need benchmarks that can assess the agentsâ€™ capabilities in these conditions.she2019openlorisobjectinclude the common challenges that the learning agents might need to deal with in the environment such as changes in illumination, occlusion, object size, camera-object distance, camera-object angle, and clutter. Including these factors from real-life environments into the benchmark creates more realistic scenarios for assessing a lifelong learning agentâ€™s capabilities.

[å›¾ç‰‡: images\image_18.png]
å›¾ç‰‡è¯´æ˜: Figure 6.8:OpenLORIS-Object benchmark.

Conventional neural networks require running a loop over a batch of i.i.d data multiple times to improve performance. Although several lifelong learning training methods are proposed to alleviate forgetting under non-i.i.d. conditions, many of these methods are inflexible and inefficient in real-world scenarios where data comes from a dynamic data distribution and constantly changes over time.

Stream-51 is used to evaluate whether an agent can robustly handle shifts and novel inputs in training data in a lifelong learning scenario. Online streaming learning is a more pragmatic approach where a model needs to learn one sample at a time that it receives from a stream of data. Stream-51 provides sufficient data classes with high-quality training instances for such an online lifelong learning setup.

This benchmark consists of temporally correlated images from 51 unique object categories and additional evaluation classes for testing novelty recognition. The evaluation samples are not provided to the model at training time. As Figure6.9shows, the model learns from a temporally correlated stream of samples. Similar to the other benchmarks, the model is evaluated on the previously seen classes and the ability of the model to detect unlearned concepts.

[å›¾ç‰‡: images\image_19.png]
å›¾ç‰‡è¯´æ˜: Figure 6.9:Stream-51

Stream-51 Specific Metrics:
In the Stream-51 benchmark6.1.5, the model receives samples in a stream of data. A few of these samples are considered unseen data or novel samples. Therefore the goal is to compute the overall classification performance and its ability to detect novel inputs[Roady_2020_Stream51]. Since Stream-51 works in the stream of data where the model is not allowed to iterate over the training samples for several epochs, the overall classification performance is computed as follows:

whereTğ‘‡Tis the total number of testing events,Î±tsubscriptğ›¼ğ‘¡\alpha_{t}is the accuracy of the streaming learner at timetğ‘¡t, andÎ±offline,tsubscriptğ›¼offlineğ‘¡\alpha_{\text{offline },t}is the accuracy of an optimized offline model at timetğ‘¡t. This metric normalizes a streaming learnerâ€™s performance using an optimized offline learner. Normalizing the streaming learnerâ€™s performance to an offline learner makes the metric easier to interpret across various orderings[Roady_2020_Stream51].
For novelty detection,Roady_2020_Stream51propose an incremental variant of the area under the OSC curve
(AUOSC) which normalizes an incremental learnerâ€™s performance to an optimized offline baseline as follows:

whereTğ‘‡Tis the total number of testing events,Î³tsubscriptğ›¾ğ‘¡\gamma_{t}is the AUOSC score of the incremental learner at timetğ‘¡t, andÎ³offline,tsubscriptğ›¾offlinet\gamma_{\text{offline },\mathrm{t}}is the AUOSC score of the optimized offline learner at timetğ‘¡t[Roady_2020_Stream51].

Abdelsalam_2021_CVPRintroduced Incremental Implicitly-Refined Classification (IIRC) as an extension to the class incremental learning setup. Unlike other lifelong learning benchmarks, IIRC breaks the assumption of having the same level of hierarchy for the samples the model should learn from through time. IIRC provides a benchmark and scenario which is more challenging and more aligned with real-life learning scenarios.

In this setup, the incoming batches of classes might have two levels of granularity: a high level (coarse) label like â€œbearâ€ and a low-level (fine) label like â€œpolar bearâ€. Only one label is provided at a time, and the model has to figure out the other label if it has already learned it. Therefore, the model should be able to expand its knowledge about a concept while not forgetting high-level information that is previously learned knowledge (at a different granularity) about the same concept. Another challenge included in the IIRC benchmark is that the classes have imbalanced sample distribution similar to real-life where not all classes are observed at the same frequency.

IIRC benchmark provides IIRC-ImageNet and IIRC-CIFAR built based on ImageNet and CIFAR datasets respectively. These two datasets are most popular in lifelong learning literature. IIRC-ImageNet simulates data diversity challenges. They provided a shorter version of IIRC-ImageNet called IIRC-ImageNet-li.Abdelsalam_2021_CVPRclearly states that the IIRC-ImageNet-lite version is not for benchmarking the model performance but only for performing and debugging experiments.

IIRC benchmark reveals the modelâ€™s ability to expand its knowledge and associate and re-associate labels over time. Figure6.10illustrates the training and evaluation paradigm in the IIRC benchmark. In the figure, the top right label is available to the label model during training, whereas the bottom label, defined as a target, is predicted by the model during evaluation. The right bottom panel also shows the set of classes used for model evaluation, and the dashed line represents the task boundary in the task incremental learning setup.

[å›¾ç‰‡: images\image_20.png]
å›¾ç‰‡è¯´æ˜: Figure 6.10:The learning and evaluation procedure in the IIRC benchmark.

IIRC Specific Metrics:

As discussed previously, the Average Accuracy metric is one of the most popular metrics used in lifelong learning. But multi-label classification setup that involves hierarchy requires a different evaluation metric. IIRC, a benchmark closer to real-life scenarios, enabled the multi-labels classification for super-classes and sub-classes that the model might see over its lifetime. In the IIRC benchmark, limiting the model to not predicting a large number of possible classes is essential, otherwise the model will tend to predict more labels to receive more positive feedback during its supervision periods.

Sorower10aliteratureproposes to use Exact-Match Ratio (MR) as a metric for the multi-label classification. The Exact-Match Ratio (MR) is defined and computed as:

whereIğ¼Iis the indicator function,Yisubscriptğ‘Œğ‘–Y_{i}are the ground truth labels for sampleiğ‘–i,Y^isubscript^ğ‘Œğ‘–\hat{Y}_{i}is the set of predictions for the corresponding sample, andnğ‘›nis the total number of samples. The Exact-Match Ratio penalizes partial correct predictions in the same way as completely incorrect ones. In other words, a partially corrected prediction will have the same score as the completely incorrect one. That is an important weakness of the Exact-Match Ratio metric.

In partial multi-labels or complete multi-label classification literature using Jaccard Similarity (JS)[Sorower10aliterature]is very common. The Jaccard Similarity (JS) computes the score for evaluating a modelâ€™s performance for the multi-labels prediction as intersection over the union of true labels. It is mathematically formalized as follows:

The precision weighted JS (pw-JS) is further weights JS by sampling precision. Similar to the other lifelong learning benchmarks, after training the model on the taskjğ‘—j, pw-JS is computed to evaluate the model performance on all tasks from the beginning to taskjğ‘—jas follow:

wherenğ‘›nis the total number of evaluation samples for all the tasks seen so far,Yisubscriptğ‘Œğ‘–Y_{i}is the set of ground truth labels of sampleiğ‘–iandYi^^subscriptğ‘Œğ‘–\hat{Y_{i}}is the set of predictions from the model for sampleiğ‘–i[Abdelsalam_2021_CVPR].
Compared to JS, the weights additionally differentiate completely correct predictions from partially correct predictions.Abdelsalam_2021_CVPRshow that algorithms that tend to generate more labels have lower pw-JS scores than JS scores.

Natural language processing is used in many day-to-day applications that take the advantage of ML to solve tasks. With the surge of attention on using lifelong learning methods in practical scenarios, having a good benchmark to evaluate proposed methods is a key element in applying the lifelong learning methods in the NLP domain. Several benchmarks have been proposed recently in this direction and we discuss some of them in this section.

hu2020drinkingcollected data from the popular social media platform Twitter to introduce a benchmark for evaluating a personalized language model for each user. Each user is considered as a different task in this setup. The goal here is to propose a Personalized Online Language Learning (POLL) method that helps in finding a personalized language model for each user over time. The key characteristic of this setup is that users are added and dropped over time. Since users tweet with different frequencies, the setup has a highly non-stationary distribution. This setup provides both multi-task and continual settings of tasks. Learning the language model for each user is an online multi-task setting since each user is considered a task in this setup. Since it requires learning a shared language model with the non-stationary data distribution, it is a lifelong learning problem[hu2020drinking].

hu2020drinkingcollected more than100100100million tweets with more than1.51.51.5billion tokens, posted by one million users over six calendar years called the FIREHOSE datasets. Since language learning is an unsupervised or semi-supervised problem in real life, this dataset does not require human or automatic labeling process to create the benchmark for evaluating lifelong learning algorithms. Therefore, FIREHOSE is a massive web-scale dataset that can also support research on POLL.hu2020drinkingprovided a smaller version of the dataset called Firehose10M and the bigger one as Firehose100M. Table6.1provides the statistic for each version.

Text classification tasks have a lot of practical and industrial usage. Therefore, having a lifelong learning benchmark to evaluate proposed methodâ€™s performance in continual text classification tasks seems quite important.de2019episodicused the publicly available text classification datasets that are collected from various domains such as news classification, sentiment analysis, Wikipedia article classification, and questions and answers categorization. They constructed a benchmark using AGNews (4 classes), Yelp (5 classes), DBPedia (14 classes), Amazon (5 classes), and Yahoo (10 classes) datasets.
Table6.2shows the benchmark specification in detail.

In Table6.2, both Yelp and Amazon dataset are used for the same purpose (sentiment classification). Therefore the selected classes are merged into the same semantic analysis task.de2019episodicprovided two versions for benchmarking lifelong learning methods: a balanced dataset that contains115,000115000115,000training and7,60076007,600test examples and an imbalanced version that contains575,000575000575,000training and38,0003800038,000test examples. The second version is imbalanced due to the different number of examples selected from each source. To study lifelong learning under the realistic scenario of a large number of tasks,mehta2021empiricalintroduces a novel suite of151515diverse text classification tasks. They showed that the introduced suite proves more challenging than the previously discussed benchmark with555datasets. Table6.3shows the benchmark datasets in detail. Future works should consider repurposing ExMix[aribandi2022ext], a massive collection of107107107supervised tasks across diverse domains to be an even more challenging and realistic lifelong language learning benchmark. To construct a lifelong learning benchmark for the question and answering tasks,de2019episodiccreate their benchmark using the dataset described in Table6.4.

Word embedding plays a huge role in improving NLP task performance. Some NLP applications may need access to distributed word vectors that can be built sequentially. In a large general-purpose corpus, finding the embedding may not be useful for domain-specific downstream tasks because languages and the meaning of vocabulary can slightly change over time. Emerging social media cause a faster change in the meaning of some vocabularies. For example, the meanings of a word used in specific situations and contexts may differ in some viral videos or posts. As a result,Biesialska_2020argue that learning word embedding poses an important lifelong learning paradigm. A lifelong learning setup can fulfill the requirements of vocabulary changes over time. According to our best knowledge, there is still a lack of a good benchmark for evaluating lifelong learning methods on learning embedding in a lifelong learning setup.

Lifelong learning in task-oriented dialogue systems has been previously studied byLee2017TowardCL, who perform lifelong learning over three tasks in an end-to-end (E2E) dialogue modeling setting.wu-etal-2019-transferablepropose a dialogue state tracking (DST) model that trains on a set of domains and can transfer knowledge to a new domain without forgetting. They evaluate DST on different domains in the MultiWOZ dataset[budzianowski2020multiwoz].mi2020continualpropose a method for lifelong learning in the natural language generation (NLG) setting, where different domains from the MultiWOZ dataset are presented in a sequence.geng-etal-2021-continualalso propose a lifelong learning method in the NLG setting that uses network pruning and expansion to adapt to new tasks. They evaluate on a benchmark of 7 tasks, composed of domains from the In-Car Assistant[eric2017keyvalue], MultiWOZ, and CamRest[wen2017networkbased]datasets. However, these methods look at lifelong learning in task-oriented dialogue with a relatively small number of tasks (3-7), with a smaller amount of data, and in a single setting.

madotto2020continualpropose a benchmark that presents 37 tasks in a sequence in 4 different settings. They consider single-domain dialogues from 4 different datasets: Taskmaster-1 and Taskmaster-2[byrne2019taskmaster1], Schema-Guided Dialogue (SGD) dataset[rastogi2020scalable], and MultiWOZ[budzianowski2020multiwoz]. Each domain in the datasets forms a single task in the lifelong learning formulation. The four settings they consider are as follows:

Intent prediction (INTENT), where an intent label has to be predicted from the given conversation history.

Dialogue state tracking (DST), where the intent and a sequence of slot-value pairs that are passed to an API call to be tracked have to be predicted from the conversation history.

Natural language generation (NLG), where a natural language response has to be predicted given the result of an API call.

End-to-end (E2E), which combines the three settings mentioned above.

madotto2020continualunify the settings described above into a sequence-to-sequence learning problem by presenting the dialogue history as a sequence of turns and serializing the API calls.

Metrics on Lifelong Dialogue Systems:
To evaluate the dialogue systems,madotto2020continualadopt a modular approach for evaluating the models in each setting with a suitable metric as follows:

INTENT: Intent accuracy, which compares the predicted intent label against the gold intent label.

DST: Joint goal accuracy (JGA) metric, which measure the proportion of samples for which all slot-value pairs are predicted correctly.

NLG: Entity error rate (EER) which measures the number of slots in the input that do not appear in the generated utterance, and the BLEU score between the generated and gold utterance.

E2E: All the above metrics.

To measure performance in the lifelong learning setting, the average value of the metric over the tasks is used. Forward or backward transfer are measured by aggregating the metrics defined above.

To achieve state-of-the-art performance in real-world scenarios, lifelong learning methods must be capable of generalization on a broad range of complex datasets comprising a large number of tasks. The goal of generalization requires lifelong learning benchmarks to be challenging and closer to real-world applications to ensure robust behavior over time.

We first described numerous vision-based benchmarks involving multiple tasks of image classification with varying levels of difficulty by inducing shifts in the data distribution. We also described NLP domain-based benchmarks. These benchmarks are designed specifically for the type of task at hand such as text classification, question answering, text representation, dialogue systems, etc.

Some of these benchmarks also required introducing new metrics for evaluations for a more intuitive evaluation of the baselines. It is worth noting that several benchmarks for lifelong learning presented in this chapter are inspired by multi-task learning benchmarks and are often used without any modification.

So far, we have looked at both a high-level overview of lifelong learning systems (definition and desiderata of lifelong learning systems and the relationship between lifelong learning and other paradigms) as well as specific instances of lifelong learning algorithms and benchmarks and metrics for evaluating lifelong learning systems. In this last section, we conclude the primer with a discussion on future challenges and important research directions in lifelong learning systems.

The first important research direction isdeveloping methods for organizing (or compartmentalizing) the systemâ€™s knowledge. This would enable the system to manipulate specific parts of its knowledge without over-writing all the previous knowledge. Such methods would be useful in the context of catastrophic forgetting and faster adaptation to new tasks as the system can choose which knowledge it wants to forget/update and retain all the other knowledge. These methods are especially relevant in the context of training large-scale models on constantly evolving datasets (like the web data where the ground truth data is continuously changing). Ability to selectively update parts of the knowledge system is a crucial requirement for deploying such systems in the real world[bommasani2021opportunities].

One promising line of work in this direction is the work onadapters[houlsby2019parameter,pfeiffer2020adapterfusion,pfeiffer2020mad]- small modules that are added to the intermediate layers of a large, pretrained transformer model. When transferring to a downstream task, only the newly added adapter modules are finetuned, and the original, pretrained transformer model is kept fixed. Previous works likehoulsby2019parameter,pfeiffer2020mad,pfeiffer2020adapterfusionhave demonstrated that finetuning just the adapter modules can also provide excellent performance on the downstream tasks. The use of adapters enables learning downstream models that arecompact(uses fewer trainable parameters for each downstream task) andextensible(uses new adapters that can be incrementally trained on downstream tasks). Adapters can be seen asknowledge modulesthat can be swapped in and out, depending on the downstream task. However, adapters are designed for the forward transfer of knowledge (start with a pre-trained model and adapt to the new tasks) and do not provide a straightforward mechanism to enable the backward transfer of knowledge (updating the knowledge in the pre-trained model, in a non-catastrophic way, while training on the new tasks). Combining insights from existing work in modular architectures(Chapter5) and adapters could provide a useful inductive bias for compartmentalizing the systems knowledge in terms of its parameters.

Another important research direction is toenable the lifelong learning systems to interface with external knowledge sources. This capability is especially relevant in the context of interactive systems (like dialogue systems or embodied agents) operating in the real world. These interactive systems are often likely to encounter new topics/objects/items and have to account for the continually changing state of the world. For example, it is not possible for a static language model, trained at one point in time, to answer â€œwhat is the current temperature in Montrealâ€ without access to an external knowledge base that can provide it the current temperature in Montreal. One way to work around this challenge is to develop lifelong learning systems that learnsskills(e.g., querying the external knowledge-bases, aggregating the results from different queries, etc.) instead of knowledge. One popular approach for teaching suchskillsis to curate a list of tasks, with or without a curriculum, such that each task requires the system to either learn a new skill or learn to combine previously learned skills[bengio2009curriculum,babyai,weston2015towards]. For example, a question-answering system may start by learning to choose relevant facts from a curated list of facts. The system may learn to create a curated list of facts using a single knowledge base as the next step. In the third task, the system may have to answer the question using the knowledge base (requiring it first to create a list of facts and then answer the question). In the next task, the system could learn to choose the relevant knowledge base(s) to query. In this way, the system learns a sequence of skills and combines them for solving new tasks. The key challenge with this approach is that the list of tasks is often hand-designed, thus limiting the scope, both in terms of the number of tasks and the spectrum of skills they cover.

We note that the system doesnâ€™t need to rely exclusively on external knowledge bases for all the information. The system could employ memory-augmented neural networks (like Neural Turning Machine[graves2014neural,luders2016continual], Differentiable Neural Computer[graves2016hybrid]or memory network[sukhbaatar2015end,sukhbaatar2015end,kumar2016ask,miller2016key,lample2019large]). The key requirement is that the system should be able to query external knowledge stores for accessing the dynamically changing information. This research direction can be seen as an extension to the first direction where the compartmentalized knowledge lives in external, well-curated knowledge bases that the system can access on-demand.

[å›¾ç‰‡: images\image_21.png]

[å›¾ç‰‡: images\image_22.png]

