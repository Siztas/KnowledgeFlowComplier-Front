###联邦学习中的隐私风险：你知道多少？


这篇文章是一篇关于联邦学习中隐私威胁及对策的综述。文章指出，尽管联邦学习被认为是一种注重隐私的机器学习方法，因为其在训练过程中不会直接交换原始数据，但仍存在隐私泄露的风险。文章详细分析了三种典型的联邦学习类型：水平联邦学习（HFL）、垂直联邦学习（VFL）和联邦迁移学习（FTL），并针对每种类型分别探讨了隐私威胁和相应的对策。

文章首先介绍了联邦学习的背景和重要性。随着计算设备的普及，人们在日常生活中产生了大量的数据。集中存储这些数据不仅成本高、耗时，还涉及用户隐私和保密问题。敏感数据如生物特征和医疗保健信息可能被用于针对性的社交广告和推荐，带来即时或潜在的隐私风险。因此，直接共享数据而不考虑隐私是不可取的。随着社会对隐私的关注度不断提高，相关的法律法规如欧盟的《通用数据保护条例》（GDPR）和《人工智能法案》也在不断出台，使得数据聚合的做法变得越来越不可行。在这种背景下，联邦学习作为一种有前景的机器学习技术应运而生，它允许每个客户端学习并发送信息到服务器，而无需直接交换私有原始数据。

文章接着详细描述了三种联邦学习类型的学习和预测方法。水平联邦学习（HFL）是最常见的联邦学习类别，由谷歌首次提出，其目标是让持有不同样本的每个客户端协作提高模型的准确性。垂直联邦学习（VFL）则允许持有相同样本但不同特征的客户端协作训练一个模型。联邦迁移学习（FTL）适用于两个客户端在样本和特征上都只有小部分重叠的情况，其目标是将拥有标签的客户端（源客户端）的知识转移到没有标签的客户端（目标客户端）。

文章详细分析了每种联邦学习类型中的隐私威胁。在水平联邦学习中，客户端数据是隐私的主要威胁。可能的攻击者包括服务器、客户端和第三方。服务器可能会对客户端发送的模型进行推理攻击以推断客户端数据；客户端可能会对从服务器接收的全局模型进行推理攻击以推断其他客户端的数据；第三方可能会窃听通过通信通道传输的模型，并通过推理攻击推断客户端数据。在垂直联邦学习中，隐私的主要威胁是通过客户端之间的身份匹配导致的身份泄露。此外，客户端发送给服务器的中间输出也可能导致客户端数据被推断。在联邦迁移学习中，隐私威胁取决于客户端之间共享的信息。当特征共享时，隐私威胁是特征类比网络和预测网络的交换；当ID共享时，隐私威胁是身份泄露和客户端之间为特征相似性所需的信息。

文章最后讨论了针对每种联邦学习类型中隐私威胁的对策。这些对策包括差分隐私、安全计算、通信加密和ID伪装。差分隐私通过在模型中添加噪声来减少训练数据的泄露；安全计算用于在客户端和服务器之间保密模型计算过程；通信加密用于防止信息泄露给第三方；ID伪装用于防止ID泄露。文章通过详细的分析和讨论，为联邦学习中的隐私保护提供了全面的视角和实用的解决方案。