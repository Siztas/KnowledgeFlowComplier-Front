###DVORL：一种通过数据估值提升离线强化学习策略泛化能力的方法

本文探讨了在离线强化学习（Offline Reinforcement Learning, Offline RL）中如何评估和筛选数据以提升策略在目标域上的泛化能力。深度强化学习依赖大量与环境交互产生的数据，但在现实世界中，数据的收集往往成本高、风险大。离线强化学习通过使用专家或监控程序采集好的数据集，避免了在线数据收集的成本，成为在实际系统中可行的学习方式。然而，当前主流的离线RL方法往往依赖与目标任务高度匹配的数据集，这使得在面对数据市场中采购的“非原生”数据时，策略难以泛化至目标域。该问题的根源在于“源-目标域失配”（source-target domain mismatch）导致的外推误差，即模型在评估未见状态-动作对时产生不切实际的价值估计。

为此，本文提出了一个基于数据估值的新方法 DVORL（Data Valuation for Offline Reinforcement Learning），以解决离线RL中由于源-目标域差异所引起的性能下降问题。DVORL 以当前的离线RL算法为基础，在给定一个固定的源数据集和一个小规模的目标数据集的前提下，通过对源数据集中的样本进行重要性评估，筛选出对目标任务最具相关性和价值的部分，从而在保证数据利用率的同时提升策略的迁移性能。该方法借鉴了监督学习中已有的数据估值方法如 Data Shapley 和 DVRL，在 MuJoCo 平台上的 Hopper 和 Walker2d 两个环境中进行实验，通过调整躯干质量和摩擦系数构造目标域差异。实验结果表明，主流的离线RL方法如 BCQ、CQL 和 TD3+BC 在面对源-目标域失配时表现明显退化，而 DVORL 能够在所有目标配置下超过这些基线方法。

在理论建模方面，本文基于马尔可夫决策过程（MDP），设为五元组 \$(\mathcal{X}, \mathcal{U}, p, r, \gamma)\$，其中 \$\mathcal{X}\$ 是状态空间，\$\mathcal{U}\$ 是动作空间，\$p(x' \mid x, u)\$ 为状态转移概率，\$r(x, u, x')\$ 是奖励函数，\$\gamma \in \[0, 1)\$ 为折扣因子。目标是最大化回报 \$R\_t = \sum\_{i=t+1}^{\infty} \gamma^i r(x\_i, u\_i, x\_{i+1})\$，策略 \$\pi:\mathcal{X} \to \mathcal{P}(\mathcal{U})\$ 映射状态到动作分布，对应的状态-动作值函数为 \$Q^{\pi}(x, u) = \mathbb{E}*{\pi}\[R\_t \mid x, u]\$，最优策略为 \$\pi^\* = \operatorname{argmax}*u Q^\*(x, u)\$。策略可通过确定性策略梯度更新，即 \$\nabla*{\vartheta} J(\vartheta) = \mathbb{E}*{x \sim p\_{\pi}}\[\nabla\_u Q^{\pi}*{\theta}(x, u) \big|*{u = \pi(x)} \nabla\_{\vartheta} \pi\_{\vartheta}(x)]\$。

此外，本文介绍了三种当前主流的离线RL方法：BCQ（通过限制动作空间降低外推误差）、CQL（引入惩罚项抑制策略对非行为策略动作的过高估值）、TD3+BC（在策略学习中加入模仿学习约束）。这些方法虽然能够缓解某些离线学习的困难，但在源-目标域不一致时依然表现不佳。DVORL的优势在于它不改变原有的离线RL算法结构，而是作为一个前置数据筛选模块，显著提升策略的泛化能力。实验验证了DVORL方法在处理异构数据来源时的有效性，并为未来离线强化学习的迁移性和鲁棒性研究提供了新的视角。
