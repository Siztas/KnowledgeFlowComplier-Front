标题：Data Valuation for Offline Reinforcement Learning

The success of deep reinforcement learning (DRL)
hinges on the availability of training data, which is typically
obtained via a large number of environment interactions. In many real-world scenarios, costs and risks are associated with gathering these data. The field of offline reinforcement learning addresses these issues through outsourcing the collection of
data to a domain expert or a carefully monitored program
and subsequently searching for a batch-constrained optimal policy. With the emergence of data markets, an alternative to constructing a dataset in-house is to purchase external data. However, while state-of-the-art offline reinforcement learning approaches have shown a lot of promise, they currently rely on carefully constructed datasets that are well aligned with
the intended target domains. This raises questions regarding the transferability and robustness of an offline reinforcement learning agent trained on externally acquired data. In this paper, we empirically evaluate the ability of the current state-of-the-art offline reinforcement learning approaches to coping with the source-target domain mismatch within two MuJoCo environments, finding that current state-of-the-art offline reinforcement learning algorithms underperform in the target domain. To address this, we propose data valuation for offline reinforcement learning (DVORL), which allows us to identify relevant
and high-quality transitions, improving the performance and transferability of policies learned by offline reinforcement learning algorithms. The
results show that our method outperforms offline reinforcement learning baselines
on two MuJoCo environments.

Offline Reinforcement Learning (RL) – also known as batch-constrained RL – is a class of RL methods that requires the agent to learn from a static dataset of pre-collected experiences without further environment interaction[1]. This learning paradigm disentangles exploration from exploitation, lending itself to the tasks in which exploration for collecting data is costly, time-consuming, or risky[2,3]. By taking advantage of pre-collected datasets, offline RL can mitigate the technical concerns associated with online data collection, and has potential benefits for a number of real environments, such as human-robot collaboration[4].

Offline reinforcement learning outsources the collection of data to a domain expert and subsequently searches for a batch-constrained optimal policy. However, this task is challenging, as offline RL methods suffer from theextrapolation error[3,5]. This pathology occurs when offline deep RL methods are trained under one distribution but evaluated on a different one. More specifically, value functions implemented by a function approximator have a tendency to predict unrealistic values for unseen state-action pairs for standard off-policy deep RL algorithms (e. g., DQN and DDPG). This raises the need for approaches that restrict the action
space, forcing the agent to learn a behavior that is close to on-policy with respect to a subset of the given source data[3].

For offline RL methods that are designed to mitigate the extrapolation error[3,5,6], there remains the challenge that external data (e. g., purchased from data markets) may not be well aligned with the intended target domain. Therefore, the learned policy induces a different visited state-action distribution that results in a degradation in the performance of the offline RL agent.

In recent years there have been a number of efforts within the paradigm of supervised learning for overcoming thesource-target domain mismatch problemvia valuating data, includingdata Shapley[7]anddata valuation using reinforcement learning(DVRL)[8]. Such methods have shown promising results on several application scenarios such as data-value quantification, corrupted sample discovery, robust learning with noisy labels, and domain adaptation[8]. This raises the question: data valuation improve the transferability and robustness of an offline RL agent trained on externally acquired data?

To investigate the above question, we propose a data valuation approach that selects a subset of samples in the source dataset that are relevant to the target task. Our main contributions can be summarized as follows:

Inspired by DVRL[8], we propose Data Valuation for Offline Reinforcement Learning (DVORL) that for a given offline RL method, a fixed source dataset, and a small target dataset, identifies those samples of the source buffer that are relevant to the target task.

We contribute a benchmark on two Gym MuJoCo domains (Hopper and Walker2d) for which parameterizations (friction and mass of torso) for the target domain are different from those of the source domain.

We show that the state-of-the-art offline RL methods fail to generalize to different target domain configurations.

We show how our data valuation approach can improve the generalizability of the RL agent to the target domain by outperforming the existing state-of-the-art methods on all considered target domain configurations.

The rest of this paper is organized as follows.Section IIgives the background on (offline) RL. InSection III, we formally define the source-target domain mismatch problem for offline RL, and provide a motivating example inSection IV.Section Vgives an overview of related work. InSection VI, we introduce our DVORL framework.Section VIIdescribes our experiment setup. We discuss our results inSection VIII, and inSection IX, we discuss our main findings. Finally inSection Xwe conclude with suggestions for future work.

The RL problem is typically modeled by a Markov decision process (MDP), formulated as a tuple(𝒳,𝒰,p,r,γ)𝒳𝒰𝑝𝑟𝛾(\mathcal{X},\mathcal{U},p,r,\gamma), with a state space𝒳𝒳\mathcal{X}, an action space𝒰𝒰\mathcal{U}, and transition dynamicsp​(x′∣x,u)𝑝conditionalsuperscript𝑥′𝑥𝑢p\left(x^{\prime}\mid x,u\right).
At each discrete time step the agent performs an actionu∈𝒰𝑢𝒰u\in\mathcal{U}in a statex∈𝒳𝑥𝒳x\in\mathcal{X}, and transitions to a new statex′∈𝒳superscript𝑥′𝒳x^{\prime}\in\mathcal{X}based on the transition dynamicsp​(x′∣x,u)𝑝conditionalsuperscript𝑥′𝑥𝑢p\left(x^{\prime}\mid x,u\right), and receives a rewardr​(x,u,x′)𝑟𝑥𝑢superscript𝑥′r\left(x,u,x^{\prime}\right). The goal of the agent is to maximize the expectation of the sum of discounted rewards, also known as the returnRt=∑i=t+1∞γi​r​(xi,ui,xi+1)subscript𝑅𝑡superscriptsubscript𝑖𝑡1superscript𝛾𝑖𝑟subscript𝑥𝑖subscript𝑢𝑖subscript𝑥𝑖1R_{t}=\sum_{i=t+1}^{\infty}\gamma^{i}r\left(x_{i},u_{i},x_{i+1}\right), which weighs future rewards with respect to the discount factorγ∈[0,1)𝛾01\gamma\in[0,1), determining the effective horizon.
The agent makes decisions via a policyπ:𝒳→𝒫​(𝒰):𝜋→𝒳𝒫𝒰\pi:\mathcal{X}\rightarrow\mathcal{P}(\mathcal{U}), which maps a given statex𝑥xto a probability distribution over the action space𝒰𝒰\mathcal{U}. For a given policyπ𝜋\pi, the value function is defined as the expected return of an agent, starting from statex𝑥x, performing actionu𝑢u, and following the policyQπ​(s,a)=𝔼π​[Rt∣x,u]superscript𝑄𝜋𝑠𝑎subscript𝔼𝜋delimited-[]conditionalsubscript𝑅𝑡𝑥𝑢Q^{\pi}(s,a)=\mathbb{E}_{\pi}\left[R_{t}\mid x,u\right]. The state-action value function can be computed through the Bellman equation of the Q function:

GivenQπsuperscript𝑄𝜋Q^{\pi}, the optimal policyπ∗=maxu⁡Q∗​(x,u)superscript𝜋subscriptmax𝑢superscript𝑄𝑥𝑢\pi^{*}=\operatorname{max}_{u}Q^{*}(x,u), can be obtained by greedy selection over the optimal value functionQ∗​(x,u)=maxπ⁡Qπ​(x,u)superscript𝑄𝑥𝑢subscript𝜋superscript𝑄𝜋𝑥𝑢Q^{*}(x,u)=\max_{\pi}Q^{\pi}(x,u).
For environments confronting agents with the curse of dimensionality the value can be estimated with a
differentiable function approximatorQθ​(x,u)subscript𝑄𝜃𝑥𝑢Q_{\theta}(x,u), with parametersθ𝜃\theta.

In this work, we focus on continuous control problems, where, given a parameterized policyπϑsubscript𝜋italic-ϑ\pi_{\vartheta}our objective is to find an optimal policyπϑ∗subscriptsuperscript𝜋italic-ϑ\pi^{*}_{\vartheta},
with respect to the parametersϑitalic-ϑ\vartheta,
which maximizes the expected return
from the start distributionJ​(ϑ)=𝔼xi∼pπ,ui∼π⁡[R0]𝐽italic-ϑsubscript𝔼formulae-sequencesimilar-tosubscript𝑥𝑖subscript𝑝𝜋similar-tosubscript𝑢𝑖𝜋subscript𝑅0J(\vartheta)=\operatorname{\mathbb{E}}_{x_{i}\sim p_{\pi},u_{i}\sim\pi}[R_{0}][9].
The policy parametersϑitalic-ϑ\varthetacan be updated by taking the gradient of the expected return∇ϑJ​(ϑ)subscript∇italic-ϑ𝐽italic-ϑ\nabla_{\vartheta}J(\vartheta).
A popular approach to optimizing the policy is to use
actor-critic methods, where the actor (policy) can be updated
through the deterministic policy gradient algorithm[10]:∇ϑJ(ϑ)=𝔼x∼pπ[∇uQθπ(x,u)|u,π​(x)∇ϑπϑ(x)]\nabla_{\vartheta}J(\vartheta)=\operatorname{\mathbb{E}}_{x\sim p_{\pi}}[\nabla_{u}Q_{\theta}^{\pi}(x,u)\rvert_{u,\pi(x)}\nabla_{\vartheta}\pi_{\vartheta}(x)], in which the value functionQθπ​(x,u)superscriptsubscript𝑄𝜃𝜋𝑥𝑢Q_{\theta}^{\pi}(x,u)is the critic.

Standard off-policy deep RL algorithms such as deep Q-learning (DQN)[11]and deep deterministic policy gradient (DDPG)[9]are applicable in batch RL as they are based on more fundamental batch RL algorithms[12]. However, they suffer from a phenomenon, known asextrapolation error, which occurs when there is a mismatch between the given fixed batch of data and true state-action visitation of the current policy[3]. This is problematic as incorrect values of state-action pairs, which are not contained in the batch of data, are propagated through temporal difference updates of most off-policy algorithms[13], resulting in poor performance of the model[14]. Below we give an overview of approaches designed
to address the extrapolation error, which will serve as our baselines.

BCQ.Batch-Constrained deep Q-learning[3]is an offline RL method for continuous control that restricts the action space, thereby eliminating actions that are unlikely to be selected by the behavioral policyπbsubscript𝜋𝑏\pi_{b}and therefore rarely observed in the batch. Given a dataset ofN𝑁Ntransitions𝒟=𝒟absent\mathcal{D}={xt,ut,rt+1,xt+1}t=0Nsuperscriptsubscriptsubscript𝑥𝑡subscript𝑢𝑡subscript𝑟𝑡1subscript𝑥𝑡1𝑡0𝑁\left\{x_{t},u_{t},r_{t+1},x_{t+1}\right\}_{t=0}^{N}, collected by a behavior policyπbsubscript𝜋𝑏\pi_{b}, BCQ consists of four parameterized networks:
a generative modelGω:𝒳→𝒰:subscript𝐺𝜔→𝒳𝒰G_{\omega}:\mathcal{X}\rightarrow\mathcal{U}, parameterized withω𝜔\omega, a perturbation modelξϕ​(x,u,Φ)subscript𝜉italic-ϕ𝑥𝑢Φ\xi_{\phi}(x,u,\Phi), parameterized withϕitalic-ϕ\phi, and two Q-networksQϑ1​(x,u)subscript𝑄subscriptitalic-ϑ1𝑥𝑢Q_{\vartheta_{1}}(x,u),Qϑ2​(x,u)subscript𝑄subscriptitalic-ϑ2𝑥𝑢Q_{\vartheta_{2}}(x,u), parameterized withϑ1subscriptitalic-ϑ1\vartheta_{1}andϑ2subscriptitalic-ϑ2\vartheta_{2}, respectively.
The generative modelGωsubscript𝐺𝜔G_{\omega}selects the most likely action given the state with respect to the data in the batch.
Since modeling the distribution of data in the high dimensional continuous control environments is not straightforward, a variational autoencoder (VAE) is used to approximate it.
The policy is defined by samplingn𝑛nactions fromGω​(x)subscript𝐺𝜔𝑥G_{\omega}(x)and selecting the highest valued action according to a Q-network as it is easier to sample fromπb​(u∣x)subscript𝜋𝑏conditional𝑢𝑥\pi_{b}(u\mid x)than modelingπb​(u∣x)subscript𝜋𝑏conditional𝑢𝑥\pi_{b}(u\mid x)in a continuous action space.
The perturbation modelξϕ​(x,u,Φ)subscript𝜉italic-ϕ𝑥𝑢Φ\xi_{\phi}(x,u,\Phi), parameterized withϕitalic-ϕ\phi, models the distribution of data in the batch and is a residual added to the sampled actions in the range[−Φ,Φ]ΦΦ[-\Phi,\Phi].
This model is trained with the DDPG[10]and can be thought of as a behavioral cloning model.
Since the perturbation model together with the sampling can be considered as a hierarchical policy, BCQ can also be considered an actor-critic method.

CQL.To prevent the training policy from overestimating the Q-values, Conservative Q-Learning (CQL)[6]utilizes a penalized empircal RL objective. More precisely, CQL optimizes the value function not only to minimize the temporal difference error based on the interactions seen on the dataset but also minimizes the value of actions that the currently trained policy takes, while at the same time maximizing the value of actions taken by the behavioral policy during data generation. This results in a conservativeQQ\mathrm{Q}-function.

TD3+BC.Twin Delayed Deep Deterministic (TD3) policy gradient with Behavior Cloning (BC) is a model-free algorithm that does not explicitly learn a model of the behavioral policy, while trains a policy to mimic the behavior policy from the data[15]. It directly penalizes Euclidean distance to the actions that were recorded in the dataset.

The training samples that machine learning (ML) models are trained with are not all equally valuable[16].
A sample can be considered low-quality due to noisy input values, a noisy label, or the source-target domain mismatch problem.
Removing low-quality samples has been shown to increase the performance of ML models[7,8]. The task of quantifying the quality of individual datum to the overall performance is referred to asdata valuation.
In supervised learning, data valuation is formally defined as follows.
Given a source (training) dataset𝒟s=subscript𝒟𝑠absent\mathcal{D}_{s}={(𝐱i,yi)}i=1Nsuperscriptsubscriptsubscript𝐱𝑖subscript𝑦𝑖𝑖1𝑁\left\{\left(\mathbf{x}_{i},y_{i}\right)\right\}_{i=1}^{N}and a target (test) dataset𝒟𝒯={(𝐱j𝒯,yj𝒯)}j=1Msubscript𝒟𝒯superscriptsubscriptsuperscriptsubscript𝐱𝑗𝒯superscriptsubscript𝑦𝑗𝒯𝑗1𝑀\mathcal{D_{T}}=\left\{\left(\mathbf{x}_{j}^{\mathcal{T}},y_{j}^{\mathcal{T}}\right)\right\}_{j=1}^{M}where𝐱∈𝒳𝐱𝒳\mathbf{x}\in\mathcal{X}is ad𝑑d-dimensional feature vector, andy∈𝒴𝑦𝒴y\in\mathcal{Y}is a corresponding label, the goal is to find a subset𝒟∗={(𝐱k,yk)∣(𝐱k,yk)∈𝒟𝒮}k=1Ksuperscript𝒟superscriptsubscriptconditional-setsubscript𝐱𝑘subscript𝑦𝑘subscript𝐱𝑘subscript𝑦𝑘subscript𝒟𝒮𝑘1𝐾\mathcal{D}^{*}=\left\{\left(\mathbf{x}_{k},y_{k}\right)\mid\left(\mathbf{x}_{k},y_{k}\right)\in\mathcal{D_{S}}\right\}_{k=1}^{K}of the source dataset𝒟𝒮subscript𝒟𝒮\mathcal{D_{S}}that maximizes the performance of the trained model on the target dataset𝒟𝒯subscript𝒟𝒯\mathcal{D_{T}}[7,17].

In this section, we formally define the data valuation problem for offline RL.

We assume the availability of a source dataset𝒟𝒮=subscript𝒟𝒮absent\mathcal{D_{S}}={(xi𝒮,ui𝒮,xi′𝒮,ri𝒮,ei𝒮)}i=1N∼𝒫𝒮similar-tosuperscriptsubscriptsuperscriptsubscript𝑥𝑖𝒮subscriptsuperscript𝑢𝒮𝑖superscriptsubscriptsuperscript𝑥′𝑖𝒮superscriptsubscript𝑟𝑖𝒮superscriptsubscript𝑒𝑖𝒮𝑖1𝑁subscript𝒫𝒮\{(x_{i}^{\mathcal{S}},u^{\mathcal{S}}_{i},{x^{\prime}_{i}}^{\mathcal{S}},{r}_{i}^{\mathcal{S}},{e}_{i}^{\mathcal{S}})\}_{i=1}^{N}\sim\mathcal{P}_{\mathcal{S}}and a
target dataset𝒟𝒯=subscript𝒟𝒯absent\mathcal{D_{T}}={(xi𝒯,ui𝒯,xi′𝒯,ri𝒯,ei𝒯)}i=1M∼𝒫𝒯similar-tosuperscriptsubscriptsuperscriptsubscript𝑥𝑖𝒯subscriptsuperscript𝑢𝒯𝑖superscriptsubscriptsuperscript𝑥′𝑖𝒯superscriptsubscript𝑟𝑖𝒯superscriptsubscript𝑒𝑖𝒯𝑖1𝑀subscript𝒫𝒯\{(x_{i}^{\mathcal{T}},u^{\mathcal{T}}_{i},{x^{\prime}_{i}}^{\mathcal{T}},{r}_{i}^{\mathcal{T}},{e}_{i}^{\mathcal{T}})\}_{i=1}^{M}\sim\mathcal{P}_{\mathcal{T}},
wherex∈ℝm𝑥superscriptℝ𝑚x\in\mathbb{R}^{m}is a state;u∈ℝn𝑢superscriptℝ𝑛u\in\mathbb{R}^{n}is the action that the agent performs at the statex𝑥x;r∈ℝ𝑟ℝr\in\mathbb{R}is the reward that the agent gets by performing the actionu𝑢uin the statex𝑥x;x′∈ℝmsuperscript𝑥′superscriptℝ𝑚x^{\prime}\in\mathbb{R}^{m}is the state that the agent transitions to (i.e. next state); ande∈{0,1}𝑒01e\in\{0,1\}indicates whether thex′superscript𝑥′x^{\prime}is a terminal state.
We assume that the target dataset𝒟𝒯subscript𝒟𝒯\mathcal{D_{T}}is much smaller than the the source dataset𝒟𝒮subscript𝒟𝒮\mathcal{D_{S}}, thereforeN≫Mmuch-greater-than𝑁𝑀N\gg M.
Furthermore, the source distribution𝒫𝒮subscript𝒫𝒮\mathcal{P}_{\mathcal{S}}can be different from the target distribution𝒫𝒯subscript𝒫𝒯\mathcal{P}_{\mathcal{T}}, confronting our learner with the source-target domain mismatch problem.
As in supervised learning, our goal is to find a (sub)set𝒟∗⊆𝒟𝒮superscript𝒟subscript𝒟𝒮\mathcal{D}^{*}\subseteq\mathcal{D_{S}}, and seek a batch-constrained policyπ𝜋\pi,
that when trained on𝒟∗superscript𝒟\mathcal{D}^{*}can generalize to the target domain used to construct𝒟𝒯subscript𝒟𝒯\mathcal{D_{T}}.
Therefore, we have a transfer learning problem.

To formally define transfer learning for offline RL, we draw on the formulation from Zhu et al.[18].
Let𝚿𝒮=subscript𝚿𝒮absent\mathbf{\Psi}_{\mathcal{S}}={Ψ𝒮∣Ψ𝒮∈𝚿𝒮}conditional-setsubscriptΨ𝒮subscriptΨ𝒮subscript𝚿𝒮\left\{{\Psi}_{\mathcal{S}}\mid{\Psi}_{\mathcal{S}}\in{\mathbf{\Psi}}_{\mathcal{S}}\right\}be a set of source domains andΨ𝒯subscriptΨ𝒯{\Psi}_{\mathcal{T}}be a target domain, where each domain corresponds to an MDP. Therefore, the MDPs in the source domainΨ𝒮subscriptΨ𝒮\Psi_{\mathcal{S}}and target domainΨ𝒯subscriptΨ𝒯\Psi_{\mathcal{T}}are defined as(𝒳𝒮,𝒰𝒮,p𝒮,r𝒮,γ𝒮)subscript𝒳𝒮subscript𝒰𝒮subscript𝑝𝒮subscript𝑟𝒮subscript𝛾𝒮(\mathcal{X}_{\mathcal{S}},\mathcal{U}_{\mathcal{S}},p_{\mathcal{S}},r_{\mathcal{S}},\gamma_{\mathcal{S}})and(𝒳𝒯,𝒰𝒯,p𝒯,r𝒯,γ𝒯)subscript𝒳𝒯subscript𝒰𝒯subscript𝑝𝒯subscript𝑟𝒯subscript𝛾𝒯(\mathcal{X}_{\mathcal{T}},\mathcal{U}_{\mathcal{T}},p_{\mathcal{T}},r_{\mathcal{T}},\gamma_{\mathcal{T}}), respectively.
We assume prior knowledge𝒟𝒮subscript𝒟𝒮\mathcal{D}_{\mathcal{S}}provided by the set of source domains𝚿𝒮subscript𝚿𝒮\mathbf{\Psi}_{\mathcal{S}}and accessible to the target domainΨ𝒯subscriptΨ𝒯{\Psi}_{\mathcal{T}}.

By leveraging the prior information𝒟𝒮subscript𝒟𝒮\mathcal{D_{S}}from the source domain𝚿𝒮subscript𝚿𝒮\mathbf{\Psi}_{\mathcal{S}}as well as information𝒟𝒯subscript𝒟𝒯\mathcal{D}_{\mathcal{T}}provided byΨ𝒯subscriptΨ𝒯\Psi_{\mathcal{T}}, transfer learning aims to learn an optimal policyπ∗superscript𝜋\pi^{*}for the target domainΨ𝒯subscriptΨ𝒯{\Psi}_{\mathcal{T}}, such that:

whereπ=ϕ​(𝒟𝒮∼Ψ𝒮,𝒟𝒯∼Ψ𝒯):𝒳𝒯→𝒰𝒯:𝜋italic-ϕformulae-sequencesimilar-tosubscript𝒟𝒮subscriptΨ𝒮similar-tosubscript𝒟𝒯subscriptΨ𝒯→subscript𝒳𝒯subscript𝒰𝒯\pi=\phi\left(\mathcal{D_{S}}\sim\Psi_{\mathcal{S}},\mathcal{D}_{\mathcal{T}}\sim\Psi_{\mathcal{T}}\right):\mathcal{X}_{\mathcal{T}}\rightarrow\mathcal{U}_{\mathcal{T}}is a function mapping the states to actions for the target domainΨ𝒯subscriptΨ𝒯\Psi_{\mathcal{T}}, learned based on information from both𝒟𝒯subscript𝒟𝒯\mathcal{D}_{\mathcal{T}}and𝒟𝒮subscript𝒟𝒮\mathcal{D}_{\mathcal{S}}.

The source and target domains can have distinct state spaces, but their action spaces have to be the same and their transition function and reward function have to be similar as they share internal dynamics. We focus on policy transfers where𝒳𝒮=𝒳𝒯subscript𝒳𝒮subscript𝒳𝒯\mathcal{X}_{\mathcal{S}}=\mathcal{X}_{\mathcal{T}},𝒰𝒮=𝒰𝒯subscript𝒰𝒮subscript𝒰𝒯\mathcal{U}_{\mathcal{S}}=\mathcal{U}_{\mathcal{T}},r𝒮=r𝒯subscript𝑟𝒮subscript𝑟𝒯r_{\mathcal{S}}=r_{\mathcal{T}},
butp𝒮≠p𝒯subscript𝑝𝒮subscript𝑝𝒯p_{\mathcal{S}}\neq p_{\mathcal{T}}.

The source and target domains can have distinct transition functions because a change in environment parameters (e. g., mass of torso and friction) results in a different probability functionp𝑝p, which itself is conditioned on(x,u,x′)𝑥𝑢superscript𝑥′(x,u,x^{\prime}), wherex𝑥x,u𝑢u, andx′superscript𝑥′x^{\prime}denote state, action, and next state, respectively.

However, since the target dataset is very small, compared to the source dataset (N≫Mmuch-greater-than𝑁𝑀N\gg M), state-action transition(x,u,x′)𝑥𝑢superscript𝑥′(x,u,x^{\prime})is too restrictive. Thus, we also consider the case(x)𝑥(x)in which only the change in the distribution of the state space is taken into account.

For our motivating example, we consider two scenarios involving a cobot, depicted inFigure 1.
In the source domain the cobot is performing pick-and-place task while the target domain confronts the cobot with a sorting task. Clearly, a learning agent trained on the source task will perform poorly on the target task. However, our hypothesis is that data valuation can help us identify samples that are relevant for both tasks. For instance, both tasks have pick-up and place actions in common. Therefore, the goal is to find the relevant subset of the source dataset that allows the agent to learn a policy that generalizes to the target domain.

The RL literature contains numerous techniques for dealing with the source-target domain mismatch problem.
Noteworthy contributions here include:
EPOPT[19],
which is a combination of policy transfer through source domain ensemble and learning from limited demonstrations for the fast adaptation to the target domain;
UP-OSI[20]trains robust agent policies using a large number of synthetic demonstrations from a simulator to deal with environments with unknown dynamics;
CAD2RL[21]learns latent representations
from observations in the source domain that are generally applicable to the target domain;
DARLA[22], a zero-shot transfer learning method that learns disentangled representations which are robust against domain shifts;
and SAFER[23], which accelerates policy learning on complex control tasks by considering safety constraints.

Meanwhile, the literature on off-policy RL includes principled experience replay memory sampling techniques. Prioritized Experience Replay (PER)[24](e.g.,[25,26,27]) attempts to sample transitions that contribute the most toward learning.
prioritized replay with weighted importance sampling
can improve BCQ.
However, the majority of the work to date on offline RL is focused on preventing the training policy from being too disjoint with the behavioral policy[3,6,28].
To increase the generalization capacity of offline RL methods, Kostrikov et al.[29]propose in-sample Q-learning (IQL), which approximates the policy improvement step by considering the state value function as a random variable with some randomness determined by the action, and then taking a state-conditional expectile of this random variable to estimate the value of the best actions in that state.

In contrast to the offline RL methods listed above, our work focuses on valuating the suitability of state transition tuples for a given target domain.
Here we draw inspiration from the literature on data valuation for supervised learning.
Ghorbani et al. in[30]propose the distributional Shapley, which is a framework in which the value of a point is defined in the context of underlying data distribution. The reformulation of the data Shapley value as a distributional quantity reduces the dependence on the specific draw of data as the valuation function does not depend on a fixed data set.
Ghorbani and Zou propose the Neuron Shapley framework[31]to quantify how individual neurons contribute to the prediction and performance of a deep neural network (DNN). However, the limitation of Shapely based methods is that it’s computationally expensive or even impossible to quantify the contribution of each individual sample to the overall performance of the model, in particular, for complex models such as DNNs[8].

Wang et al.[32]propose a minimax game-based transfer learning technique for selective transfer learning that consists of a selector, a discriminator, and a transfer learning module, playing a minimax game to find useful source data.
Yoon et al.[8]introduce a framework for data valuation in supervised learning tasks, making use of RL to determine how likely each training sample is used in the training of the predictor model.
This method integrates data valuation into the training procedure of the predictor model, making the predictor and data value estimator able to improve each other’s performance.

Despite the success of the existing works on data valuation, they are only applicable to the supervised learning tasks in which the availability of labels is assumed. Therefore, they are not directly applicable to the RL setting where there is no ground-truth for the transitions.
This emphasizes a need for a data valuation method that is applicable to the RL setting.

DVORL consists of two learnable functions: (1) a data value estimator (DVE) modelvϕsubscript𝑣italic-ϕv_{\phi}and (2) an offline reinforcement learning model.
Inspired by DVRL[8], we adapt the REINFORCE algorithm[33]and use it as the DVE.
We use a DNN for the DVE.
The goal is to find the parametersϕ∗superscriptitalic-ϕ\phi^{*}of the DNN so that the network returns the optimal probability distribution over the sample space.

The DVE modelvϕ:𝒳×𝒰×𝒳′×ℛ×ℰ→[0,1]:subscript𝑣italic-ϕ→𝒳𝒰superscript𝒳′ℛℰ01v_{\phi}:\mathcal{X}\times\mathcal{U}\times\mathcal{X^{\prime}}\times\mathcal{R}\times\mathcal{E}\rightarrow[0,1]is optimized to output data values corresponding to the relevance of training samples to the target task. We formulate the corresponding optimization problem as:

maxϕ⁡J​(πϕ)=𝔼(x𝒮,u𝒮,x𝒮′)∼P𝒮(x𝒯,u𝒯,x𝒯′)∼P𝒯⁡[rϕ​((x𝒮,u𝒮,x𝒮′),(x𝒯,u𝒯,x𝒯′))]subscriptitalic-ϕ𝐽subscript𝜋italic-ϕsubscript𝔼similar-tosuperscript𝑥𝒮superscript𝑢𝒮superscript𝑥superscript𝒮′superscript𝑃𝒮similar-tosuperscript𝑥𝒯superscript𝑢𝒯superscript𝑥superscript𝒯′superscript𝑃𝒯subscript𝑟italic-ϕsuperscript𝑥𝒮superscript𝑢𝒮superscript𝑥superscript𝒮′superscript𝑥𝒯superscript𝑢𝒯superscript𝑥superscript𝒯′\max_{\phi}J\left(\pi_{\phi}\right)=\operatorname{\mathbb{E}}_{\begin{subarray}{c}(x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\sim P^{\mathcal{S}}\\
(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}})\sim P^{\mathcal{T}}\end{subarray}}\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right],

where

corresponds to the reciprocal of the KL divergence between the batch of source dataset and target dataset.
Therefore, the objective of the network is to assign high probabilities to samples whose reward function valuerϕ​((x𝒮,u𝒮,x𝒮′),(x𝒯,u𝒯,x𝒯′))subscript𝑟italic-ϕsuperscript𝑥𝒮superscript𝑢𝒮superscript𝑥superscript𝒮′superscript𝑥𝒯superscript𝑢𝒯superscript𝑥superscript𝒯′r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))is high.

Training.As shown inAlgorithm 1(lines 4 to 10), all the samples of source buffer𝒟𝒮subscript𝒟𝒮\mathcal{D_{S}}are divided into batches and each batch𝒟𝒮′superscriptsubscript𝒟𝒮′\mathcal{D_{S}}^{\prime}is given as input to the DVE (with shared parameters across the batch).
The KL divergence between the distribution of the state-action transition(x,u,x′)𝑥𝑢superscript𝑥′(x,u,x^{{}^{\prime}})of the given batch and that of the small target buffer𝒟𝒯subscript𝒟𝒯\mathcal{D_{T}}is calculated and used as the reward signalrs​i​gsubscript𝑟𝑠𝑖𝑔r_{sig}for training the DVE.
Letw=vϕ​(xi𝒮,ui𝒮,xi′𝒮,ri𝒮,ei𝒮)𝑤subscript𝑣italic-ϕsuperscriptsubscript𝑥𝑖𝒮subscriptsuperscript𝑢𝒮𝑖superscriptsubscriptsuperscript𝑥′𝑖𝒮superscriptsubscript𝑟𝑖𝒮superscriptsubscript𝑒𝑖𝒮w=v_{\phi}(x_{i}^{\mathcal{S}},u^{\mathcal{S}}_{i},{x^{\prime}_{i}}^{\mathcal{S}},{r}_{i}^{\mathcal{S}},{e}_{i}^{\mathcal{S}})denote the probability that the samplei𝑖iof the source buffer is used for training the offline reinforcement learning model.

Our adapted version of REINFORCE algorithm, has the following object function for the policyπϕsubscript𝜋italic-ϕ\pi_{\phi}:

J​(πϕ)=𝔼(x𝒮,u𝒮,x𝒮′)∼P𝒮(x𝒯,u𝒯,x𝒯′)∼P𝒯w∼πϕ​(𝒟𝒮′,⋅)⁡[rϕ​((x𝒮,u𝒮,x𝒮′),(x𝒯,u𝒯,x𝒯′))]=∫P𝒯​((x𝒮,u𝒮,x𝒮′))​∑w∈[0,1]Nπϕ​(𝒟𝒮′,w)⋅[rϕ​((x𝒮,u𝒮,x𝒮′),(x𝒯,u𝒯,x𝒯′))]​d​((x𝒮,u𝒮,x𝒮′)).𝐽subscript𝜋italic-ϕabsentsubscript𝔼similar-tosuperscript𝑥𝒮superscript𝑢𝒮superscript𝑥superscript𝒮′superscript𝑃𝒮similar-tosuperscript𝑥𝒯superscript𝑢𝒯superscript𝑥superscript𝒯′superscript𝑃𝒯similar-to𝑤subscript𝜋italic-ϕsuperscriptsubscript𝒟𝒮′⋅subscript𝑟italic-ϕsuperscript𝑥𝒮superscript𝑢𝒮superscript𝑥superscript𝒮′superscript𝑥𝒯superscript𝑢𝒯superscript𝑥superscript𝒯′missing-subexpressionabsentsuperscript𝑃𝒯superscript𝑥𝒮superscript𝑢𝒮superscript𝑥superscript𝒮′subscript𝑤superscript01𝑁subscript𝜋italic-ϕsuperscriptsubscript𝒟𝒮′𝑤missing-subexpression⋅absentdelimited-[]subscript𝑟italic-ϕsuperscript𝑥𝒮superscript𝑢𝒮superscript𝑥superscript𝒮′superscript𝑥𝒯superscript𝑢𝒯superscript𝑥superscript𝒯′𝑑superscript𝑥𝒮superscript𝑢𝒮superscript𝑥superscript𝒮′\begin{aligned} J\left(\pi_{\phi}\right)&=\operatorname{\mathbb{E}}_{\begin{subarray}{c}\begin{subarray}{c}(x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\sim P^{\mathcal{S}}\\
(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}})\sim P^{\mathcal{T}}\\
w\sim\pi_{\phi}(\mathcal{D_{S}}^{\prime},\cdot)\end{subarray}\end{subarray}}\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right]\\
&=\int P^{\mathcal{T}}\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\sum_{w\in[0,1]^{N}}\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\\
&\quad\cdot\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right]d\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right).\end{aligned}

In the above equation,πϕ​(𝒟𝒮′,w)subscript𝜋italic-ϕsuperscriptsubscript𝒟𝒮′𝑤\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)is the probability that the selection probability vectorw𝑤woccurs. In this way, the policy directly uses the values output by the DVE. This is different from the DVRL[8], which uses a binary selection vector𝐬=(s1,…,sBs)𝐬subscript𝑠1…subscript𝑠subscript𝐵𝑠\mathbf{s}=\left(s_{1},\ldots,s_{B_{s}}\right)wheresBssubscript𝑠subscript𝐵𝑠s_{B_{s}}denotes the batch size,si∈{0,1}subscript𝑠𝑖01s_{i}\in\{0,1\}, andP​(si=1)=wi𝑃subscript𝑠𝑖1subscript𝑤𝑖P\left(s_{i}=1\right)=w_{i}. Thus, in our training, the DVE has no control over exploration and just provides weightings for the given samples and is tuned accordingly.

It should be noted that we use the whole input information of the source buffer (i.e.,(x,u,x′,r,e)𝑥𝑢superscript𝑥′𝑟𝑒\left(x,u,x^{\prime},{r},{e}\right)) for calculating the data values (i.e.,𝒟𝒮′superscriptsubscript𝒟𝒮′\mathcal{D_{S}}^{\prime}in policyπϕ​(𝒟𝒮′,𝐰)subscript𝜋italic-ϕsuperscriptsubscript𝒟𝒮′𝐰\pi_{\phi}(\mathcal{D_{S}}^{\prime},\mathbf{w})); however, we only use the information of the state-action transition (x,u,x′𝑥𝑢superscript𝑥′x,u,x^{\prime}) for calculating the reward signal, used for updating the DVE, that is consistent with our formulation of transfer learning wherep𝒮≠p𝒯subscript𝑝𝒮subscript𝑝𝒯p_{\mathcal{S}}\neq p_{\mathcal{T}}as the transition probability function is conditioned on the state-action transition (x,u,x′𝑥𝑢superscript𝑥′x,u,x^{\prime}).

We calculate the gradient of the above objective function in the following.

∇ϕJ​(πϕ)=∫PT​((x𝒮,u𝒮,x𝒮′))​∑w∈[0,1]N∇ϕπϕ​(𝒟𝒮′,w)⋅[rϕ((x𝒮,u𝒮,x𝒮′),(x𝒯,u𝒯,x𝒯′)))]d((x𝒮,u𝒮,x𝒮′))=∫PT((x𝒮,u𝒮,x𝒮′))[∑x∈[0,1]N∇ϕπϕ​(𝒟𝒮′,w)πϕ​(𝒟𝒮′,w)⋅πϕ(𝒟𝒮′,w)[rϕ((x𝒮,u𝒮,x𝒮′),(x𝒯,u𝒯,x𝒯′))]]d((x𝒮,u𝒮,x𝒮′))=∫PT((x𝒮,u𝒮,x𝒮′))[∑w∈[0,1]N∇ϕlog(πϕ(𝒟𝒮′,w))⋅πϕ(𝒟𝒮′,w)[rϕ((x𝒮,u𝒮,x𝒮′),(x𝒯,u𝒯,x𝒯′))]]d((x𝒮,u𝒮,x𝒮′))=𝔼(x𝒮,u𝒮,x𝒮′)∼PS(x𝒯,u𝒯,x𝒯′)∼PTw∼πϕ​(𝒟S,⋅)​[rϕ​((x𝒮,u𝒮,x𝒮′),(x𝒯,u𝒯,x𝒯′))]⋅∇ϕlog(πϕ(𝒟𝒮′,w)).\begin{aligned} \nabla_{\phi}J\left(\pi_{\phi}\right)&=\int P^{T}\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\sum_{w\in[0,1]^{N}}\nabla_{\phi}\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\\
&\quad\cdot\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}})))\right]d\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\\
&=\int P^{T}\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\left[\sum_{x\in[0,1]^{N}}\frac{\nabla_{\phi}\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)}{\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)}\right.\\
&\quad\left.\cdot\;\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right]\vphantom{\sum_{x\in[0,1]^{N}}}\right]d\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\\
&=\int P^{T}\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\left[\sum_{w\in[0,1]^{N}}\nabla_{\phi}\log\left(\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\right)\right.\\
&\quad\left.\cdot\;\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right]\vphantom{\sum_{w\in[0,1]^{N}}}\right]d\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\\
&=\underset{\begin{subarray}{c}(x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\sim P^{S}\\
(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}})\sim P^{T}\\
w\sim\pi_{\phi}(\mathcal{D}_{S},\cdot)\end{subarray}}{\mathbb{E}}\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right]\\
&\quad\cdot\nabla_{\phi}\log\left(\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\right).\end{aligned}

To enhance the stability of the DVE, we use the moving averagerr​o​l​l​i​n​gsubscript𝑟𝑟𝑜𝑙𝑙𝑖𝑛𝑔r_{rolling}of the previous signal rewards with the window sizeω𝜔\omegaas the baseline. The baseline reduces the variance of the gradient estimates[34].

Inference.As shown inAlgorithm 1(lines 11 to 18), after all the samples of the source buffer are used for training the DVE, the fully-trained DVE is used for outputting the data values of the original source buffer. The samples whose corresponding data values are lower than the selection thresholdϵitalic-ϵ\epsilonare filtered out and the remaining subset of samples form the new source buffer𝒟𝒮∗superscriptsubscript𝒟𝒮\mathcal{D_{S}^{*}}that is relevant to the target domain:

𝒟𝒮∗={(xi𝒮,ui𝒮,xi′𝒮,ri𝒮,ei𝒮)∈𝒟𝒮|i=1,…,N;wi≥ϵ}superscriptsubscript𝒟𝒮conditional-setsuperscriptsubscript𝑥𝑖𝒮subscriptsuperscript𝑢𝒮𝑖superscriptsubscriptsuperscript𝑥′𝑖𝒮superscriptsubscript𝑟𝑖𝒮superscriptsubscript𝑒𝑖𝒮subscript𝒟𝒮formulae-sequence𝑖1…𝑁subscript𝑤𝑖italic-ϵ\mathcal{D_{S}^{*}}=\left\{\left(x_{i}^{\mathcal{S}},u^{\mathcal{S}}_{i},{x^{\prime}_{i}}^{\mathcal{S}},{r}_{i}^{\mathcal{S}},{e}_{i}^{\mathcal{S}}\right)\in\mathcal{D_{S}}\;|\;i=1,\ldots,N;w_{i}\geq\epsilon\right\}.

Finally, the buffer𝒟𝒮∗superscriptsubscript𝒟𝒮\mathcal{D_{S}^{*}}is given to an offline RL model for training.

[图片: images\image_1.png]
图片说明: Figure 1:Illustration of the DVORL framework. (1) A batch of source buffer samples is given as input to the data value estimator (DVE). (2) KL Divergence between the distribution of the state-action transitions of the given batch and the distribution of the state-action transitions of target buffer (whose transition items are collected by a domain expert on the target domain) is calculated and (3) used as the reward signal for updating the DVE. (4) After all the samples of the source buffer are used for training the DVE, (5) the fully trained DVE is used for outputting the data values of the source buffer (6) that are filtered out by removing those values being lower than the selection threshold. (7) This results in a subset of the source buffer that is relevant to the target domain and is used for training the given base offline RL algorithm.

In this work, we use the following offline RL methods discussed inSection IIas baselines: (Vanilla) BCQ, CQL and TD3+BC. We also evaluated the performance of BEAR[5]on our considered domains. However, as found by[6], we found CQL and TD3+BC outperformed BEAR, and therefore focused our evaluation on the above three methods and our DVORL.

We use the (Vanilla) BCQ as the base model in our DVORL, and we refer to it as Data Valuation based BCQ (DVBCQ). The reason for using the Vanilla BCQ is that it is the most commonly used offline RL algorithm, and we also intend to show how the selection of relevant transitions can help a base model, underperforming other methods in most cases, outperform the state-of-the-art methods in terms of transferability of learned policy to different target configurations. We consider two versions of DVBCQ: i) DVBCQ(x)𝑥(x)using information of states, and ii) DVBCQ(x,u,x′)𝑥𝑢superscript𝑥′(x,u,x^{{}^{\prime}})using information of state-action transition, for calculating the similarity between source and target buffers.

The DVORL agent learns from a dataset collected by a domain expert. In our experiments, for each domain and domain parametrization, we trained a DDPG agent for one million iterations and used the fully-trained agent for generating the buffers with the size one million and ten thousand for the source and target, respectively.

In this work, we use the following two MuJoCo domains:

Hopper-v3:The Hopper is a simulated monopod robot with 4 body links including the torso, upper leg, lower leg,
and foot, together with 3 actuated joints. This domain has an 11-dimensional state space
including joint angles and joint velocities and a 3-dimensional action space corresponding to torques at the joints. The goal is to make the hopper hop forward as fast as possible.

Walker2d-v3:The Walker is a simulated bipedal robot consisting of 7 body links including to two legs and a torso, along with 6 actuated joints. This domain has a 17-dimensional state space including joint angles and joint velocities and a 4-dimensional action space corresponding to joint torques. The goal is to make the robot walk forward as fast as possible.

DomainSource ConfigTarget ConfigNameHopper-v3F: 2.0T: 0.05F: 2.0T: 0.05Hopper-SourceF: 2.5T: 0.075Hopper-Target1F: 3.0T: 0.075Hopper-Target2Walker2d-v3F: 0.9T: 0.05F: 0.9T: 0.05Walker2d-SourceF: 1.125T: 0.075Walker2d-Target1F: 1.35T: 0.075Walker2d-Target2

For our experiments, we shall distinguish betweensourceandtargetdomains. The source domain is the one within which the samples are gathered by a fully-trained DDPG. The target domain is the domain within which the DVORL agent is to be deployed. To study the extent to which DVBCQ can cope with modified domain configurations, we consider two scenarios with respect to the source and target domains:

Identical Source and Target Domains:Domain configuration for gathering samples and training DVORL agent remain the same. This is the simplest setting where the DDPG agent gathers samples in an environment with a domain parameterization identical to the domain within which the DVORL agent will be deployed. For this setting, we consider two datasets “Hopper-Source” and “Walker-Source”.

Transfer Learning:Samples are gathered from a source domain with a parameterization that differs from the target domain. More precisely, the target domain will have differentmass of torsoandfrictioncoefficients compared with the source domain. For this setting, we consider four datasets “Hopper-Target1”, “Hopper-Target2”, “Walker2d-Target1”, and “Walker2d-Target2”.

All the considered source and target domain configurations are presented inTable I.

For all the competitors, we used the default parameters values reported in the corresponding papers. Hyperparameters of DVORL are selected by grid search. Since we used the BCQ in our DVORL method, we report the used parameter values of Data Valuation based BCQ (DVBCQ), listed inTable II. The parameters values of the baseline (Vanilla) BCQ are the same as those of the base agent in our DVBCQ.

ParameterValueDescriptiondve_batch_size200batch size for DVEdve_hidden_layers[128, 128]Number of nodes in hidden layersmoving_average_window_size20Window size for the moving averageselection_threshold0.1selection thresholdmini_batch_size100Batch size for offline RL modeldiscount0.9Discount factortau0.005Target network update rate of BCQlambda0.75Weighting for clipped double Q-learningphi0.05Max perturbation parameter for BCQ

Our implementation of the DVORL builds
on OpenAI gym’s[35]control environments with the MuJoCo[36]physics simulator.

Figure 2shows the performance of BCQ, CQL, TD3+BC, DVBCQ(x)𝑥(x)and DVBCQ(x,u,x′)𝑥𝑢superscript𝑥′(x,u,x^{{}^{\prime}})on the source domain and two different target domain configurations described inSectionVII-D. The models are trained for 1M iterations, and the results are averaged over ten runs on target domain environments with randomly-chosen seeds.

Identical source-domain:For DVBCQ and other baselines, we report the average return achieved by the best policy with respect to checkpoints saved throughout the run. For the identical source-domain setting, both DVBCQ(x)𝑥(x)and DVBCQ(x,u,x′)𝑥𝑢superscript𝑥′(x,u,x^{{}^{\prime}})significantly outperform all baselines on Hopper environment, and their performance is superior to BCQ and CQL, while underperforming TD3+BC on Walker2d environment.

Transfer learning:For transfer learning setting, DVBCQ(x)𝑥(x)outperforms both target domains whose configurations (mass of torso and friction) differ from those of the source domain, on both Hopper and Walker2d environments. However, DVBCQ(x,u,x′)𝑥𝑢superscript𝑥′(x,u,x^{{}^{\prime}})underperforms CQL on Hopper environment and TD3+BC on Walker environment but it has competitive performance compared with other baselines.

It should be noted that the there is a significant difference between the performance of DVBCQ and its base model BCQ.

[图片: images\image_2.png]
图片说明: Figure 2:Performance of BCQ, CQL, TD3+BC, and DVBCQ on the source domain and two different target domain configurations (described inSectionVII-D), where the models are trained for 1M iterations, and the results are averaged over ten runs on target domain environments with randomly-chosen seeds. For DVBCQ and other baselines, we report the performance achieved by the best policy with respect to checkpoints saved throughout the run.

Figure 3shows the performance of BCQ models trained on the source dataset with different selection thresholds and evaluated on a different target domain configuration (Walker2d-Target2), where all the models are trained for 200K iterations, and a fixed seed is used for the evaluation environment. We consider five thresholds (0.0, 0.1, …, 0.4) for excluding the high/low-value data samples of the source dataset. In addition, we report the average return of the best policy (with respect to checkpoints saved throughout the run) learned for each point.

As shown inFigure 3, removing low-value samples from the source dataset can help the RL agent learn only those transitions relevant to the target domain configuration and therefore achieve better performance on the target domain (green line). On the other hand, removing high-value samples from the source dataset significantly deteriorates the RL agent’s performance (red line).

The findings inFigure 3support the opening hypothesis that excluding high-value samples worsens the performance of the offline RL methods.

[图片: images\image_3.png]
图片说明: Figure 3:Performance of BCQ models trained on the source
dataset with different selection thresholds and evaluated on a different target domain configuration (Walker2d-Target2), where all the models are trained for 200.000 iterations, and a fixed seed is used for the evaluation environment. Excluding high-value samples (red line) aggravates the performance of the offline RL methods. However, excluding low-value samples (green line) does not deteriorate the performance as much as that of the high-value samples.

Our results suggest that DVORL can improve the offline reinforcement learning methods on both identical source-target and transfer learning settings. In addition, our method helps the offline RL methods achieve significantly higher performance with fewer iterations, making them more efficient. Furthermore, our method can identify the relevant samples of the source domain to different target domain configurations. This is of high importance and has many use cases, such as learning from an externally acquired dataset and safe RL.

It should be noted that our goal is not to show that our proposed method outperforms all the state-of-the-art offline RL methods on both source and target domains, but to show that the data valuation for the offline reinforcement learning (DVORL) framework can improve the performance of the baseline algorithms.

For future work, we aim to examine whether the size of target buffer plays a role in the performance of DVORL. We intend conduct some experiments on real-world domains and compare our results to other data valuation methods like Data Shapely. Moreover, we plan to improve our reward function by taking into account dynamics of the model.

We also aim to investigate the extent to which DVORL can identify the safe transitions within a safe reinforcement learning setting. We also plan to apply the idea of transition valuation to the safe multi-agent reinforcement learning[37], where different data value estimators are optimized for the corresponding agent with respect to the tasks that they need to perform. In addition, we aim to incorporate a mechanism for auto-tuning the selection threshold into the training as the optimal value for this parameter may vary from one domain configuration to another one.

In this work, we proposed a data valuation framework that selects a subset of samples in the source dataset that are relevant to the target task. The data values are estimated using a deep neural network, trained using reinforcement learning with a reward that corresponds to the similarity between the distribution of the state-action transition of the given data and the target dataset.
We show that DVORL outperforms baselines on different target domain configurations and has a competitive performance on the source domain in which the reinforcement learning agent is trained. We find that our method can identify relevant and high-quality transitions and improve the performance and transferability of policy learned by offline RL algorithms. Moreover, we contributed a benchmark on two Gym MuJoCo domains (Hopper and Walker2d) for which domain configurations (friction and mass of torso) for the target domain differ from those of the source domain.

The authors gratefully acknowledge, that the proposed research is a result of the
research project “IIP-Ecosphere”, granted by the German Federal Ministry for
Economics and Climate Action (BMWK) via funding code 01MK20006A.

[图片: images\image_4.png]

[图片: images\image_5.png]

