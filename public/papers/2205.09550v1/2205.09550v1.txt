æ ‡é¢˜ï¼šData Valuation for Offline Reinforcement Learning

The success of deep reinforcement learning (DRL)
hinges on the availability of training data, which is typically
obtained via a large number of environment interactions. In many real-world scenarios, costs and risks are associated with gathering these data. The field of offline reinforcement learning addresses these issues through outsourcing the collection of
data to a domain expert or a carefully monitored program
and subsequently searching for a batch-constrained optimal policy. With the emergence of data markets, an alternative to constructing a dataset in-house is to purchase external data. However, while state-of-the-art offline reinforcement learning approaches have shown a lot of promise, they currently rely on carefully constructed datasets that are well aligned with
the intended target domains. This raises questions regarding the transferability and robustness of an offline reinforcement learning agent trained on externally acquired data. In this paper, we empirically evaluate the ability of the current state-of-the-art offline reinforcement learning approaches to coping with the source-target domain mismatch within two MuJoCo environments, finding that current state-of-the-art offline reinforcement learning algorithms underperform in the target domain. To address this, we propose data valuation for offline reinforcement learning (DVORL), which allows us to identify relevant
and high-quality transitions, improving the performance and transferability of policies learned by offline reinforcement learning algorithms. The
results show that our method outperforms offline reinforcement learning baselines
on two MuJoCo environments.

Offline Reinforcement Learning (RL) â€“ also known as batch-constrained RL â€“ is a class of RL methods that requires the agent to learn from a static dataset of pre-collected experiences without further environment interaction[1]. This learning paradigm disentangles exploration from exploitation, lending itself to the tasks in which exploration for collecting data is costly, time-consuming, or risky[2,3]. By taking advantage of pre-collected datasets, offline RL can mitigate the technical concerns associated with online data collection, and has potential benefits for a number of real environments, such as human-robot collaboration[4].

Offline reinforcement learning outsources the collection of data to a domain expert and subsequently searches for a batch-constrained optimal policy. However, this task is challenging, as offline RL methods suffer from theextrapolation error[3,5]. This pathology occurs when offline deep RL methods are trained under one distribution but evaluated on a different one. More specifically, value functions implemented by a function approximator have a tendency to predict unrealistic values for unseen state-action pairs for standard off-policy deep RL algorithms (e.â€‰g.,Â DQN and DDPG). This raises the need for approaches that restrict the action
space, forcing the agent to learn a behavior that is close to on-policy with respect to a subset of the given source data[3].

For offline RL methods that are designed to mitigate the extrapolation error[3,5,6], there remains the challenge that external data (e.â€‰g.,Â purchased from data markets) may not be well aligned with the intended target domain. Therefore, the learned policy induces a different visited state-action distribution that results in a degradation in the performance of the offline RL agent.

In recent years there have been a number of efforts within the paradigm of supervised learning for overcoming thesource-target domain mismatch problemvia valuating data, includingdata Shapley[7]anddata valuation using reinforcement learning(DVRL)[8]. Such methods have shown promising results on several application scenarios such as data-value quantification, corrupted sample discovery, robust learning with noisy labels, and domain adaptation[8]. This raises the question: data valuation improve the transferability and robustness of an offline RL agent trained on externally acquired data?

To investigate the above question, we propose a data valuation approach that selects a subset of samples in the source dataset that are relevant to the target task. Our main contributions can be summarized as follows:

Inspired by DVRL[8], we propose Data Valuation for Offline Reinforcement Learning (DVORL) that for a given offline RL method, a fixed source dataset, and a small target dataset, identifies those samples of the source buffer that are relevant to the target task.

We contribute a benchmark on two Gym MuJoCo domains (Hopper and Walker2d) for which parameterizations (friction and mass of torso) for the target domain are different from those of the source domain.

We show that the state-of-the-art offline RL methods fail to generalize to different target domain configurations.

We show how our data valuation approach can improve the generalizability of the RL agent to the target domain by outperforming the existing state-of-the-art methods on all considered target domain configurations.

The rest of this paper is organized as follows.SectionÂ IIgives the background on (offline) RL. InSectionÂ III, we formally define the source-target domain mismatch problem for offline RL, and provide a motivating example inSectionÂ IV.SectionÂ Vgives an overview of related work. InSectionÂ VI, we introduce our DVORL framework.SectionÂ VIIdescribes our experiment setup. We discuss our results inSectionÂ VIII, and inSectionÂ IX, we discuss our main findings. Finally inSectionÂ Xwe conclude with suggestions for future work.

The RL problem is typically modeled by a Markov decision process (MDP), formulated as a tuple(ğ’³,ğ’°,p,r,Î³)ğ’³ğ’°ğ‘ğ‘Ÿğ›¾(\mathcal{X},\mathcal{U},p,r,\gamma), with a state spaceğ’³ğ’³\mathcal{X}, an action spaceğ’°ğ’°\mathcal{U}, and transition dynamicspâ€‹(xâ€²âˆ£x,u)ğ‘conditionalsuperscriptğ‘¥â€²ğ‘¥ğ‘¢p\left(x^{\prime}\mid x,u\right).
At each discrete time step the agent performs an actionuâˆˆğ’°ğ‘¢ğ’°u\in\mathcal{U}in a statexâˆˆğ’³ğ‘¥ğ’³x\in\mathcal{X}, and transitions to a new statexâ€²âˆˆğ’³superscriptğ‘¥â€²ğ’³x^{\prime}\in\mathcal{X}based on the transition dynamicspâ€‹(xâ€²âˆ£x,u)ğ‘conditionalsuperscriptğ‘¥â€²ğ‘¥ğ‘¢p\left(x^{\prime}\mid x,u\right), and receives a rewardrâ€‹(x,u,xâ€²)ğ‘Ÿğ‘¥ğ‘¢superscriptğ‘¥â€²r\left(x,u,x^{\prime}\right). The goal of the agent is to maximize the expectation of the sum of discounted rewards, also known as the returnRt=âˆ‘i=t+1âˆÎ³iâ€‹râ€‹(xi,ui,xi+1)subscriptğ‘…ğ‘¡superscriptsubscriptğ‘–ğ‘¡1superscriptğ›¾ğ‘–ğ‘Ÿsubscriptğ‘¥ğ‘–subscriptğ‘¢ğ‘–subscriptğ‘¥ğ‘–1R_{t}=\sum_{i=t+1}^{\infty}\gamma^{i}r\left(x_{i},u_{i},x_{i+1}\right), which weighs future rewards with respect to the discount factorÎ³âˆˆ[0,1)ğ›¾01\gamma\in[0,1), determining the effective horizon.
The agent makes decisions via a policyÏ€:ğ’³â†’ğ’«â€‹(ğ’°):ğœ‹â†’ğ’³ğ’«ğ’°\pi:\mathcal{X}\rightarrow\mathcal{P}(\mathcal{U}), which maps a given statexğ‘¥xto a probability distribution over the action spaceğ’°ğ’°\mathcal{U}. For a given policyÏ€ğœ‹\pi, the value function is defined as the expected return of an agent, starting from statexğ‘¥x, performing actionuğ‘¢u, and following the policyQÏ€â€‹(s,a)=ğ”¼Ï€â€‹[Rtâˆ£x,u]superscriptğ‘„ğœ‹ğ‘ ğ‘subscriptğ”¼ğœ‹delimited-[]conditionalsubscriptğ‘…ğ‘¡ğ‘¥ğ‘¢Q^{\pi}(s,a)=\mathbb{E}_{\pi}\left[R_{t}\mid x,u\right]. The state-action value function can be computed through the Bellman equation of the Q function:

GivenQÏ€superscriptğ‘„ğœ‹Q^{\pi}, the optimal policyÏ€âˆ—=maxuâ¡Qâˆ—â€‹(x,u)superscriptğœ‹subscriptmaxğ‘¢superscriptğ‘„ğ‘¥ğ‘¢\pi^{*}=\operatorname{max}_{u}Q^{*}(x,u), can be obtained by greedy selection over the optimal value functionQâˆ—â€‹(x,u)=maxÏ€â¡QÏ€â€‹(x,u)superscriptğ‘„ğ‘¥ğ‘¢subscriptğœ‹superscriptğ‘„ğœ‹ğ‘¥ğ‘¢Q^{*}(x,u)=\max_{\pi}Q^{\pi}(x,u).
For environments confronting agents with the curse of dimensionality the value can be estimated with a
differentiable function approximatorQÎ¸â€‹(x,u)subscriptğ‘„ğœƒğ‘¥ğ‘¢Q_{\theta}(x,u), with parametersÎ¸ğœƒ\theta.

In this work, we focus on continuous control problems, where, given a parameterized policyÏ€Ï‘subscriptğœ‹italic-Ï‘\pi_{\vartheta}our objective is to find an optimal policyÏ€Ï‘âˆ—subscriptsuperscriptğœ‹italic-Ï‘\pi^{*}_{\vartheta},
with respect to the parametersÏ‘italic-Ï‘\vartheta,
which maximizes the expected return
from the start distributionJâ€‹(Ï‘)=ğ”¼xiâˆ¼pÏ€,uiâˆ¼Ï€â¡[R0]ğ½italic-Ï‘subscriptğ”¼formulae-sequencesimilar-tosubscriptğ‘¥ğ‘–subscriptğ‘ğœ‹similar-tosubscriptğ‘¢ğ‘–ğœ‹subscriptğ‘…0J(\vartheta)=\operatorname{\mathbb{E}}_{x_{i}\sim p_{\pi},u_{i}\sim\pi}[R_{0}][9].
The policy parametersÏ‘italic-Ï‘\varthetacan be updated by taking the gradient of the expected returnâˆ‡Ï‘Jâ€‹(Ï‘)subscriptâˆ‡italic-Ï‘ğ½italic-Ï‘\nabla_{\vartheta}J(\vartheta).
A popular approach to optimizing the policy is to use
actor-critic methods, where the actor (policy) can be updated
through the deterministic policy gradient algorithm[10]:âˆ‡Ï‘J(Ï‘)=ğ”¼xâˆ¼pÏ€[âˆ‡uQÎ¸Ï€(x,u)|u,Ï€â€‹(x)âˆ‡Ï‘Ï€Ï‘(x)]\nabla_{\vartheta}J(\vartheta)=\operatorname{\mathbb{E}}_{x\sim p_{\pi}}[\nabla_{u}Q_{\theta}^{\pi}(x,u)\rvert_{u,\pi(x)}\nabla_{\vartheta}\pi_{\vartheta}(x)], in which the value functionQÎ¸Ï€â€‹(x,u)superscriptsubscriptğ‘„ğœƒğœ‹ğ‘¥ğ‘¢Q_{\theta}^{\pi}(x,u)is the critic.

Standard off-policy deep RL algorithms such as deep Q-learning (DQN)[11]and deep deterministic policy gradient (DDPG)[9]are applicable in batch RL as they are based on more fundamental batch RL algorithms[12]. However, they suffer from a phenomenon, known asextrapolation error, which occurs when there is a mismatch between the given fixed batch of data and true state-action visitation of the current policy[3]. This is problematic as incorrect values of state-action pairs, which are not contained in the batch of data, are propagated through temporal difference updates of most off-policy algorithms[13], resulting in poor performance of the model[14]. Below we give an overview of approaches designed
to address the extrapolation error, which will serve as our baselines.

BCQ.Batch-Constrained deep Q-learning[3]is an offline RL method for continuous control that restricts the action space, thereby eliminating actions that are unlikely to be selected by the behavioral policyÏ€bsubscriptğœ‹ğ‘\pi_{b}and therefore rarely observed in the batch. Given a dataset ofNğ‘Ntransitionsğ’Ÿ=ğ’Ÿabsent\mathcal{D}={xt,ut,rt+1,xt+1}t=0Nsuperscriptsubscriptsubscriptğ‘¥ğ‘¡subscriptğ‘¢ğ‘¡subscriptğ‘Ÿğ‘¡1subscriptğ‘¥ğ‘¡1ğ‘¡0ğ‘\left\{x_{t},u_{t},r_{t+1},x_{t+1}\right\}_{t=0}^{N}, collected by a behavior policyÏ€bsubscriptğœ‹ğ‘\pi_{b}, BCQ consists of four parameterized networks:
a generative modelGÏ‰:ğ’³â†’ğ’°:subscriptğºğœ”â†’ğ’³ğ’°G_{\omega}:\mathcal{X}\rightarrow\mathcal{U}, parameterized withÏ‰ğœ”\omega, a perturbation modelÎ¾Ï•â€‹(x,u,Î¦)subscriptğœ‰italic-Ï•ğ‘¥ğ‘¢Î¦\xi_{\phi}(x,u,\Phi), parameterized withÏ•italic-Ï•\phi, and two Q-networksQÏ‘1â€‹(x,u)subscriptğ‘„subscriptitalic-Ï‘1ğ‘¥ğ‘¢Q_{\vartheta_{1}}(x,u),QÏ‘2â€‹(x,u)subscriptğ‘„subscriptitalic-Ï‘2ğ‘¥ğ‘¢Q_{\vartheta_{2}}(x,u), parameterized withÏ‘1subscriptitalic-Ï‘1\vartheta_{1}andÏ‘2subscriptitalic-Ï‘2\vartheta_{2}, respectively.
The generative modelGÏ‰subscriptğºğœ”G_{\omega}selects the most likely action given the state with respect to the data in the batch.
Since modeling the distribution of data in the high dimensional continuous control environments is not straightforward, a variational autoencoder (VAE) is used to approximate it.
The policy is defined by samplingnğ‘›nactions fromGÏ‰â€‹(x)subscriptğºğœ”ğ‘¥G_{\omega}(x)and selecting the highest valued action according to a Q-network as it is easier to sample fromÏ€bâ€‹(uâˆ£x)subscriptğœ‹ğ‘conditionalğ‘¢ğ‘¥\pi_{b}(u\mid x)than modelingÏ€bâ€‹(uâˆ£x)subscriptğœ‹ğ‘conditionalğ‘¢ğ‘¥\pi_{b}(u\mid x)in a continuous action space.
The perturbation modelÎ¾Ï•â€‹(x,u,Î¦)subscriptğœ‰italic-Ï•ğ‘¥ğ‘¢Î¦\xi_{\phi}(x,u,\Phi), parameterized withÏ•italic-Ï•\phi, models the distribution of data in the batch and is a residual added to the sampled actions in the range[âˆ’Î¦,Î¦]Î¦Î¦[-\Phi,\Phi].
This model is trained with the DDPG[10]and can be thought of as a behavioral cloning model.
Since the perturbation model together with the sampling can be considered as a hierarchical policy, BCQ can also be considered an actor-critic method.

CQL.To prevent the training policy from overestimating the Q-values, Conservative Q-Learning (CQL)[6]utilizes a penalized empircal RL objective. More precisely, CQL optimizes the value function not only to minimize the temporal difference error based on the interactions seen on the dataset but also minimizes the value of actions that the currently trained policy takes, while at the same time maximizing the value of actions taken by the behavioral policy during data generation. This results in a conservativeQQ\mathrm{Q}-function.

TD3+BC.Twin Delayed Deep Deterministic (TD3) policy gradient with Behavior Cloning (BC) is a model-free algorithm that does not explicitly learn a model of the behavioral policy, while trains a policy to mimic the behavior policy from the data[15]. It directly penalizes Euclidean distance to the actions that were recorded in the dataset.

The training samples that machine learning (ML) models are trained with are not all equally valuable[16].
A sample can be considered low-quality due to noisy input values, a noisy label, or the source-target domain mismatch problem.
Removing low-quality samples has been shown to increase the performance of ML models[7,8]. The task of quantifying the quality of individual datum to the overall performance is referred to asdata valuation.
In supervised learning, data valuation is formally defined as follows.
Given a source (training) datasetğ’Ÿs=subscriptğ’Ÿğ‘ absent\mathcal{D}_{s}={(ğ±i,yi)}i=1Nsuperscriptsubscriptsubscriptğ±ğ‘–subscriptğ‘¦ğ‘–ğ‘–1ğ‘\left\{\left(\mathbf{x}_{i},y_{i}\right)\right\}_{i=1}^{N}and a target (test) datasetğ’Ÿğ’¯={(ğ±jğ’¯,yjğ’¯)}j=1Msubscriptğ’Ÿğ’¯superscriptsubscriptsuperscriptsubscriptğ±ğ‘—ğ’¯superscriptsubscriptğ‘¦ğ‘—ğ’¯ğ‘—1ğ‘€\mathcal{D_{T}}=\left\{\left(\mathbf{x}_{j}^{\mathcal{T}},y_{j}^{\mathcal{T}}\right)\right\}_{j=1}^{M}whereğ±âˆˆğ’³ğ±ğ’³\mathbf{x}\in\mathcal{X}is adğ‘‘d-dimensional feature vector, andyâˆˆğ’´ğ‘¦ğ’´y\in\mathcal{Y}is a corresponding label, the goal is to find a subsetğ’Ÿâˆ—={(ğ±k,yk)âˆ£(ğ±k,yk)âˆˆğ’Ÿğ’®}k=1Ksuperscriptğ’Ÿsuperscriptsubscriptconditional-setsubscriptğ±ğ‘˜subscriptğ‘¦ğ‘˜subscriptğ±ğ‘˜subscriptğ‘¦ğ‘˜subscriptğ’Ÿğ’®ğ‘˜1ğ¾\mathcal{D}^{*}=\left\{\left(\mathbf{x}_{k},y_{k}\right)\mid\left(\mathbf{x}_{k},y_{k}\right)\in\mathcal{D_{S}}\right\}_{k=1}^{K}of the source datasetğ’Ÿğ’®subscriptğ’Ÿğ’®\mathcal{D_{S}}that maximizes the performance of the trained model on the target datasetğ’Ÿğ’¯subscriptğ’Ÿğ’¯\mathcal{D_{T}}[7,17].

In this section, we formally define the data valuation problem for offline RL.

We assume the availability of a source datasetğ’Ÿğ’®=subscriptğ’Ÿğ’®absent\mathcal{D_{S}}={(xiğ’®,uiğ’®,xiâ€²ğ’®,riğ’®,eiğ’®)}i=1Nâˆ¼ğ’«ğ’®similar-tosuperscriptsubscriptsuperscriptsubscriptğ‘¥ğ‘–ğ’®subscriptsuperscriptğ‘¢ğ’®ğ‘–superscriptsubscriptsuperscriptğ‘¥â€²ğ‘–ğ’®superscriptsubscriptğ‘Ÿğ‘–ğ’®superscriptsubscriptğ‘’ğ‘–ğ’®ğ‘–1ğ‘subscriptğ’«ğ’®\{(x_{i}^{\mathcal{S}},u^{\mathcal{S}}_{i},{x^{\prime}_{i}}^{\mathcal{S}},{r}_{i}^{\mathcal{S}},{e}_{i}^{\mathcal{S}})\}_{i=1}^{N}\sim\mathcal{P}_{\mathcal{S}}and a
target datasetğ’Ÿğ’¯=subscriptğ’Ÿğ’¯absent\mathcal{D_{T}}={(xiğ’¯,uiğ’¯,xiâ€²ğ’¯,riğ’¯,eiğ’¯)}i=1Mâˆ¼ğ’«ğ’¯similar-tosuperscriptsubscriptsuperscriptsubscriptğ‘¥ğ‘–ğ’¯subscriptsuperscriptğ‘¢ğ’¯ğ‘–superscriptsubscriptsuperscriptğ‘¥â€²ğ‘–ğ’¯superscriptsubscriptğ‘Ÿğ‘–ğ’¯superscriptsubscriptğ‘’ğ‘–ğ’¯ğ‘–1ğ‘€subscriptğ’«ğ’¯\{(x_{i}^{\mathcal{T}},u^{\mathcal{T}}_{i},{x^{\prime}_{i}}^{\mathcal{T}},{r}_{i}^{\mathcal{T}},{e}_{i}^{\mathcal{T}})\}_{i=1}^{M}\sim\mathcal{P}_{\mathcal{T}},
wherexâˆˆâ„mğ‘¥superscriptâ„ğ‘šx\in\mathbb{R}^{m}is a state;uâˆˆâ„nğ‘¢superscriptâ„ğ‘›u\in\mathbb{R}^{n}is the action that the agent performs at the statexğ‘¥x;râˆˆâ„ğ‘Ÿâ„r\in\mathbb{R}is the reward that the agent gets by performing the actionuğ‘¢uin the statexğ‘¥x;xâ€²âˆˆâ„msuperscriptğ‘¥â€²superscriptâ„ğ‘šx^{\prime}\in\mathbb{R}^{m}is the state that the agent transitions to (i.e. next state); andeâˆˆ{0,1}ğ‘’01e\in\{0,1\}indicates whether thexâ€²superscriptğ‘¥â€²x^{\prime}is a terminal state.
We assume that the target datasetğ’Ÿğ’¯subscriptğ’Ÿğ’¯\mathcal{D_{T}}is much smaller than the the source datasetğ’Ÿğ’®subscriptğ’Ÿğ’®\mathcal{D_{S}}, thereforeNâ‰«Mmuch-greater-thanğ‘ğ‘€N\gg M.
Furthermore, the source distributionğ’«ğ’®subscriptğ’«ğ’®\mathcal{P}_{\mathcal{S}}can be different from the target distributionğ’«ğ’¯subscriptğ’«ğ’¯\mathcal{P}_{\mathcal{T}}, confronting our learner with the source-target domain mismatch problem.
As in supervised learning, our goal is to find a (sub)setğ’Ÿâˆ—âŠ†ğ’Ÿğ’®superscriptğ’Ÿsubscriptğ’Ÿğ’®\mathcal{D}^{*}\subseteq\mathcal{D_{S}}, and seek a batch-constrained policyÏ€ğœ‹\pi,
that when trained onğ’Ÿâˆ—superscriptğ’Ÿ\mathcal{D}^{*}can generalize to the target domain used to constructğ’Ÿğ’¯subscriptğ’Ÿğ’¯\mathcal{D_{T}}.
Therefore, we have a transfer learning problem.

To formally define transfer learning for offline RL, we draw on the formulation from Zhu et al.[18].
Letğš¿ğ’®=subscriptğš¿ğ’®absent\mathbf{\Psi}_{\mathcal{S}}={Î¨ğ’®âˆ£Î¨ğ’®âˆˆğš¿ğ’®}conditional-setsubscriptÎ¨ğ’®subscriptÎ¨ğ’®subscriptğš¿ğ’®\left\{{\Psi}_{\mathcal{S}}\mid{\Psi}_{\mathcal{S}}\in{\mathbf{\Psi}}_{\mathcal{S}}\right\}be a set of source domains andÎ¨ğ’¯subscriptÎ¨ğ’¯{\Psi}_{\mathcal{T}}be a target domain, where each domain corresponds to an MDP. Therefore, the MDPs in the source domainÎ¨ğ’®subscriptÎ¨ğ’®\Psi_{\mathcal{S}}and target domainÎ¨ğ’¯subscriptÎ¨ğ’¯\Psi_{\mathcal{T}}are defined as(ğ’³ğ’®,ğ’°ğ’®,pğ’®,rğ’®,Î³ğ’®)subscriptğ’³ğ’®subscriptğ’°ğ’®subscriptğ‘ğ’®subscriptğ‘Ÿğ’®subscriptğ›¾ğ’®(\mathcal{X}_{\mathcal{S}},\mathcal{U}_{\mathcal{S}},p_{\mathcal{S}},r_{\mathcal{S}},\gamma_{\mathcal{S}})and(ğ’³ğ’¯,ğ’°ğ’¯,pğ’¯,rğ’¯,Î³ğ’¯)subscriptğ’³ğ’¯subscriptğ’°ğ’¯subscriptğ‘ğ’¯subscriptğ‘Ÿğ’¯subscriptğ›¾ğ’¯(\mathcal{X}_{\mathcal{T}},\mathcal{U}_{\mathcal{T}},p_{\mathcal{T}},r_{\mathcal{T}},\gamma_{\mathcal{T}}), respectively.
We assume prior knowledgeğ’Ÿğ’®subscriptğ’Ÿğ’®\mathcal{D}_{\mathcal{S}}provided by the set of source domainsğš¿ğ’®subscriptğš¿ğ’®\mathbf{\Psi}_{\mathcal{S}}and accessible to the target domainÎ¨ğ’¯subscriptÎ¨ğ’¯{\Psi}_{\mathcal{T}}.

By leveraging the prior informationğ’Ÿğ’®subscriptğ’Ÿğ’®\mathcal{D_{S}}from the source domainğš¿ğ’®subscriptğš¿ğ’®\mathbf{\Psi}_{\mathcal{S}}as well as informationğ’Ÿğ’¯subscriptğ’Ÿğ’¯\mathcal{D}_{\mathcal{T}}provided byÎ¨ğ’¯subscriptÎ¨ğ’¯\Psi_{\mathcal{T}}, transfer learning aims to learn an optimal policyÏ€âˆ—superscriptğœ‹\pi^{*}for the target domainÎ¨ğ’¯subscriptÎ¨ğ’¯{\Psi}_{\mathcal{T}}, such that:

whereÏ€=Ï•â€‹(ğ’Ÿğ’®âˆ¼Î¨ğ’®,ğ’Ÿğ’¯âˆ¼Î¨ğ’¯):ğ’³ğ’¯â†’ğ’°ğ’¯:ğœ‹italic-Ï•formulae-sequencesimilar-tosubscriptğ’Ÿğ’®subscriptÎ¨ğ’®similar-tosubscriptğ’Ÿğ’¯subscriptÎ¨ğ’¯â†’subscriptğ’³ğ’¯subscriptğ’°ğ’¯\pi=\phi\left(\mathcal{D_{S}}\sim\Psi_{\mathcal{S}},\mathcal{D}_{\mathcal{T}}\sim\Psi_{\mathcal{T}}\right):\mathcal{X}_{\mathcal{T}}\rightarrow\mathcal{U}_{\mathcal{T}}is a function mapping the states to actions for the target domainÎ¨ğ’¯subscriptÎ¨ğ’¯\Psi_{\mathcal{T}}, learned based on information from bothğ’Ÿğ’¯subscriptğ’Ÿğ’¯\mathcal{D}_{\mathcal{T}}andğ’Ÿğ’®subscriptğ’Ÿğ’®\mathcal{D}_{\mathcal{S}}.

The source and target domains can have distinct state spaces, but their action spaces have to be the same and their transition function and reward function have to be similar as they share internal dynamics. We focus on policy transfers whereğ’³ğ’®=ğ’³ğ’¯subscriptğ’³ğ’®subscriptğ’³ğ’¯\mathcal{X}_{\mathcal{S}}=\mathcal{X}_{\mathcal{T}},ğ’°ğ’®=ğ’°ğ’¯subscriptğ’°ğ’®subscriptğ’°ğ’¯\mathcal{U}_{\mathcal{S}}=\mathcal{U}_{\mathcal{T}},rğ’®=rğ’¯subscriptğ‘Ÿğ’®subscriptğ‘Ÿğ’¯r_{\mathcal{S}}=r_{\mathcal{T}},
butpğ’®â‰ pğ’¯subscriptğ‘ğ’®subscriptğ‘ğ’¯p_{\mathcal{S}}\neq p_{\mathcal{T}}.

The source and target domains can have distinct transition functions because a change in environment parameters (e.â€‰g.,Â mass of torso and friction) results in a different probability functionpğ‘p, which itself is conditioned on(x,u,xâ€²)ğ‘¥ğ‘¢superscriptğ‘¥â€²(x,u,x^{\prime}), wherexğ‘¥x,uğ‘¢u, andxâ€²superscriptğ‘¥â€²x^{\prime}denote state, action, and next state, respectively.

However, since the target dataset is very small, compared to the source dataset (Nâ‰«Mmuch-greater-thanğ‘ğ‘€N\gg M), state-action transition(x,u,xâ€²)ğ‘¥ğ‘¢superscriptğ‘¥â€²(x,u,x^{\prime})is too restrictive. Thus, we also consider the case(x)ğ‘¥(x)in which only the change in the distribution of the state space is taken into account.

For our motivating example, we consider two scenarios involving a cobot, depicted inFigureÂ 1.
In the source domain the cobot is performing pick-and-place task while the target domain confronts the cobot with a sorting task. Clearly, a learning agent trained on the source task will perform poorly on the target task. However, our hypothesis is that data valuation can help us identify samples that are relevant for both tasks. For instance, both tasks have pick-up and place actions in common. Therefore, the goal is to find the relevant subset of the source dataset that allows the agent to learn a policy that generalizes to the target domain.

The RL literature contains numerous techniques for dealing with the source-target domain mismatch problem.
Noteworthy contributions here include:
EPOPT[19],
which is a combination of policy transfer through source domain ensemble and learning from limited demonstrations for the fast adaptation to the target domain;
UP-OSI[20]trains robust agent policies using a large number of synthetic demonstrations from a simulator to deal with environments with unknown dynamics;
CAD2RL[21]learns latent representations
from observations in the source domain that are generally applicable to the target domain;
DARLA[22], a zero-shot transfer learning method that learns disentangled representations which are robust against domain shifts;
and SAFER[23], which accelerates policy learning on complex control tasks by considering safety constraints.

Meanwhile, the literature on off-policy RL includes principled experience replay memory sampling techniques. Prioritized Experience Replay (PER)[24](e.g.,[25,26,27]) attempts to sample transitions that contribute the most toward learning.
prioritized replay with weighted importance sampling
can improve BCQ.
However, the majority of the work to date on offline RL is focused on preventing the training policy from being too disjoint with the behavioral policy[3,6,28].
To increase the generalization capacity of offline RL methods, Kostrikov et al.[29]propose in-sample Q-learning (IQL), which approximates the policy improvement step by considering the state value function as a random variable with some randomness determined by the action, and then taking a state-conditional expectile of this random variable to estimate the value of the best actions in that state.

In contrast to the offline RL methods listed above, our work focuses on valuating the suitability of state transition tuples for a given target domain.
Here we draw inspiration from the literature on data valuation for supervised learning.
Ghorbani et al. in[30]propose the distributional Shapley, which is a framework in which the value of a point is defined in the context of underlying data distribution. The reformulation of the data Shapley value as a distributional quantity reduces the dependence on the specific draw of data as the valuation function does not depend on a fixed data set.
Ghorbani and Zou propose the Neuron Shapley framework[31]to quantify how individual neurons contribute to the prediction and performance of a deep neural network (DNN). However, the limitation of Shapely based methods is that itâ€™s computationally expensive or even impossible to quantify the contribution of each individual sample to the overall performance of the model, in particular, for complex models such as DNNs[8].

Wang et al.[32]propose a minimax game-based transfer learning technique for selective transfer learning that consists of a selector, a discriminator, and a transfer learning module, playing a minimax game to find useful source data.
Yoon et al.[8]introduce a framework for data valuation in supervised learning tasks, making use of RL to determine how likely each training sample is used in the training of the predictor model.
This method integrates data valuation into the training procedure of the predictor model, making the predictor and data value estimator able to improve each otherâ€™s performance.

Despite the success of the existing works on data valuation, they are only applicable to the supervised learning tasks in which the availability of labels is assumed. Therefore, they are not directly applicable to the RL setting where there is no ground-truth for the transitions.
This emphasizes a need for a data valuation method that is applicable to the RL setting.

DVORL consists of two learnable functions: (1) a data value estimator (DVE) modelvÏ•subscriptğ‘£italic-Ï•v_{\phi}and (2) an offline reinforcement learning model.
Inspired by DVRL[8], we adapt the REINFORCE algorithm[33]and use it as the DVE.
We use a DNN for the DVE.
The goal is to find the parametersÏ•âˆ—superscriptitalic-Ï•\phi^{*}of the DNN so that the network returns the optimal probability distribution over the sample space.

The DVE modelvÏ•:ğ’³Ã—ğ’°Ã—ğ’³â€²Ã—â„›Ã—â„°â†’[0,1]:subscriptğ‘£italic-Ï•â†’ğ’³ğ’°superscriptğ’³â€²â„›â„°01v_{\phi}:\mathcal{X}\times\mathcal{U}\times\mathcal{X^{\prime}}\times\mathcal{R}\times\mathcal{E}\rightarrow[0,1]is optimized to output data values corresponding to the relevance of training samples to the target task. We formulate the corresponding optimization problem as:

maxÏ•â¡Jâ€‹(Ï€Ï•)=ğ”¼(xğ’®,uğ’®,xğ’®â€²)âˆ¼Pğ’®(xğ’¯,uğ’¯,xğ’¯â€²)âˆ¼Pğ’¯â¡[rÏ•â€‹((xğ’®,uğ’®,xğ’®â€²),(xğ’¯,uğ’¯,xğ’¯â€²))]subscriptitalic-Ï•ğ½subscriptğœ‹italic-Ï•subscriptğ”¼similar-tosuperscriptğ‘¥ğ’®superscriptğ‘¢ğ’®superscriptğ‘¥superscriptğ’®â€²superscriptğ‘ƒğ’®similar-tosuperscriptğ‘¥ğ’¯superscriptğ‘¢ğ’¯superscriptğ‘¥superscriptğ’¯â€²superscriptğ‘ƒğ’¯subscriptğ‘Ÿitalic-Ï•superscriptğ‘¥ğ’®superscriptğ‘¢ğ’®superscriptğ‘¥superscriptğ’®â€²superscriptğ‘¥ğ’¯superscriptğ‘¢ğ’¯superscriptğ‘¥superscriptğ’¯â€²\max_{\phi}J\left(\pi_{\phi}\right)=\operatorname{\mathbb{E}}_{\begin{subarray}{c}(x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\sim P^{\mathcal{S}}\\
(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}})\sim P^{\mathcal{T}}\end{subarray}}\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right],

where

corresponds to the reciprocal of the KL divergence between the batch of source dataset and target dataset.
Therefore, the objective of the network is to assign high probabilities to samples whose reward function valuerÏ•â€‹((xğ’®,uğ’®,xğ’®â€²),(xğ’¯,uğ’¯,xğ’¯â€²))subscriptğ‘Ÿitalic-Ï•superscriptğ‘¥ğ’®superscriptğ‘¢ğ’®superscriptğ‘¥superscriptğ’®â€²superscriptğ‘¥ğ’¯superscriptğ‘¢ğ’¯superscriptğ‘¥superscriptğ’¯â€²r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))is high.

Training.As shown inAlgorithmÂ 1(lines 4 to 10), all the samples of source bufferğ’Ÿğ’®subscriptğ’Ÿğ’®\mathcal{D_{S}}are divided into batches and each batchğ’Ÿğ’®â€²superscriptsubscriptğ’Ÿğ’®â€²\mathcal{D_{S}}^{\prime}is given as input to the DVE (with shared parameters across the batch).
The KL divergence between the distribution of the state-action transition(x,u,xâ€²)ğ‘¥ğ‘¢superscriptğ‘¥â€²(x,u,x^{{}^{\prime}})of the given batch and that of the small target bufferğ’Ÿğ’¯subscriptğ’Ÿğ’¯\mathcal{D_{T}}is calculated and used as the reward signalrsâ€‹iâ€‹gsubscriptğ‘Ÿğ‘ ğ‘–ğ‘”r_{sig}for training the DVE.
Letw=vÏ•â€‹(xiğ’®,uiğ’®,xiâ€²ğ’®,riğ’®,eiğ’®)ğ‘¤subscriptğ‘£italic-Ï•superscriptsubscriptğ‘¥ğ‘–ğ’®subscriptsuperscriptğ‘¢ğ’®ğ‘–superscriptsubscriptsuperscriptğ‘¥â€²ğ‘–ğ’®superscriptsubscriptğ‘Ÿğ‘–ğ’®superscriptsubscriptğ‘’ğ‘–ğ’®w=v_{\phi}(x_{i}^{\mathcal{S}},u^{\mathcal{S}}_{i},{x^{\prime}_{i}}^{\mathcal{S}},{r}_{i}^{\mathcal{S}},{e}_{i}^{\mathcal{S}})denote the probability that the sampleiğ‘–iof the source buffer is used for training the offline reinforcement learning model.

Our adapted version of REINFORCE algorithm, has the following object function for the policyÏ€Ï•subscriptğœ‹italic-Ï•\pi_{\phi}:

Jâ€‹(Ï€Ï•)=ğ”¼(xğ’®,uğ’®,xğ’®â€²)âˆ¼Pğ’®(xğ’¯,uğ’¯,xğ’¯â€²)âˆ¼Pğ’¯wâˆ¼Ï€Ï•â€‹(ğ’Ÿğ’®â€²,â‹…)â¡[rÏ•â€‹((xğ’®,uğ’®,xğ’®â€²),(xğ’¯,uğ’¯,xğ’¯â€²))]=âˆ«Pğ’¯â€‹((xğ’®,uğ’®,xğ’®â€²))â€‹âˆ‘wâˆˆ[0,1]NÏ€Ï•â€‹(ğ’Ÿğ’®â€²,w)â‹…[rÏ•â€‹((xğ’®,uğ’®,xğ’®â€²),(xğ’¯,uğ’¯,xğ’¯â€²))]â€‹dâ€‹((xğ’®,uğ’®,xğ’®â€²)).ğ½subscriptğœ‹italic-Ï•absentsubscriptğ”¼similar-tosuperscriptğ‘¥ğ’®superscriptğ‘¢ğ’®superscriptğ‘¥superscriptğ’®â€²superscriptğ‘ƒğ’®similar-tosuperscriptğ‘¥ğ’¯superscriptğ‘¢ğ’¯superscriptğ‘¥superscriptğ’¯â€²superscriptğ‘ƒğ’¯similar-toğ‘¤subscriptğœ‹italic-Ï•superscriptsubscriptğ’Ÿğ’®â€²â‹…subscriptğ‘Ÿitalic-Ï•superscriptğ‘¥ğ’®superscriptğ‘¢ğ’®superscriptğ‘¥superscriptğ’®â€²superscriptğ‘¥ğ’¯superscriptğ‘¢ğ’¯superscriptğ‘¥superscriptğ’¯â€²missing-subexpressionabsentsuperscriptğ‘ƒğ’¯superscriptğ‘¥ğ’®superscriptğ‘¢ğ’®superscriptğ‘¥superscriptğ’®â€²subscriptğ‘¤superscript01ğ‘subscriptğœ‹italic-Ï•superscriptsubscriptğ’Ÿğ’®â€²ğ‘¤missing-subexpressionâ‹…absentdelimited-[]subscriptğ‘Ÿitalic-Ï•superscriptğ‘¥ğ’®superscriptğ‘¢ğ’®superscriptğ‘¥superscriptğ’®â€²superscriptğ‘¥ğ’¯superscriptğ‘¢ğ’¯superscriptğ‘¥superscriptğ’¯â€²ğ‘‘superscriptğ‘¥ğ’®superscriptğ‘¢ğ’®superscriptğ‘¥superscriptğ’®â€²\begin{aligned} J\left(\pi_{\phi}\right)&=\operatorname{\mathbb{E}}_{\begin{subarray}{c}\begin{subarray}{c}(x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\sim P^{\mathcal{S}}\\
(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}})\sim P^{\mathcal{T}}\\
w\sim\pi_{\phi}(\mathcal{D_{S}}^{\prime},\cdot)\end{subarray}\end{subarray}}\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right]\\
&=\int P^{\mathcal{T}}\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\sum_{w\in[0,1]^{N}}\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\\
&\quad\cdot\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right]d\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right).\end{aligned}

In the above equation,Ï€Ï•â€‹(ğ’Ÿğ’®â€²,w)subscriptğœ‹italic-Ï•superscriptsubscriptğ’Ÿğ’®â€²ğ‘¤\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)is the probability that the selection probability vectorwğ‘¤woccurs. In this way, the policy directly uses the values output by the DVE. This is different from the DVRL[8], which uses a binary selection vectorğ¬=(s1,â€¦,sBs)ğ¬subscriptğ‘ 1â€¦subscriptğ‘ subscriptğµğ‘ \mathbf{s}=\left(s_{1},\ldots,s_{B_{s}}\right)wheresBssubscriptğ‘ subscriptğµğ‘ s_{B_{s}}denotes the batch size,siâˆˆ{0,1}subscriptğ‘ ğ‘–01s_{i}\in\{0,1\}, andPâ€‹(si=1)=wiğ‘ƒsubscriptğ‘ ğ‘–1subscriptğ‘¤ğ‘–P\left(s_{i}=1\right)=w_{i}. Thus, in our training, the DVE has no control over exploration and just provides weightings for the given samples and is tuned accordingly.

It should be noted that we use the whole input information of the source buffer (i.e.,(x,u,xâ€²,r,e)ğ‘¥ğ‘¢superscriptğ‘¥â€²ğ‘Ÿğ‘’\left(x,u,x^{\prime},{r},{e}\right)) for calculating the data values (i.e.,ğ’Ÿğ’®â€²superscriptsubscriptğ’Ÿğ’®â€²\mathcal{D_{S}}^{\prime}in policyÏ€Ï•â€‹(ğ’Ÿğ’®â€²,ğ°)subscriptğœ‹italic-Ï•superscriptsubscriptğ’Ÿğ’®â€²ğ°\pi_{\phi}(\mathcal{D_{S}}^{\prime},\mathbf{w})); however, we only use the information of the state-action transition (x,u,xâ€²ğ‘¥ğ‘¢superscriptğ‘¥â€²x,u,x^{\prime}) for calculating the reward signal, used for updating the DVE, that is consistent with our formulation of transfer learning wherepğ’®â‰ pğ’¯subscriptğ‘ğ’®subscriptğ‘ğ’¯p_{\mathcal{S}}\neq p_{\mathcal{T}}as the transition probability function is conditioned on the state-action transition (x,u,xâ€²ğ‘¥ğ‘¢superscriptğ‘¥â€²x,u,x^{\prime}).

We calculate the gradient of the above objective function in the following.

âˆ‡Ï•Jâ€‹(Ï€Ï•)=âˆ«PTâ€‹((xğ’®,uğ’®,xğ’®â€²))â€‹âˆ‘wâˆˆ[0,1]Nâˆ‡Ï•Ï€Ï•â€‹(ğ’Ÿğ’®â€²,w)â‹…[rÏ•((xğ’®,uğ’®,xğ’®â€²),(xğ’¯,uğ’¯,xğ’¯â€²)))]d((xğ’®,uğ’®,xğ’®â€²))=âˆ«PT((xğ’®,uğ’®,xğ’®â€²))[âˆ‘xâˆˆ[0,1]Nâˆ‡Ï•Ï€Ï•â€‹(ğ’Ÿğ’®â€²,w)Ï€Ï•â€‹(ğ’Ÿğ’®â€²,w)â‹…Ï€Ï•(ğ’Ÿğ’®â€²,w)[rÏ•((xğ’®,uğ’®,xğ’®â€²),(xğ’¯,uğ’¯,xğ’¯â€²))]]d((xğ’®,uğ’®,xğ’®â€²))=âˆ«PT((xğ’®,uğ’®,xğ’®â€²))[âˆ‘wâˆˆ[0,1]Nâˆ‡Ï•log(Ï€Ï•(ğ’Ÿğ’®â€²,w))â‹…Ï€Ï•(ğ’Ÿğ’®â€²,w)[rÏ•((xğ’®,uğ’®,xğ’®â€²),(xğ’¯,uğ’¯,xğ’¯â€²))]]d((xğ’®,uğ’®,xğ’®â€²))=ğ”¼(xğ’®,uğ’®,xğ’®â€²)âˆ¼PS(xğ’¯,uğ’¯,xğ’¯â€²)âˆ¼PTwâˆ¼Ï€Ï•â€‹(ğ’ŸS,â‹…)â€‹[rÏ•â€‹((xğ’®,uğ’®,xğ’®â€²),(xğ’¯,uğ’¯,xğ’¯â€²))]â‹…âˆ‡Ï•log(Ï€Ï•(ğ’Ÿğ’®â€²,w)).\begin{aligned} \nabla_{\phi}J\left(\pi_{\phi}\right)&=\int P^{T}\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\sum_{w\in[0,1]^{N}}\nabla_{\phi}\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\\
&\quad\cdot\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}})))\right]d\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\\
&=\int P^{T}\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\left[\sum_{x\in[0,1]^{N}}\frac{\nabla_{\phi}\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)}{\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)}\right.\\
&\quad\left.\cdot\;\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right]\vphantom{\sum_{x\in[0,1]^{N}}}\right]d\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\\
&=\int P^{T}\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\left[\sum_{w\in[0,1]^{N}}\nabla_{\phi}\log\left(\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\right)\right.\\
&\quad\left.\cdot\;\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right]\vphantom{\sum_{w\in[0,1]^{N}}}\right]d\left((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\right)\\
&=\underset{\begin{subarray}{c}(x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}})\sim P^{S}\\
(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}})\sim P^{T}\\
w\sim\pi_{\phi}(\mathcal{D}_{S},\cdot)\end{subarray}}{\mathbb{E}}\left[r_{\phi}((x^{\mathcal{S}},u^{\mathcal{S}},x^{{}^{\prime}\mathcal{S}}),(x^{\mathcal{T}},u^{\mathcal{T}},x^{{}^{\prime}\mathcal{T}}))\right]\\
&\quad\cdot\nabla_{\phi}\log\left(\pi_{\phi}(\mathcal{D_{S}}^{\prime},w)\right).\end{aligned}

To enhance the stability of the DVE, we use the moving averagerrâ€‹oâ€‹lâ€‹lâ€‹iâ€‹nâ€‹gsubscriptğ‘Ÿğ‘Ÿğ‘œğ‘™ğ‘™ğ‘–ğ‘›ğ‘”r_{rolling}of the previous signal rewards with the window sizeÏ‰ğœ”\omegaas the baseline. The baseline reduces the variance of the gradient estimates[34].

Inference.As shown inAlgorithmÂ 1(lines 11 to 18), after all the samples of the source buffer are used for training the DVE, the fully-trained DVE is used for outputting the data values of the original source buffer. The samples whose corresponding data values are lower than the selection thresholdÏµitalic-Ïµ\epsilonare filtered out and the remaining subset of samples form the new source bufferğ’Ÿğ’®âˆ—superscriptsubscriptğ’Ÿğ’®\mathcal{D_{S}^{*}}that is relevant to the target domain:

ğ’Ÿğ’®âˆ—={(xiğ’®,uiğ’®,xiâ€²ğ’®,riğ’®,eiğ’®)âˆˆğ’Ÿğ’®|i=1,â€¦,N;wiâ‰¥Ïµ}superscriptsubscriptğ’Ÿğ’®conditional-setsuperscriptsubscriptğ‘¥ğ‘–ğ’®subscriptsuperscriptğ‘¢ğ’®ğ‘–superscriptsubscriptsuperscriptğ‘¥â€²ğ‘–ğ’®superscriptsubscriptğ‘Ÿğ‘–ğ’®superscriptsubscriptğ‘’ğ‘–ğ’®subscriptğ’Ÿğ’®formulae-sequenceğ‘–1â€¦ğ‘subscriptğ‘¤ğ‘–italic-Ïµ\mathcal{D_{S}^{*}}=\left\{\left(x_{i}^{\mathcal{S}},u^{\mathcal{S}}_{i},{x^{\prime}_{i}}^{\mathcal{S}},{r}_{i}^{\mathcal{S}},{e}_{i}^{\mathcal{S}}\right)\in\mathcal{D_{S}}\;|\;i=1,\ldots,N;w_{i}\geq\epsilon\right\}.

Finally, the bufferğ’Ÿğ’®âˆ—superscriptsubscriptğ’Ÿğ’®\mathcal{D_{S}^{*}}is given to an offline RL model for training.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1:Illustration of the DVORL framework. (1) A batch of source buffer samples is given as input to the data value estimator (DVE). (2) KL Divergence between the distribution of the state-action transitions of the given batch and the distribution of the state-action transitions of target buffer (whose transition items are collected by a domain expert on the target domain) is calculated and (3) used as the reward signal for updating the DVE. (4) After all the samples of the source buffer are used for training the DVE, (5) the fully trained DVE is used for outputting the data values of the source buffer (6) that are filtered out by removing those values being lower than the selection threshold. (7) This results in a subset of the source buffer that is relevant to the target domain and is used for training the given base offline RL algorithm.

In this work, we use the following offline RL methods discussed inSectionÂ IIas baselines: (Vanilla) BCQ, CQL and TD3+BC. We also evaluated the performance of BEAR[5]on our considered domains. However, as found by[6], we found CQL and TD3+BC outperformed BEAR, and therefore focused our evaluation on the above three methods and our DVORL.

We use the (Vanilla) BCQ as the base model in our DVORL, and we refer to it as Data Valuation based BCQ (DVBCQ). The reason for using the Vanilla BCQ is that it is the most commonly used offline RL algorithm, and we also intend to show how the selection of relevant transitions can help a base model, underperforming other methods in most cases, outperform the state-of-the-art methods in terms of transferability of learned policy to different target configurations. We consider two versions of DVBCQ: i)Â DVBCQ(x)ğ‘¥(x)using information of states, and ii)Â DVBCQ(x,u,xâ€²)ğ‘¥ğ‘¢superscriptğ‘¥â€²(x,u,x^{{}^{\prime}})using information of state-action transition, for calculating the similarity between source and target buffers.

The DVORL agent learns from a dataset collected by a domain expert. In our experiments, for each domain and domain parametrization, we trained a DDPG agent for one million iterations and used the fully-trained agent for generating the buffers with the size one million and ten thousand for the source and target, respectively.

In this work, we use the following two MuJoCo domains:

Hopper-v3:The Hopper is a simulated monopod robot with 4 body links including the torso, upper leg, lower leg,
and foot, together with 3 actuated joints. This domain has an 11-dimensional state space
including joint angles and joint velocities and a 3-dimensional action space corresponding to torques at the joints. The goal is to make the hopper hop forward as fast as possible.

Walker2d-v3:The Walker is a simulated bipedal robot consisting of 7 body links including to two legs and a torso, along with 6 actuated joints. This domain has a 17-dimensional state space including joint angles and joint velocities and a 4-dimensional action space corresponding to joint torques. The goal is to make the robot walk forward as fast as possible.

DomainSource ConfigTarget ConfigNameHopper-v3F: 2.0T: 0.05F: 2.0T: 0.05Hopper-SourceF: 2.5T: 0.075Hopper-Target1F: 3.0T: 0.075Hopper-Target2Walker2d-v3F: 0.9T: 0.05F: 0.9T: 0.05Walker2d-SourceF: 1.125T: 0.075Walker2d-Target1F: 1.35T: 0.075Walker2d-Target2

For our experiments, we shall distinguish betweensourceandtargetdomains. The source domain is the one within which the samples are gathered by a fully-trained DDPG. The target domain is the domain within which the DVORL agent is to be deployed. To study the extent to which DVBCQ can cope with modified domain configurations, we consider two scenarios with respect to the source and target domains:

Identical Source and Target Domains:Domain configuration for gathering samples and training DVORL agent remain the same. This is the simplest setting where the DDPG agent gathers samples in an environment with a domain parameterization identical to the domain within which the DVORL agent will be deployed. For this setting, we consider two datasets â€œHopper-Sourceâ€ and â€œWalker-Sourceâ€.

Transfer Learning:Samples are gathered from a source domain with a parameterization that differs from the target domain. More precisely, the target domain will have differentmass of torsoandfrictioncoefficients compared with the source domain. For this setting, we consider four datasets â€œHopper-Target1â€, â€œHopper-Target2â€, â€œWalker2d-Target1â€, and â€œWalker2d-Target2â€.

All the considered source and target domain configurations are presented inTableÂ I.

For all the competitors, we used the default parameters values reported in the corresponding papers. Hyperparameters of DVORL are selected by grid search. Since we used the BCQ in our DVORL method, we report the used parameter values of Data Valuation based BCQ (DVBCQ), listed inTableÂ II. The parameters values of the baseline (Vanilla) BCQ are the same as those of the base agent in our DVBCQ.

ParameterValueDescriptiondve_batch_size200batch size for DVEdve_hidden_layers[128, 128]Number of nodes in hidden layersmoving_average_window_size20Window size for the moving averageselection_threshold0.1selection thresholdmini_batch_size100Batch size for offline RL modeldiscount0.9Discount factortau0.005Target network update rate of BCQlambda0.75Weighting for clipped double Q-learningphi0.05Max perturbation parameter for BCQ

Our implementation of the DVORL builds
on OpenAI gymâ€™s[35]control environments with the MuJoCo[36]physics simulator.

FigureÂ 2shows the performance of BCQ, CQL, TD3+BC, DVBCQ(x)ğ‘¥(x)and DVBCQ(x,u,xâ€²)ğ‘¥ğ‘¢superscriptğ‘¥â€²(x,u,x^{{}^{\prime}})on the source domain and two different target domain configurations described inSectionVII-D. The models are trained for 1M iterations, and the results are averaged over ten runs on target domain environments with randomly-chosen seeds.

Identical source-domain:For DVBCQ and other baselines, we report the average return achieved by the best policy with respect to checkpoints saved throughout the run. For the identical source-domain setting, both DVBCQ(x)ğ‘¥(x)and DVBCQ(x,u,xâ€²)ğ‘¥ğ‘¢superscriptğ‘¥â€²(x,u,x^{{}^{\prime}})significantly outperform all baselines on Hopper environment, and their performance is superior to BCQ and CQL, while underperforming TD3+BC on Walker2d environment.

Transfer learning:For transfer learning setting, DVBCQ(x)ğ‘¥(x)outperforms both target domains whose configurations (mass of torso and friction) differ from those of the source domain, on both Hopper and Walker2d environments. However, DVBCQ(x,u,xâ€²)ğ‘¥ğ‘¢superscriptğ‘¥â€²(x,u,x^{{}^{\prime}})underperforms CQL on Hopper environment and TD3+BC on Walker environment but it has competitive performance compared with other baselines.

It should be noted that the there is a significant difference between the performance of DVBCQ and its base model BCQ.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2:Performance of BCQ, CQL, TD3+BC, and DVBCQ on the source domain and two different target domain configurations (described inSectionVII-D), where the models are trained for 1M iterations, and the results are averaged over ten runs on target domain environments with randomly-chosen seeds. For DVBCQ and other baselines, we report the performance achieved by the best policy with respect to checkpoints saved throughout the run.

FigureÂ 3shows the performance of BCQ models trained on the source dataset with different selection thresholds and evaluated on a different target domain configuration (Walker2d-Target2), where all the models are trained for 200K iterations, and a fixed seed is used for the evaluation environment. We consider five thresholds (0.0, 0.1, â€¦, 0.4) for excluding the high/low-value data samples of the source dataset. In addition, we report the average return of the best policy (with respect to checkpoints saved throughout the run) learned for each point.

As shown inFigureÂ 3, removing low-value samples from the source dataset can help the RL agent learn only those transitions relevant to the target domain configuration and therefore achieve better performance on the target domain (green line). On the other hand, removing high-value samples from the source dataset significantly deteriorates the RL agentâ€™s performance (red line).

The findings inFigureÂ 3support the opening hypothesis that excluding high-value samples worsens the performance of the offline RL methods.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3:Performance of BCQ models trained on the source
dataset with different selection thresholds and evaluated on a different target domain configuration (Walker2d-Target2), where all the models are trained for 200.000 iterations, and a fixed seed is used for the evaluation environment. Excluding high-value samples (red line) aggravates the performance of the offline RL methods. However, excluding low-value samples (green line) does not deteriorate the performance as much as that of the high-value samples.

Our results suggest that DVORL can improve the offline reinforcement learning methods on both identical source-target and transfer learning settings. In addition, our method helps the offline RL methods achieve significantly higher performance with fewer iterations, making them more efficient. Furthermore, our method can identify the relevant samples of the source domain to different target domain configurations. This is of high importance and has many use cases, such as learning from an externally acquired dataset and safe RL.

It should be noted that our goal is not to show that our proposed method outperforms all the state-of-the-art offline RL methods on both source and target domains, but to show that the data valuation for the offline reinforcement learning (DVORL) framework can improve the performance of the baseline algorithms.

For future work, we aim to examine whether the size of target buffer plays a role in the performance of DVORL. We intend conduct some experiments on real-world domains and compare our results to other data valuation methods like Data Shapely. Moreover, we plan to improve our reward function by taking into account dynamics of the model.

We also aim to investigate the extent to which DVORL can identify the safe transitions within a safe reinforcement learning setting. We also plan to apply the idea of transition valuation to the safe multi-agent reinforcement learning[37], where different data value estimators are optimized for the corresponding agent with respect to the tasks that they need to perform. In addition, we aim to incorporate a mechanism for auto-tuning the selection threshold into the training as the optimal value for this parameter may vary from one domain configuration to another one.

In this work, we proposed a data valuation framework that selects a subset of samples in the source dataset that are relevant to the target task. The data values are estimated using a deep neural network, trained using reinforcement learning with a reward that corresponds to the similarity between the distribution of the state-action transition of the given data and the target dataset.
We show that DVORL outperforms baselines on different target domain configurations and has a competitive performance on the source domain in which the reinforcement learning agent is trained. We find that our method can identify relevant and high-quality transitions and improve the performance and transferability of policy learned by offline RL algorithms. Moreover, we contributed a benchmark on two Gym MuJoCo domains (Hopper and Walker2d) for which domain configurations (friction and mass of torso) for the target domain differ from those of the source domain.

The authors gratefully acknowledge, that the proposed research is a result of the
research project â€œIIP-Ecosphereâ€, granted by the German Federal Ministry for
Economics and Climate Action (BMWK) via funding code 01MK20006A.

[å›¾ç‰‡: images\image_4.png]

[å›¾ç‰‡: images\image_5.png]

