æ ‡é¢˜ï¼šOffline Trajectory Generalization for Offline Reinforcement Learning

Offline reinforcement learning (RL) aims to learn policies from static datasets of previously collected trajectories.
Existing methods for offline RL either constrain the learned policy to the support of offline data or utilize model-based virtual environments to generate simulated rollouts.
However, these methods suffer from(i)poor generalization to unseen states; and(ii)trivial improvement from low-qualified rollout simulation.In this paper, we proposeofflinetrajectory generalization through worldtransformers foroffline reinforcement learning (OTTO).
Specifically, we use casual Transformers, a.k.a. World Transformers, to predict state dynamics and the immediate reward. Then we propose four strategies to use World Transformers to generate high-rewarded trajectory simulation by perturbing the offline data. Finally, we jointly use offline data with simulated data to train an offline RL algorithm.
OTTO serves as a plug-in module and can be integrated with existing offline RL methods to enhance them with better generalization capability of transformers and high-rewarded data augmentation.
Conducting extensive experiments on D4RL benchmark datasets, we verify that OTTO significantly outperforms state-of-the-art offline RL methods.

Offline reinforcement learning (RL) refers to training RL agents from pre-collected datasets, without the need for real-time interactions or online explorations(Levine etÂ al.,2020).
This paradigm plays a crucial role in real-world scenarios where collecting online interactions can be expensive or risky, as observed in domains like healthcare(Liu etÂ al.,2020), robotics(Singh etÂ al.,2022), autonomous driving(Yu etÂ al.,2020a), and recommendation systems(Swaminathan & Joachims,2015).
Standard RL methods often fail in such offline setting due to erroneous estimation of value functions(Fujimoto etÂ al.,2019).

Existing offline RL methods can be classified into two categories: model-free methods and model-based methods.
Model-free methods incorporate conservatism into the value function estimation(Fujimoto etÂ al.,2019; Kumar etÂ al.,2019; Fujimoto etÂ al.,2018; Siegel etÂ al.,2020; Kumar etÂ al.,2020; Kostrikov etÂ al.,2021a).
For example, CQL(Kumar etÂ al.,2020)adds a regularization term into the Q-function update. The conservatism downgrades the value function for unseen states and thus helps to avoid the over-estimation issue. As a result, the learned policy is constrained to near the offline data distribution. Besides, the overly conservatism could lead to underestimation of potentially good actions. Such issues lead to the poor generalization capability of model-free offline RL methods.

Unlike model-free RL methods,
model-based RL algorithms learn a dynamics model based on the offline dataset. The dynamics model can be utilized as a simulated environment to generate additional interaction data, providing new potential to further enhance the learned policy(Yu etÂ al.,2020b; Kidambi etÂ al.,2020; Yu etÂ al.,2021).
However, we found that the simulated data generated by existing model-based RL methods is of low qualification, resulting in trivial improvement for policy learning.
Specifically, existing model-based RL approaches can only perform short-horizon model rollouts, resulting in marginal generalization improvement only near the support data, as shown in Figure1(a).
Besides, when we perform the simulation of long-horizon trajectories using the environment model, the
average reward of each interaction step often becomes lower for longer steps, as shown in Figure1(b). As a result, the performance of the learned policy suffers a sharp decline, as shown in Figure1(c).
The above observation indicates that existing model-based offline RL methods suffer from trivial improvement from low-qualified rollout simulation.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: (a)Existing model-based methods vs OTTO.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: (a)Existing model-based methods vs OTTO.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: (a)Existing model-based methods vs OTTO.

To address the aforementioned issues, we proposeofflinetrajectory generalization through worldtransformers foroffline reinforcement learning (OTTO). Specifically, we first learn the state dynamics transition and reward function by a pair of Transformers, a.k.a. World Transformers, based on the offline dataset.
It is observed that the Transformer architecture achieves state-of-the-art performance in wide domains, including natural language processing (NLP)(Devlin etÂ al.,2018; Raffel etÂ al.,2020)and computer vision(He etÂ al.,2022; Dosovitskiy etÂ al.,2020). The Transformer particularly stands out for its outstanding generalization capability.
As a result, the proposed World Transformers are utilized to improve the generalization capability for environment simulation.
Then, four strategies are proposed to use World Transformers to generate long trajectory simulation by perturbing the offline data. The generalization capability of World Transformers, together with the prior knowledge in the offline dataset, ensures that the generated long-horizon trajectories are high-rewarded and thus helps the model to learn potential good actions.
Finally, offline data together with the simulated trajectories are jointly used to train an offline RL algorithm.
Note that OTTO serves as a plug-in module and can be integrated with a wide range of offline RL methods to enhance the learned policy further.

Our contributions in this paper are as follows:

We propose World Transformers, which utilize the sequential modeling of Transformers to learn state dynamics transition and reward functions with good generalization capability.

We propose four trajectory simulation strategies, which can autoregressively generate high-qualified long-horizon trajectory augmentation by using the World Transformers.

Our experiments on D4RL benchmarks show that OTTO can effectively improve the performance of representative model-free offline RL algorithms and outperforms strong model-based baselines.

Offline RL:A Markov decision process is defined by the tupleM=(ğ’®,ğ’œ,T,r,Î¼0,Î³)ğ‘€ğ’®ğ’œğ‘‡ğ‘Ÿsubscriptğœ‡0ğ›¾\mathit{M}=(\mathcal{S},\mathcal{A},\mathit{T},r,\mu_{0},\gamma), whereğ’®,ğ’œğ’®ğ’œ\mathcal{S},\mathcal{A}refer to state space and action space, respectively.
A policyÏ€â€‹(a|s)ğœ‹conditionalğ‘ğ‘ \pi(a|s)defines a mapping from statesâˆˆğ’®ğ‘ ğ’®s\in\mathcal{S}to a probability distribution over actionaâˆˆğ’œğ‘ğ’œa\in\mathcal{A}.
Given a(s,a)ğ‘ ğ‘(s,a)pair,Tâ€‹(sâ€²|s,a)ğ‘‡conditionalsuperscriptğ‘ â€²ğ‘ ğ‘T(s^{\prime}|s,a)andrâ€‹(s,a)ğ‘Ÿğ‘ ğ‘r(s,a)represent the distribution of next statesâ€²superscriptğ‘ â€²s^{\prime}and the obtained immediate reward, respectively.Î¼0subscriptğœ‡0\mu_{0}is the initial state distribution andÎ³âˆˆ(0,1)ğ›¾01\gamma\in(0,1)is the discount factor for future reward.
The value functionVMÏ€â€‹(s):=ğ”¼Ï€,T[âˆ‘t=0âˆÎ³tâ€‹râ€‹(st,at)|s0=s]assignsuperscriptsubscriptğ‘‰ğ‘€ğœ‹ğ‘ subscriptğ”¼ğœ‹ğ‘‡delimited-[]conditionalsuperscriptsubscriptğ‘¡0superscriptğ›¾ğ‘¡ğ‘Ÿsubscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ‘ 0ğ‘ V_{\mathit{M}}^{\pi}(s):=\mathop{\mathbb{E}}\limits_{\pi,\mathit{T}}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})|s_{0}=s]represents the expected cumulative reward from executingÏ€ğœ‹\pifrom statesğ‘ s.
We defineVMÏ€superscriptsubscriptğ‘‰ğ‘€ğœ‹V_{\mathit{M}}^{\pi}as the value function under the initial state distribution, i.e.VMÏ€=âˆ‘sâˆˆğ’®Î¼0â€‹(s)â€‹VMÏ€â€‹(s)superscriptsubscriptğ‘‰ğ‘€ğœ‹subscriptğ‘ ğ’®subscriptğœ‡0ğ‘ superscriptsubscriptğ‘‰ğ‘€ğœ‹ğ‘ V_{\mathit{M}}^{\pi}=\sum_{s\in\mathcal{S}}\mu_{0}(s)V_{\mathit{M}}^{\pi}(s).
The goal of RL is to find an optimal policyÏ€âˆ—=argmaxÏ€â€‹VMÏ€superscriptğœ‹subscriptargmaxğœ‹superscriptsubscriptğ‘‰ğ‘€ğœ‹\pi^{*}={\text{argmax}}_{\pi}V_{\mathit{M}}^{\pi}that maximizes the expected cumulative reward.

Conventional RL methods learn the value function through sampling(s,a)ğ‘ ğ‘(s,a)according to the target policyÏ€ğœ‹\pi, which is an online learning manner with error explorations. However, online learning is usually expensive under practical usage.
To this end, offline RL has emerged.
In offline RL, instead of obtaining interaction data via online environment explorations, the policy is learned purely from a static datasetğ’Ÿğ’Ÿ\mathcal{D}consisting of trajectories pre-collected with an unknown behavior policyÏ€Î²subscriptğœ‹ğ›½\pi_{\beta}. The goal of offline RL is to find the optimal policy using the static offline dataset.

Model-based Offline RL:Model-based offline RL aims to learn the environment model and utilize it to aid policy search. The model-based approaches typically train a state dynamics modelT^^ğ‘‡\hat{T}and a reward functionr^â€‹(s,a)^ğ‘Ÿğ‘ ğ‘\hat{r}(s,a)based on the given datasetğ’Ÿğ’Ÿ\mathcal{D}. Subsequently, an estimated MDPM^=(ğ’®,ğ’œ,T^,r^,Î¼0,Î³)^ğ‘€ğ’®ğ’œ^ğ‘‡^ğ‘Ÿsubscriptğœ‡0ğ›¾\hat{M}=(\mathcal{S},\mathcal{A},\mathit{\hat{T}},\hat{r},\mu_{0},\gamma)which has the same state and action space as the true MDP but uses the simulated state transition and reward function is constructed. Following this, the policy is learned through simulated interactions to avoid expensive online explorations.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: Figure 2:Trajectories generation with World Transformers. World Transformers consist of Reward Transformerâ„³rsubscriptâ„³ğ‘Ÿ\mathcal{M}_{r}and State Transformerâ„³ssubscriptâ„³ğ‘ \mathcal{M}_{s}. Given an original trajectory and the augmentation stept=i+1ğ‘¡ğ‘–1t=i+1(as shown in black dashed box), random noise of range(âˆ’Ïµ,Ïµ)italic-Ïµitalic-Ïµ(-\epsilon,\epsilon)is introduced toai+1subscriptğ‘ğ‘–1a_{i+1}(as shown asai+1âˆ—superscriptsubscriptğ‘ğ‘–1a_{i+1}^{*}in red circle) to form a new action. Then the correspondingri+1âˆ—superscriptsubscriptğ‘Ÿğ‘–1r_{i+1}^{*}andsi+2âˆ—superscriptsubscriptğ‘ ğ‘–2s_{i+2}^{*}can be inferred fromâ„³rsubscriptâ„³ğ‘Ÿ\mathcal{M}_{r}andâ„³ssubscriptâ„³ğ‘ \mathcal{M}_{s}, respectively. This process is repeated forhâ„htimes to generate the new trajectory.

This section presents OTTO, a novel model-based framework for offline RL. We first provide an overview in section3.1.
Then, the World Transformers are described in section3.2.
Finally, the process of long-horizon trajectory generalization is described in section3.3.

Offline RL aims to learn a policy from a static datasetğ’Ÿğ’Ÿ\mathcal{D}, which contains trajectoriesÏ„={st,at,rt}t=0|Ï„|ğœsuperscriptsubscriptsubscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ‘Ÿğ‘¡ğ‘¡0ğœ\tau=\{s_{t},a_{t},r_{t}\}_{t=0}^{|\tau|}pre-collected with an unknown behavior policy.tğ‘¡tdenotes the interaction step in the trajectoryÏ„ğœ\tau.
However, offline RL algorithms often fail to tackle unseen states and actions during online inference in a real environment.
In this work, we aim to improve the generalization capability of offline RL methods through generating
high-qualified simulated trajectoriesÏ„âˆ—={stâˆ—,atâˆ—,rtâˆ—}t=0|Ï„âˆ—|superscriptğœsuperscriptsubscriptsubscriptsuperscriptğ‘ ğ‘¡subscriptsuperscriptğ‘ğ‘¡subscriptsuperscriptğ‘Ÿğ‘¡ğ‘¡0superscriptğœ\tau^{*}=\{s^{*}_{t},a^{*}_{t},r^{*}_{t}\}_{t=0}^{|\tau^{*}|}, which enlarge the observed states and actions during offline model training.

To augment the trajectories, we first train World Transformers (â„³â„³\mathcal{M}) consisting of a State Transformer (â„³ssubscriptâ„³ğ‘ \mathcal{M}_{s}) and a Reward Transformer (â„³rsubscriptâ„³ğ‘Ÿ\mathcal{M}_{r}) based onğ’Ÿğ’Ÿ\mathcal{D}, which predict the next state and reward for a given(s,a)ğ‘ ğ‘(s,a)pair, respectively, to serve as the simulated environment.
Then, we introduce noises to perturb the original trajectoriesÏ„ğœ\tauand use World Transformers to generate simulated trajectoriesÏ„âˆ—superscriptğœ\tau^{*}, forming the generated datasetğ’Ÿgâ€‹eâ€‹nsubscriptğ’Ÿğ‘”ğ‘’ğ‘›\mathcal{D}_{gen}.
The new offline datasetğ’Ÿnâ€‹eâ€‹wsubscriptğ’Ÿğ‘›ğ‘’ğ‘¤\mathcal{D}_{new}is obtained by mixingğ’Ÿğ’Ÿ\mathcal{D}andğ’Ÿgâ€‹eâ€‹nsubscriptğ’Ÿğ‘”ğ‘’ğ‘›\mathcal{D}_{gen}, i.e.,ğ’Ÿnâ€‹eâ€‹w=ğ’Ÿâˆªğ’Ÿgâ€‹eâ€‹nsubscriptğ’Ÿğ‘›ğ‘’ğ‘¤ğ’Ÿsubscriptğ’Ÿğ‘”ğ‘’ğ‘›\mathcal{D}_{new}=\mathcal{D}\cup\mathcal{D}_{gen}.
Finally, a model-free offline RL algorithm is trained uponğ’Ÿnâ€‹eâ€‹wsubscriptğ’Ÿğ‘›ğ‘’ğ‘¤\mathcal{D}_{new}.Î´=|ğ’Ÿgâ€‹eâ€‹n|/|ğ’Ÿ|ğ›¿subscriptğ’Ÿğ‘”ğ‘’ğ‘›ğ’Ÿ\delta=|\mathcal{D}_{gen}|/|\mathcal{D}|is used as the augmentation ratio.

To perform trajectory augmentation, we need to implement an environment simulator which models the state transition distribution and the reward function.
However, existing model-based methods fail to generate long-horizon and high-rewarded trajectories, as shown in Figure1(b), leading to trivial improvement.
Inspired by recent work using the Transformer architecture by casting the RL problem as conditional sequence modeling tasks(Chen etÂ al.,2021), we propose to model the environment dynamics through Transformers, aka World Transformers. It has been shown that Transformers can effectively learn from long input sequences and produce satisfying outputs.

Model Inputs.The key factors to construct the environment model inputs are: (1) the model input should contain all the factors determining the environment state transitions; (2) the model should enable Transformers to learn from continuous sequential interactions; (3) the model input should be as simple as possible to avoid unnecessary noise. Considering that in online RL the environment provides feedback simply based on the agentâ€™s action and the current state, the model input for World Transformers contains multiple continuous state and action tuples(s,a)ğ‘ ğ‘(s,a). Formally, we define the model input as

Both the State Transformer and Reward Transformer take the above model inputs for environment simulation.

Environment Modeling.We use the State Transformer and the Reward Transformer to model the next state transition and the reward function, respectively.
Particularly, we introduce a learnable embedding matrix to represent the interaction steptğ‘¡t, which is similar to the positional embedding in language modeling. Noticing that GPT-series models have achieved remarkable success across various NLP tasks(Radford etÂ al.,2018; Brown etÂ al.,2020; Radford etÂ al.,2019), we deployed two GPT models to model the states transition and reward function. At timesteptğ‘¡t, the State Transformer and Reward Transformer model the following distributions:

where{s,a}tâˆ’L+1tsuperscriptsubscriptğ‘ ğ‘ğ‘¡ğ¿1ğ‘¡{\{{s},{a}\}}_{t-L+1}^{t}represents the model input sequence{stâˆ’L+1,atâˆ’L+1,stâˆ’L+2,atâˆ’L+2,â€¦,st,at}subscriptğ‘ ğ‘¡ğ¿1subscriptğ‘ğ‘¡ğ¿1subscriptğ‘ ğ‘¡ğ¿2subscriptğ‘ğ‘¡ğ¿2â€¦subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡\{{s}_{t-L+1},{a}_{t-L+1},{s}_{t-L+2},{a}_{t-L+2},...,{s}_{t},{a}_{t}\}as shown in Eq.(1).Ls,Lrsubscriptğ¿ğ‘ subscriptğ¿ğ‘ŸL_{s},L_{r}represent the hyperparameters that define the length of the model inputs forâ„³s,â„³rsubscriptâ„³ğ‘ subscriptâ„³ğ‘Ÿ\mathcal{M}_{s},\mathcal{M}_{r}respectively.
We also tried to model both distributions in one unified Transformer, but we did not observe a better performance.

Bothâ„³ssubscriptâ„³ğ‘ \mathcal{M}_{s}andâ„³rsubscriptâ„³ğ‘Ÿ\mathcal{M}_{r}are trained in a supervised manner upon the offline dataset. The prediction output at the position ofatsubscriptğ‘ğ‘¡{a}_{t}is trained to predicts^t+1subscript^ğ‘ ğ‘¡1\hat{s}_{t+1}orr^tsubscript^ğ‘Ÿğ‘¡\hat{r}_{t}inâ„³ssubscriptâ„³ğ‘ \mathcal{M}_{s}orâ„³rsubscriptâ„³ğ‘Ÿ\mathcal{M}_{r}, respectively. We use mean-squared error for both the next state prediction and reward prediction. World Transformers are effective in conducting long sequence modeling and thus can be used to generate long-horizon simulation.

We now discuss how to generate trajectories with World Transformers. The key idea in trajectory generation is to expand the observed state-action tuples, thereby improving the performance of offline RL. Motivated by this, we introduce uniform noise to the actionatsubscriptğ‘ğ‘¡a_{t}in the original trajectory, forming a perturbed actionatâˆ—subscriptsuperscriptğ‘ğ‘¡a^{*}_{t}. In particular, we add a random number of range(âˆ’Ïµ,Ïµ)italic-Ïµitalic-Ïµ(-\epsilon,\epsilon)to each dimension ofatsubscriptğ‘ğ‘¡a_{t}. Then we predict the corresponding next statest+1âˆ—subscriptsuperscriptğ‘ ğ‘¡1s^{*}_{t+1}and rewardrtâˆ—subscriptsuperscriptğ‘Ÿğ‘¡r^{*}_{t}with the World Transformer. The alteration of action results in changes to the next state, therefore a new tuple{atâˆ—,rtâˆ—,st+1âˆ—}subscriptsuperscriptğ‘ğ‘¡subscriptsuperscriptğ‘Ÿğ‘¡subscriptsuperscriptğ‘ ğ‘¡1\{a^{*}_{t},r^{*}_{t},s^{*}_{t+1}\}is generated in a new trajectory. We repeat this process of introducing noise to the original action and predicting the next state and reward from timestept=tsğ‘¡subscriptğ‘¡ğ‘ t=t_{s}tot=ts+hâˆ’1ğ‘¡subscriptğ‘¡ğ‘ â„1t=t_{s}+h-1, wherehâ„his a hyperparameter that defines the length of simulation. The whole process is visualized in Figure2. The generated trajectory effectively expands the observed state-action space. Besides, the generated trajectory is still continuous, which enables World Transformers to learn autoregressively from previous interaction steps, thereby avoiding overly erroneous estimations for long-horizon simulation.
The perturbation of offline data also helps OTTO to utilize the prior knowledge of offline data, thus generating high-rewarded trajectories.

Generating for all the timesteps in the original offline dataset is not appropriate because (1) this may also introduce extra noise to the offline dataset and (2) not every tuple{st,at,rt}subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ‘Ÿğ‘¡\{s_{t},a_{t},r_{t}\}in a trajectory is worthy of augmenting. We prefer to generate trajectories with high total rewards, which are good demonstrations for policy improvement. In this paper, we propose four trajectory generation strategies:Random :For each trajectoryÏ„={st,at,rt}t=0|Ï„|ğœsubscriptsuperscriptsubscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ‘Ÿğ‘¡ğœğ‘¡0\tau=\{s_{t},a_{t},r_{t}\}^{|\tau|}_{t=0}in offline dataset, we randomly select the start timesteptsâˆˆ[0,|Ï„|âˆ’h+1]subscriptğ‘¡ğ‘ 0ğœâ„1t_{s}\in[0,|\tau|-h+1], and generate trajectoryÏ„âˆ—superscriptğœ\tau^{*}based on the trajectory segment{st,at,rt}t=tsts+hâˆ’1subscriptsuperscriptsubscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ‘Ÿğ‘¡subscriptğ‘¡ğ‘ â„1ğ‘¡subscriptğ‘¡ğ‘ \{s_{t},a_{t},r_{t}\}^{t_{s}+h-1}_{t=t_{s}}.Top-Kğ¾K:We first split all original trajectories into segments of lengthhâ„h. Then we sort these segments by their cumulative rewards in descending order and generate trajectories based on the top-Kğ¾Ksegments, whereK=|ğ’Ÿgâ€‹eâ€‹n|/hğ¾subscriptğ’Ÿğ‘”ğ‘’ğ‘›â„K=|\mathcal{D}_{gen}|/h.Softmax :We also split the trajectory segments as the Top-Kğ¾Kstrategy, but we choose the segments according to their probabilities calculated by a softmax function over the cumulative rewards of the segments.BeamSearch :Different with Top-Kğ¾Kand Softmax strategies which perturb each segment one time, the BeamSearch strategy perturbs each segment multiple times and selects the trajectories with highest rewards as the augmented data. The details are summarized in AlgorithmLABEL:alg:beamsearchin the appendix.

Finally, a model-free offline RL algorithm, e.g., DT(Chen etÂ al.,2021), is trained upon both the original dataset and the augmented dataset to learn a policy.

Offline RL is the task of learning policies from a static dataset of pre-collected trajectories. It has been applied to various domains including robotics(Kumar etÂ al.,2021; Kalashnikov etÂ al.,2018), healthcare(Wang etÂ al.,2018; Tang & Wiens,2021; Tang etÂ al.,2022)and NLP(Snell etÂ al.,2023; Verma etÂ al.,2022; Wu & Hu,2018). Existing offline RL methods can be categorized into model-free methods and model-based methods.

Model-free methods.As the learned policy might differ from the behavior policy, offline RL encounters the distributional shift issue. Most approaches for model-free offline RL are based on restricting the target policy, including constraining the learned policy to near the behavior policy(Fujimoto etÂ al.,2019; Kostrikov etÂ al.,2021b; Fujimoto & Gu,2021; Wu etÂ al.,2019; Siegel etÂ al.,2020), incorporating value pessimism(Kumar etÂ al.,2020; Xie etÂ al.,2021; Kostrikov etÂ al.,2021a), importance sampling(Liu etÂ al.,2019; Nachum etÂ al.,2019; Sutton etÂ al.,2016), and uncertainty punishment for value functions(Sinha etÂ al.,2022; An etÂ al.,2021; Kumar etÂ al.,2019).
However, these methods only use the offline dataset, which prevents them from broader generalization. On the contrary, we use both the
offline data as well as the high-qualified augmentation data, which can further enhance the performance of these existing methods.

Recently,Chen etÂ al. (2021); Janner etÂ al. (2021)proposed to cast offline RL into a sequence modeling problem and use Transformers to solve it in a supervised manner.
Their methods directly output the action but are limited to the support of offline data. However, the proposed OTTO aims to model the environment through World Transformers, leading to better generalization.

Model-based methods.Model-based offline RL methods(Yu etÂ al.,2020b; Kidambi etÂ al.,2020; Yu etÂ al.,2021; Rigter etÂ al.,2022; Zhan etÂ al.,2022; Swazinna etÂ al.,2021; Rafailov etÂ al.,2020; Matsushima etÂ al.,2020)provide an alternative approach that learns a model of environment and generate synthetic rollouts from the model to optimize the policy.
Model-based methods provide the potential for broader generalization. However, we found the trajectories generated by these methods are extremely short-horizon (with simulation lengthh=5â„5h=5or evenh=1â„1h=1), resulting in limited generalization improvement.
On the contrary, OTTO can generate long-horizon trajectories with high rewards by World Transformers, which provides more potential for better generalization.Micheli etÂ al. (2022)also proposed to use Transformers to model the environment. However, their methods are not tailored for offline RL.
Besides, OTTO can serve as a plug-in module that can be integrated with existing model-free methods to provide further improvement.

In this section, we conduct experiments to answer the following research questions: (1) Can OTTO effectively improve the performance of existing model-free offline RL methods? (2) How does OTTO perform compared to state-of-the-art model-based offline RL methods? (3) How does the design of OTTO affect the performance?

To answer question (1), we integrate OTTO with
several representative model-free offline RL approaches and compare the agent performance with or without OTTO. These model-free offline RL approaches include DT(Chen etÂ al.,2021), CQL(Kumar etÂ al.,2020)and BCQ(Fujimoto etÂ al.,2019).
DT employs Transformers to cast RL as a sequence modeling task. The CQL and BCQ are both time-difference (TD) learning algorithms and CQL is regarded as one of the state-of-the-art model-free approaches. To answer question (2), we choose COMBO(Yu etÂ al.,2021), MOPO(Yu etÂ al.,2020b)and MOReL(Kidambi etÂ al.,2020)as our baselines. These model-based methods also aim to use a model to simulate the environment and generate short-horizon rollouts to improve the policy learning, among which COMBO achieved the best performance.

We evaluate all the methods on continuous control tasks in OpenAI Gym MuJoCo environments(Brockman etÂ al.,2016)from the D4RL benchmark(Fu etÂ al.,2020), which contains three environments (halfcheetah, hopper, and walker2d) and four datasets for each environment (expert, medium, medium-replay, and medium-expert).

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: Figure 3:The average immediate reward of each interaction step in simulated trajectories withh=50â„50h=50. Random, Top-Kğ¾K, Softmax, and BeamSearch correspond to four strategies of OTTO. MOPO refers to the trajectories generated by MOPO(Yu etÂ al.,2020b).

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: Figure 4:The average immediate reward of each interaction step in the trajectories generated by OTTO. The dashed line represents predicted immediate reward by using the World Transformers. The solid line represents real immediate reward by calculating in online environments.

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: Figure 4:The average immediate reward of each interaction step in the trajectories generated by OTTO. The dashed line represents predicted immediate reward by using the World Transformers. The solid line represents real immediate reward by calculating in online environments.

[å›¾ç‰‡: images\image_8.png]
å›¾ç‰‡è¯´æ˜: Figure 4:The average immediate reward of each interaction step in the trajectories generated by OTTO. The dashed line represents predicted immediate reward by using the World Transformers. The solid line represents real immediate reward by calculating in online environments.

We implement three representative model-free methods and train them on both the original datasetğ’Ÿğ’Ÿ\mathcal{D}andğ’Ÿnâ€‹eâ€‹wsubscriptğ’Ÿğ‘›ğ‘’ğ‘¤\mathcal{D}_{new}augmented by OTTO. TableLABEL:tb:model_freeshows the summary of our experimental results. Firstly, we can see that with the augmented trajectories from OTTO, all algorithms achieve better scores than their original scores in 30 out of total 36 settings, and achieve comparable results in remaining settings, suggesting that OTTO is robust to different datasets and can generalize to multiple model-free algorithms. Secondly, DT is the most suitable algorithm for OTTO among the three algorithms as it outperforms Ori in all settings, especially inhopper_medium-replay(+17.2 scores). We guess the reason could be that DT can hone in on high-rewarded actions(Chen etÂ al.,2021), resulting in better utilization of the generated trajectories. Thirdly, for the other two TD-learning algorithms, OTTO also achieves remarkable performance improvement (2.2 average improvement for CQL and 2.8 average improvement for BCQ), suggesting that OTTO is robust to different model-free offline RL algorithms. Finally, we find that OTTO achieves significant improvement inhopperenvironment, while its performance is not as remarkable inhalfcheetahenvironment. We attribute this performance gap to the differences between environments, more details are shown in the error analysis of section5.3.

To conclude, the proposed OTTO can help to further improve the performance of existing model-free methods.

In this section, we compare OTTO and existing model-based methods. We integrate OTTO with CQL as our methods. For baselines,
COMBO(Yu etÂ al.,2021)improved the Q-function used in CQL and achieved current state-of-the-art results in a wide range of tasks. We also include MOPO and MORel as baselines.
TableLABEL:tb:model_basedshows the comparison results.
We can see that CQL+OTTO significantly outperforms COMBO in all datasets ofhopperandwalker2denvironments and achieves a higher average score (90.5 vs 82.0). While COMBO outperforms CQL+OTTO inhalfcheetahenvironments, we discuss the possible reasons in the error analysis of section5.3.

Besides, we compare the quality between the trajectories generated by our method and MOPO as shown in Figure3.
Particularly, we calculate the real reward for each state-action tuple(st,at)subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡(s_{t},a_{t})at timesteptğ‘¡tin the trajectories generated by MOPO and OTTO in an online environment. We also calculate the average reward in the offline dataset as a reference. We can see that the immediate reward of MOPO initially hovers around the mean score during the initial five timesteps but suffers a rapid decline as the timesteps progress. The trajectories generated by MOPO are quite low-rewarded compared to the mean of offline data, which leads to trivial improvement for policy learning. On the contrary, all the four strategies in OTTO generate high-rewarded long-horizon trajectories. The rewards for all four strategies remain relatively stable in all timesteps. Three of them consistently remain above the offline mean score, while the remaining one hovers around the mean score.

To conclude, OTTO can generate high-qualified trajectories and outperforms existing model-based methods.

In this section, we evaluate the four trajectory generation strategies using DT as the following model-free RL method. TableLABEL:tb:four_strategysummarizes the experimental results. To ensure a fair comparison, we setÎ´ğ›¿\deltato be the same for all strategies (Î´=1/8ğ›¿18\delta=1/8for the medium-replay dataset andÎ´=1/10ğ›¿110\delta=1/10for other datasets). Firstly, we can see that all the strategies can outperform their original performance, which suggests that all four strategies can enhance the performance of existing offline RL algorithms.
Secondly, we found the overall performance of Top-Kğ¾Kis similar to Softmax. Top-Kğ¾Kperforms slightly better in datasets with high-rewarded trajectories (medium-expert and expert), while Softmax performs better in other datasets (medium-replay and medium).
Besides, we find the Random strategy performs best in expert and medium datasets.
As noted inYu etÂ al. (2020b); Rafailov etÂ al. (2020), medium-replay and medium-expert are collected with a wide range of policies while expert and medium datasets are collected with fewer policies and have narrow state-action distributions.
In such a case, the Random strategy can more effectively enlarge the observed state-action space through perturbation on randomly selected actions, thus leading to better performance.

To investigate the quality of generated trajectories, we aim to address the following two questions: (i) can OTTO generate high-rewarded trajectories by using manually designed trajectory generation strategies; (ii) how is the estimated error of reward prediction? To answer these questions, we calculate the immediate reward of each interaction step in the trajectories generated by OTTO as shown in Figure4. We can see that for both predicted rewards and accurate rewards, BeamSearch strategy generates the highest-rewarded trajectories in all settings, Softmax and Top-Kğ¾Kstrategies perform comparably in the middle, while the Random strategy generates the lowest-rewarded trajectory in most settings. This suggests that three manually designed trajectory generation strategies can effectively generate high-rewarded trajectories by selecting the specific segments. However, we can see that these manually designed trajectory generation strategies also lead to a bigger gap between the real reward and the predicted reward, which affects policy performance as well.
In datasets with narrow state-action distribution such as the expert dataset, the estimated error of immediate reward is relatively higher.
As a result, the Random strategy performs better in expert dataset.
While in datasets with a wide range of policies like medium-replay, we can see BeamSearch can generate high-rewarded trajectories with little estimated error, resulting in better performance as shown in TableLABEL:tb:four_strategy. These observations suggest that we should navigate a trade-off between low-error, low-reward strategy (Random) and high-error, high-reward strategies (Top-Kğ¾K, Softmax, BeamSearch) based on the specific dataset.

TableLABEL:tb:deltashows the impact of the augmentation ratio.
Firstly, we can see that for all augmentation ratios varying fromÎ´=1/8ğ›¿18\delta=1/8toÎ´=2ğ›¿2\delta=2, OTTO outperforms its original performance. This suggests that DT+OTTO is robust to theÎ´ğ›¿\delta.
Secondly, for all environments, we observed that as the augmentation ratio increases, the performance of OTTO improves because the mixed dataset with more augmented data is able to provide more high-rewarded trajectories to enlarge the observed states and actions.
But the performance will drop whenÎ´ğ›¿\deltais too large. The reason is that too largeÎ´ğ›¿\deltaintroduces more noise.

[å›¾ç‰‡: images\image_9.png]
å›¾ç‰‡è¯´æ˜: Figure 5:Comparison of the average loss of state estimation and reward estimation between three environments.

In experiments, we found that OTTO can significantly improve the performance inhopperandwalker2denvironments, but only perform comparably inhalfcheetah.
To further analyze this performance gap, we compare the estimated error of environment modeling between these three environments as shown in Figure5.
Particularly, we define the loss as mean absolute deviation.
We can see that the loss of next state estimation and reward estimation inhalfcheetahare both higher than that in the other two environments. Especially the standard deviation of reward loss is extremely high.
To conclude, the environment simulation inhalfcheetahis not as accurate as the other two environments, resulting in a smaller improvement of OTTO.
Such observation indicates that the accuracy of the environment model is essential for model-based methods.

In this work, we have presented offline trajectory generalization through world transformers for offline reinforcement learning (OTTO). In particular, we have trained the World Transformers to simulate the environment and proposed four trajectory generation strategies to generate high-qualified trajectories based on World Transformers.
The RL agent is trained upon both the original data and augmented trajectories from World Transformers.
Extensive experiments on D4RL benchmarks show that OTTO improves the performance of representative model-free offline RL algorithms and outperforms strong model-based baselines. Despite the advantages of OTTO, there are a few challenges left such as the relatively high estimation error of World Transformers in complex environments. We leave them for future studies. Additionally, noticing that large language models (LLMs) have achieved promising results in a variety of tasks, we also want to investigate the prospective utilization of LLMs as environmental models in future research.

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

We summarized the details of BeamSearch strategy as shown in AlgorithmLABEL:alg:beamsearch.

[å›¾ç‰‡: images\image_10.png]
å›¾ç‰‡è¯´æ˜: Figure 6:The average L1 loss of next state estimation and reward estimation. We do experiments on walker2d-expert and walker2d-medium-replay dataset and use the BeamSearch strategy.

[å›¾ç‰‡: images\image_11.png]
å›¾ç‰‡è¯´æ˜: Figure 6:The average L1 loss of next state estimation and reward estimation. We do experiments on walker2d-expert and walker2d-medium-replay dataset and use the BeamSearch strategy.

[å›¾ç‰‡: images\image_12.png]
å›¾ç‰‡è¯´æ˜: Figure 6:The average L1 loss of next state estimation and reward estimation. We do experiments on walker2d-expert and walker2d-medium-replay dataset and use the BeamSearch strategy.

[å›¾ç‰‡: images\image_13.png]
å›¾ç‰‡è¯´æ˜: Figure 6:The average L1 loss of next state estimation and reward estimation. We do experiments on walker2d-expert and walker2d-medium-replay dataset and use the BeamSearch strategy.

We consider the range of noiseÏµitalic-Ïµ\epsilonwe introduce to the action as an important parameter in OTTO, as it determines the scope of searching for optimal actions. In this section, we evaluate the hyperparameter searches for the range of noiseÏµitalic-Ïµ\epsilonon DT as shown in TableLABEL:tb:epsilon.

We can see that when we introduce a wide range of noise to the actions (Ïµ=0.5italic-Ïµ0.5\epsilon=0.5), OTTO performs better in medium-replay dataset. When the perturbation is small, we found a better performance in expert dataset. Ideally, a larger perturbation range leads to a broader generalization. However, this will also lead to more inaccurate estimations. We calculate the average L1 loss of next state estimation and reward estimation for each interaction step as shown in Figure6. We can see that in expert dataset, either reward L1 loss or next state L1 loss is relatively high whenÏµ=0.5italic-Ïµ0.5\epsilon=0.5compared toÏµ=0.02italic-Ïµ0.02\epsilon=0.02andÏµ=0.1italic-Ïµ0.1\epsilon=0.1. While in medium-replay dataset, there is no significant difference in both reward L1 loss and next state L1 loss between three values ofÏµitalic-Ïµ\epsilon. This suggests that we should choose the appropriate value ofÏµitalic-Ïµ\epsilonbased on the specific dataset.

In this section, we discuss the hyperparameters that we use for OTTO. Similar toChen etÂ al. (2021), the World Transformers is based on the implementation of GPT-2(Radford etÂ al.,2019). TableLABEL:tb:hyperlists the hyperparameters we use to train the World Transformers. We use the same hyperparameters for both the State Transformer and Reward Transformer.

We now list the additional hyperparameters of trajectory generation as follows.

Trajectory lengthhâ„h.Unlike existing model-based offline RL only perform short-horizon rollouts withhâ‰¤5â„5h\leq 5, OTTO performs long-horizon trajectories withh=50â„50h=50. However, we found whenlğ‘™lis too large, it will also lead to inaccurate estimation.

Augmentation ratioÎ´ğ›¿\delta.For DT, we simply chooseÎ´=0.1ğ›¿0.1\delta=0.1in medium, medium-expert and expert dataset, as we didnâ€™t find significant improvement whenÎ´ğ›¿\deltaincreases. We find the total timesteps in medium-replay dataset are relatively low, so we chooseÎ´=0.25ğ›¿0.25\delta=0.25to augment more trajectories. We findÎ´ğ›¿\deltais sensitive to other offline RL methods which are based on restricting the learning policy such as CQL and BCQ. When theÎ´ğ›¿\deltais too large, these methods have either poor performance or large variance. Therefore, we chooseÎ´âˆˆ{0.05,0.1}ğ›¿0.050.1\delta\in\{0.05,0.1\}to avoid too much noise.

Range of noiseÏµitalic-Ïµ\epsilon.We selectÏµitalic-Ïµ\epsilonfrom the set{0.02,0.1,0.5}0.020.10.5\{0.02,0.1,0.5\}, which correspond to low perturbation range, medium perturbation range and high perturbation range. In the D4RL experiments, we found thatÏµ=0.02italic-Ïµ0.02\epsilon=0.02works well for the medium and expert dataset, which have narrow state-action distributions. And we chooseÏµ=0.1italic-Ïµ0.1\epsilon=0.1for medium-expert dataset andÏµ=0.5italic-Ïµ0.5\epsilon=0.5for medium-replay dataset, which is collected by a wide range of policies.

BeamSearch widthBğµB.We simply chooseB=10ğµ10B=10for all settings. This takes into account both the efficiency of generating trajectories and the high qualification of generated trajectories.

[å›¾ç‰‡: images\image_14.png]

[å›¾ç‰‡: images\image_15.png]

