标题：Offline Trajectory Generalization for Offline Reinforcement Learning

Offline reinforcement learning (RL) aims to learn policies from static datasets of previously collected trajectories.
Existing methods for offline RL either constrain the learned policy to the support of offline data or utilize model-based virtual environments to generate simulated rollouts.
However, these methods suffer from(i)poor generalization to unseen states; and(ii)trivial improvement from low-qualified rollout simulation.In this paper, we proposeofflinetrajectory generalization through worldtransformers foroffline reinforcement learning (OTTO).
Specifically, we use casual Transformers, a.k.a. World Transformers, to predict state dynamics and the immediate reward. Then we propose four strategies to use World Transformers to generate high-rewarded trajectory simulation by perturbing the offline data. Finally, we jointly use offline data with simulated data to train an offline RL algorithm.
OTTO serves as a plug-in module and can be integrated with existing offline RL methods to enhance them with better generalization capability of transformers and high-rewarded data augmentation.
Conducting extensive experiments on D4RL benchmark datasets, we verify that OTTO significantly outperforms state-of-the-art offline RL methods.

Offline reinforcement learning (RL) refers to training RL agents from pre-collected datasets, without the need for real-time interactions or online explorations(Levine et al.,2020).
This paradigm plays a crucial role in real-world scenarios where collecting online interactions can be expensive or risky, as observed in domains like healthcare(Liu et al.,2020), robotics(Singh et al.,2022), autonomous driving(Yu et al.,2020a), and recommendation systems(Swaminathan & Joachims,2015).
Standard RL methods often fail in such offline setting due to erroneous estimation of value functions(Fujimoto et al.,2019).

Existing offline RL methods can be classified into two categories: model-free methods and model-based methods.
Model-free methods incorporate conservatism into the value function estimation(Fujimoto et al.,2019; Kumar et al.,2019; Fujimoto et al.,2018; Siegel et al.,2020; Kumar et al.,2020; Kostrikov et al.,2021a).
For example, CQL(Kumar et al.,2020)adds a regularization term into the Q-function update. The conservatism downgrades the value function for unseen states and thus helps to avoid the over-estimation issue. As a result, the learned policy is constrained to near the offline data distribution. Besides, the overly conservatism could lead to underestimation of potentially good actions. Such issues lead to the poor generalization capability of model-free offline RL methods.

Unlike model-free RL methods,
model-based RL algorithms learn a dynamics model based on the offline dataset. The dynamics model can be utilized as a simulated environment to generate additional interaction data, providing new potential to further enhance the learned policy(Yu et al.,2020b; Kidambi et al.,2020; Yu et al.,2021).
However, we found that the simulated data generated by existing model-based RL methods is of low qualification, resulting in trivial improvement for policy learning.
Specifically, existing model-based RL approaches can only perform short-horizon model rollouts, resulting in marginal generalization improvement only near the support data, as shown in Figure1(a).
Besides, when we perform the simulation of long-horizon trajectories using the environment model, the
average reward of each interaction step often becomes lower for longer steps, as shown in Figure1(b). As a result, the performance of the learned policy suffers a sharp decline, as shown in Figure1(c).
The above observation indicates that existing model-based offline RL methods suffer from trivial improvement from low-qualified rollout simulation.

[图片: images\image_1.png]
图片说明: (a)Existing model-based methods vs OTTO.

[图片: images\image_2.png]
图片说明: (a)Existing model-based methods vs OTTO.

[图片: images\image_3.png]
图片说明: (a)Existing model-based methods vs OTTO.

To address the aforementioned issues, we proposeofflinetrajectory generalization through worldtransformers foroffline reinforcement learning (OTTO). Specifically, we first learn the state dynamics transition and reward function by a pair of Transformers, a.k.a. World Transformers, based on the offline dataset.
It is observed that the Transformer architecture achieves state-of-the-art performance in wide domains, including natural language processing (NLP)(Devlin et al.,2018; Raffel et al.,2020)and computer vision(He et al.,2022; Dosovitskiy et al.,2020). The Transformer particularly stands out for its outstanding generalization capability.
As a result, the proposed World Transformers are utilized to improve the generalization capability for environment simulation.
Then, four strategies are proposed to use World Transformers to generate long trajectory simulation by perturbing the offline data. The generalization capability of World Transformers, together with the prior knowledge in the offline dataset, ensures that the generated long-horizon trajectories are high-rewarded and thus helps the model to learn potential good actions.
Finally, offline data together with the simulated trajectories are jointly used to train an offline RL algorithm.
Note that OTTO serves as a plug-in module and can be integrated with a wide range of offline RL methods to enhance the learned policy further.

Our contributions in this paper are as follows:

We propose World Transformers, which utilize the sequential modeling of Transformers to learn state dynamics transition and reward functions with good generalization capability.

We propose four trajectory simulation strategies, which can autoregressively generate high-qualified long-horizon trajectory augmentation by using the World Transformers.

Our experiments on D4RL benchmarks show that OTTO can effectively improve the performance of representative model-free offline RL algorithms and outperforms strong model-based baselines.

Offline RL:A Markov decision process is defined by the tupleM=(𝒮,𝒜,T,r,μ0,γ)𝑀𝒮𝒜𝑇𝑟subscript𝜇0𝛾\mathit{M}=(\mathcal{S},\mathcal{A},\mathit{T},r,\mu_{0},\gamma), where𝒮,𝒜𝒮𝒜\mathcal{S},\mathcal{A}refer to state space and action space, respectively.
A policyπ​(a|s)𝜋conditional𝑎𝑠\pi(a|s)defines a mapping from states∈𝒮𝑠𝒮s\in\mathcal{S}to a probability distribution over actiona∈𝒜𝑎𝒜a\in\mathcal{A}.
Given a(s,a)𝑠𝑎(s,a)pair,T​(s′|s,a)𝑇conditionalsuperscript𝑠′𝑠𝑎T(s^{\prime}|s,a)andr​(s,a)𝑟𝑠𝑎r(s,a)represent the distribution of next states′superscript𝑠′s^{\prime}and the obtained immediate reward, respectively.μ0subscript𝜇0\mu_{0}is the initial state distribution andγ∈(0,1)𝛾01\gamma\in(0,1)is the discount factor for future reward.
The value functionVMπ​(s):=𝔼π,T[∑t=0∞γt​r​(st,at)|s0=s]assignsuperscriptsubscript𝑉𝑀𝜋𝑠subscript𝔼𝜋𝑇delimited-[]conditionalsuperscriptsubscript𝑡0superscript𝛾𝑡𝑟subscript𝑠𝑡subscript𝑎𝑡subscript𝑠0𝑠V_{\mathit{M}}^{\pi}(s):=\mathop{\mathbb{E}}\limits_{\pi,\mathit{T}}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})|s_{0}=s]represents the expected cumulative reward from executingπ𝜋\pifrom states𝑠s.
We defineVMπsuperscriptsubscript𝑉𝑀𝜋V_{\mathit{M}}^{\pi}as the value function under the initial state distribution, i.e.VMπ=∑s∈𝒮μ0​(s)​VMπ​(s)superscriptsubscript𝑉𝑀𝜋subscript𝑠𝒮subscript𝜇0𝑠superscriptsubscript𝑉𝑀𝜋𝑠V_{\mathit{M}}^{\pi}=\sum_{s\in\mathcal{S}}\mu_{0}(s)V_{\mathit{M}}^{\pi}(s).
The goal of RL is to find an optimal policyπ∗=argmaxπ​VMπsuperscript𝜋subscriptargmax𝜋superscriptsubscript𝑉𝑀𝜋\pi^{*}={\text{argmax}}_{\pi}V_{\mathit{M}}^{\pi}that maximizes the expected cumulative reward.

Conventional RL methods learn the value function through sampling(s,a)𝑠𝑎(s,a)according to the target policyπ𝜋\pi, which is an online learning manner with error explorations. However, online learning is usually expensive under practical usage.
To this end, offline RL has emerged.
In offline RL, instead of obtaining interaction data via online environment explorations, the policy is learned purely from a static dataset𝒟𝒟\mathcal{D}consisting of trajectories pre-collected with an unknown behavior policyπβsubscript𝜋𝛽\pi_{\beta}. The goal of offline RL is to find the optimal policy using the static offline dataset.

Model-based Offline RL:Model-based offline RL aims to learn the environment model and utilize it to aid policy search. The model-based approaches typically train a state dynamics modelT^^𝑇\hat{T}and a reward functionr^​(s,a)^𝑟𝑠𝑎\hat{r}(s,a)based on the given dataset𝒟𝒟\mathcal{D}. Subsequently, an estimated MDPM^=(𝒮,𝒜,T^,r^,μ0,γ)^𝑀𝒮𝒜^𝑇^𝑟subscript𝜇0𝛾\hat{M}=(\mathcal{S},\mathcal{A},\mathit{\hat{T}},\hat{r},\mu_{0},\gamma)which has the same state and action space as the true MDP but uses the simulated state transition and reward function is constructed. Following this, the policy is learned through simulated interactions to avoid expensive online explorations.

[图片: images\image_4.png]
图片说明: Figure 2:Trajectories generation with World Transformers. World Transformers consist of Reward Transformerℳrsubscriptℳ𝑟\mathcal{M}_{r}and State Transformerℳssubscriptℳ𝑠\mathcal{M}_{s}. Given an original trajectory and the augmentation stept=i+1𝑡𝑖1t=i+1(as shown in black dashed box), random noise of range(−ϵ,ϵ)italic-ϵitalic-ϵ(-\epsilon,\epsilon)is introduced toai+1subscript𝑎𝑖1a_{i+1}(as shown asai+1∗superscriptsubscript𝑎𝑖1a_{i+1}^{*}in red circle) to form a new action. Then the correspondingri+1∗superscriptsubscript𝑟𝑖1r_{i+1}^{*}andsi+2∗superscriptsubscript𝑠𝑖2s_{i+2}^{*}can be inferred fromℳrsubscriptℳ𝑟\mathcal{M}_{r}andℳssubscriptℳ𝑠\mathcal{M}_{s}, respectively. This process is repeated forhℎhtimes to generate the new trajectory.

This section presents OTTO, a novel model-based framework for offline RL. We first provide an overview in section3.1.
Then, the World Transformers are described in section3.2.
Finally, the process of long-horizon trajectory generalization is described in section3.3.

Offline RL aims to learn a policy from a static dataset𝒟𝒟\mathcal{D}, which contains trajectoriesτ={st,at,rt}t=0|τ|𝜏superscriptsubscriptsubscript𝑠𝑡subscript𝑎𝑡subscript𝑟𝑡𝑡0𝜏\tau=\{s_{t},a_{t},r_{t}\}_{t=0}^{|\tau|}pre-collected with an unknown behavior policy.t𝑡tdenotes the interaction step in the trajectoryτ𝜏\tau.
However, offline RL algorithms often fail to tackle unseen states and actions during online inference in a real environment.
In this work, we aim to improve the generalization capability of offline RL methods through generating
high-qualified simulated trajectoriesτ∗={st∗,at∗,rt∗}t=0|τ∗|superscript𝜏superscriptsubscriptsubscriptsuperscript𝑠𝑡subscriptsuperscript𝑎𝑡subscriptsuperscript𝑟𝑡𝑡0superscript𝜏\tau^{*}=\{s^{*}_{t},a^{*}_{t},r^{*}_{t}\}_{t=0}^{|\tau^{*}|}, which enlarge the observed states and actions during offline model training.

To augment the trajectories, we first train World Transformers (ℳℳ\mathcal{M}) consisting of a State Transformer (ℳssubscriptℳ𝑠\mathcal{M}_{s}) and a Reward Transformer (ℳrsubscriptℳ𝑟\mathcal{M}_{r}) based on𝒟𝒟\mathcal{D}, which predict the next state and reward for a given(s,a)𝑠𝑎(s,a)pair, respectively, to serve as the simulated environment.
Then, we introduce noises to perturb the original trajectoriesτ𝜏\tauand use World Transformers to generate simulated trajectoriesτ∗superscript𝜏\tau^{*}, forming the generated dataset𝒟g​e​nsubscript𝒟𝑔𝑒𝑛\mathcal{D}_{gen}.
The new offline dataset𝒟n​e​wsubscript𝒟𝑛𝑒𝑤\mathcal{D}_{new}is obtained by mixing𝒟𝒟\mathcal{D}and𝒟g​e​nsubscript𝒟𝑔𝑒𝑛\mathcal{D}_{gen}, i.e.,𝒟n​e​w=𝒟∪𝒟g​e​nsubscript𝒟𝑛𝑒𝑤𝒟subscript𝒟𝑔𝑒𝑛\mathcal{D}_{new}=\mathcal{D}\cup\mathcal{D}_{gen}.
Finally, a model-free offline RL algorithm is trained upon𝒟n​e​wsubscript𝒟𝑛𝑒𝑤\mathcal{D}_{new}.δ=|𝒟g​e​n|/|𝒟|𝛿subscript𝒟𝑔𝑒𝑛𝒟\delta=|\mathcal{D}_{gen}|/|\mathcal{D}|is used as the augmentation ratio.

To perform trajectory augmentation, we need to implement an environment simulator which models the state transition distribution and the reward function.
However, existing model-based methods fail to generate long-horizon and high-rewarded trajectories, as shown in Figure1(b), leading to trivial improvement.
Inspired by recent work using the Transformer architecture by casting the RL problem as conditional sequence modeling tasks(Chen et al.,2021), we propose to model the environment dynamics through Transformers, aka World Transformers. It has been shown that Transformers can effectively learn from long input sequences and produce satisfying outputs.

Model Inputs.The key factors to construct the environment model inputs are: (1) the model input should contain all the factors determining the environment state transitions; (2) the model should enable Transformers to learn from continuous sequential interactions; (3) the model input should be as simple as possible to avoid unnecessary noise. Considering that in online RL the environment provides feedback simply based on the agent’s action and the current state, the model input for World Transformers contains multiple continuous state and action tuples(s,a)𝑠𝑎(s,a). Formally, we define the model input as

Both the State Transformer and Reward Transformer take the above model inputs for environment simulation.

Environment Modeling.We use the State Transformer and the Reward Transformer to model the next state transition and the reward function, respectively.
Particularly, we introduce a learnable embedding matrix to represent the interaction stept𝑡t, which is similar to the positional embedding in language modeling. Noticing that GPT-series models have achieved remarkable success across various NLP tasks(Radford et al.,2018; Brown et al.,2020; Radford et al.,2019), we deployed two GPT models to model the states transition and reward function. At timestept𝑡t, the State Transformer and Reward Transformer model the following distributions:

where{s,a}t−L+1tsuperscriptsubscript𝑠𝑎𝑡𝐿1𝑡{\{{s},{a}\}}_{t-L+1}^{t}represents the model input sequence{st−L+1,at−L+1,st−L+2,at−L+2,…,st,at}subscript𝑠𝑡𝐿1subscript𝑎𝑡𝐿1subscript𝑠𝑡𝐿2subscript𝑎𝑡𝐿2…subscript𝑠𝑡subscript𝑎𝑡\{{s}_{t-L+1},{a}_{t-L+1},{s}_{t-L+2},{a}_{t-L+2},...,{s}_{t},{a}_{t}\}as shown in Eq.(1).Ls,Lrsubscript𝐿𝑠subscript𝐿𝑟L_{s},L_{r}represent the hyperparameters that define the length of the model inputs forℳs,ℳrsubscriptℳ𝑠subscriptℳ𝑟\mathcal{M}_{s},\mathcal{M}_{r}respectively.
We also tried to model both distributions in one unified Transformer, but we did not observe a better performance.

Bothℳssubscriptℳ𝑠\mathcal{M}_{s}andℳrsubscriptℳ𝑟\mathcal{M}_{r}are trained in a supervised manner upon the offline dataset. The prediction output at the position ofatsubscript𝑎𝑡{a}_{t}is trained to predicts^t+1subscript^𝑠𝑡1\hat{s}_{t+1}orr^tsubscript^𝑟𝑡\hat{r}_{t}inℳssubscriptℳ𝑠\mathcal{M}_{s}orℳrsubscriptℳ𝑟\mathcal{M}_{r}, respectively. We use mean-squared error for both the next state prediction and reward prediction. World Transformers are effective in conducting long sequence modeling and thus can be used to generate long-horizon simulation.

We now discuss how to generate trajectories with World Transformers. The key idea in trajectory generation is to expand the observed state-action tuples, thereby improving the performance of offline RL. Motivated by this, we introduce uniform noise to the actionatsubscript𝑎𝑡a_{t}in the original trajectory, forming a perturbed actionat∗subscriptsuperscript𝑎𝑡a^{*}_{t}. In particular, we add a random number of range(−ϵ,ϵ)italic-ϵitalic-ϵ(-\epsilon,\epsilon)to each dimension ofatsubscript𝑎𝑡a_{t}. Then we predict the corresponding next statest+1∗subscriptsuperscript𝑠𝑡1s^{*}_{t+1}and rewardrt∗subscriptsuperscript𝑟𝑡r^{*}_{t}with the World Transformer. The alteration of action results in changes to the next state, therefore a new tuple{at∗,rt∗,st+1∗}subscriptsuperscript𝑎𝑡subscriptsuperscript𝑟𝑡subscriptsuperscript𝑠𝑡1\{a^{*}_{t},r^{*}_{t},s^{*}_{t+1}\}is generated in a new trajectory. We repeat this process of introducing noise to the original action and predicting the next state and reward from timestept=ts𝑡subscript𝑡𝑠t=t_{s}tot=ts+h−1𝑡subscript𝑡𝑠ℎ1t=t_{s}+h-1, wherehℎhis a hyperparameter that defines the length of simulation. The whole process is visualized in Figure2. The generated trajectory effectively expands the observed state-action space. Besides, the generated trajectory is still continuous, which enables World Transformers to learn autoregressively from previous interaction steps, thereby avoiding overly erroneous estimations for long-horizon simulation.
The perturbation of offline data also helps OTTO to utilize the prior knowledge of offline data, thus generating high-rewarded trajectories.

Generating for all the timesteps in the original offline dataset is not appropriate because (1) this may also introduce extra noise to the offline dataset and (2) not every tuple{st,at,rt}subscript𝑠𝑡subscript𝑎𝑡subscript𝑟𝑡\{s_{t},a_{t},r_{t}\}in a trajectory is worthy of augmenting. We prefer to generate trajectories with high total rewards, which are good demonstrations for policy improvement. In this paper, we propose four trajectory generation strategies:Random :For each trajectoryτ={st,at,rt}t=0|τ|𝜏subscriptsuperscriptsubscript𝑠𝑡subscript𝑎𝑡subscript𝑟𝑡𝜏𝑡0\tau=\{s_{t},a_{t},r_{t}\}^{|\tau|}_{t=0}in offline dataset, we randomly select the start timestepts∈[0,|τ|−h+1]subscript𝑡𝑠0𝜏ℎ1t_{s}\in[0,|\tau|-h+1], and generate trajectoryτ∗superscript𝜏\tau^{*}based on the trajectory segment{st,at,rt}t=tsts+h−1subscriptsuperscriptsubscript𝑠𝑡subscript𝑎𝑡subscript𝑟𝑡subscript𝑡𝑠ℎ1𝑡subscript𝑡𝑠\{s_{t},a_{t},r_{t}\}^{t_{s}+h-1}_{t=t_{s}}.Top-K𝐾K:We first split all original trajectories into segments of lengthhℎh. Then we sort these segments by their cumulative rewards in descending order and generate trajectories based on the top-K𝐾Ksegments, whereK=|𝒟g​e​n|/h𝐾subscript𝒟𝑔𝑒𝑛ℎK=|\mathcal{D}_{gen}|/h.Softmax :We also split the trajectory segments as the Top-K𝐾Kstrategy, but we choose the segments according to their probabilities calculated by a softmax function over the cumulative rewards of the segments.BeamSearch :Different with Top-K𝐾Kand Softmax strategies which perturb each segment one time, the BeamSearch strategy perturbs each segment multiple times and selects the trajectories with highest rewards as the augmented data. The details are summarized in AlgorithmLABEL:alg:beamsearchin the appendix.

Finally, a model-free offline RL algorithm, e.g., DT(Chen et al.,2021), is trained upon both the original dataset and the augmented dataset to learn a policy.

Offline RL is the task of learning policies from a static dataset of pre-collected trajectories. It has been applied to various domains including robotics(Kumar et al.,2021; Kalashnikov et al.,2018), healthcare(Wang et al.,2018; Tang & Wiens,2021; Tang et al.,2022)and NLP(Snell et al.,2023; Verma et al.,2022; Wu & Hu,2018). Existing offline RL methods can be categorized into model-free methods and model-based methods.

Model-free methods.As the learned policy might differ from the behavior policy, offline RL encounters the distributional shift issue. Most approaches for model-free offline RL are based on restricting the target policy, including constraining the learned policy to near the behavior policy(Fujimoto et al.,2019; Kostrikov et al.,2021b; Fujimoto & Gu,2021; Wu et al.,2019; Siegel et al.,2020), incorporating value pessimism(Kumar et al.,2020; Xie et al.,2021; Kostrikov et al.,2021a), importance sampling(Liu et al.,2019; Nachum et al.,2019; Sutton et al.,2016), and uncertainty punishment for value functions(Sinha et al.,2022; An et al.,2021; Kumar et al.,2019).
However, these methods only use the offline dataset, which prevents them from broader generalization. On the contrary, we use both the
offline data as well as the high-qualified augmentation data, which can further enhance the performance of these existing methods.

Recently,Chen et al. (2021); Janner et al. (2021)proposed to cast offline RL into a sequence modeling problem and use Transformers to solve it in a supervised manner.
Their methods directly output the action but are limited to the support of offline data. However, the proposed OTTO aims to model the environment through World Transformers, leading to better generalization.

Model-based methods.Model-based offline RL methods(Yu et al.,2020b; Kidambi et al.,2020; Yu et al.,2021; Rigter et al.,2022; Zhan et al.,2022; Swazinna et al.,2021; Rafailov et al.,2020; Matsushima et al.,2020)provide an alternative approach that learns a model of environment and generate synthetic rollouts from the model to optimize the policy.
Model-based methods provide the potential for broader generalization. However, we found the trajectories generated by these methods are extremely short-horizon (with simulation lengthh=5ℎ5h=5or evenh=1ℎ1h=1), resulting in limited generalization improvement.
On the contrary, OTTO can generate long-horizon trajectories with high rewards by World Transformers, which provides more potential for better generalization.Micheli et al. (2022)also proposed to use Transformers to model the environment. However, their methods are not tailored for offline RL.
Besides, OTTO can serve as a plug-in module that can be integrated with existing model-free methods to provide further improvement.

In this section, we conduct experiments to answer the following research questions: (1) Can OTTO effectively improve the performance of existing model-free offline RL methods? (2) How does OTTO perform compared to state-of-the-art model-based offline RL methods? (3) How does the design of OTTO affect the performance?

To answer question (1), we integrate OTTO with
several representative model-free offline RL approaches and compare the agent performance with or without OTTO. These model-free offline RL approaches include DT(Chen et al.,2021), CQL(Kumar et al.,2020)and BCQ(Fujimoto et al.,2019).
DT employs Transformers to cast RL as a sequence modeling task. The CQL and BCQ are both time-difference (TD) learning algorithms and CQL is regarded as one of the state-of-the-art model-free approaches. To answer question (2), we choose COMBO(Yu et al.,2021), MOPO(Yu et al.,2020b)and MOReL(Kidambi et al.,2020)as our baselines. These model-based methods also aim to use a model to simulate the environment and generate short-horizon rollouts to improve the policy learning, among which COMBO achieved the best performance.

We evaluate all the methods on continuous control tasks in OpenAI Gym MuJoCo environments(Brockman et al.,2016)from the D4RL benchmark(Fu et al.,2020), which contains three environments (halfcheetah, hopper, and walker2d) and four datasets for each environment (expert, medium, medium-replay, and medium-expert).

[图片: images\image_5.png]
图片说明: Figure 3:The average immediate reward of each interaction step in simulated trajectories withh=50ℎ50h=50. Random, Top-K𝐾K, Softmax, and BeamSearch correspond to four strategies of OTTO. MOPO refers to the trajectories generated by MOPO(Yu et al.,2020b).

[图片: images\image_6.png]
图片说明: Figure 4:The average immediate reward of each interaction step in the trajectories generated by OTTO. The dashed line represents predicted immediate reward by using the World Transformers. The solid line represents real immediate reward by calculating in online environments.

[图片: images\image_7.png]
图片说明: Figure 4:The average immediate reward of each interaction step in the trajectories generated by OTTO. The dashed line represents predicted immediate reward by using the World Transformers. The solid line represents real immediate reward by calculating in online environments.

[图片: images\image_8.png]
图片说明: Figure 4:The average immediate reward of each interaction step in the trajectories generated by OTTO. The dashed line represents predicted immediate reward by using the World Transformers. The solid line represents real immediate reward by calculating in online environments.

We implement three representative model-free methods and train them on both the original dataset𝒟𝒟\mathcal{D}and𝒟n​e​wsubscript𝒟𝑛𝑒𝑤\mathcal{D}_{new}augmented by OTTO. TableLABEL:tb:model_freeshows the summary of our experimental results. Firstly, we can see that with the augmented trajectories from OTTO, all algorithms achieve better scores than their original scores in 30 out of total 36 settings, and achieve comparable results in remaining settings, suggesting that OTTO is robust to different datasets and can generalize to multiple model-free algorithms. Secondly, DT is the most suitable algorithm for OTTO among the three algorithms as it outperforms Ori in all settings, especially inhopper_medium-replay(+17.2 scores). We guess the reason could be that DT can hone in on high-rewarded actions(Chen et al.,2021), resulting in better utilization of the generated trajectories. Thirdly, for the other two TD-learning algorithms, OTTO also achieves remarkable performance improvement (2.2 average improvement for CQL and 2.8 average improvement for BCQ), suggesting that OTTO is robust to different model-free offline RL algorithms. Finally, we find that OTTO achieves significant improvement inhopperenvironment, while its performance is not as remarkable inhalfcheetahenvironment. We attribute this performance gap to the differences between environments, more details are shown in the error analysis of section5.3.

To conclude, the proposed OTTO can help to further improve the performance of existing model-free methods.

In this section, we compare OTTO and existing model-based methods. We integrate OTTO with CQL as our methods. For baselines,
COMBO(Yu et al.,2021)improved the Q-function used in CQL and achieved current state-of-the-art results in a wide range of tasks. We also include MOPO and MORel as baselines.
TableLABEL:tb:model_basedshows the comparison results.
We can see that CQL+OTTO significantly outperforms COMBO in all datasets ofhopperandwalker2denvironments and achieves a higher average score (90.5 vs 82.0). While COMBO outperforms CQL+OTTO inhalfcheetahenvironments, we discuss the possible reasons in the error analysis of section5.3.

Besides, we compare the quality between the trajectories generated by our method and MOPO as shown in Figure3.
Particularly, we calculate the real reward for each state-action tuple(st,at)subscript𝑠𝑡subscript𝑎𝑡(s_{t},a_{t})at timestept𝑡tin the trajectories generated by MOPO and OTTO in an online environment. We also calculate the average reward in the offline dataset as a reference. We can see that the immediate reward of MOPO initially hovers around the mean score during the initial five timesteps but suffers a rapid decline as the timesteps progress. The trajectories generated by MOPO are quite low-rewarded compared to the mean of offline data, which leads to trivial improvement for policy learning. On the contrary, all the four strategies in OTTO generate high-rewarded long-horizon trajectories. The rewards for all four strategies remain relatively stable in all timesteps. Three of them consistently remain above the offline mean score, while the remaining one hovers around the mean score.

To conclude, OTTO can generate high-qualified trajectories and outperforms existing model-based methods.

In this section, we evaluate the four trajectory generation strategies using DT as the following model-free RL method. TableLABEL:tb:four_strategysummarizes the experimental results. To ensure a fair comparison, we setδ𝛿\deltato be the same for all strategies (δ=1/8𝛿18\delta=1/8for the medium-replay dataset andδ=1/10𝛿110\delta=1/10for other datasets). Firstly, we can see that all the strategies can outperform their original performance, which suggests that all four strategies can enhance the performance of existing offline RL algorithms.
Secondly, we found the overall performance of Top-K𝐾Kis similar to Softmax. Top-K𝐾Kperforms slightly better in datasets with high-rewarded trajectories (medium-expert and expert), while Softmax performs better in other datasets (medium-replay and medium).
Besides, we find the Random strategy performs best in expert and medium datasets.
As noted inYu et al. (2020b); Rafailov et al. (2020), medium-replay and medium-expert are collected with a wide range of policies while expert and medium datasets are collected with fewer policies and have narrow state-action distributions.
In such a case, the Random strategy can more effectively enlarge the observed state-action space through perturbation on randomly selected actions, thus leading to better performance.

To investigate the quality of generated trajectories, we aim to address the following two questions: (i) can OTTO generate high-rewarded trajectories by using manually designed trajectory generation strategies; (ii) how is the estimated error of reward prediction? To answer these questions, we calculate the immediate reward of each interaction step in the trajectories generated by OTTO as shown in Figure4. We can see that for both predicted rewards and accurate rewards, BeamSearch strategy generates the highest-rewarded trajectories in all settings, Softmax and Top-K𝐾Kstrategies perform comparably in the middle, while the Random strategy generates the lowest-rewarded trajectory in most settings. This suggests that three manually designed trajectory generation strategies can effectively generate high-rewarded trajectories by selecting the specific segments. However, we can see that these manually designed trajectory generation strategies also lead to a bigger gap between the real reward and the predicted reward, which affects policy performance as well.
In datasets with narrow state-action distribution such as the expert dataset, the estimated error of immediate reward is relatively higher.
As a result, the Random strategy performs better in expert dataset.
While in datasets with a wide range of policies like medium-replay, we can see BeamSearch can generate high-rewarded trajectories with little estimated error, resulting in better performance as shown in TableLABEL:tb:four_strategy. These observations suggest that we should navigate a trade-off between low-error, low-reward strategy (Random) and high-error, high-reward strategies (Top-K𝐾K, Softmax, BeamSearch) based on the specific dataset.

TableLABEL:tb:deltashows the impact of the augmentation ratio.
Firstly, we can see that for all augmentation ratios varying fromδ=1/8𝛿18\delta=1/8toδ=2𝛿2\delta=2, OTTO outperforms its original performance. This suggests that DT+OTTO is robust to theδ𝛿\delta.
Secondly, for all environments, we observed that as the augmentation ratio increases, the performance of OTTO improves because the mixed dataset with more augmented data is able to provide more high-rewarded trajectories to enlarge the observed states and actions.
But the performance will drop whenδ𝛿\deltais too large. The reason is that too largeδ𝛿\deltaintroduces more noise.

[图片: images\image_9.png]
图片说明: Figure 5:Comparison of the average loss of state estimation and reward estimation between three environments.

In experiments, we found that OTTO can significantly improve the performance inhopperandwalker2denvironments, but only perform comparably inhalfcheetah.
To further analyze this performance gap, we compare the estimated error of environment modeling between these three environments as shown in Figure5.
Particularly, we define the loss as mean absolute deviation.
We can see that the loss of next state estimation and reward estimation inhalfcheetahare both higher than that in the other two environments. Especially the standard deviation of reward loss is extremely high.
To conclude, the environment simulation inhalfcheetahis not as accurate as the other two environments, resulting in a smaller improvement of OTTO.
Such observation indicates that the accuracy of the environment model is essential for model-based methods.

In this work, we have presented offline trajectory generalization through world transformers for offline reinforcement learning (OTTO). In particular, we have trained the World Transformers to simulate the environment and proposed four trajectory generation strategies to generate high-qualified trajectories based on World Transformers.
The RL agent is trained upon both the original data and augmented trajectories from World Transformers.
Extensive experiments on D4RL benchmarks show that OTTO improves the performance of representative model-free offline RL algorithms and outperforms strong model-based baselines. Despite the advantages of OTTO, there are a few challenges left such as the relatively high estimation error of World Transformers in complex environments. We leave them for future studies. Additionally, noticing that large language models (LLMs) have achieved promising results in a variety of tasks, we also want to investigate the prospective utilization of LLMs as environmental models in future research.

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

We summarized the details of BeamSearch strategy as shown in AlgorithmLABEL:alg:beamsearch.

[图片: images\image_10.png]
图片说明: Figure 6:The average L1 loss of next state estimation and reward estimation. We do experiments on walker2d-expert and walker2d-medium-replay dataset and use the BeamSearch strategy.

[图片: images\image_11.png]
图片说明: Figure 6:The average L1 loss of next state estimation and reward estimation. We do experiments on walker2d-expert and walker2d-medium-replay dataset and use the BeamSearch strategy.

[图片: images\image_12.png]
图片说明: Figure 6:The average L1 loss of next state estimation and reward estimation. We do experiments on walker2d-expert and walker2d-medium-replay dataset and use the BeamSearch strategy.

[图片: images\image_13.png]
图片说明: Figure 6:The average L1 loss of next state estimation and reward estimation. We do experiments on walker2d-expert and walker2d-medium-replay dataset and use the BeamSearch strategy.

We consider the range of noiseϵitalic-ϵ\epsilonwe introduce to the action as an important parameter in OTTO, as it determines the scope of searching for optimal actions. In this section, we evaluate the hyperparameter searches for the range of noiseϵitalic-ϵ\epsilonon DT as shown in TableLABEL:tb:epsilon.

We can see that when we introduce a wide range of noise to the actions (ϵ=0.5italic-ϵ0.5\epsilon=0.5), OTTO performs better in medium-replay dataset. When the perturbation is small, we found a better performance in expert dataset. Ideally, a larger perturbation range leads to a broader generalization. However, this will also lead to more inaccurate estimations. We calculate the average L1 loss of next state estimation and reward estimation for each interaction step as shown in Figure6. We can see that in expert dataset, either reward L1 loss or next state L1 loss is relatively high whenϵ=0.5italic-ϵ0.5\epsilon=0.5compared toϵ=0.02italic-ϵ0.02\epsilon=0.02andϵ=0.1italic-ϵ0.1\epsilon=0.1. While in medium-replay dataset, there is no significant difference in both reward L1 loss and next state L1 loss between three values ofϵitalic-ϵ\epsilon. This suggests that we should choose the appropriate value ofϵitalic-ϵ\epsilonbased on the specific dataset.

In this section, we discuss the hyperparameters that we use for OTTO. Similar toChen et al. (2021), the World Transformers is based on the implementation of GPT-2(Radford et al.,2019). TableLABEL:tb:hyperlists the hyperparameters we use to train the World Transformers. We use the same hyperparameters for both the State Transformer and Reward Transformer.

We now list the additional hyperparameters of trajectory generation as follows.

Trajectory lengthhℎh.Unlike existing model-based offline RL only perform short-horizon rollouts withh≤5ℎ5h\leq 5, OTTO performs long-horizon trajectories withh=50ℎ50h=50. However, we found whenl𝑙lis too large, it will also lead to inaccurate estimation.

Augmentation ratioδ𝛿\delta.For DT, we simply chooseδ=0.1𝛿0.1\delta=0.1in medium, medium-expert and expert dataset, as we didn’t find significant improvement whenδ𝛿\deltaincreases. We find the total timesteps in medium-replay dataset are relatively low, so we chooseδ=0.25𝛿0.25\delta=0.25to augment more trajectories. We findδ𝛿\deltais sensitive to other offline RL methods which are based on restricting the learning policy such as CQL and BCQ. When theδ𝛿\deltais too large, these methods have either poor performance or large variance. Therefore, we chooseδ∈{0.05,0.1}𝛿0.050.1\delta\in\{0.05,0.1\}to avoid too much noise.

Range of noiseϵitalic-ϵ\epsilon.We selectϵitalic-ϵ\epsilonfrom the set{0.02,0.1,0.5}0.020.10.5\{0.02,0.1,0.5\}, which correspond to low perturbation range, medium perturbation range and high perturbation range. In the D4RL experiments, we found thatϵ=0.02italic-ϵ0.02\epsilon=0.02works well for the medium and expert dataset, which have narrow state-action distributions. And we chooseϵ=0.1italic-ϵ0.1\epsilon=0.1for medium-expert dataset andϵ=0.5italic-ϵ0.5\epsilon=0.5for medium-replay dataset, which is collected by a wide range of policies.

BeamSearch widthB𝐵B.We simply chooseB=10𝐵10B=10for all settings. This takes into account both the efficiency of generating trajectories and the high qualification of generated trajectories.

[图片: images\image_14.png]

[图片: images\image_15.png]

