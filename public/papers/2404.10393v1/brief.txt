离线轨迹泛化：强化离线强化学习的能力

离线强化学习（Offline RL）旨在从静态数据集中的预先收集的轨迹学习策略。然而，现有的离线RL方法通常存在两大问题：一是对未见状态的泛化能力差，二是通过模型生成的模拟数据对策略改进贡献有限。为了解决这些问题，本文提出了一种基于世界转换器（World Transformers）的离线轨迹泛化方法（OTTO）。OTTO通过利用世界转换器预测状态动态和即时奖励，结合四种策略生成高奖励的轨迹模拟，最终通过结合离线数据和模拟数据来训练离线强化学习算法。

OTTO通过模拟轨迹的生成来解决模型生成数据质量低、无法有效改进策略的问题。传统的离线RL方法，尤其是基于模型的方法，在生成长时间跨度的轨迹时，通常会遇到奖励逐渐降低的问题，这导致策略学习效果显著下降。而OTTO通过世界转换器在生成长轨迹时引入噪声扰动，确保了生成的轨迹不仅具有高奖励，而且能够显著提升模型的泛化能力。

OTTO作为一个插件模块，能够与现有的离线RL方法结合使用，增强策略学习的效果。通过在D4RL基准数据集上进行的广泛实验，OTTO显示出显著优于当前最先进的离线RL方法，能够有效提升策略的泛化能力。

该方法的核心创新在于引入了Transformer架构，尤其是其在自然语言处理和计算机视觉中的成功应用，来模拟环境动态并生成高质量的模拟轨迹。世界转换器的应用使得离线RL能够更好地应对从未见过的状态，同时避免了传统方法中因过度保守而造成的策略性能下降问题。

OTTO不仅解决了传统方法中由于低质量模拟数据造成的训练瓶颈，还为未来的离线强化学习模型提供了新的思路，进一步推动了该领域的研究进展。
