æ ‡é¢˜ï¼šOffline Robot Reinforcement Learning with Uncertainty-Guided Human Expert Sampling

Recent advances in batch (offline) reinforcement learning have shown promising results in learning from available offline data and proved offline reinforcement learning to be an essential toolkit in learning control policies in a model-free setting. An offline reinforcement learning algorithm applied to a dataset collected by a suboptimal non-learning-based algorithm can result in a policy that outperforms the behavior agent used to collect the data. Such a scenario is frequent in robotics, where existing automation is collecting operational data. Although offline learning techniques can learn from data generated by a sub-optimal behavior agent, there is still an opportunity to improve the sample complexity of existing offline reinforcement learning algorithms by strategically introducing human demonstration data into the training process. To this end, we propose a novel approach that uses uncertainty estimation to trigger the injection of human demonstration data and guide policy training towards optimal behavior while reducing overall sample complexity. Our experiments show that this approach is more sample efficient when compared to a naive way of combining expert data with data collected from a sub-optimal agent. We augmented an existing offline reinforcement learning algorithm Conservative Q-Learning with our approach and performed experiments on data collected from MuJoCo and OffWorld Gym learning environments.

The field ofoffline reinforcement learninghas emerged in the past few years as a way of training reinforcement learning (RL) agents from previously collected experiences and without the need for an interactive feedback loop with the environment[20]. Since the first breakthrough in the field of deep reinforcement learning[24], this field has seen considerable progress over the years, achieving super-human performance in computer games[23,32,31,35], solving robotics control problem[2,19,11], improving recommendation systems[4,13], healthcare[12], autonomous driving[25], and process optimization[27]. However, one of the limitations of online reinforcement learning is that it relies on interaction with a dynamical system or environment, and such online interactions can be expensive, especially in domains such as real-world robotics. Similar to supervised learning, where an algorithm trains a model by performing batch model updates, offline reinforcement learning performs model updates by sampling batches from a dataset of state, action, reward and next state (SARS) tuples. The model is then trained towards the standard reinforcement learning objective of maximizing expected future discounted reward, but also with a secondary objective of either keeping the learned policy close to the distribution of thebehavior policythat was used to collect the data[9,16].

We propose a method that allows to reduce the overall offline learning sample complexity by tracking model uncertainty measured from an ensemble of Q-networks. High level of uncertainty shows that the agent has entered an unexplored part of the state-action space indicating that introducing human demonstrations at this moment would have the most impact on learning. The algorithm measures the uncertainty and strategically introduces human demonstration data in-between the batches sampled from sub-optima non-expert data. Through our experiments in a simulated MuJoCo[34]and OffWorld Gym[15]environments, we demonstrate that our approach significantly reduces sample complexity compared to a model trained on a naively mixed dataset. The proposed method is directly transferable to the physical world without any additional assumptions on the properties of the environment or the learning algorithm.

Our work is most effective when applied with an algorithm that addresses some of the inherent issues of offline reinforcement learning like extrapolation error, q-value overestimation, and others. Extrapolation error is caused when there is a mismatch between the distribution of the dataset collected by a behavior agent and the state-action visitation of the policy being trained. Fujimoto et al.[9]introduced an algorithm called Batch-Constrained Q-Learning where they propose a solution to reduce the extrapolation error in reinforcement learning by training a policy such that it is constrained to stay close to the behavior policy. Techniques like BEAR-QL[16]prevent overestimation of q-value by bootstrapping error reduction, while CQL, and algorithm introduced by Kumar et al.[17], introduced a lower bound on the q-value to prevent over-estimation.

This work assumes that this algorithm is applied on a static dataset. To the best of our knowledge, the current state of the art algorithms like CQL and Implicit Q-Learning (IQL)[14]have been tested on static datasets only and there is no research work that demonstrates the application of offline reinforcement learning techniques on dynamic datasets.

Over the recent years, significant contributions have been made to the field of offline reinforcement learning towards solving such inherent challenges with the technique as deadly-triad issue[33], the issue with q-value function overestimation for out-of-distribution (OOD) state-action pairs[9], overfitting and underfitting issues[6], to name a few. To solve the problem of q-value function overestimation for OOD state-action pair, algorithms like conservative Q-learning (CQL) implement a learning objective that favors conservative estimates of the q-value function. The deadly-triad problem was solved in various works, viz. Fujimoto et al.[9], Lillicrap et al.[22]and Haarnoja et al.[11]. The formulation of overfitting and underfitting has been established in work by Kumar et al.[18], where several approaches to overcome these problems have been identified.

Since this work is based on mixing data generated by different behavior agents to create a single dataset, we rely on the results of Schweighofer et al.[28], Fu et al.[7], and Gulcehre et al.[10]that demonstrate the feasibility of training an offline RL model on a mixed dataset that mixes SARS tuples obtained by different behavior policies.

In the online RL setting, uncertainty estimation has been leveraged for efficient exploration[5], and for improving exploration by reducing the uncertainty of learned implicit reward function[21]. In the offline RL setting, the work by An et al.[1]quantifies the uncertainty of Q-value estimates by using an ensemble of Q-networks. However, in this work, uncertainty estimation is leveraged as a penalization term in Q-learning, while in our proposed method uncertainty acts as an indicator of the learning progress. Wu et al.[36]proposed the Uncertainty Weighted Actor-Critic algorithm that down-weights the contribution of OOD state-action pairs in training. This work achieves good performance gains on a dataset with narrow human demonstrations, however, in their work, data mixing is done by mixing the human demonstration data with data generated by imitating the human data. Some recent work[29],[8]have explored sampling strategies in offline RL using rank-based sampling or sampling prioritized experience replay where priority is given to sample transitions with lower epistemic uncertainty. Such methods mayover-samplegood samples.

Reinforcement learningis a machine learning technique through which an agent learns to solve a task by learning from interactions with an environment. In the domain of robotic control and behavior most of the algorithms are based on the Markov Decision Process. A Markov Decision Process or MDP is defined as a tuple comprising of seven elements â€“ (ğ’®ğ’®\mathcal{S},ğ’œğ’œ\mathcal{A},ğ’¯ğ’¯\mathcal{T},r,Î³ğ›¾\gamma,ğ’®0subscriptğ’®0\mathcal{S}_{0},â„‹â„‹\mathcal{H}), whereğ’®ğ’®\mathcal{S}is the state space,ğ’œğ’œ\mathcal{A}is the action space,ğ’¯ğ’¯\mathcal{T}is the state transition probability functionğ’¯ğ’¯\mathcal{T}=ğ’«â€‹(st+1|st,at)ğ’«conditionalsubscriptğ‘ ğ‘¡1subscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡\mathcal{P}({s_{t+1}}|{s_{t}},{a_{t}}),ris the environment reward functionr:ğ’®Ã—ğ’œâ†’â„1â†’ğ’®ğ’œsuperscriptâ„1\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{1},Î³ğ›¾\gammais the discount value,ğ’®0subscriptğ’®0\mathcal{S}_{0}is the start state distribution andâ„‹â„‹\mathcal{H}is the horizon length. In this work, we consider an MDP setting with a non-zero time horizon and discounted cumulative rewards. A reinforcement learning algorithm interacts with an environment to learn a policyÏ€ğœ‹\pi, which maximizes the reinforcement learning objectiveğ’¥â€‹(Ï€)=ğ”¼â€‹[âˆ‘t=0HÎ³tâ€‹rt]ğ’¥ğœ‹ğ”¼delimited-[]superscriptsubscriptğ‘¡0ğ»superscriptğ›¾ğ‘¡subscriptğ‘Ÿğ‘¡\scriptstyle\mathcal{J}(\pi)=\mathbb{E}[\sum\limits_{t=0}^{H}\gamma^{t}r_{t}]. The two important functions that help estimate the value of a state or a state and action pair are thevalue functionğ’±Ï€â€‹(s)=ğ”¼â€‹[âˆ‘t=0HÎ³tâ€‹rt|s0=s]superscriptğ’±ğœ‹ğ‘ ğ”¼delimited-[]conditionalsuperscriptsubscriptğ‘¡0ğ»superscriptğ›¾ğ‘¡subscriptğ‘Ÿğ‘¡subscriptğ‘ 0ğ‘ \scriptstyle\mathcal{V}^{\pi}(s)=\mathbb{E}[\sum\limits_{t=0}^{H}\gamma^{t}r_{t}|s_{0}=s]and theQ-functionğ’¬Ï€â€‹(s,a)=ğ”¼â€‹[âˆ‘t=0HÎ³tâ€‹rt|s0=s,a0=a]superscriptğ’¬ğœ‹ğ‘ ğ‘ğ”¼delimited-[]formulae-sequenceconditionalsuperscriptsubscriptğ‘¡0ğ»superscriptğ›¾ğ‘¡subscriptğ‘Ÿğ‘¡subscriptğ‘ 0ğ‘ subscriptğ‘0ğ‘\scriptstyle\mathcal{Q}^{\pi}(s,a)=\mathbb{E}[\sum\limits_{t=0}^{H}\gamma^{t}r_{t}|s_{0}=s,a_{0}=a]. There are two broad categories of online reinforcement learning algorithms, viz.on-policyandoff-policy. In an on-policy setting, the algorithm generates an action for a particular state based on the current policy. The recorded trajectory data is then used to update the current policy. Whereas in an off-policy setting there is a concept of a replay bufferğ’Ÿğ’Ÿ\mathcal{D}, that accumulates data from different policies and the policy updates are made by sampling from the replay buffer instead of generating actions from the current policy. The standard Q-learning objective is to the minimize the Bellman error which is defined asâ„’â€‹(Î¸)=(ğ’¬Î¸â€‹(st,at)âˆ’(rt+Î³â€‹maxaâ¡ğ’¬â€‹(st+1,a)))â„’ğœƒsubscriptğ’¬ğœƒsubscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡subscriptğ‘Ÿğ‘¡ğ›¾subscriptğ‘ğ’¬subscriptğ‘ ğ‘¡1ğ‘\scriptstyle\mathcal{L}(\theta)=(\mathcal{Q}_{\theta}(s_{t},a_{t})-(r_{t}+\gamma\max_{a}\mathcal{Q}(s_{t+1},a)))[33].

In offline reinforcement learning,ğ’Ÿğ’Ÿ\mathcal{D}represents the entire replay dataset and in Q-learning setting the objective is to minimize the following loss function based on Bellman error:

whereÎ¸â€²superscriptğœƒâ€²\theta^{\prime}are the parameters of the target Q-network, softly updated for algorithm stability, whileÎ¸ğœƒ\thetaare the parameters of the running Q-network. In addition to the general reinforcement learning objective or the Q-learning objective, the algorithms usually have a secondary objective of keeping the learned policy close to the state-action distribution of the behavior policy. This is done to avoid overestimation of the Q-value, which happens when the Q-estimate error propagates during the bootstrapping, as shown in Equation1. Over-estimation of Q-values can destabilize the training.

Sub-optimal agent (SOA)is represented by a behavior policy trained by QR-DQN learning algorithm to approximately 60% of optimal performance. While we assume that a human is an optimal (or close to) agent, the SOA represents a data source that has lower performance guarantees, but is easier to obtain.

The problem we are addressing in this work is the reduction of sample complexity of existing offline RL algorithms. In robotics, solving a task with a minimal number of samples is a goal most practitioners would appreciate. In many cases, there is either an absence of an optimal approach or a human is an expert, but creating a human demonstration dataset is prohibitively expensive. Combining data from expert and non-expert demonstrations into amixed datasetcan significantly reduce the overall sample complexity[28], however in many practical applications the amount of human demonstrations necessary to reach close to optimal behavior remains prohibitively high.

In this work, we propose a method to strategically choose the order in which we pick samples from two replay buffers, one with data collected by a sub-optimal agent (ğ’ŸSOAsubscriptğ’ŸSOA\mathcal{D}_{\text{SOA}}) and another with data collected from human demonstrations (ğ’ŸHsubscriptğ’ŸH\mathcal{D}_{\text{H}}). Both are available ahead of time. This allows us to boost the speed of learning twice and reduce the overall sample complexity by 5 times, minimizing the required total sample size of both replay buffers required to reach close-to-optimal performance.

Offline RL algorithms learn a policy by performing policy updates on data sampled from an existing dataset. A dataset consists of trajectories collected by performing rollouts in an environment using a behavior policyÎ¼ğœ‡\mu. A dataset is usually of the form:[(s01,a01,r01,â€¦,sâ„‹1,aâ„‹1,râ„‹1),â€¦,(s01,a01,r01,â€¦,sâ„‹N,aâ„‹N,râ„‹N)]superscriptsubscriptğ‘ 01superscriptsubscriptğ‘01superscriptsubscriptğ‘Ÿ01â€¦superscriptsubscriptğ‘ â„‹1superscriptsubscriptğ‘â„‹1superscriptsubscriptğ‘Ÿâ„‹1â€¦superscriptsubscriptğ‘ 01superscriptsubscriptğ‘01superscriptsubscriptğ‘Ÿ01â€¦superscriptsubscriptğ‘ â„‹ğ‘superscriptsubscriptğ‘â„‹ğ‘superscriptsubscriptğ‘Ÿâ„‹ğ‘[(s_{0}^{1},a_{0}^{1},r_{0}^{1},\ldots,s_{\mathcal{H}}^{1},a_{\mathcal{H}}^{1},r_{\mathcal{H}}^{1}),\ldots,(s_{0}^{1},a_{0}^{1},r_{0}^{1},\ldots,s_{\mathcal{H}}^{N},a_{\mathcal{H}}^{N},r_{\mathcal{H}}^{N})], whereNğ‘Nis the total number of episodes. A sample-efficient algorithm requires a smaller dataset than an algorithm with high sample complexity. Sample complexity bounds are usually computed based on MDP attributes such as the horizon lengthâ„‹â„‹\mathcal{H}, the state spaceğ’®ğ’®\mathcal{S}, the action spaceğ’œğ’œ\mathcal{A}, etc. We will make the single policy concentrability assumption about the behavior policy as defined in[26,37].

Thesingle policy concentrabilityassumption is defined as follows: given a reference policyÎ¼ğœ‡\muand an optimal policyÏ€âˆ—superscriptğœ‹\pi^{*},Î¼ğœ‡\muis said to satisfy the assumption when

for some deterministic optimal policyÏ€âˆ—superscriptğœ‹\pi^{*}and coefficientCâˆ—superscriptğ¶C^{*}, wheredis the state-action distribution of a policy. Intuitively the assumption requires there is a constantCâˆ—superscriptğ¶C^{*}(concentrability coefficient) such that for every possible state-action pair the ratio of probabilities of said state-action being indÏ€superscriptğ‘‘ğœ‹d^{\pi}anddÎ¼superscriptğ‘‘ğœ‡d^{\mu}is not higher than the constantCâˆ—superscriptğ¶C^{*}.

The concentrability coefficientCâˆ—âˆˆ[1,âˆ)superscriptğ¶1C^{*}\in[1,\infty)is the smallest value that satisfies Equation2. When the reference policy is exactly equal to the optimal policyÎ¼=Ï€âˆ—ğœ‡superscriptğœ‹\mu=\pi^{*}the concentrability coefficientCâˆ—=1superscriptğ¶1C^{*}=1.Câˆ—superscriptğ¶C^{*}estimates the difference between the state-action distribution density under of reference (behavior) policy and the optimal policy. Recent works in the field of sample complexity analysis of offline RL algorithms are based on the pessimism principle for value iterations[30,37]and express the sample complexity of an offline RL algorithm as a function ofğ’®ğ’®\mathcal{S},ğ’œğ’œ\mathcal{A},â„‹â„‹\mathcal{H}andCâˆ—superscriptğ¶C^{*}, where the upper bound on the number of episodes in the datasetğ’Ÿğ’Ÿ\mathcal{D}isOâ€‹(fâ€‹(ğ’®,ğ’œ,â„‹,Câˆ—))ğ‘‚ğ‘“ğ’®ğ’œâ„‹superscriptğ¶O(f(\mathcal{S},\mathcal{A},\mathcal{H},C^{*})). The theoretical foundation of our approach relies on two assumptions. First, the algorithms need to satisfy thesingle policy concentrabilityassumption, so that their sample complexity could be expressed as a function ofğ’®ğ’®\mathcal{S},ğ’œğ’œ\mathcal{A},â„‹â„‹\mathcal{H}andCâˆ—superscriptğ¶C^{*}. Second, we assume that the human demonstrator is an expert and the behavior policyÎ¼Hsubscriptğœ‡ğ»\mu_{H}corresponding to human policy is very close to an optimal policyÏ€âˆ—superscriptğœ‹\pi^{*}such that the following condition is satisfied,

whereâ„³â„³\mathcal{M}is a family of non-expert behavior policies. Intuitively, inequality3states that if an algorithmâ€™s sample complexity can be expressed in terms of concentrability, and the assumption that human expert is close to optimal is satisfied, then the concentrability coefficient is lower when the algorithm is trained on human data.

In online reinforcement learning, uncertainty estimation has been leveraged for effective exploration strategies such as upper confidence bound exploration via Q-ensembles[5]. In an offline reinforcement learning setting, uncertainty estimation has been leveraged to act conservatively, and select paths with low uncertainty[3]. Our approach defines uncertainty in offline reinforcement learning as estimated variance of the Q-estimate of a given(s,a)ğ‘ ğ‘(s,a)pair over an ensemble ofMğ‘€MQ-networks trained on the same data. In an ensemble ofMğ‘€MQ-networks that are trained to minimize the objective defined in Equation1we define point estimators for the meanÎ¼=1Mâ€‹âˆ‘i=1M[QÎ¸iâ€‹(s,a)]ğœ‡1ğ‘€superscriptsubscriptğ‘–1ğ‘€delimited-[]subscriptğ‘„superscriptğœƒğ‘–ğ‘ ğ‘\scriptstyle\mu=\frac{1}{M}\sum_{i=1}^{M}[Q_{\theta^{i}}(s,a)]and varianceÏƒ2=1Mâ€‹âˆ‘i=1M(QÎ¸iâ€‹(s,a)âˆ’Î¼â€‹(s,a))2superscriptğœ21ğ‘€superscriptsubscriptğ‘–1ğ‘€superscriptsubscriptğ‘„superscriptğœƒğ‘–ğ‘ ğ‘ğœ‡ğ‘ ğ‘2\scriptstyle\sigma^{2}=\frac{1}{M}\sum\limits_{i=1}^{M}(Q_{\theta^{i}}(s,a)-\mu(s,a))^{2}of the Q-estimates. The varianceÏƒ2superscriptğœ2\sigma^{2}characterizes the uncertainty in Q-function estimates and reflects the state of the training. At the start of the training, the Q-function estimates are expected to be noisy and have high variance. As the training progresses, the Q-network estimates become more accurate in the parts of the state space that were visited by the learning algorithm as the critic or Q-estimator improves.

Introducing expert demonstrations into the training dataset helps reduce the sample complexity and also helps improve performance when compared to a mixed dataset. Having defined sample complexity analysis and uncertainty estimation in the previous sections, we propose the Uncertainty-Guided Expert Sampling (UGES) algorithm (see Supplementary Algorithm1) that leads to more efficient use of available human data in conjunction with offline data collected from a sub-optimal behavior policy. Above-threshold uncertainty in the model ensemble triggers the algorithm to sample the next SARS tuple from the replay bufferğ’ŸHsubscriptğ’Ÿğ»\mathcal{D}_{H}containing human demonstrations.

Strategic sampling allows to keep the buffer of human demonstrations small while providing the same benefit to the overall learning process as a naive sampling on a larger human dataset would.

We experimentally validate that uncertainty-based sampling strategy allows for a significant reduction in overall sample complexity and explore the benefits of combining the two sources of offline data. We compare two ways of mixing the data during learning: a naive (random) combination strategy is compared against the UGES method in terms of speed of learning, resulting agent performance, overall sample complexity, and amount of human data required to reach close-to-optimal performance.

[å›¾ç‰‡: images\image_1.jpg]
å›¾ç‰‡è¯´æ˜: (a)MuJoCoCheetah

[å›¾ç‰‡: images\image_2.jpg]
å›¾ç‰‡è¯´æ˜: (a)MuJoCoCheetah

[å›¾ç‰‡: images\image_3.jpg]
å›¾ç‰‡è¯´æ˜: (a)MuJoCoCheetah

The experiments were performed using datasets generated from three learning tasks in simulated environments shown on Figure1. In addition to the standard MuJoCo tasksCheetahandAnt(D4RL benchmark datasets[7]), UGES was tested in a simulated version of the OffWorld GymMonolithenvironment that contains a vision-based task: a mobile robot explores the environment and gets rewarded when it reaches a monolith (goal) in the center of the field. A sparse reward of +1 is given when the robot is within a small distance from the goal with no step penalty.

To facilitate a fair comparison, we usethe number of successful trajectoriesas the characteristic of a size of a dataset. A successful trajectory is defined as one that leads to a positive reward (see Supplementary Figure3a for a comparison between training on human demonstrations (expert data) versus on data collected by a sub-optimal agent. In all experiment we maintain the 5:1 ratio of suboptimal to expert data. InCheetahandAntexperiments the dataset was320,000320000320,000successful trajectories from the suboptimal agent and80,0008000080,000expert ones. In theMonolithenvironment the offline data consisted of10,0001000010,000SOA trajectories and2,50025002,500expert human demonstrations. In our experiments, the uncertainty threshold,Ïµitalic-Ïµ\epsilon, was tuned experimentally and the best threshold value was used in all three environments.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: (a)Uncertainty-based sampling leads to more than 2x faster convergence in MuJoCOCheetahenvironment and reaches better overall level of performance.

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: (a)Uncertainty-based sampling leads to more than 2x faster convergence in MuJoCOCheetahenvironment and reaches better overall level of performance.

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: (a)Uncertainty-based sampling leads to more than 2x faster convergence in MuJoCOCheetahenvironment and reaches better overall level of performance.

Across all experiments we can see a significant improvement of data utilization efficiency when sampled are picked based on agentâ€™s uncertainty estimate. Subsequent experiments in theMonolithenvironment that allowed the naive mixing strategy to have access to a larger dataset of human expert trajectories showed that in this environment the naive strategy needed approximately 5x more successful human expert trajectories to follow UGESâ€™ learning curve (see Supplementary Figure3b).

We compare the episodic returns of online evaluations performed on agents trained using UGES versus Conservative Q-Learning (CQL)[17]agents trained on a combined dataset with 80% successful trajectories coming from the SOA agent and 20% being human expert trajectories. The result shown on Figure2demonstrates that already in the early stages of learning the algorithm proposed in this work makes a more efficient use of available samples, learning faster than a naive dataset combination strategy and consistently reaching higher level of performance.

Generating a large dataset for offline RL can could be time-consuming, and algorithms that reduce sample requirements can make a difference. Human demonstrations provide great learning signal for Offline RL, but collecting such data is prohibitively expensive, especially in real-world robotics. These constraints lead us to consider using a combination of two sources of offline data: a limited amount of â€œexpensive" human demonstrations with a dataset of â€œcheap" autonomously collected experiences. In this work we show how uncertainty estimation can guide strategic introduction of the human samples into the learning process leading to significant reduction in overall sample complexity.

The theoretical analysis in Section3.1is based on the assumption that the sample complexity of the CQL algorithm (and other offline RL algorithms that are being used in practice) can be expressed in terms of a set of MDP characteristics and the concentrability coefficient. However, to our knowledge, this assumption has not been explicitly proven for the CQL algorithm.

Due to a wide range of experimental conditions, we relied on data generated by interactions with a simulated environment, while our main aim is to make offline RL sample complexity sufficiently low to facilitate learning in the real physical world. The robotic benchmark learning environment chosen for this maintains a close relationship between its real and simulated versions of the environment. Since our results do not depend on the properties of the environment we believe that the empirical result shows in this work is directly transferable to the physical environment. Working with simulated data has allowed us to experiment with a broader set of experimental conditions than would not be possible otherwise. Our next immediate step is to validate the method on a real robot.

The authors would like to thank Dylan Wishner and Felix Lu for advice and early version of the Tianshou code adaptation, and for suggestions and discussions they contributed during our work on this manuscript.

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: (a)With the same number of successful trajectories in the dataset, the trajectories provided by a human expert lead to faster learning and higher final performance.

[å›¾ç‰‡: images\image_8.png]
å›¾ç‰‡è¯´æ˜: (a)With the same number of successful trajectories in the dataset, the trajectories provided by a human expert lead to faster learning and higher final performance.

The table2contains the values of various parameters used in our MuJoCo experiments.

Parameters used in the Monolith environment are in table3.

[å›¾ç‰‡: images\image_9.png]

[å›¾ç‰‡: images\image_10.png]

