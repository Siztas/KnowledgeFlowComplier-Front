æ ‡é¢˜ï¼šTowards Balanced Active Learning for Multimodal Classification

Training multimodal networks requires a vast amount of data due to their larger parameter space compared to unimodal networks. Active learning is a widely used technique for reducing data annotation costs by selecting only those samples that could contribute to improving model performance. However, current active learning strategies are mostly designed for unimodal tasks, and when applied to multimodal data, they often result in biased sample selection from the dominant modality. This unfairness hinders balanced multimodal learning, which is crucial for achieving optimal performance. To address this issue, we propose three guidelines for designing a more balanced multimodal active learning strategy. Following these guidelines, a novel approach is proposed to achieve more fair data selection by modulating the gradient embedding with the dominance degree among modalities. Our studies demonstrate that the proposed method achieves more balanced multimodal learning by avoiding greedy sample selection from the dominant modality. Our approach outperforms existing active learning strategies on a variety of multimodal classification tasks. Overall, our work highlights the importance of balancing sample selection in multimodal active learning and provides a practical solution for achieving more balanced active learning for multimodal classification.

Multimodal classification, as one of the classical multimodal learning tasks, aims to exploit complementary information inherent in multimodal data to achieve better classification performance. To this end, deep learning strategies have been implemented to train large-scale multimodal deep neural networks(Baltrusaitis etÂ al.,2019; Guo etÂ al.,2019). However, such networks require an enormous amount of data to learn from, given their huge number of parameters. To reduce data cost, active learning (AL) is used to select a subset of more informative and distinctive unlabeled data samples for label assignment by oracles. Consequently, large networks can maintain performance while utilizing a smaller labeling budget. Most existing active learning algorithms are designed for unimodal tasks such as image classification(Sener and Savarese,2018; Beluch etÂ al.,2018), object detection(Liu etÂ al.,2021; Yuan etÂ al.,2021)and language modeling(Yuan etÂ al.,2020; Margatina etÂ al.,2021). The objective is to select samples that have high uncertainty in them, carry novel knowledge for model training and those with distinctive features. However, there has been significantly less research reported on the design of effective active learning strategies for multimodal learning(Ren etÂ al.,2022).

In this paper, we initially examine the performance of existing active learning strategies in selecting multimodal data. Our experiments reveal that these strategies tend to focus more on the dominant modality rather than fairly considering all modalities. For instance, in an image-text classification task, if the text contributes more to model optimization, active learning strategies may exhibit a bias towards the more distinguishable text modality by selecting valuable text samples and disregarding the informativeness of image samples. As a result, the selected multimodal dataset could become unbalanced, with insufficient information from the image modality, potentially leading to a degraded image model backbone. Recent works(Peng etÂ al.,2022; Wang etÂ al.,2020; Huang etÂ al.,2022; Wu etÂ al.,2022)point out that balancing the training and optimization of all modalities is a key factor for successful multimodal learning. Similarly, it is crucial to design active learning strategies that can select multimodal data with fairness among all modalities to assist balanced multimodal learning.

Based on our findings, we develop aBalancedMultimodalActiveLearning (BMMAL) algorithm that selects multimodal data by fairly considering each modality present in the data. In our approach, we choose the gradient embedding of model parameters, as it reflects the impact on model training and captures the diversity of data samples. However, we examine how the previous gradient embedding method(Ash etÂ al.,2020)fails to select balanced multimodal data. To ensure fairness, we individually assess the contribution of each modality feature by examining the Shapley value, which attributes its contribution to the final multimodal prediction. We then apply modulation on the gradient embedding to penalize samples with dominant modalities. Lastly, a clustering seed initialization algorithm is employed to select diverse multimodal data with a significant influence on model training.

In summary, our main contributions are as follows:

We empirically show that most existing active learning strategies fail to select a balanced multimodal dataset. We analyze how to improve the current gradient embedding based active learning strategy to rectify this.

We propose a method to modulate the gradient embedding on sample-level to select more balanced multimodal candidates.

We conduct experiments on three multimodal datasets to show that our proposed method treats multimodal data more equally and achieves better performance.

Uncertainty-aware strategies attempt to utilize the data uncertainty or the model uncertainty as a criterion to locate unlabeled data points that the current model has less confidence about. One strategy is to utilize the posterior classification probability distribution by measuring its entropy(Settles,2012; Wang etÂ al.,2015), or the margin between the most confident class and the second most confident class(Roth and Small,2006). In addition, uncertainty can be evaluated as the variance of predictions generated by an ensemble of models(Beluch etÂ al.,2018)or by multiple inferences with Monte-Carlo dropout as an alternative Bayesian approximation for static networks(Gal and Ghahramani,2016). Moreover, ALFA-Mix(Parvaneh etÂ al.,2022)evaluates unlabeled samples by mixing their features with labeled samples and observing whether there is inconsistency among predictions from mixed features. DFAL(Ducoffe and Precioso,2018)incorporates adversarial attack techniques(Moosavi-Dezfooli etÂ al.,2016)to select unlabeled data samples located close to the classification boundaries.

Diversity-aware strategies tend to select unlabeled data points whose features are as diverse as possible to minimize data redundancy.(Nguyen and Smeulders,2004)utilizes K-medoid algorithm(Kaufman and Rousseeuw,1990)to select representative data centroids that minimize the total distance from other data samples to the nearest centroids. CoreSet(Sener and Savarese,2018)greedily selects unlabeled data samples that have maximum distances from their nearest neighbors.(Biyik etÂ al.,2019)adopts the determinantal point process (DPP) to evaluate the diversity by calculating the determinant of the similarity matrix. Diversity-aware strategies can also be considered in the context of distribution matching, which aims to reduce the gap between the distributions of labeled and unlabeled samples in latent space or feature space. VAAL(Sinha etÂ al.,2019)trains a variational auto-encoder to construct the latent distribution of labeled samples and an adversarial network to distinguish labeled samples and unlabeled samples in the latent space. Moreover, the maximum mean discrepancy (MMD)(Viering etÂ al.,2019), theâ„‹â„‹\mathcal{H}-divergence(Su etÂ al.,2020)and the Wasserstein distance(Shui etÂ al.,2020)are used to measure the distribution gap.

To achieve a better trade-off between informativeness and diversity, hybrid methods are developed with an awareness of both. Since diversity-aware strategies are orthogonal to most of uncertainty-aware strategies(Hacohen etÂ al.,2022), they could be easily combined together. ALFA-Mix(Parvaneh etÂ al.,2022)adopts K-means clustering to further filter out samples to enhance diversity. BADGE(Ash etÂ al.,2020)represents unlabeled data samples via gradient embedding of parameters of the last classifier layer and applies K-means++(Arthur and Vassilvitskii,2007)to form a diverse data selection which still carries high uncertainty.

Our work considers joint multimodal learning for classifications. Here, it has been found that the best unimodal networks could potentially outperform multimodal networks regardless of fusion mechanisms or regularization methods(Wang etÂ al.,2020). Recent works show that the degradation of multimodal learning could be due to unbalanced optimization among different modalities. In(Huang etÂ al.,2022), the failure of multimodal learning is attributed to modality competition where only dominant modalities are fully explored by joint training. Similarly,(Wu etÂ al.,2022)demonstrates that multimodal learning greedily optimizes the dominant modalities and chooses to balance their training speeds.(Wang etÂ al.,2020)propose to blend gradients with weights that are disproportional to the overfitting and generalization ratio of each modality so that each modality could be optimized in a balanced manner.(Peng etÂ al.,2022)finds that fusion mechanisms such as concatenation and summation encourage the dominant modality to learn faster and thus develops gradient modulation to adaptively balance the training speed of each modality.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1.General active learning process. The dashed lines represent model training. The solid lines represent data selection.

The general active learning process is shown inFigure1. Initially, we are given a large unlabeled data poolX0U={(xm1,â€¦,xmM)1â€‹â€¦â€‹n}superscriptsubscriptğ‘‹0ğ‘ˆsubscriptsubscriptğ‘¥subscriptğ‘š1â€¦subscriptğ‘¥subscriptğ‘šğ‘€1â€¦ğ‘›X_{0}^{U}=\{(x_{m_{1}},\dots,x_{m_{M}})_{1\dots n}\}ofnğ‘›ninput data withMğ‘€Mmodalities and an empty labeled data poolX0L=âˆ…superscriptsubscriptğ‘‹0ğ¿X_{0}^{L}=\emptyset. The labeling budget of each round is set toBğµB. In the first round of active learning, since there is no trained model to evaluate with, a subsetX1Lsuperscriptsubscriptğ‘‹1ğ¿X_{1}^{L}containingBğµBmultimodal data is randomly selected fromX0Usuperscriptsubscriptğ‘‹0ğ‘ˆX_{0}^{U}, and they will be assigned with true labelsY1Lsuperscriptsubscriptğ‘Œ1ğ¿Y_{1}^{L}. After data selection, the unlabeled dataset becomesX1U=X0Uâˆ–X1Lsuperscriptsubscriptğ‘‹1ğ‘ˆsuperscriptsubscriptğ‘‹0ğ‘ˆsuperscriptsubscriptğ‘‹1ğ¿X_{1}^{U}=X_{0}^{U}\setminus X_{1}^{L}. The training dataset for the first round of model training consists ofX1Lsuperscriptsubscriptğ‘‹1ğ¿X_{1}^{L}andY1Lsuperscriptsubscriptğ‘Œ1ğ¿Y_{1}^{L}. Starting from the second round, an active learning strategySAâ€‹Lsubscriptğ‘†ğ´ğ¿S_{AL}evaluates the trained model and unlabeled data in the last round using an acquisition function and selects a batch of candidates for label assignment to construct a new training dataset for the current round of model training. The processes of data selection and model training continue until the total labeling budget is run out or the target performance of the trained model is reached.

We then introduce our multimodal learning framework for classification task.xm1subscriptğ‘¥subscriptğ‘š1x_{m_{1}}andxm2subscriptğ‘¥subscriptğ‘š2x_{m_{2}}represent the input data from two different modalities. They are processed through encodersÏ†m1subscriptğœ‘subscriptğ‘š1\varphi_{m_{1}}andÏ†m2subscriptğœ‘subscriptğ‘š2\varphi_{m_{2}}respectively to extract unimodal featureszm1âˆˆâ„Dmâ€‹1subscriptğ‘§subscriptğ‘š1superscriptâ„subscriptğ·ğ‘š1z_{m_{1}}\in\mathbb{R}^{D_{m1}}andzm2âˆˆâ„Dm2subscriptğ‘§subscriptğ‘š2superscriptâ„subscriptğ·subscriptğ‘š2z_{m_{2}}\in\mathbb{R}^{D_{m_{2}}}. We adopt concatenation, a wildly used late-fusion mechanism, to construct multimodal featureszmâ€‹m=zm1âŠ•zm2subscriptğ‘§ğ‘šğ‘šdirect-sumsubscriptğ‘§subscriptğ‘š1subscriptğ‘§subscriptğ‘š2z_{mm}=z_{m_{1}}\oplus z_{m_{2}}.111Other fusion mechanisms such as summation and NL-gate are implemented in our further experiments.The unimodal and multimodal features are fed to unimodal classifiersCm1subscriptğ¶subscriptğ‘š1C_{m_{1}},Cm2subscriptğ¶subscriptğ‘š2C_{m_{2}}and multimodal classifierCmâ€‹msubscriptğ¶ğ‘šğ‘šC_{mm}respectively to produce logitsfm1subscriptğ‘“subscriptğ‘š1f_{m_{1}},fm2subscriptğ‘“subscriptğ‘š2f_{m_{2}}andfmâ€‹msubscriptğ‘“ğ‘šğ‘šf_{mm}for classification. The final loss is the average cross-entropy lossâ„’Câ€‹Esubscriptâ„’ğ¶ğ¸\mathcal{L}_{CE}of unimodal and multimodal logits with true labelsyğ‘¦y:

Once the model is trained, the unlabeled data samples are evaluated using an acquisition function and filtered for labeling.

We introduce one of the state-of-the-art active learning algorithm BADGE(Ash etÂ al.,2020)and provide analysis of its imbalanced data selection over multimodal data samples. BADGE was the first to propose the replacement of features for embedding with the gradient of the weight of the last FC layer, which acts as the classifier. In our case, the last FC layer for multimodal classification is the multimodal classifierCmâ€‹msubscriptğ¶ğ‘šğ‘šC_{mm}. The weight of classifierWğ‘ŠWis a 2-dimensional matrix of sizeKÃ—Dmâ€‹mğ¾subscriptğ·ğ‘šğ‘šK\times D_{mm}, whereKğ¾Kis the number of classes andDmâ€‹msubscriptğ·ğ‘šğ‘šD_{mm}is the dimension of concatenated multimodal featureDm1+Dm2subscriptğ·subscriptğ‘š1subscriptğ·subscriptğ‘š2D_{m_{1}}+D_{m_{2}}. The corresponding multimodal cross-entropy loss can be expanded as

whereÏƒğœ\sigmais softmax function andzmâ€‹mâ‹…WiTâ‹…subscriptğ‘§ğ‘šğ‘šsuperscriptsubscriptğ‘Šğ‘–ğ‘‡{z_{mm}\cdot W_{i}^{T}}is theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}element of logitsfmâ€‹msubscriptğ‘“ğ‘šğ‘šf_{mm}. The gradient embedding is defined asg=âˆ‚â„’mâ€‹mâˆ‚Wğ‘”subscriptâ„’ğ‘šğ‘šğ‘Šg=\frac{\partial\mathcal{L}_{mm}}{\partial W}, and it is a 2-D matrix of sizeKÃ—Dmâ€‹mğ¾subscriptğ·ğ‘šğ‘šK\times D_{mm}where theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}row is

wherey^mâ€‹m=argmaxiâˆˆ[K]â€‹[(fmâ€‹m)i]subscript^ğ‘¦ğ‘šğ‘šğ‘–delimited-[]ğ¾argmaxdelimited-[]subscriptsubscriptğ‘“ğ‘šğ‘šğ‘–\hat{y}_{mm}=\underset{i\in[K]}{\mathrm{argmax}}[({f_{mm}})_{i}]is the pseudo label for unlabeled data samples. The gradient embedding is flattened into a vector for sampling. It not only carries the uncertainty of classification from the margin between logitsfisubscriptğ‘“ğ‘–f_{i}and pseudo labelsy^mâ€‹msubscript^ğ‘¦ğ‘šğ‘š\hat{y}_{mm}, but also is representative enough due to the information present inzmâ€‹msubscriptğ‘§ğ‘šğ‘šz_{mm}.

However, in multimodal learning settings, identifying the source of uncertainty can be challenging. Upon examining the calculation of multimodal logitsfi=zmâ€‹mâ‹…WiT=zm1â‹…(Wi)m1T+zm2â‹…(Wi)m2Tsubscriptğ‘“ğ‘–â‹…subscriptğ‘§ğ‘šğ‘šsuperscriptsubscriptğ‘Šğ‘–ğ‘‡â‹…subscriptğ‘§subscriptğ‘š1superscriptsubscriptsubscriptğ‘Šğ‘–subscriptğ‘š1ğ‘‡â‹…subscriptğ‘§subscriptğ‘š2superscriptsubscriptsubscriptğ‘Šğ‘–subscriptğ‘š2ğ‘‡f_{i}={z_{mm}\cdot W_{i}^{T}}=z_{m_{1}}\cdot({W_{i}})_{m_{1}}^{T}+z_{m_{2}}\cdot({W_{i}})_{m_{2}}^{T}, whereWisubscriptğ‘Šğ‘–W_{i}is divided into two matrices(Wi)m1subscriptsubscriptğ‘Šğ‘–subscriptğ‘š1({W_{i}})_{m_{1}}and(Wi)m2subscriptsubscriptğ‘Šğ‘–subscriptğ‘š2({W_{i}})_{m_{2}}, it is difficult to determine which modality carries more uncertainty and which carries less. To illustrate, for a visual event such as drawing, the visual modality contains more information and contributes more to multimodal logits by generating a larger output. The multimodal uncertainty calculation is thus skewing the visual uncertainty instead of considering both visual and auditory uncertainties fairly. FromSection4.4, we find that BADGE does pay more attention to the dominant modality, which might potentially damage the performance of joint multimodal learning. Another limitation of BADGE is its inability to distinguish modality contributions. For instance, given two data samples with identical logits, we should prioritize the one with a more balanced contribution during data selection to facilitate balanced multimodal learning. However, the current BADGE algorithm cannot achieve this. Similarly, most conventional active learning algorithms lack this capability.

Hence, we develop a balanced multimodal active learning method that could avoid biased data selection towards the dominant modality to mitigate modality competition and assure that the trained multimodal network would not easily degenerate to the dominant modality. While our designed method is encouraged to pay more attention to the weaker modality, it is essential to ensure that it does not overly lean towards the weaker modality, as this may also harm the multimodal classification performance.

To make existing AL strategies more suitable for balanced multimodal learning, it is necessary to inspect the individual modality contribution and reduce the contribution gap among different modalities. We empirically propose three guidelines for designing active learning strategies that treat each modality more equally. LetÎ¦miâ€‹(x)subscriptÎ¦subscriptğ‘šğ‘–ğ‘¥\Phi_{m_{i}}(x)represent the contribution of theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}modality of data samplexğ‘¥xto the final model outcome , which should satisfy:

We introduce the dominance degreeÏâ€‹(x)ğœŒğ‘¥\rho(x)to quantify how severely a data samplexğ‘¥xis dominated by the strongest modality:

We further partition the entire unlabeled dataset into multiple subsetsX={X1,â€¦,XM}ğ‘‹subscriptğ‘‹1â€¦subscriptğ‘‹ğ‘€X=\{X_{1},...,X_{M}\}for the ease of discussion. In each subsetXisubscriptğ‘‹ğ‘–X_{i}, modalitymisubscriptğ‘šğ‘–m_{i}contributes the most:

Guideline 1: For two multimodal data samplesxisubscriptğ‘¥ğ‘–x_{i}andxjsubscriptğ‘¥ğ‘—x_{j}, if their acquisition scores of conventional active learning (CAL) strategies are equal, the one with more balanced unimodal contributions should have higher acquisition scores of balanced multimodal active learning strategies,

By following Guideline 1, data samples with more equal unimodal contributions are more likely to be selected. However, this does not guarantee that the stronger modality will be suppressed, nor does it ensure that the weaker modality will not be overly encouraged. Therefore, we introduce two additional guidelines.

Guideline 2: To avoid biased data selection favoring the stronger modality, the gap between the average acquisition scores of data samples dominated by the stronger modality and those dominated by the weaker modality should be reduced. In a two-modality case, wherem1subscriptğ‘š1m_{1}is the weaker modality andm2subscriptğ‘š2m_{2}is the stronger modality (i.e. the average contribution ofm1subscriptğ‘š1m_{1}over the entire dataset is less than that ofm2subscriptğ‘š2m_{2},1|X|â€‹âˆ‘xâˆˆXÎ¦m1â€‹(x)<1|X|â€‹âˆ‘xâˆˆXÎ¦m2â€‹(x)1ğ‘‹subscriptğ‘¥ğ‘‹subscriptÎ¦subscriptğ‘š1ğ‘¥1ğ‘‹subscriptğ‘¥ğ‘‹subscriptÎ¦subscriptğ‘š2ğ‘¥\frac{1}{|X|}\sum_{x\in X}\Phi_{m_{1}}(x)<\frac{1}{|X|}\sum_{x\in X}\Phi_{m_{2}}(x)), we have

Guideline 3: Lastly, to prevent biased data selection towards the weaker modality, it is necessary to ensure that the contribution of each modality to the acquisition score functionaBâ€‹Mâ€‹Mâ€‹Aâ€‹Lsubscriptğ‘ğµğ‘€ğ‘€ğ´ğ¿a_{BMMAL}is still proportional to its modality contribution to the model outcome on the sample-level. It ensures that the data samples are selected in a way that fairly represents the contributions of each modality to the actual model outcome.

In summary, Guideline 1 prioritizes the samples with more equal unimodal contributions. Guideline 2 and 3 work together to punish the stronger modality on the dataset-level but maintain the relationship between strong and weak modality on the sample-level, avoiding biases towards either the stronger or weaker modalities.

We show how we compute modality contributionÎ¦Î¦\Phi. In the context of multimodal classification, balanced active learning should select data samples that fairly contribute to the performance of all modalities. To achieve this, it is essential to estimate the degree to which each modality of a given data sample contributes to the final multimodal prediction.
One approach involves assessing modality importance by computing the disparity in model performance before and after the incorporation of a particular modality. Researchers have proposed various techniques to remove the information of one modality, such as masking(Frank etÂ al.,2021), permutation(Gat etÂ al.,2021), and empirical multimodally-additive projection (EMAP)(Hessel and Lee,2020). Nonetheless, these attribution methods are ill-suited for active learning as they require ground truth labels to calculate model performance metrics, such as accuracy. As a result, these methods cannot be employed for estimating modality contribution for unlabeled data due to the absence of ground truth labels.

Therefore, we choose to use the Shapley value to estimate modality contribution without the need for true labels. The Shapley value(SHAPLEY,1953)was proposed to fairly attribute payouts among group of cooperative players based on their contributions to the total payout in game theory. In deep learning, SHapley Additive exPlanations (SHAP) value(Lundberg and Lee,2017)considers each feature as a player and the model prediction as the total payout to estimate feature contributions. Letâ„³={zm1,â€¦,zmM}â„³subscriptğ‘§subscriptğ‘š1â€¦subscriptğ‘§subscriptğ‘šğ‘€\mathcal{M}=\{z_{m_{1}},...,z_{m_{M}}\}represent the set of all modality features,ğ’®ğ’®\mathcal{S}denote the subset, andVğ‘‰Vsymbolize the model outcome. Here, we use features instead of raw data inputs since features are utilized in active learning. To estimate the Shapley value ofitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}modality featurezmisubscriptğ‘§subscriptğ‘šğ‘–z_{m_{i}}, we compute the marginal contribution to the subsetğ’®ğ’®\mathcal{S}and average over all possible subset selections:

We use the largest predicted class probabilitypy^subscriptğ‘^ğ‘¦p_{\hat{y}}provided byfmâ€‹msubscriptğ‘“ğ‘šğ‘šf_{mm}as the model outcomeVğ‘‰V, wherey^^ğ‘¦\hat{y}is the pseudo class. For the most common two-modality case, the Shapley values of modality features can be computed as follows (âˆ…\emptysetrepresents a zero vector):

The Shapley value could be positive, negative or zero. While the sign indicates in which direction of each modality contributes, our primary interest lies in the extend of its contribution. Hence, we define modality contribution as follows:

Following the proposed guidelines, we redesign the BADGE for multimodal classification scenarios with two modalities,m1subscriptğ‘š1m_{1}andm2subscriptğ‘š2m_{2}, to achieve more balanced data selection. Theitâ€‹hsuperscriptğ‘–ğ‘¡â„i^{th}row of gradient embedding in Eq.3could be derived as concatenation of two unimodal gradient embeddings:

We then design two weightswm1subscriptğ‘¤subscriptğ‘š1w_{m_{1}}andwm2subscriptğ‘¤subscriptğ‘š2w_{m_{2}}, and scale each unimodal gradient embedding by them respectively:

Here,Ï=|Î¦m1âˆ’Î¦m2|ğœŒsubscriptÎ¦subscriptğ‘š1subscriptÎ¦subscriptğ‘š2\rho=|\Phi_{m_{1}}-\Phi_{m_{2}}|is the difference between contributions of two modalities. Note that the gradient embedding of larger l2 norm will be selected more easily by K-Means++ algorithm(Ash etÂ al.,2020). Therefore, by multiplying with these weights, the magnitude of gradient embedding will be suppressed more if their unimodal contributions are more unbalanced. It aligns with our Guideline 1 where we want to punish the samples with unbalanced contributions.

Moreover, we observe that the averageÏğœŒ\rhoof the subset in which the weaker modality dominates is smaller than that of the subset where the stronger modality dominates. SeeFigure6and our discussion inSec4.5. Ifm1subscriptğ‘š1m_{1}is the weaker modality regarding the entire dataset, then we will have1|X1|â€‹âˆ‘xâˆˆX1Ïâ€‹(x)<1|X2|â€‹âˆ‘xâˆˆX2Ïâ€‹(x)1subscriptğ‘‹1subscriptğ‘¥subscriptğ‘‹1ğœŒğ‘¥1subscriptğ‘‹2subscriptğ‘¥subscriptğ‘‹2ğœŒğ‘¥\frac{1}{|X_{1}|}\sum_{x\in X_{1}}\rho(x)<\frac{1}{|X_{2}|}\sum_{x\in X_{2}}\rho(x)for two subsetsX1subscriptğ‘‹1X_{1}andX2subscriptğ‘‹2X_{2}dominated bym1subscriptğ‘š1m_{1}andm2subscriptğ‘š2m_{2}respectively. It means that the subset where the stronger modality dominates will be suppressed more, and it follows our Guideline 2 to punish the stronger modality on the dataset-level.

Finally, the Guideline 3 is also adhered to. For each sample, the modality with a higher contribution to the model outcome is always assigned a greater weight, resulting in a higher magnitude of unimodal gradient embedding. This ensures that the contribution to data selection is proportional to the contribution to the model outcome and model optimization if selected.

In the end, we perform K-Means++ over the scaled gradient embedding to select candidates for labeling. As a result, our BMMAL strategy could achieve more balanced active learning on multimodal classification than BADGE. It could prevent biased selection towards either the stronger or weaker modalities, thus benefiting multimodal learning.

Food101(Wang etÂ al.,2015)is a multi-class food recipe dataset with 101 kinds of food. Each recipe consists of a food image and textual recipe description. The dataset consists of 45,719 samples for training and 15,294 samples for testing.

KineticsSound(Arandjelovic and Zisserman,2017)is a sub-dataset containing 31 action classes selected from Kinetics-400(Kay etÂ al.,2017). These action classes are considered to be correlated to both visual and auditory content. This dataset contains 14,739 clips for training and 2,594 clips for testing.

VGGSound(Chen etÂ al.,2020)is a large-scale video dataset with 309 classes. Each video clip is 10-second and captures the object making the sound. We are only able to download 180,911 clips for training and 14,843 clips for testing due to the unavailability of YouTube videos.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2.Modality contributionÎ¦Î¦\Phiacross different AL iterations on the Food101 test set.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3.Modality contributionÎ¦Î¦\Phiacross different AL iterations on the KineticsSound test set.

We consider seven existing active learning strategies as baselines.Randomselects the data samples randomly from the unlabeled data pool.Entropy(Settles,2012)selects data samples with the highest entropy of multimodal classification probabilities.CoreSet(Sener and Savarese,2018)filters out a subset of unlabeled data with representative multimodal features via K-center greedy algorithm.BADGE(Ash etÂ al.,2020)is a hybrid method that selects diverse data samples by K-means++ sampler over their gradient embedding of multimodal classifier.BALD(Gal etÂ al.,2017)is a Bayesian method to evaluate the mutual information between model predictions and model parameters. Since our model is static, we run five rounds of model forwarding with enabled dropout to obtain the entropy of model parameters.DeepFool(Ducoffe and Precioso,2018)adopts an adversarial-like approach that adds small perturbations over multimodal features and selects data whose predictions are flipped.GCNAL(Caramalau etÂ al.,2021)learns an extra graph convolution network to distinguish labelled and unlabelled samples and selects unlabelled samples that are sufficiently different from labelled ones.

Image-text Classification:For the Food101 dataset, we adopt ResNet-101 pre-trained on ImageNet as the image backbone and pre-trained Bert-base model(Devlin etÂ al.,2019)as the text backbone. All unimodal and multimodal classifiers are single FC layers. We use AdamW(Loshchilov and Hutter,2019)as the optimizer and train the model for 15 epochs in each AL round and adopt random crop, random horizontal flip and random grey scale for image augmentation.

Video Classification:For VGGSound and KineticsSound, we utilize ResNet2P1D-18(Tran etÂ al.,2018)as visual backbone. The difference is that it is pre-trained on Kinetics-400 for VGGSound, while it is randomly initialized for KineticsSound. We use the randomly initialized ResNet-18 as an auditory backbone whose input channel is modified from 3 to 1. The video is uniformly sampled into 10 frames at the rate of one frame per second. The audio clip is transformed into a spectrogram with a window length of 512 and an overlap length of 353. For video augmentation, we randomly sample 5 frames out of 10 frames and apply image augmentation techniques on each frame. For audio augmentation, we randomly extract a 5-second audio fragment from the whole audio clip. We use Adam as optimizer and train the model for 45 epochs in each round.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_8.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_9.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_10.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

The experiment is repeated 5 times for image-text classification and 3 times for video classification to remove the randomness of the initial querying. For multimodal fusion, we apply concatenation which is a widely used multimodal fusion mechanism on all tasks. In addition, we implement summation and NL-gate(Wang etÂ al.,2018)that is similar to multi-head attention(Vaswani etÂ al.,2017)in further experiments.

A fair and good AL strategy ought to select important multimodal data that could contribute to multimodal tasks and, simultaneously, pay fair attention to weaker modalities and strong modalities to prevent the trained multimodal network from degenerating into only a good unimodal network. We run conventional active learning strategies along with our proposed method BMMAL on several multimodal datasets, and compare their multimodal and unimodal classification accuracy.

We firstly draw the trend of modality contributions to the predicted probability over the ground truth class on test dataset across different active learning iterations inFigure2andFigure3. As shown in the figures, the textual modal contributes more than the imagery model on the Food101 after second iteration, and the auditory modal contributes more than the visual modal on the KineticsSound. More importantly, the difference between two unimodal contributions of BMMAL is overall smaller than both BADGE and Random. It means that two modalities contribute more equally in the models trained by the data selected by BMMAL.

The performance comparison of each AL iteration on the Food101 dataset is shown inFigure10(a)and10(c). Note that textual modality is the stronger modality since iteration 2. Our method outperforms all baselines except BADGE in multimodal classification. In text classification, BMMAL, BADGE and CoreSet achieve good performance. In image classification, our method is superior to most of the baselines except Random. From the above comparison, we can tell that BADGE and CoreSet mainly focus on selecting valuable samples over the stronger text modality and ignore the weaker image modality. Although Random uniformly selects multimodal data without any weighting in image classification, it is considered unfair concerning the text modality.

The performance comparison of each AL iteration on the KineticsSound dataset is shown inFigure10(b)and10(d). Note that auditory modality is the stronger modality. Our method outperforms all baselines in multimodal classification. BADGE performs the best on audio classification on many iterations, However, its performance declines on video classification indicating that biased data selection might negatively affect multimodal classification. It shows that BADGE tends to assign more importance to audio modality during data selection and such behavior might negatively affect multimodal joint training.

The performance comparison of each AL iteration on the VGGSound dataset is shown inFigure4(c)and4(f). Note that auditory modality is the stronger modality. Our method outperforms BADGE in not only multimodal classification but also in two unimodal classification by an obvious margin.

[å›¾ç‰‡: images\image_11.png]
å›¾ç‰‡è¯´æ˜: (a)Pairwise comparison on multimodal classification.

[å›¾ç‰‡: images\image_12.png]
å›¾ç‰‡è¯´æ˜: (a)Pairwise comparison on multimodal classification.

[å›¾ç‰‡: images\image_13.png]
å›¾ç‰‡è¯´æ˜: (a)Pairwise comparison on multimodal classification.

Findings.Our first finding is that AL methods such as BADGE and BALD which win at classification of the stronger modality could stand a good chance of failing at classification of the weak modality. This may be due to biased data selection towards the stronger modality, and it is undesirable for balanced multimodal learning. Our second finding is that Random and CoreSet could perform better in the weaker modality, whereas they are inferior in multimodal classification because random selection treats every sample with absolute fairness and CoreSet focuses too much on the weak modality which are both unfair concerning the stronger modality. Finally, our method achieves a fairer multimodal data selection with a better trade-off between weak and strong modalities.

Pairwise Comparison.We illustrate the results across various experimental settings in matrixPğ‘ƒPinFigure5(Ash etÂ al.,2020). We compute the t-score for each repeated experiment and use the two-sided t-test to compare the performance of paired strategies on the test set with a 0.9 confidence interval. If strategyiğ‘–isignificantly outperforms strategyjğ‘—j, we add1/L1ğ¿1/LtoPi,jsubscriptğ‘ƒğ‘–ğ‘—P_{i,j}, whereLğ¿Lis the total number of iterations for a single experiment setting. The maximum cell value equals the total number of experiment settings.Pi,jsubscriptğ‘ƒğ‘–ğ‘—P_{i,j}indicates the number of times strategyiğ‘–isignificantly outperforms strategyjğ‘—j. We compute the matrix for both multimodal and unimodal classification for stronger (text for Food101, audio for KineticsSound and VGGSound) and weaker modalities (image for Food101, video for KineticsSound and VGGSound). The three matrices demonstrate that our proposed method outperforms most baselines across settings. Specifically, BMMAL surpasses BADGE in multimodal classification and unimodal classification on weaker modalities, while performing comparably with BADGE in unimodal classification on stronger modalities. This suggests that the performance improvement of BMMAL in multimodal classification mainly stems from enhancing weaker modalities while maintaining stable performance in stronger modalities.

[å›¾ç‰‡: images\image_14.png]
å›¾ç‰‡è¯´æ˜: (a)Average weightwisubscriptğ‘¤ğ‘–w_{i}inXtsubscriptğ‘‹ğ‘¡X_{t}and average weightwtsubscriptğ‘¤ğ‘¡w_{t}inXisubscriptğ‘‹ğ‘–X_{i}on the Food101 dataset.

[å›¾ç‰‡: images\image_15.png]
å›¾ç‰‡è¯´æ˜: (a)Average weightwisubscriptğ‘¤ğ‘–w_{i}inXtsubscriptğ‘‹ğ‘¡X_{t}and average weightwtsubscriptğ‘¤ğ‘¡w_{t}inXisubscriptğ‘‹ğ‘–X_{i}on the Food101 dataset.

Dominance Degree. As described inEq.6, we divide the entire unlabeled dataset into multiple sub-datasets in which modalitymisubscriptğ‘šğ‘–m_{i}contributes the most. The Food101 dataset is divided intoXtsubscriptğ‘‹ğ‘¡X_{t}andXisubscriptğ‘‹ğ‘–X_{i}dominated by text and image modality, respectively. InFigure6(a), the average weight values of the weaker modality are showed. As shown before inFigure2, text modality is the stronger one starting from the second iteration. The average value ofwisubscriptğ‘¤ğ‘–w_{i}inXtsubscriptğ‘‹ğ‘¡X_{t}accordingly becomes less than that ofwtsubscriptğ‘¤ğ‘¡w_{t}inXisubscriptğ‘‹ğ‘–X_{i}from the second iteration, meaning that the average difference valueÏğœŒ\rhobetween two unimodal contributions inXtsubscriptğ‘‹ğ‘¡X_{t}is larger than inXisubscriptğ‘‹ğ‘–X_{i}. The KineticsSound dataset is divided intoXvsubscriptğ‘‹ğ‘£X_{v}andXasubscriptğ‘‹ğ‘X_{a}dominated by video and audio modality, respectively. InFigure6(b), the average weight values of the weaker modality are showed. Similarly, the average difference valueÏğœŒ\rhobetween two unimodal contributions inXasubscriptğ‘‹ğ‘X_{a}is larger than inXvsubscriptğ‘‹ğ‘£X_{v}. Consequently, on the dataset-level, the sub-dataset dominated by the weaker modality receives less punishment compared to the sub-dataset dominated by the stronger modality.

[å›¾ç‰‡: images\image_16.png]
å›¾ç‰‡è¯´æ˜: Figure 7.Multimodal and unimodal classification performance comparison with NL-gate fusion method on the VGGSound dataset.

Different Fusion Mechanisms.We perform experiment by changing the fusion method from concatenation into summation on Food101 and KineticsSound, while keeping other settings unchanged. We include the performance comparison in the pairwise comparison and present the iterative comparison in the supplementary materials. Furthermore, we change concatenation to NL-gate for mixing video and audio features on the VGGSound dataset, setting the initial budget to 5,000 and the AL budget for each round to 2,000, as NL-gate requires more data to demonstrate its efficiency in fusion. We provide the implementation details in the supplementary materials. As shown inFigure7, our method achieves comparable multimodal classification performance to BADGE and becomes worse on auditory classification. However, for the weaker visual classification, our method outperforms the others, demonstrating its effectiveness in balancing weak and strong modalities.

Large-scale Active Learning.We conduct experiment on VGGSound with larger budget size of 5,000 to validate our method on large-scale active learning for multimodal video classification. The results are averaged and shown inTable1. On video classification, the performance of BADGE degrades and becomes worse than random selection, while our method achieves improvement over BADGE and random selection. On audio classification, BADGE and our method are comparable and are both better than random selection. As a result, our method performs better than BADGE and can save around 5k labels compared with random selection if target multimodal classification top-1 accuracy is set to 0.435.

Classwise Performance Comparison. We show the classwise performance comparison on the KineticsSound dataset. As shown inFigure8, the gain is more significant than the drop. Moreover, improved classes such as â€™chopping woodâ€™, â€™bowlingâ€™ and â€™shoveling snowâ€™ carry more visual information, and dropped classes are mostly dominated by the auditory modality. Note that KineticsSound is a dataset where audio contributes more than vision, which means that BMMAL avoids biased selection over auditory modality and focuses more on the weaker visual modality.

[å›¾ç‰‡: images\image_17.png]
å›¾ç‰‡è¯´æ˜: Figure 8.Top 10 improved and dropped classes based on the improvement of BMMAL to BADGE on multimodal classification accuracy on KineticsSound with 5K labeled samples. Bars represent multimodal classification accuracy. Stems represent unimodal classification accuracy.

In this paper, we evaluate how existing active learning strategies perform on multimodal classification. Our empirical studies show that they might treat different modalities unfairly, and it could lead to performance degradation for multimodal learning. We propose BMMAL to mitigate this unfairness by separately scaling unimodal gradient embeddings, which avoids mixing all unimodal information and well retain characteristics of each modality. The method performs well on multiple datasets and can be potentially applied on large-scale multimodal active learning.

Computing the Shapley values of each unimodal feature requires to perform inference2Msuperscript2ğ‘€2^{M}times in total, whereMğ‘€Mis the number of modalities. In our two-modality learning case, we need to perform inference four times with different combination of unimodal features to obtain the Shapley values, which is acceptable. Then, given the computed gradient embedding ofNğ‘Nunlabeled samples, the sampling time complexity of BMMAL isğ’ªâ€‹(Nâ€‹Bâ€‹Dâ€‹K)ğ’ªğ‘ğµğ·ğ¾\mathcal{O}(NBDK), whereBğµBis the query budget of each AL round,Dğ·Dis the size of weight matrix of the last linear classifier andKğ¾Kis the number of classes.

NL-gate(Wang etÂ al.,2018)is a mid-fusion mechanism that behaves similar to multi-head attention. We implement it in the video classification task, where Resnet-18 is utilized as the audio backbone and Resnet2P1D-18 is utilized as the video backbone. Note that both Resnet-18 and Resnet2P1D-18 have four blocks. We extract the middle 2D audio features from the third block of Resnet-18 and the middle 3D video features from the third block of Resnet2P1D-18 as inputs to the NL-gate.

We show the implementation of NL-gate inFigure9. The 3D video feature is average pooled over the spatial channels into a 1D video feature. It is then tiled over the frequency channel into a 2D video feature that has the same size as the 2D audio feature. The concatenation of the 2D video feature and the 2D audio feature is used as key and value in NL-gate. The original 3D video feature is used as query in NL-gate. After audio and video features are mixed, they will be processed with a random initialized module with the same layout as the fourth Block of Resnet2P1D-18 to produce the final feature. To compute the marginal unimodal contribution, we choose to compute the Shapley values of the features generated by the last shared convolution layers (Câ€‹oâ€‹nâ€‹vVğ¶ğ‘œğ‘›subscriptğ‘£ğ‘‰Conv_{V}andCâ€‹oâ€‹nâ€‹vAğ¶ğ‘œğ‘›subscriptğ‘£ğ´Conv_{A}) before the NL-gate fusion module.

In large-scale AL experiments, the gradient embedding produced by all unlabeled data samples could be too large to be stored in the memory. To address this issue, we split the unlabeled data pool intoSğ‘†Ssmaller pools to save memory space, whereSğ‘†Sis the split size. After splitting, we queryNSğ‘ğ‘†\frac{N}{S}unlabeled samples from each smaller pool and aggregate them to form the final query set. The space complexity of BMMAL is correspondingly reduced bySğ‘†Stimes. Moreover, the sampling time complexity becomesğ’ªâ€‹(NSâ€‹BSâ€‹Dâ€‹Kâ€‹S)=ğ’ªâ€‹(1Sâ€‹Nâ€‹Bâ€‹Dâ€‹K)ğ’ªğ‘ğ‘†ğµğ‘†ğ·ğ¾ğ‘†ğ’ª1ğ‘†ğ‘ğµğ·ğ¾\mathcal{O}(\frac{N}{S}\frac{B}{S}DKS)=\mathcal{O}(\frac{1}{S}NBDK), which is also reduced bySğ‘†Stimes compared with original time complexity. We use split size of eight in the large-scale AL experiment with the VGGSound-full dataset. Although splitting might affect the AL performance, we observe that both BMMAL and BADGE still perform better than random data selection. It indicates that splitting the unlabeled data pool is acceptable in large-scale AL.

[å›¾ç‰‡: images\image_18.png]
å›¾ç‰‡è¯´æ˜: Figure 9.The implementation of NL-gate. We use the 3D video feature as query and the 2D concatenated audio and video feature as key and value.

We visualize the performance comparison of all baselines with our proposed method in all AL rounds on Food101 and KineticsSound with fusion mechanism of summation inFigure10, and the unimodal contribution among BMMAL, BADGE and Random inFigure11. As shown in the figures, our proposed method outperforms BADGE on Food101 and achieves more balanced unimodal contribution than BADGE. While on KineticsSound, our proposed method is comparable with BADGE, and it may be due to the weak fusion ability of summation.

[å›¾ç‰‡: images\image_19.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_20.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_21.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_22.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_23.png]
å›¾ç‰‡è¯´æ˜: (a)Multimodal performance comparison across AL iterations on Food101.

[å›¾ç‰‡: images\image_24.png]
å›¾ç‰‡è¯´æ˜: (a)Modality contributionÎ¦Î¦\Phiacross different AL iterations on the Food101 test set.

[å›¾ç‰‡: images\image_25.png]
å›¾ç‰‡è¯´æ˜: (a)Modality contributionÎ¦Î¦\Phiacross different AL iterations on the Food101 test set.

[å›¾ç‰‡: images\image_26.png]

[å›¾ç‰‡: images\image_27.png]

