###提示窃取攻击：一种针对大型语言模型的攻击方法与防御策略研究

本文提出了一种新的针对大型语言模型（LLMs）的攻击方法，称为“提示窃取攻击”（prompt stealing attacks）。随着诸如ChatGPT等LLMs在多个领域的广泛应用，“提示工程”（prompt engineering）成为提升模型输出质量的关键技术。为了获得高质量的回答，许多公司和个人都投入了大量的资源来优化提示设计，因此在这个背景下，攻击者可能会试图窃取这些精心设计的提示，以便达到相同的效果。

提示窃取攻击的目标是根据LLM生成的回答，推测出原始提示内容。攻击方法主要包括两个模块：参数提取器（parameter extractor）和提示重建器（prompt reconstructor）。参数提取器通过分析生成的回答，识别出提示的类型，并进一步提取出提示中使用的角色或上下文等细节信息。具体来说，提示可以分为三类：直接提示（direct prompt）、基于角色的提示（role-based prompt）和基于上下文的提示（in-context prompt）。参数提取器的任务是通过回答来判断原始提示的类型，并提取出相关参数。

在参数提取完成后，提示重建器则根据提取到的信息重新构造出与原始提示相似的反向提示。对于直接提示，重建器直接生成与原始提示相似的内容；对于基于角色的提示，重建器会加入提示中的角色信息；对于基于上下文的提示，重建器会根据提取到的上下文信息生成类似的问题和答案。这一过程的核心目的是通过重建的提示生成与原始提示相似的回答，从而达到“窃取”原始提示的效果。

实验结果表明，所提出的攻击方法在多个基准数据集上表现出色。参数提取器能够准确地预测提示的类型，重建出的反向提示与原始提示在语义和结构上具有较高的相似性，且生成的回答也能与原始回答高度一致。此外，针对这种攻击，文章还提出了两种防御策略，分别是通过修改提示内容和调整生成的回答来减少攻击效果，实验结果表明，这些防御方法虽然能够在一定程度上降低攻击的相似度，但仍无法完全防止提示窃取攻击。

本文的研究为LLM的安全性问题提供了新的视角，揭示了提示工程背后的潜在风险，并呼吁更多的关注和研究，以确保LLM系统的安全性与鲁棒性。
