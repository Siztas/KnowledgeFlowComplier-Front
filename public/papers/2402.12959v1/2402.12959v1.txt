Ê†áÈ¢òÔºöPrompt Stealing Attacks Against Large Language Models

The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ‚Äúprompt engineering,‚Äù a technology to improve the quality of model outputs.
With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge.
In this paper, we propose a novel attack against LLMs, named prompt stealing attacks.
Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers.
The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstructor.
The goal of the parameter extractor is to figure out the properties of the original prompts.
We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt.
Our parameter extractor first tries to distinguish the type of prompts based on the generated answers.
Then, it can further predict which role or how many contexts are used based on the types of prompts.
Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features.
The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts.
Our experimental results show the remarkable performance of our proposed attacks.
Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.

With the rapid development of machine learning, large language models (LLMs) like GPT-4[23]have risen to prominence as important tools in various fields, ranging from customer support[52]and academic paper writing[11,42]to complex programming tasks[39,4].
These models exhibit a remarkable ability to generate human-like text, providing immense value in diverse applications.
A growing number of individuals and organizations across different areas are beginning to take advantage of these LLMs in their daily work.
Nonetheless, the successful utilization of LLMs is not simply about deploying the models but entails a subtler, often overlooked aspect known as ‚Äúprompt engineering.‚Äù
The quality of output from these models heavily depends on the input queries or prompts provided to them.
Crafting high-quality prompts that can guide the model toward generating precise and meaningful responses is both an art and a science.

Recognizing the significance of high-quality prompts, an increasing number of companies are investing huge resources into hiring experts specialized in prompt engineering.
The expertise of these professionals lies in understanding the behavior pattern of LLMs and manipulating their inputs to achieve desired outputs.
For instance, company TikTok, among others, has established a dedicated team of prompt engineers, which has drastically improved the effectiveness of its AI-driven services.

Concurrent with this, there have been a lot of paid tutorials and courses aimed at educating individuals on the techniques of high-quality prompt design.
These educational resources underscore the growing interest and demand in the market for skills related to AI and machine learning, specifically in areas such as prompt engineering.
Several established prompt engineering techniques are currently in use, including direct prompts, where the model is directly instructed on what to do; in-context prompts, where additional information is provided to guide the response; and role-based prompts, where the prompts are designed as if the model is playing specific roles.
However, the application of these techniques and the creation of innovative prompts are heavily dependent on the nature of the tasks at hand.

[ÂõæÁâá: images\image_1.png]
ÂõæÁâáËØ¥Êòé: Figure 1:The overview of the proposed prompt stealing attacks.
The users take advantage of prompt engineering to get the desired answers from LLM.
Then, the adversary tries to reverse the original prompts through the parameter extractor and the prompt reconstructor.

As an increasing number of companies begin to make money by generating high-quality prompts for the users, a natural research problem emerged: is it possible to extrapolate these prompts based on the responses generated by LLMs?
Viewing this from another perspective, such reverse-engineered prompts could be advantageous, serving as a stepping stone for auxiliary tasks like detecting fake text[21,12,13]and generating jailbreak prompts[16,6,19,34].
Therefore, in this work, we make the initial attempt to propose prompt stealing attacks against LLMs.

Methodology.Our proposed attack is shown inFigure¬†1.
As we have stated before, the goal of the prompt stealing attacks against LLMs is to reverse the original prompts according to the given answers.
To achieve this goal, we designed two modules that can work together: the parameter extractor and the prompt reconstructor.
The goal of the parameter extractor is to steal the detailed parameter information of the prompts according to the generated answer.
We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt.
Direct prompt represents the simplest form where the user directly asks their desired questions.
Role-based prompt refers to the prompt that asks the LLMs to impersonate a specific role, such as when users use ChatGPT for movie reviews, starting prompts with ‚ÄúAssume you are a movie commentator.‚Äù
In-context prompt involves providing certain contexts to help the LLM understand the desired topic.
Based on the above observation, we hope our parameter extractor can figure out which kind of prompts the original prompts are based on the generated answers.
Our parameter extractor first incorporates a three-class classifier named the primary classifier for the type determination.
Additionally, we construct two specialized sub-classifiers for the role-based prompts and the in-context prompts.
For role-based prompts, our sub-classifier is supposed to identify which role the LLM is pretending to play according to its generated answer.
We collect 15 of the most popular roles for the LLMs based on the questionnaires to cover the most common usage scenarios.
We then use ChatGPT to decide which different prompts are based on what roles and connect them as the new prompts.
The new prompts will be used to train our 15-class sub-classifier.
For the in-context prompts, we try to train another sub-classifier that can predict the number of contexts given by the original prompts.

Upon completing the parameter extraction stage, we have detailed insights into the original prompts.
We now try to reconstruct all the prompts based on the given answers and the features we obtained before the prompt reconstruction phase.
We first leverage ChatGPT to generate the directly reversed prompts based on the given answers.
Then, if the original prompts are predicted to be direct prompts according to the parameter extractor, we will just consider the output of ChatGPT to be the prompts we need.
If the original prompts are predicted to be role-based prompts, we will add the role prompt: ‚ÄúAssume you are the predicted role,‚Äù to the output of ChatGPT, and the added prompt can be considered as the new reversed prompt.
If the original prompts are predicted to be the in-context prompts, we will also leverage the ChatGPT again to generate a certain number of similar questions and answers according to the context number we have predicted.
Similar questions and answers together can be added to the output of ChatGPT as the new reversed prompts.

Evaluation.We perform experiments on two benchmark Question-Answering datasets, RetrievalQA[2]and Alpaca-GPT4[25], and two popular LLMs including ChatGPT[1]and LLaMA[43].

In the parameter extraction phase, our experimental results indicate that the proposed parameter extractor can achieve remarkable performance.
To be more specific, the primary classifier of the parameter extractor can achieve 0.833 accuracy to predict which type of prompts the original prompts are on RetrievalQA generated by ChatGPT.
Based on the results of the primary classifier, our two sub-classifiers for the role-based prompts and the in-context prompts can also achieve great performance.
For instance, the role-based sub-classifier can achieve 0.711 accuracy on the role-based prompts generated from the RetrievalQA datasets on LLaMA.
With the help of the primary classifier and the sub-classifiers, our parameter extractor can achieve overall excellent performance.
Note that to show the necessity of the primary classifier and the sub-classifiers, we consider the baseline model, which only takes the generated answer as the input and directly outputs the detailed information, i.e., how many contexts are used.
Our experimental results show that the current parameter extractor can achieve much better results than the baseline model, which indicates that hierarchical prediction is necessary.

In this prompt reconstruction phase, we consider two metrics to measure the attack‚Äôs performance: prompt similarity and answer similarity.
Prompt similarity computes the cosine similarity between the reversed prompts and the original prompts.
Answer similarity refers to the similarity between the answers generated by the reversed prompts and the answers generated by the original prompts.
Note that we compute all the similarity based on the embedding generated by the sentence transformer[31].
Our experimental results show that the proposed prompt reconstructor can have great performance.
For instance, on the prompts from RetrievalQA, the prompt reconstructor can achieve 0.832 prompt similarity on the answers generated by ChatGPT.
We also show that, armed with the information provided by the parameter extractor, the prompt reconstructor can achieve better performance.
For instance, if there is no parameter extractor, the prompt reconstructor can only achieve 0.616 answer similarity on the RetrievalQA datasets generated by ChatGPT for role-based prompts, but the similarity can be improved to 0.703 once we know the detailed parameter information.

We then make the first attempt to mitigate the vulnerabilities caused by the prompt stealing attacks proposed in this paper.
We consider two defense strategies.
The first defense needs the defender to add some prompts on the original prompts to prevent the generated answer containing the information from the prompts.
The second defense leverages ChatGPT to modify the generated answer to fool the prompt-stealing attacks with certain prompts.
Our extensive results indicate that both defenses can make the attack similarity drop.
However, compared to the baseline, the prompt similarity and the answer similarity are still high.
Therefore, we can argue that the proposed prompt stealing attack has a certain level of robustness.

Implications.In this paper, we propose the first prompt stealing attacks against language learning models (LLMs).
Our work uncovers the risks associated with these attacks on widely used LLMs.
We believe that the research findings presented here can provide valuable guidance for developing robust LLM systems.
Furthermore, we aim to heighten attention and increase awareness about the safety and security issues inherent to LLMs.

LLMs are generation models that can generate texts conditioned by giving them the designed prompts.
These kinds of models are trained on a large number of online texts with unsupervised or reinforcement learning.
The proposal of transformer[44]can be regarded as the starting point of the advancement of LLMs.
The key idea behind the transformer is the self-attention mechanisms, which allow the models to focus on important parts of the input instead of the entire text.
Based on transformers, a generative pre-trained model (GPT)[28]was proposed to deal with different downstream tasks in the NLP domain.
It takes advantage of the pre-training and then fine-tuning module for different tasks.
GPT achieves great performance in a wide range of fields.
Based on the structure of GPT, GPT2[29]and GPT3[3]were proposed one after another with larger parameters and larger training datasets.
These models have shown an excellent ability to deal with tasks in different fields.
At the same time, they also initially showed that they have a certain ability for independent inference and thinking.
What makes LLMs really popular is the proposal of ChatGPT in November 2022.
It is based on the GPT-3.5 architecture and optimized with Reinforcement Learning from Human Feedback (RLHF)[5,41]to make the model truly intelligent.
ChatGPT[1]shows revolutionary capabilities when facing almost all daily life scenarios.

Prompt engineering is a newly existing technology for developing and optimizing prompts to better leverage LLMs for users‚Äô specific tasks.
Typically, prompts can be classified as direct prompts, role-based prompts, and in-context prompts.
For the role-based prompts, previous work has shown that with the proper role, LLMs can be used to generate toxic contents[10,9,38], game designing[24], and so on.
Also, most jail-break prompts are also role-based prompts[15].
For the in-context prompts, previous works have found that LLMs have the ability to conduct few-shot learning.
There are many new proposed in-context learning technology to boost the performance of LLMs like chain-of-thought context[48,47]and tree-of-thought context[51,50].
However, since there is no consensus on which type of context is better, in this work, we only consider the most obvious in-context prompts: directly using the text as the context.

In this paper, we refer to the prompts that were used to generate the given answer as original prompts and the prompts we have stolen as reversed prompts.
We will first introduce the adversary‚Äôs goal and then emphasize the background knowledge the adversary can obtain.

To successfully steal the original prompts based on the given answer, the goal of prompt stealing attacks should follow the following points.

Exact Parameter Information.The first goal of the adversary is to try to infer the exact parameter information according to the generated answers.
To be more specific, the adversary should be able to predict which kind of prompts the original prompts are.
The knowledge about parameter information can foster the whole process of prompt stealing.

Similar Reversed Prompt.The reversed prompts should be similar to the original prompts.
They should possess similar semantics and share similar structures.
Also, the reversed prompt should be able to generate a similar answer as the original prompts.
This is also the primary goal of our proposed prompt stealing attacks.

We assume the adversary has the following background knowledge regarding the original prompts.

Access to the Generated Answers.We assume the adversary can have access to the answers generated by the original prompts.
Note that the adversary should have no knowledge about the original prompts.

Knowledge About LLMs.Regarding the knowledge of LLMs that are leveraged to generate answers, we assume the adversary can know what kind of LLMs are used.
It is a reasonable assumption as most of the answers shared on the internet have classified which model they are used to generate.

In this section, we introduce the proposed prompt stealing attacks against large language models.
We first describe how we steal the detailed parameter information through the proposed parameter extractor module.
Then, we further introduce the prompt reconstructor module, where we can reverse the original prompts.

[ÂõæÁâá: images\image_2.png]
ÂõæÁâáËØ¥Êòé: Figure 2:The overview of the method we have proposed.
It involves two stages: parameter extraction and prompt reconstruction.
In the parameter extraction phase, the extractor tries to predict the type of prompts and further information.
With the parameter information and the given answers, the prompt reconstructor tries to generate the reversed prompts.

Overview.The basic idea of parameter extraction is to infer the parameters of the prompts according to the generated answers.
The detailed parameters of the prompt can be defined as the type and the further structure information of the original prompt.
To effectively implement the functionality of parameter extraction, we first conduct the prompt categorization through the questionnaire answered by 10 users who use ChatGPT frequently.
Based on the results of our questionnaire, we find that most popular prompts fall into three categories, including direct prompts, role-based prompts, and in-context prompts.
Then, we construct the training dataset containing different kinds of prompts to train our classifiers.
Our parameter extractor contains three classifiers: one primary classifier and two sub-classifiers.
The primary classifier aims to determine which type of prompt is.
The sub-classifiers aim to determine further information based on the prediction of the primary classifier.
In the end, the proposed parameter extraction can be used to predict which kinds of prompts the original prompts are and the detailed structure information.

Prompts Categorization.We first categorize prompts based on their types and structures.
We designed the questionnaire to ask 10 users who use ChatGPT frequently about what types of prompts they always use.
Then, we collect the answers and summarize the results.
According to the results of the questionnaire, we find that the most popular prompts fall into three main types: direct prompts, role-based prompts, and in-context prompts.
We show some examples inTable¬†1.
Based on the categorization, we will next construct the datasets and the parameter extractor.

Data Collection.We then try to collect different prompts to construct our dataset.
In order to align different prompts in their distribution, we leverage one direct prompt dataset and expand it to the role-based prompt and in-context prompt.
We take the following steps for the expansion:

Role-Based Prompts.We first collect 15 roles that are most commonly used recently.
It is collected by the questionnaire answered by 10 users who use ChatGPT frequently.
It is concluded inTable¬†8in the Appendix.
Regarding how to assign different roles to different prompts, we consider two cases.
In the first case, we assign the roles to different prompts according to their relevance according to the rating of ChatGPT.
In this case, we design the prompt: ‚ÄúWhich following role [listed role] are you assumed to be.‚Äù
This is the most common case due to that the role-based prompt tends to find the most suitable role.
In the second case, we assign the collected roles at random to observe how the role affects the generated answer.

In-Context Prompts.To generate the in-context prompts, we randomly select a certain number of existing prompts and corresponding answers as the context and add them to the original prompts.¬†We try to figure out whether different contexts have impacts on the generated answer.

Parameter Extraction.We then construct our parameter extractor according to the above categorization and data collection as shown inFigure¬†2.
Our parameter extractor consists of one primary classifier and two sub-classifiers.
The primary classifier aims to predict which kind of prompt the original prompt is.
We build the primary classifier (i.e., a three-class classifier) that accepts the generated answers as the input and outputs the predicted type, i.e., direct prompt.
We take advantage of the pre-trained BERT[8]model as the backbone of our primary stealer.
Note that to evaluate the generalizability of the trained primary classifier, we test the primary classifier on unknown prompt datasets.
Based on the prediction result of the primary classifier, we train two sub-classifiers for the detailed parameter information.
If the prompt is predicted to be a role-based prompt, one sub-classifier will be leveraged to determine which role the prompt belongs to.
This sub-classifier is trained on the role-based data we built before.
If the prompt is predicted to be the in-context prompt, another sub-classifier can be used to further tell how many contexts are used in the original prompts.
This sub-classifier is trained on the in-context data mentioned before.
In general, the proposed parameter extractor can be used to infer the parameter information of the original prompts through the well-constructed primary classifier and two sub-classifiers.

Overview.In the prompt reconstruction phase, we aim to reconstruct the whole prompt, which should be similar to the original prompts.
We take advantage of the previous classifier‚Äôs results by designing a prompt reconstruction module.
We first leverage ChatGPT to directly generate the reversed prompts according to the given answers.
Then, we add different information based on the output of the previous parameter extractor.
We will show later in the experiment phase that the proposed prompt reconstruction process can achieve great performance.

Direct Prompt Reconstruction.We leverage ChatGPT To generate direct prompts based on the given answers.
We design the reconstruction prompts: ‚ÄúWhat question are you asked if you can generate the following answer?‚Äù
Then we input the given answer, and the ChatGPT will return the directly reversed prompt.

Assembly Composition.After the generation, we can construct the reversed prompt based on the detailed parameter information we obtained before.
To be more specific, we take the following steps for different types of prompts:

Role-Based Prompt.If the original prompt is the role-based prompt, we will add ‚ÄúAssume you are a ‚Äù together with the predicted role to the directly reversed prompt to get a new role-based reversed prompt.

In-Context Prompt.If the original prompt is the in-context prompt, we will leverage ChatGPT to generate other contexts based on the number of previously predicted results.
Note that the prompt to generate context can be designed as ‚ÄúCan you generate [predicted number] questions and corresponding answers similar to [the directly reversed prompt].‚Äù
The generated context, together with the directly reversed prompt, can be considered as the new reversed prompt.

In conclusion, our proposed prompt stealing attacks against LLMs are composed of the parameter extractor and the prompt reconstructor.
The goal of our parameter extraction is to predict which type of prompts is used to generate the given answers.
We construct our parameter extractor using one primary classifier and two sub-classifiers.
The primary classifier can be used to predict whether the original prompts belong to the direct prompt, role-based prompt, or in-context prompt.
Then, based on the output of the primary classifier, we further develop two sub-classifiers to further determine which role is used if the original prompt is the role-based prompt and how many contexts are used if the original prompt is the in-context prompt.
Together, the combination of the primary classifier and the sub-classifiers can be considered our parameter extractor.
The goal of the prompt reconstructor is to reverse the exact prompt that should be similar to the original prompts.
We take advantage of the information provided by the parameter extractor and leverage ChatGPT to generate the prompts.
We show in further experiments that the proposed parameter extractor and prompt reconstructor can have much better performance than the baseline.

In this section, we first introduce the experimental setup for the proposed attacks, including the target models, datasets, and evaluation metrics.
Then, we introduce the performance of the parameter extractor and the prompt reconstruction.

Large Language Models.To evaluate the effectiveness of the proposed prompt stealing attacks, we leverage two different kinds of large language models, including ChatGPT and LLaMA, for the experiments.

ChatGPT[1].ChatGPT is the advanced large language model based on GPT-3.5‚Äôs architecture.
It can generate human-like answers for different tasks.
ChatGPT is optimized by the Reinforcement Learning from Human Feedback (RLHF) to achieve great performance.

LLaMA[43].LLaMA is another state-of-the-art large language model released by Meta.
It is designed to generate human-like texts, solve mathematical theorems, and other tasks.
LLaMA is also based on the transformer and can be considered as the next word predictor.
Different from ChatGPT, LLaMA is open source and can be deployed on a personal server.

Note that all of our models will only be trained on the ChatGPT‚Äôs data.
LLaMA will be used to test the generalizability of the proposed attacks, as it can serve as a different target model.

Prompt Datasets.Currently, there are numerous studies that aim to evaluate the performance of LLMs.
Therefore, in this work, we take advantage of existing question-answering datasets to conduct our attack.

Alpaca-GPT4[25].The Alpaca-GPT4 was introduced to replicate the performance of ChatGPT and GPT-4.
It covers a wide range of usage scenarios for LLMs.
Therefore, Alpaca-GPT4 is used in this work as the original prompts datasets.

RetrievalQA[2].RetrievalQA is another dataset built for the evaluation of using generative LLMs to augment training data for retrieval models in different domains.
It contains question prompts from different aspects of daily life.

Note that due to the fact that querying ChatGPT costs lots of time and money, we only randomly selected 600 prompts from the two datasets.
For the RetrievalQA, we divide 500 prompts as the training prompts for our attack.
The remaining prompts from RetrievalQA and all prompts we have selected from Alpaca-GPT4 are considered test prompts.

Evaluation Metric.For the parameter extraction, we use accuracy, recall, precision, and AUC to evaluate different prediction tasks.
For the prompt reconstruction, we leverage the following metrics for the evaluation.

Prompts Similarity (PS).As our primary goal is to make the reversed prompts similar to the original prompts, computing the similarity between the two prompts is the most obvious metric.
We leverage the sentence transformer to generate the embeddings for both reversed and original prompts and then compute the similarities between them.
More specifically, we compute the similarities as follows:

whereR‚Äã_‚ÄãPùëÖ_ùëÉR\_Prepresents reversed prompts andO‚Äã_‚ÄãPùëÇ_ùëÉO\_Prepresents original prompts.

Answers Similarity (AS).Note that the motivation for the prompt stealing is to reverse the prompt, which can have similar answers to the original prompts.
Therefore, evaluating the similarities between the generated answers of reversed prompts and original prompts can also be considered an effective metric.
To compute such similarity, we first generate two answers based on the reversed and original prompts.
Then, similar to the above answer similarity computing process, we leverage a sentence transformer to generate two embeddings and compute the similarities as follows:

where R_A represents answers generated by reversed prompts and O_A represents answers generated by original prompts.

In this section, we first show the performance of the primary classifier for parameter extraction.
Then, we evaluate the proposed two sub-classifiers based on the prediction results from the primary classifier.

Evaluation Results for Primary Classifier.We show the evaluation result for the primary classifier inTable¬†2.
the goal of the primary classifier is to determine whether the given answer is generated by the direct prompt, role-based prompt, or in-context prompt.
Note that due to the limitation of our computing resources and the input limitation of LLaMA, the LLaMA model cannot take the long context as the input.
Therefore, in the results shown inTable¬†2, the primary classifier for LLaMA can be considered as the two-class classifier‚Äôs results which only contain direct prompts and role-based prompts.

From the table, it can be seen that the proposed classifier can achieve great performance in all the cases.
For instance, to determine the prompts of the answers generated by ChatGPT, the classifier can achieve 0.833 accuracy on RetrievalQA datasets and 0.811 performance on Alpaca-GPT4 datasets.
We emphasize here that for different models (ChatGPT and LLaMA), we train the classifier on one dataset (RetrievalQA) and then test it on both the RetrievalQA test set and Alpaca-GPT4.
The performance on Alpaca-GPT4 further demonstrates that the trained primary classifier shows great generalization ability against the different distributions of original prompts.
Also, we can find that, overall, the performance of the primary classifier (3-class classification) on ChatGPT is similar to the performance on LLaMA, while the primary classifier for LLaMA is the 2-class classification.
That is because, compared to ChatGPT, the output generated by LLaMA is not that clear and related to the original prompts.
We show one example inTable¬†3.
Therefore, the difference between the performance of the proposed attacks on the two models can be considered as the difference between the generation abilities of the two models.
It can be concluded that the proposed primary classifier can work very well on the two current popular large language models.

Evaluation Results for Sub-Classifiers.Based on the prediction results, we train our sub-classifiers for different downstream tasks.
We now show the performance of our sub-classifiers in this section.
Our proposed sub-classifiers have different functions for different prediction results.
For instance, if the primary classifier predicts that the given answers are generated by the role-based prompt, then one sub-classifier can be used to predict which role is used in the original prompts.
If the prediction results of the primary classifier are the in-context prompts, then another sub-classifier should be leveraged to predict the context number used in the original prompts.
Otherwise, if the original prompts are just direct prompts, then no sub-classifier is needed for further prediction.Table¬†2can summarize the performance of sub-classifiers for different tasks.

It can be seen from the table that our sub-classifiers for different tasks can still achieve outstanding performance.
For instance, to predict which role is used in the original prompts, the sub-classifier can achieve 0.732 accuracy on answers from RetrievalQA generated by ChatGPT, which is a 15-class classification task.
From the results, we can also conclude that the sub-classifier for the context number has worse performance than the sub-classifiers for the role.
For instance, as the 13-class classification, the sub-classifier for the role can still have better performance (0.732) than the sub-classifier for context number (0.614), which is the 4-class classification.
The results reveal that the set role has much more impact on the quality of the generated answers than the number of contexts.
However, whether the prompts contain the context or not can still have a significant impact on the generated answers because the primary classifier can achieve great performance.
Also, as LLaMA cannot take inputs that are too large, we only test the direct prompt and role-based prompt, as we stated before.

[ÂõæÁâá: images\image_3.png]
ÂõæÁâáËØ¥Êòé: (a)RetrievalQA

[ÂõæÁâá: images\image_4.png]
ÂõæÁâáËØ¥Êòé: (a)RetrievalQA

Overall Performance.Together with the primary classifier and the sub-classifiers, we construct our parameter extractor.
The above section shows the outstanding performance of different models used in the parameter extractor.
In this section, we show the overall performance of the whole module.

The results are summarized inFigure¬†3.
It can be seen that with different classifiers together, our proposed parameter extractor can have great performance.
Note that we show the baseline results by directly training the 20-class classifier as the prompt extractor.
The baseline model can directly predict which role or how many contexts are contained in the original prompts in one model as the 20 classification tasks.
We can see from the figure that the idea of using different classifiers to conduct the hierarchical prediction can largely boost the performance of the parameter extractor.
For instance, the overall performance on RetrievalQA generated by ChatGPT can achieve 0.693 accuracy, while the baseline can only achieve 0.234 accuracy.
Note that the hierarchical parameter extractor excels not just in terms of performance but also has extra advantages when utilized with new kinds of prompts associated with frequently employed roles.

Takeaways.In conclusion, the proposed parameter extractor can achieve great performance on the existing popular LLMs and different distributed datasets.
To be more specific, as the first part of our extractor, the primary classifier aims to predict which type of original prompts can achieve outstanding performance even on the different distributed data.
The following sub-classifiers can also work very well in their different tasks.
Together with the primary classifier and the sub-classifiers, our parameter extractor can work very well to steal the parameter information of the original prompts based on the generated answers.

We show the great performance of the proposed parameter extractor in the previous section.
In this section, based on the results of the parameter extractor, we further show the performance of the proposed prompt reconstructor.
Our proposed prompt reconstructor aims to reverse the exact prompts that should be similar to the original prompts.
We leverage prompts similarity (PS) and the generated answer similarity (AS) as the metrics to conduct the evaluation.

We show the results inTable¬†4.
It can be seen that apart from the sentence similarity, the structure and the tone used in the original sentence are also perfectly imitated.
As we take advantage of ChatGPT for the reconstruction task, it can be concluded that ChatGPT has a remarkable ability in reverse engineering.
For instance, when using ChatGPT to generate the reversed prompt for the direct prompt, the model can achieve 0.832 PS and 0.768 AS on ChatGPT.
Although ChatGPT is good at prompt reconstruction, we can also conclude from the results that the information provided by the parameter extractor can boost the performance of prompt reconstruction.
To show the necessity of the previous parameter extractor, we consider the naive version of the prompt stealing attack as the baseline for the prompt reconstruction, which just uses the generated answers to generate the reversed prompts.
Note that as the baseline is just the case for the direct prompt, the results of the baseline and the direct prompt are the same.
It can be seen from the table that the proposed prompt reconstruction can achieve great performance with high PS and high AS.
For instance, the reconstructor can achieve 0.803 PS and 0.703 AS when reversing the role-based prompt from RetrievalQA generated by ChatGPT.
Hence, we can postulate that together with the ChatGPT‚Äôs remarkable reversing ability and the information we obtained from the parameter extractor, we can conduct very successful prompt stealing attacks against LLMs.

Note that as we can not reverse the exact context for the in-context prompt, the PS for the in-context prompt is very low.
This is due to the fact that the answer response to the in-context prompt cannot reveal the context information that is randomly selected from the original datasets.
The role of the context is to help the model better understand and grasp the train of thought for problem-solving.
Therefore, although the PS is very low, the AS for the in-context prompt is good enough to be considered a successful attack.

Different from the in-context prompt, the reversed role-based prompt can achieve better performance in both PS and AS.
This result indicates that for the role-based prompt if the reversed prompt is more similar to the original prompt, the generated answer can be more similar to the original answer.
If we take into account all the results collectively, it can be concluded that the AS is the better metric for prompt reconstruction tasks.
This is due to the fact that the final goal of the prompt stealing attacks is to generate prompts that can have similar corresponding answers as the original prompts.
Therefore, even if the PS is not that high (i.e., reversed in-context prompt), the similar generated answer can still be regarded as a successful attack.

Takeaways.In conclusion, we perform the evaluation on the prompt reconstruction.
Our results show that the proposed prompt reconstructor can have great performance in reversing the exact prompts.
Also, we find that AS can be considered a better metric to measure the quality of the reversed prompts than the prompt similarity.

[ÂõæÁâá: images\image_5.png]
ÂõæÁâáËØ¥Êòé: (a)ChatGPT

[ÂõæÁâá: images\image_6.png]
ÂõæÁâáËØ¥Êòé: (a)ChatGPT

[ÂõæÁâá: images\image_7.png]
ÂõæÁâáËØ¥Êòé: (a)ChatGPT

[ÂõæÁâá: images\image_8.png]
ÂõæÁâáËØ¥Êòé: (a)ChatGPT

Impacts of the Training Sets Size.Due to the limited computational resources, we only leverage 500 of the prompts and their corresponding answers as the training set to train our parameter extractor.
In this part, we will analyze the impacts of the training set size on the parameter extractor‚Äôs performance.
We show our results of different numbers of training prompts of the primary classifier inFigure¬†4.
We also show the results for the sub-classifier for role-based prompts inFigure¬†5and for the sub-classifier for in-context prompts inFigure¬†6.
Note that, as we stated before, due to the input limitation of LLaMA, we do not have the results of in-context prompts on LLaMA.
It can be seen from the figure that with more training prompts, our parameter extractor can achieve better performance.
For instance, for the primary classifier on the RetrievalQA generated by ChatGPT, with only 100 training prompts, the classifier can only achieve 0.507 accuracy, while it can be improved to 0.833 if there are 500 prompts on RetrvialQA.
It can also be observed that even the number of prompts has a great impact on the classifier‚Äôs performance.
When this number comes to a certain stage, i.e., 200, the impacts will be very limited.
For instance, when the number of training prompts increases from 100 to 300, the performance of the primary classifier for Alpaca-GPT4 generated by ChatGPT can be improved from 0.412 to 0.793.
However, when the training prompts number increases from 300 to 500, the improvement in the performance is just 0.08.
Therefore, it can be concluded that although increasing the number of prompts can further improve the parameter extractors‚Äô performance, the current number of prompts is sufficient to verify the feasibility of the attack.

[ÂõæÁâá: images\image_9.png]
ÂõæÁâáËØ¥Êòé: Figure 6:Performance of the sub-classifier for in-context prompts of the parameter extractor with different numbers of training prompts.

Roles Assigned Based on Semantics V.S.¬†Roles Assigned Randomly.In the previous section, for the role-based prompt, we assign roles to different prompts according to the semantic correlation computed by ChatGPT.
In this section, we evaluate the performance of the sub-classifier for the role-based prompts if the roles are randomly assigned.
We show our results inTable¬†5.
We can see from the table that when the roles are randomly assigned, the classifier cannot make the right predictions according to the answers.
For instance, on RetrievalQA, the classifier can only achieve 0.087 performance on the answers generated by ChatGPT.
Note that the poor performance of our sub-classifier on the randomly assigned roles has limited influence on the feasibility of our proposed attack.
This is because the role assigned under normal scenarios should be aligned with the following prompt.
Nevertheless, the results reveal that if the role is assigned to be aligned with the given prompt, it can boost the performance of LLMs as our classifier can easily figure out the difference between the direct prompt and the role-based prompt.
However, if the role is assigned randomly, the role-based prompt cannot improve the performance of the LLMs.

Different Prompts for the Prompt Reconstruction.As we have stated before, for prompt reconstruction, we leverage ChatGPT to generate the naive reversed prompt based on the answer.
However, the prompts that are used to generate the reversed prompt (we call them the reconstruction prompt) may influence the final performance.
Therefore, in this section, we study different prompts‚Äô impacts on prompt reconstruction.

As our role-based prompts and in-context prompts are generated based on the naive reversed prompts, in this section, for the sake of simplicity in the experiment, we only consider the direct prompts as the original prompts.
We collect three different reconstruction prompts and compute the similarities.
Our reconstruction prompts are designed based on the direct prompt, role-based prompt, and in-context prompt.
We show the results inTable¬†6.
It can be seen that different prompts have limited influence on the reversed prompt.
For instance, the direct reconstruction prompt can achieve 0.832 similarity while the performance of the role-based reconstruction prompt is similar (0.865 similarity).
We also provide some examples of the reversed prompt inTable¬†6.
It can be seen that although different reversed prompts have different structures, they all capture the key contents in the original prompts: movie and newborn baby.
Through the example, we can more intuitively see that different reconstruction prompts do not make significant impacts on the reversed prompts.

As we have shown the outstanding performance of the proposed prompt stealing attacks, in this section, we discuss the potential defenses to mitigate these novel attacks.
We first summarize the defender‚Äôs goal and capabilities to mitigate the prompt stealing attacks.
We propose two defense strategies, named prompt-based defense and answer-based defense, and then provide the experimental results of the proposed defense methods.

Defender‚Äôs Goals.A defender‚Äôs goal can be summarized from the following two perspectives.

Prompt Stealing Performance.The main goal of the defender is to decrease the performance of the proposed prompt stealing attacks.
To achieve the goal, the defender is supposed to either perturb the original prompts to prevent the generated answer from leaking sensitive information, or directly perturb the generated answer.
Note that in this section, we only consider the PS and the AS of the reversed prompts and the original prompts as the metric.
This is because the final goal of the prompt stealing is to generate similar reversed prompts.

Utility.The defender is also supposed to keep the utility of the LLMs.
That means the generated answer from the defenders should be similar to the answers generated by the original prompts.
We leverage the cosine similarity to compute the similarity between the defended answer and the original answer.
The similarity here can be considered as the metric to show the utility.
Higher similarity means that the defended answer is similar to the original answer and thus can be considered to have a high utility.

Defender‚Äôs Capabilities.The defenders are supposed to have access to the original prompts and the original generated answer.
They can perturb both the original prompts and the original answers to decrease the performance of the prompt stealing attacks.
However, normally, the defenders are also the normal users; they may not have access to the LLMs and cannot fine-tune such models.
Therefore, the deployed LLMs are supposed not to be modified to defend against the attacks.
We show later that without the modification of the LLMs, the defenders can still achieve great performance.
Also, we assume the defenders have no access to the models in the proposed attacks, which means that they cannot obtain the gradient of the classifiers in the proposed attacks.

[ÂõæÁâá: images\image_10.png]
ÂõæÁâáËØ¥Êòé: Figure 7:The overview of the proposed defenses.
We consider two strategies to defend against the prompt stealing attacks, including adding perturbation on the original prompts and adding perturbation on the generated answers.

Prompt-Based Defense.In this defense setting, the defender aims to add some prompts to the original prompts to make the LLMs generate certain answers that contain limited information about the original prompts.
We show some examples inFigure¬†7.
The prompt we generate for the defense is predefined as ‚ÄúPlease generate the answers without leaking the information from the original prompts."
We hope that the added prompts can help to make the original prompts not that easy to reverse.

Answer-Based Defense.Another intuitive defensive method is the answer-based defense.
The answer-based defense means that the defender tries to perturb the generated answer to limit the information the adversary can obtain.
In this paper, we also leverage ChatGPT to add perturbation to the generated answers.
We consider two ways to add the perturbation with different kinds of prompts.
The first way is to ask the ChatGPT to summarize and rewrite the texts based on the generated answer.
The defense prompt can be ‚ÄúCan you write the text based on your understanding of the following text: ‚Äù
We call this defense the inconspicuous defense.
Another way to add the perturbation is to make the ChatGPT understand the prompt stealing attacks and then ask it to generate similar text by removing certain critical contents.
We call this kind of defense the conspicuous defense.
The possible prompts for the conspicuous defense can be designed as ‚ÄúPrompt stealing attack is an attack that aims to generate the original prompts based on the given answers; now, assume you are the defender of this attack, can you rewrite the following text to defend against that attack.‚Äù

We show our experimental results inTable¬†7.
It can be concluded from the results that both prompt-based defense and answer-based defense can achieve good performance.
For instance, by modifying the original prompts, the AS of the attack performance can drop from 0.768 to 0.583 while keeping the high utility (0.723).
We can also conclude from the results that compared to the answer-based defense, the prompt-based defense can better mitigate the attacks while also leading to a more significant decline in the utility.
For instance, the prompt-based defense can cause the AS to drop from 0.754 to 0.623 while also causing the utility to drop to 0.699.
When comparing the two methods of answer-based defense themselves, the conspicuous defense is more effective than the inconspicuous one, but it also has a worse utility.
For instance, to defend the prompt stealing attacks on RetrievalQA prompts generated by ChatGPT, the conspicuous defense can make the AS drop from 0.768 to 0.486, while the inconspicuous defense can only cause the drop of 0.077.
However, the utility of conspicuous defense (0.813) is lower than that of the inconspicuous defense (0.980).
It is due to the fact that the inconspicuous defense does not forcibly require the ChatGPT to abandon some of the key information in the original answer, which may leak the information of the original prompts.
From the above results, we can also conclude that the defense against prompt stealing attacks is the trade-off game between the attack similarity and the utility.
If the proposed defense is very effective, then it will cause an obvious drop in the utility.
If the defense does not cause the utility to drop a lot, it tends to not be that effective in defending against the proposed attacks.
Note that all of the proposed defense methods take a lot of time to ask LLMs to make the modification for the users.
Therefore, more automated defense, such as combining all the defense prompts into the system, is needed to provide effective defense without significantly increasing the user‚Äôs operational difficulty.

Takeaway.In conclusion, we propose three different defense strategies against the proposed attacks.
Our experimental results show that all proposed defenses can reduce the risks of the proposed attacks to a certain extent.
However, they can also cause a drop of the utility.
It can be concluded that the defense strategies we currently propose only achieve a subpar trade-off.
Therefore, we call for better defense methods.

Recent advancements in LLMs have facilitated a broad spectrum of applications, ranging from chatbots[9,32,37,54,49]to the medical diagnoses[46].
As LLMs become more prevalent in various domains, researchers begin to study and address the potential vulnerabilities of the LLMs.
Currently, the security research regarding the LLMs develops around the following aspects: toxicity, stereotypes, adversary robustness, and privacy issues.
Previous studies.[7,18,26]have discussed the possible prompts to trigger the LLMs to generate the toxic contents.
Deshpande et al.[7]found that with the proper roles assigned to the LLMs, it can be very mean when generating normal contents.
Liang et al.[18]study what types of stereotypes are more likely to be generated by the LLMs.
Regarding adversary robustness, AdvGLUE[45]was proposed to benchmark the LLMs with different state-of-the-art adversary example attacks.
There are also several papers to discuss the privacy issues, such as membership inference attacks[36,14]or attribution inference attacks[20,40]against LLMs.
However, currently, there is no paper that views the security issues of LLMs from the perspective of the prompts.
In this paper, we propose the first prompt stealing attacks to prove that the security issues of the prompts are also a significant part of the security issues of LLMs.

Before the advent of the current work, a substantial amount of research was dedicated to conducting prompt stealing attacks against text-to-image generation models[33,30].
The simplest way[17,22]is to leverage the image captioning models.
In particular, a remarkable proposal by Mokady et al.[22]introduced ClipCap, a strategy that capitalizes on CLIP[27]to translate an image into a prompt effectively.

Taking this concept further, Shen et al.[35]went on to propose specialized datasets that could be employed to carry out prompt stealing attacks against text-to-image models.
These datasets provided a novel resource for researchers, opening up new opportunities for exploring the vulnerabilities of text-to-image models and refining the techniques used for prompt stealing attacks.

With respect to LLMs, Zhang et al.[53]made the intriguing discovery that the system prompt can be readily reversed.
This finding implies that LLMs, while robust and powerful, may still be vulnerable to prompt stealing attacks, potentially presenting a significant concern in LLM security.
However, there are no prompt stealing attacks based on the generated answers from LLMs currently.

In this paper, we propose the first prompt stealing attacks against LLMs.
Our prompt stealing attacks aim to steal the prompts based on the corresponding answers generated by the LLMs.
Our attacks contain two hierarchical parts: the parameter extractor and the prompt reconstructor.
The goal of the parameter extractor is to predict the properties of the original prompts.
We first observe that most of the prompts fall into three categories: direct prompt, role-based prompt, and in-context prompt.
Therefore, the first classifier in the proposed parameter extractor is to predict which kind of prompts the original prompts are.
Based on that prediction, two sub-classifiers are developed to determine the further role or the number of contexts.
Following the parameter extractor, the prompt reconstructor is developed to generate the reversed prompts, which are assumed to be similar to the target prompts.
We leverage ChatGPT as the backbone of our prompt reconstructor to generate the naive reversed prompts based on the given answers.
Then, the previous information is used to modify the naive reversed prompts.
To be more specific, if the parameter extractor predicts the original prompts to be role-based prompts, then we will add the role-based prompt to the naive prompts to construct the new reversed prompts.
If the original prompts are predicted to be the in-context prompts, then a certain number of the contexts will be generated and added to the naive prompts as the reversed prompts.
Extensive results show that the proposed prompt stealing attacks can have great performance.
In particular, the parameter can successfully predict the type of the original prompts and further information.
Armed with the parameter information, the prompt reconstructor can generate more similar reversed prompts.
We also propose two possible defenses against prompt stealing attacks, and the results show there is a fair trade-off between the defense performance and the utility.

Our research emphasizes that prompts are not secrets and can be easily stolen from their generated responses, leading us to urge the scientific community to focus on the exploration and design of more effective defenses against prompt stealing attacks.
We also look forward to more security research and discussions on LLMs from the perspective of the prompt.

[ÂõæÁâá: images\image_11.png]

[ÂõæÁâá: images\image_12.png]

