标题：Multimodal Learning with Transformers:A Survey

Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks.
Thanks to the recent prevalence of multimodal applications and big data,
Transformer-based multimodal learning has become a hot topic in AI research.
This paper presents a comprehensive survey of Transformer techniques oriented at
multimodal data.
The main contents of this survey include:
(1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era,
(2) a systematic review ofVanillaTransformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective,
(3) a review of multimodal Transformer applications, via
two important paradigms,i.e., for multimodal pretraining and for specific multimodal tasks,
(4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and
(5) a discussion of open problems and potential research directions for the community.

The initial inspiration of Artificial Intelligence (AI)
is to imitate human perception,e.g., seeing, hearing, touching, smelling.
In general, a modality is often associated with a specific sensor that creates a unique communication channel, such as vision and language[1].
In humans,
a fundamental mechanism in our sensory perception is the ability to leverage multiple modalities of perception data collectively in order to engage ourselves properly with the world under dynamic unconstrained circumstances, with each modality serving as a distinct information source characterized by different statistical properties.
For example, an image gives the visual appearance of an “elephants playing in water” scene via thousands of pixels, whilst the corresponding text describes this moment with a sentence using discrete words.
Fundamentally, a multimodal AI system needs to ingest, interpret, and reason about multimodal information sources to realize similar human level perception abilities.
Multimodal learning (MML) is a general approach to building AI models that can extract and relate information from multimodal data[1].

This survey focuses on multimodal learning with Transformers[2](as demonstrated in Figure1), inspired by their intrinsic advantages and scalability in modelling different modalities (e.g., language, visual, auditory) and tasks (e.g., language translation, image recognition, speech recognition) with fewer modality-specific architectural assumptions (e.g., translation invariance and local grid attention bias in vision)[3].
Concretely, the input to a Transformer could encompass one or multiple sequences of tokens, and each sequence’s attribute (e.g., the modality label, the sequential order),
naturally allowing for MML without architectural modification[4].
Further, learning per-modal specificity and inter-modal correlation
can be simply realized by controlling the input pattern of self-attention.
Critically, there is a recent surge of research attempts and activities across distinct disciplines exploring the Transformer architectures, resulting in a large number of novel MML methods being developed in recent years, along with significant and diverse advances in various areas[4,5,6,7,8].
This calls for a timely review and summary of representative methods to enable researchers to understand the global picture of the MML field across related disciplines and more importantly to capture a holistic structured picture of current achievements as well as major challenges.

TaxonomyFor better readability and reachability from and across different disciplines,
we adopt a two-tier structured taxonomy
based on the application and challenge dimensions respectively.
This has several benefits:
(1) Researchers with expertise in specific applications
can find
those applications appropriate to their own research domain
before connecting to other related domains.
(2) Similar model designs and architectures developed in different domains
can be summarized in an abstract, formula-driven perspective so that the mathematical ideas of various models formed in different applications can be correlated and contrasted on common ground, crossing domain-specific restrictions.
Crucially, our taxonomy offers an interesting stereo-view of individual works
with the insights in both application specificity and formulation generality.
It is hoped that this can help to break down domain boundaries
and foster more effective idea communication and exchange across modalities.
By using the prompt modelling strategy[9,10]as a basis for investigation, we also include
the classical classification problem (e.g., image classification) – usually regarded
as a single modality learning application in conventional MML surveys[1,11,12]–
as a special MML application.
This has the potential to significantly enrich MML, as the classification problem
is an AI topic amongst the most extensive studies in the literature[13].

ScopeThis survey will discuss the multimodality specific designs of Transformer architecture including, but not limited to, the following modalities:
RGB image[5], depth image[14],multispectral image[15], video[7], audio/speech/music[16,17,14],
table[18], scene graph/layout[19,20,21,22],
pose skeleton[23], SQL[24,25],
recipe[26], programming language[27],
sign language[28,29,30],
point cloud[31],
symbolic knowledge (graph)[32,33], multimodal knowledge graph[34], sketch drawing[35,36,37,38], 3D object/scene[39,40,41], document[42,43], programming code[44]and Abstract Syntax Tree (AST) – a kind of graph[45], optical flow[46],
medical knowledge (e.g., diagnosis code ontology[47]).
Note that this survey will not discuss the multimodal papers where Transformer is used simply as the feature extractor without multimodal designs.

Related SurveysWe relate this paper to existing surveys
of the two specific dimensions MML and Transformers.
There exist a few MML surveys[1,11,12].
In particular,[1]proposed a structured, acknowledged taxonomy by five challenges, which we also adopt
as part of our structure.
Unlike[1,11], and[12], which review general machine learning models,
we instead focus on Transformer architectures and their self-attention mechanisms.
Several surveys dedicated to Transformers have been recently introduced, with a range of emphases including
general Transformers[48],
efficient designs[49],
visualization[50],
computer vision tasks[51,52,53,54],
medical imaging[55],
video tasks[56], and
vision language pretraining[57].
While[51,53,55,54]consider MML,
their reviews are somewhat limited in the scope, taxonomy, and coverage.
To our knowledge, only a few surveys on video-language pretraining (VLP)[57,58,59]are relevant to MML.
However, VLP is only a subdomain of MML.
In this survey, we focus solely on the intersection of multimodal learning and Transformers.

[图片: images\image_1.png]
图片说明: Figure 1:Overview of Transformer[2].

FeaturesTo our knowledge, this paper is the first comprehensive review of the state of Transformer based multimodal machine learning.
The major features of this survey include

(1)We highlight that Transformers have the advantage that they can work in a modality-agnostic way. Thus, they are compatible with various modalities (and combinations of modalities). 
To support this view, we, for the first time, offer an understanding of the intrinsic traits of Transformers in a multimodal context from a geometrically topological perspective.
We suggest that self-attention be treated as a graph style modelling, which models the input sequence (both uni-modal and multimodal) as a fully-connected graph.
Specifically, self-attention models the embedding of arbitrary tokens from an arbitrary modality as a graph node.

(2)We discuss the key components of Transformers in a multimodal context as mathematically as possible.

(3)Based on Transformers, cross-modal interactions (e.g.,
fusion, alignment) are essentially processed by self-attention
and its variants.
In this paper, we extract the mathematical essence and formulations of Transformer based MML practices, from the perspective of self-attention designs.

ContributionsHaving presented our review of the landscape of multimodal learning, Transformer ecosystem, and multimodal big data era in Section2,
we summarize our main contributions as the follows.

(1)In Section3, we present a systematic reviewing ofVanillaTransformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective.

(2)We contribute a taxonomy for Transformer based MML from two complementary perspectives,i.e., application based and challenge based.
In Section4,
we provide a review of multimodal Transformer applications, via
two important paradigms,i.e., for multimodal pretraining and for specific multimodal tasks.
In Section5,
we summarize the common challenges and designs shared by the various multimodal Transformer models and applications.

(3)In Section6, we discuss current bottlenecks, existing problems, and potential research directions for Transformer based MML.

MML[60,1,61]has been an important research area in recent decades; an early multimodal application –
audio-visual speech recognition was studied in 1980s[62].
MML is key to human societies.
The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal[63].
For instance, an AI navigation robot
needs multimodal sensors to perceive the real-world environment[64,65,66],e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer.
Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including
multimodal emotion recognition[67], multimodal event representation[68],
understanding multimodal humor[69],
face-body-voice based video person-clustering[70],etc.

Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging.
In modern life, we can see various multimodal applications, including commercial services (e.g., e-commerce/commodity retrieval[71], vision-and-language navigation (VLN)[72,73,74,75,76]), communication (e.g., lip reading[77], sign language translation[28,29]), human-computer interaction[78], healthcare AI[79,80], surveillance AI[81],etc.

Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and
Transformers[2]are a highly competitive architecture family, bringing new challenges and opportunities to MML.In particular, the recent success of large language models and their multimodal derivatives[82,83,84,85,86]further demonstrates the potential of Transformers in multimodal foundation models.

Transformers are emerging as promising learners.VanillaTransformer[2]benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art  on various NLP tasks.
Following the great success ofVanillaTransformer, a lot of derivative models have been proposed,e.g., BERT[4], BART[87], GPT[88],
Longformer[43],
Transformer-XL[89],
XLNet[90].

Transformers currently stand at the dominant position in NLP domains, and this motivates
researchers try to apply Transformers to other modalities, such as visual domains.
In early attempts for visual domain, the general pipeline
is “CNN features + standard Transformer encoder”, and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence[91].

Vision Transformer (ViT)[5]is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images.
Both ViT and its variants have been widely applied to various computer vision tasks, including
low-level tasks[92], recognition[93], detection[94], segmentation[95],etc, and also work well for both supervised[93]and self-supervised[96,97,98]visual learning.
Moreover, some recently-released works provide further theoretical understanding for ViT,e.g., its internal representation robustness[99], the continuous behaviour of its latent representation propagation[100,101].

Motivated by the great success of Transformer,
VideoBERT[7]is a breakthrough work that is the first work to extend Transformer to the multimodal tasks. VideoBERT demonstrates the great potential of Transformer in multimodal context.
Following VideoBERT, a lot of Transformer based multimodal pretraining models (e.g., ViLBERT[102],
LXMERT[103], VisualBERT[104], VL-BERT[105], UNITER[106], CBT[107], Unicoder-VL[108], B2T2[109], VLP[110], 12-in-1[111], Oscar[112], Pixel-BERT[113], ActBERT[114], ImageBERT[115], HERO[116], UniVL[117]) have become research topics of increasing interest in the field of machine learning.

In 2021, CLIP[9]was proposed. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition.
Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning.
Recently, the idea of CLIP is further studied,e.g., CLIP pretrained model based zero-shot semantic segmentation[118],
ALIGN[119], CLIP-TD[120], ALBEF[121], and CoCa[122].

In the past decade, with the rapid development of internet applications such as social media and online retail,
massive multimodal datasets have been proposed,e.g.,
Conceptual Captions[123], COCO[124], VQA[125], Visual Genome[126], SBU Captions[127], Cooking312K[7], LAIT[115],
e-SNLI-VE[128],
ARCH[129], Adversarial VQA[130],
OTT-QA[18],
MULTIMODALQA (MMQA)[131],
VALUE[132],
Fashion IQ[133],
LRS2-BBC[134],
ActivityNet[135],
VisDial[136].

Some emergent new trends among the recently released multimodal datasets are:

(1)Data scales are larger.
Various recently released datasets are million-scale,e.g.,
Product1M[137],
Conceptual 12M[138],
RUC-CAS-WenLan[139](30M),
HowToVQA69M[140],
HowTo100M[141],
ALT200M[142],
LAION-400M[143].

(2)More modalities.
In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging,e.g.,
Pano-AVQA[144]– the first large-scale spatial and audio-visual question
answering dataset on360∘superscript360360^{\circ}videos,
YouTube-360 (YT-360)[145](360∘superscript360360^{\circ}videos),
AIST++[146](a new multimodal dataset of 3D
dance motion and music),
Artemis[147](affective language for visual arts). In particular, MultiBench[148]provides a dataset including 10 modalities.

(3)More scenarios.
In addition to common caption and QA datasets,
more applications and scenarios have been studied,e.g.,
CIRR[149](real-life images),
Product1M[137],
Bed and Breakfast (BnB)[150](vision-and-language navigation),
M3A[151](financial dataset),
X-World[152](autonomous drive).

(4)Tasks are more difficult.
Beyond the straightforward tasks,
more abstract multimodal tasks are proposed,e.g., MultiMET[153](a multimodal dataset for metaphor understanding),
Hateful Memes[154](hate speech in multimodal memes).

(5)Instructional videos have become increasingly popular,e.g., cooking video YouCookII[155]. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task[156,7].Pretext tasks are pre-designed problems to force the models to learn representation by solving them.

Similar to other deep neural network architectures, Transformers are also data hungry.
Therefore, their high-capacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning.
For instance, big data bring zero-shot learning capability to VLP Transformer models.

In this section, we use mathematical formulations to review the key techniques ofVanillaTransformer[2], Vision Transformer[5], and multimodal Transformers111In this survey, “multimodal Transformer” means “Transformer in multimodal learning context”., including tokenized inputs, self-attention, multi-head attention, basic Transformer layers/blocks,etc.
We highlight thatVanillaTransformers can be understood from a geometrically topological perspective[157], because due to the self-attention mechanism, given each tokenized input from any modalities,Vanillaself-attention (Transformer) can model it as a fully-connected graph in topological geometry space[158].
Compared with other deep networks (for instance, CNN is restricted in the aligned grid spaces/matrices),
Transformers intrinsically have a more general and flexible modelling space.
This is a notable advantage of Transformers for multimodal tasks.
Sections3.1,3.2, and3.3will review the key designs ofVanillaTransformer, Vision Transformer, and multimodal Transformers, respectively.

VanillaTransformer has an encoder-decoder structure and is the origin of the Transformer-based research field.
It takes tokenized input (see Section3.1.1).
Both its encoder and decoder are stacked by the Transformer layers/blocks,
as demonstrated in Figure1.
Each block has two sub-layers,i.e., a multi-head self-attention (MHSA) layer (see Section3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section3.1.3).
To help the back propagation of the gradient, both MHSA and FFN use Residual Connection[159](given an inputx𝑥x, the residual connection of any mappingf​(⋅)𝑓⋅f(\cdot)is defined asx←f​(x)+x←𝑥𝑓𝑥𝑥x\leftarrow f(x)+x), followed by normalization layer.
Thus, assuming that the input tensor is𝐙𝐙\mathbf{Z}, the output of MHSA and FFN sub-layers can be formulated as:

wheres​u​b​l​a​y​e​r​(⋅)𝑠𝑢𝑏𝑙𝑎𝑦𝑒𝑟⋅{sublayer}(\cdot)is the mapping implemented by the sub-layer
itself andN​(⋅)𝑁⋅N(\cdot)denotes normalization,e.g.,B​N​(⋅)𝐵𝑁⋅BN(\cdot)[160],L​N​(⋅)𝐿𝑁⋅LN(\cdot)[161].

DiscussionThere is an important unsolved problem that is post-normalization versus pre-normalization.The originalVanillaTransformer uses post-normalization for each MHSA and FFN sub-layer.
However, if we consider this from the mathematical perspective, pre-normalization makes more sense[162].
This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection,e.g., Gram–Schmidt process222https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process.
This problem should be studied further by both theoretical research and experimental validation.

TokenizationVanillaTransformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input.
As mentioned previously, the original self-attention can model an arbitrary input as a fully-connected graph, independently of modalities.
Specifically, bothVanillaand variant Transformers take in the tokenized sequences, where each token can be regarded as a node of the graph.

Special/Customized TokensIn Transformers,
various special/customized tokens can be semantically defined as place-holders in the token sequences,e.g., mask token[MASK][4].
Some common special tokens are summarized in appendix. Special tokens can be used in both uni-modal and multimodal Transformers.

Position EmbeddingPosition embeddings are added
to the token embeddings to retain positional information[4].VanillaTransformer uses sine and cosine functions to produce position embedding.
To date, various implementations of position embedding have been proposed.
The concrete solutions are outside the focus of this survey.

DiscussionThe main advantages of input tokenization include the following:

(1)Tokenization is a more general approach from a geometrically topological perspective, achieved by minimizing constraints caused by different modalities.
In general, every modality has intrinsic constraints on modelling.
For instance, sentences have sequential structures that are well-suited by RNN, and photos are restricted in aligned grid matrices that CNN works well for.
Tokenization helps Transformers inherently to process different modalities universally via irregular sparse structures. Thus evenVanillaTransformer can encode multimodal inputs flexibly by just concatenation, weighted summation, even without any multimodal tailor-made modifications.

(2)Tokenization is a more flexible approach to organize the input information via concatenation/stack, weighted summation,etc.VanillaTransformer injects temporal information to the token embedding by summing position embedding.
For instance, when use Transformer to model free-hand sketch drawing[163], each input token can integrate various drawing stroke patterns,e.g., stroke coordinates, stroke ordering, pen state (start/end).

(3)Tokenization is compatible with the task-specific customized tokens,e.g.,[MASK]token[4]for Masked Language Modelling,[CLASS]token[5]for classification.

DiscussionHow to understand position embedding to Transformers is an open problem.It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer.
For cloud point[164]and sketch drawing stroke[163], their token element is already a coordinate, meaning that position embedding is optional, not necessary.
Furthermore, position embedding can be regarded as a kind of general additional information.
In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding,e.g.,
the pen state of sketch drawing stroke[163],
cameras and viewpoints in surveillance[165].
There is a comprehensive survey[166]discussing the position information in Transformers.
For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular),
position embeddings help Transformers to learn or encode the underlying structures.
Considered from the mathematical perspective of self-attention,i.e., scaled
dot-product attention,
attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing.Thus, in most cases, position embedding is necessary for Transformers.

The core component ofVanillaTransformer is the Self-Attention (SA) operation[2]that is also termed “Scaled Dot-Product Attention”.
Assume that𝐗=[𝐱1,𝐱2,⋯]∈ℝN×d𝐗subscript𝐱1subscript𝐱2⋯superscriptℝ𝑁𝑑\mathbf{X}=[\mathbf{x}_{1},\mathbf{x}_{2},\cdots]\in\mathbb{R}^{N\times d}is an input sequence ofN𝑁Nelements/tokens,
and an optional preprocessing is positional encoding by point-wise summation𝐙←𝐗⊕P​o​s​i​t​i​o​n​E​m​b​e​d​d​i​n​g←𝐙direct-sum𝐗𝑃𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝐸𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔\mathbf{Z}\leftarrow\mathbf{X}\oplus PositionEmbeddingor concatenation𝐙←c​o​n​c​a​t​(𝐗,P​o​s​i​t​i​o​n​E​m​b​e​d​d​i​n​g)←𝐙𝑐𝑜𝑛𝑐𝑎𝑡𝐗𝑃𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝐸𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔\mathbf{Z}\leftarrow concat(\mathbf{X},PositionEmbedding).

Self-Attention (SA)After preprocessing, embedding𝐙𝐙\mathbf{Z}will
go through three projection matrices (𝐖Q∈ℝd×dqsuperscript𝐖𝑄superscriptℝ𝑑subscript𝑑𝑞\mathbf{W}^{Q}\in\mathbb{R}^{d\times d_{q}},𝐖K∈ℝd×dksuperscript𝐖𝐾superscriptℝ𝑑subscript𝑑𝑘\mathbf{W}^{K}\in\mathbb{R}^{d\times d_{k}}, and𝐖V∈ℝd×dvsuperscript𝐖𝑉superscriptℝ𝑑subscript𝑑𝑣\mathbf{W}^{V}\in\mathbb{R}^{d\times d_{v}},dq=dksubscript𝑑𝑞subscript𝑑𝑘d_{q}=d_{k}) to generate three embeddings𝐐𝐐\mathbf{Q}(Query),𝐊𝐊\mathbf{K}(Key), and𝐕𝐕\mathbf{V}(Value):

The output of self-attention is defined as

Given an input sequence, self-attention allows each element to attend to all the other elements, so that self-attention encodes the input as a fully-connected graph. Therefore, the encoder ofVanillaTransformer can be regarded as a fully-connected GNN encoder,
and the Transformer family has the non-local ability of global perception, similar to the Non-Local Network[167].

Masked Self-Attention (MSA)In practice, modification
of self-attention is needed to help the decoder of Transformer to learn contextual dependence, to prevent positions from attending to subsequent positions, as

where𝐌𝐌\mathbf{M}is a masking matrix.
For instance, in GPT[88], an upper triangular mask to enable look-ahead attention
where each token can only look at the past tokens.
Masking can be used in both encoder[168,163]and decoder of Transformer, and has flexible implementations,e.g., 0-1 hard mask[163], soft mask[168].

In both uni-modal and multimodal practices,
specific masks are designed based on domain knowledge and prior knowledge.
Essentially, MSA is used to inject additional knowledge to Transformer models,e.g.,[163,169,24,170].

Multi-Head Self-Attention (MHSA)In practice, multiple self-attention sub-layers can be stacked in parallel and their concatenated outputs are fused by a projection matrix𝐖𝐖\mathbf{W}, to form a structure named Multi-Head Self-Attention:

where each head𝐙h=S​A​(𝐐h,𝐊h​𝐕h)subscript𝐙ℎ𝑆𝐴subscript𝐐ℎsubscript𝐊ℎsubscript𝐕ℎ\mathbf{Z}_{h}={SA}(\mathbf{Q}_{h},\mathbf{K}_{h}\mathbf{V}_{h})andh∈[1,H]ℎ1𝐻h\in[1,H],
andWis a linear projection matrix.
The idea of MHSA is a kind of ensemble.
MHSA helps the model to jointly attend to information from multiple representation
sub-spaces.

The output of the multi-head attention sub-layer will go through the position-wise Feed-Forward Network (FFN) that consists of successive linear layers with non-linear activation.
For instance, a two-layer FFN can be formulated as

where𝐖1subscript𝐖1\mathbf{W}_{1},𝐛1subscript𝐛1\mathbf{b}_{1},𝐖2subscript𝐖2\mathbf{W}_{2}, and𝐛2subscript𝐛2\mathbf{b}_{2}denote the weights and biases of the two linear transformations, whileσ​(⋅)𝜎⋅\sigma(\cdot)is non-linear activation,e.g.,ReLU​(⋅)ReLU⋅\text{ReLU}(\cdot)[171],G​E​L​U​(⋅)𝐺𝐸𝐿𝑈⋅GELU(\cdot)[172]. In some Transformer literature, FFN is also termed Multi-Layer Perceptron (MLP).

Vision Transformer (ViT)[5]has an image-specific input pipeline in which the input image must be split into fixed-size (e.g.,16×16161616\times 16,32×32323232\times 32) patches.
After going through the linearly embedded layer and adding the position embeddings, all the patch-wise sequences will be encoded by a standard Transformer encoder.
Given an image𝐗∈ℝH×W×C𝐗superscriptℝ𝐻𝑊𝐶\mathbf{X}\in\mathbb{R}^{H\times W\times C}(H𝐻Hheight,W𝑊Wwidth,C𝐶Cchannels), ViT needs to reshape𝐗𝐗\mathbf{X}into a sequence of flattened 2D patches:𝐱p∈ℝ𝐍×(𝐏𝟐⋅𝐂)subscript𝐱𝑝superscriptℝ𝐍⋅superscript𝐏2𝐂\mathbf{x}_{p}\in\mathbb{R}^{\mathbf{N\times(P^{2}\cdot C)}}, where(P×P)𝑃𝑃(P\times P)is the patch resolution andN=H​W/P2𝑁𝐻𝑊superscript𝑃2N=HW/P^{2}.
To perform classification, a standard approach is to
prepend an extra learnable embedding “classification token”[CLASS]to the sequence of embedded patches:

where𝐖𝐖\mathbf{W}denotes the projection.

Recently, a large number of Transformers have been studied extensively for various multimodal tasks, and shown to be compatible with various modalities in both discriminative and generative tasks.

In this section, we will review the key techniques/designs of the existing multimodal Transformer models, from the perspectives of multimodal input (Section3.3.1), self-attention variants (Section3.3.2), and network architectures (Section3.3.3).

The Transformer family is a general architecture that can be formulated as a type of general graph neural network.
Specifically, self-attention can process each input as a fully-connected graph, by attending to the global (non-local) patterns.
Therefore, this intrinsic trait helps Transformers can work in a modality
agnostic pipeline that is compatible with various modalities by treating the embedding of each token as a node of the graph.

Tokenization and Embedding ProcessingGiven an input from an arbitrary modality,
users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers.
In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives.
For instance, given an image, the solution of tokenizing and embedding is not unique.
Users can choose or design tokenization at multiple granularity levels – coarse-grained vs. fine-grained.e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings[102], use patches and linear projection as tokens and token embeddings[5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings[181].
Given a tokenization plan, the subsequent embedding approaches can be diverse.
For example, for video input, a common tokenization is to treat the non-overlapping
windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs,e.g.,
VideoBERT[7], CBT[107], and UniVL[117]use S3D[186],
ActBERT uses ResNet-3D[187].

TableIsummarizes some common practices of multimodal inputs for Transformers, including
RGB,
video,
audio/speech/music,
text,
graph,etc.

DiscussionWhen considered from the perspective of geometric topology,
each of the modalities listed in TableIcan be regarded as a graph.
An RGB image is essentially a neat grid graph in the pixel space.
Both video and audio are clip/segment based graphs over a complex space involving temporal and semantic patterns.
Both 2D and 3D drawing sketches[163,78]are a kind of sparse graph if we consider their key points along the drawing strokes.
Similar to sketches, the human pose also is a kind of graph.
3D point cloud is a graph in which each coordinate is a node.Other abstract modalities also can be interpreted as graphs,e.g.,
source code[44],
data flow of source code[44],
table[18],
SQL database schema[25],
text question graph[24], and
electronic health records (EHRs)[184].

Token Embedding FusionIn practice,
Transformers allow each token position to contain multiple embeddings.
This is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models. (This will be discussed further in subsequent sections.)
The most common fusion is the token-wise summing of the multiple embeddings,e.g., a specific token embedding⊕direct-sum\oplusposition embedding.
Similar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications.
In[81], token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI.
In particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by token-wise operators,e.g.,
in VisualBERT[104]and Unicoder-VL[108], segment embeddings are token-wise added to indicate which modality (vision or language) each token is from,
VL-BERT[105]injects global visual context to linguistic domain by “linguistic token embedding⊕direct-sum\oplusfull image visual feature
embedding”,
InterBERT[188]adds location information for ROI by “ROI embedding⊕direct-sum\opluslocation embedding”,
in ImageBERT[115], five kinds of embeddings are fused “image embedding⊕direct-sum\oplusposition embedding⊕direct-sum\opluslinguistic embedding⊕direct-sum\oplussegment embedding⊕direct-sum\oplussequence position embedding”.

In multimodal Transformers,
cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants.
Thus, in this section, we will review the main multimodal modelling practices of Transformers, from a perspective of self-attention designs, including
(1) early summation (token-wise, weighted),
(2) early concatenation,
(3) hierarchical attention (multi-stream to one-stream),
(4) hierarchical attention (one-stream to multi-stream),
(5) cross-attention, and
(6) cross-attention to concatenation. See TableIIand Figure2.

For brevity, we will state and compare the mathematical formulations in two-modality cases. Please note that all discussed self-attention and its variants are such flexible that can be extended to multiple modality cases.
Specifically, the following formulations are modality-, tokenization-, and embedding- agnostic, as self-attention models the embedding of arbitrary token from arbitrary modality as a node of a graph.

Given inputs𝐗Asubscript𝐗A\mathbf{X}_{\texttt{A}}and𝐗Bsubscript𝐗B\mathbf{X}_{\texttt{B}}from two arbitrary modalities,𝐙(A)subscript𝐙A\mathbf{Z}_{(\texttt{A})}and𝐙(B)subscript𝐙B\mathbf{Z}_{(\texttt{B})}denote their respective token embeddings.
Let𝐙𝐙\mathbf{Z}denoting the token embedding (sequence) produced by the multimodal interactions.T​f​(⋅)𝑇𝑓⋅Tf(\cdot)stands for the processing of Transformer layers/blocks.

[图片: images\image_2.png]
图片说明: Figure 2:Transformer-based cross-modal interactions: (a) Early Summation, (b) Early Concatenation, (c) Hierarchical Attention (multi-stream to one-stream), (d) Hierarchical Attention (one-stream to multi-stream), (e) Cross-Attention, and (f) Cross-Attention to Concatenation. “Q”: Query embedding; “K”: Key embedding; “V”: Value embedding. “TL”: Transformer Layer. Best viewed in colour.

[图片: images\image_3.png]
图片说明: Figure 2:Transformer-based cross-modal interactions: (a) Early Summation, (b) Early Concatenation, (c) Hierarchical Attention (multi-stream to one-stream), (d) Hierarchical Attention (one-stream to multi-stream), (e) Cross-Attention, and (f) Cross-Attention to Concatenation. “Q”: Query embedding; “K”: Key embedding; “V”: Value embedding. “TL”: Transformer Layer. Best viewed in colour.

[图片: images\image_4.png]
图片说明: Figure 2:Transformer-based cross-modal interactions: (a) Early Summation, (b) Early Concatenation, (c) Hierarchical Attention (multi-stream to one-stream), (d) Hierarchical Attention (one-stream to multi-stream), (e) Cross-Attention, and (f) Cross-Attention to Concatenation. “Q”: Query embedding; “K”: Key embedding; “V”: Value embedding. “TL”: Transformer Layer. Best viewed in colour.

[图片: images\image_5.png]
图片说明: Figure 2:Transformer-based cross-modal interactions: (a) Early Summation, (b) Early Concatenation, (c) Hierarchical Attention (multi-stream to one-stream), (d) Hierarchical Attention (one-stream to multi-stream), (e) Cross-Attention, and (f) Cross-Attention to Concatenation. “Q”: Query embedding; “K”: Key embedding; “V”: Value embedding. “TL”: Transformer Layer. Best viewed in colour.

[图片: images\image_6.png]
图片说明: Figure 2:Transformer-based cross-modal interactions: (a) Early Summation, (b) Early Concatenation, (c) Hierarchical Attention (multi-stream to one-stream), (d) Hierarchical Attention (one-stream to multi-stream), (e) Cross-Attention, and (f) Cross-Attention to Concatenation. “Q”: Query embedding; “K”: Key embedding; “V”: Value embedding. “TL”: Transformer Layer. Best viewed in colour.

[图片: images\image_7.png]
图片说明: Figure 2:Transformer-based cross-modal interactions: (a) Early Summation, (b) Early Concatenation, (c) Hierarchical Attention (multi-stream to one-stream), (d) Hierarchical Attention (one-stream to multi-stream), (e) Cross-Attention, and (f) Cross-Attention to Concatenation. “Q”: Query embedding; “K”: Key embedding; “V”: Value embedding. “TL”: Transformer Layer. Best viewed in colour.

(1) Early SummationIn practice, early summation[46,81]is a simple and effective multimodal interaction, where the token embeddings from multiple modalities can be weighted summed at each token position and then processed by Transformer layers:

where⊕direct-sum\oplusis element-wise sum, andα𝛼\alphaandβ𝛽\betaare weightings.
Concretely,𝐐(AB)=(α​𝐙(A)⊕β​𝐙(B))​𝐖(AB)Qsubscript𝐐ABdirect-sum𝛼subscript𝐙A𝛽subscript𝐙Bsubscriptsuperscript𝐖𝑄AB\mathbf{Q}_{(\texttt{AB})}=(\alpha\mathbf{Z}_{(\texttt{A})}\oplus\beta\mathbf{Z}_{(\texttt{B})})\mathbf{W}^{Q}_{(\texttt{AB})},𝐊(AB)=(α​𝐙(A)⊕β​𝐙(B))​𝐖(AB)Ksubscript𝐊ABdirect-sum𝛼subscript𝐙A𝛽subscript𝐙Bsubscriptsuperscript𝐖𝐾AB\mathbf{K}_{(\texttt{AB})}=(\alpha\mathbf{Z}_{(\texttt{A})}\oplus\beta\mathbf{Z}_{(\texttt{B})})\mathbf{W}^{K}_{(\texttt{AB})}, and𝐕(AB)=(α​𝐙(A)⊕β​𝐙(B))​𝐖(AB)Vsubscript𝐕ABdirect-sum𝛼subscript𝐙A𝛽subscript𝐙Bsubscriptsuperscript𝐖𝑉AB\mathbf{V}_{(\texttt{AB})}=(\alpha\mathbf{Z}_{(\texttt{A})}\oplus\beta\mathbf{Z}_{(\texttt{B})})\mathbf{W}^{V}_{(\texttt{AB})}.
Its main advantage is that it does not increase computational complexity.
However, its main disadvantage is due to the manually set weightings.
As discussed in Section3.1.1and3.3.1, summing position embedding is intrinsically a case of early summation.

(2) Early ConcatenationAnother straightforward solution is early concatenation[7,44,178,180]that the token embedding sequences from multiple modalities are concatenated and input into Transformer layers as

Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities.
VideoBERT[7]is the one of the first multimodal Transformer works, where video and text are fused via early concatenation that can encode the global multimodal context well[188].
However, the longer sequence after concatenation will increase computational complexity.
Early concatenation is also termed “all-attention” or “Co-Transformer”[137].

(3) Hierarchical Attention(multi-stream to one-stream)
Transformer layers can be combined hierarchically to attend to the cross-modal interactions.
A common practice is that multimodal inputs are encoded by independent Transformer streams and their outputs are concatenated and fused by another Transformer[146]:

This kind of hierarchical attention is an implementation of late interaction/fusion, and can be treated as a special case of early concatenation.

(4) Hierarchical Attention(one-stream to multi-stream)
InterBERT[188]is another good practice of hierarchical attention where concatenated multimodal inputs are encoded by a shared single-stream Transformer that is followed by two separate Transformer streams.
This flow can be formulated as

This method perceives the cross-modal interactions and meanwhile preserves the independence of uni-modal representation.

(5) Cross-AttentionFor two-stream Transformers, if the𝐐𝐐\mathbf{Q}(Query) embeddings are exchanged/swapped in a cross-stream manner, the cross-modal interactions can also be perceived.
This method is termed cross-attention or co-attention[190], which was first proposed in VilBERT[102]:

Cross-attention attends to each modality
conditioned on the other and does not cause higher computational complexity, however if considered for each modality, this method fails to perform cross-modal attention globally and thus loses the whole context.
As discussed in[188],
two-stream cross-attention can learn cross-modal interaction, whereas there is no self-attention to the self-context inside each modality.

(6) Cross-Attention to ConcatenationThe two streams of cross-attention[102]can be further concatenated and processed by another Transformer to model the global context.
This kind of hierarchically cross-modal interaction is also widely studied[189,137], and alleviates the drawback of cross-attention.

DiscussionAll these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks.
Specifically, these interactions can be flexibly combined and nested.
For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model[191]T​f2𝑇subscript𝑓2Tf_{2}andT​f3𝑇subscript𝑓3Tf_{3}of Eq.11are implemented by cross-attention defined in Eq.12.
Moreover, they can be extended to multiple (≥3absent3\geq 3) modalities.
TriBERT[183]is a tri-modal cross-attention (co-attention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities. Cross-attention to concatenation is applied to three modalities (i.e., language,
video, and audio) in[189].

Essentially,
various multimodal Transformers work due to their internal multimodal attentions that are
the aforementioned self-attention variants.
Meanwhile, as illustrated in Figure2, these attentions determine the external network structures of the multimodal Transformers where they are embedded.

In general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multi-streams, (3) hierarchical attention and cross-attention to concatenation work in hybrid-streams.
Thus,
multimodal Transformers can be divided into
single-stream (e.g., Uniter[106], Visualbert[104], Vl-bert[105], Unified VLP[110]), multi-stream (e.g., ViLBERT[102], Lxmert[103], ActBERT[114]), hybrid-stream (e.g., InterBERT[188]),etc.

From the perspective of timing of interaction, these multimodal attentions fall into three categories,i.e., early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention, cross-attention to concatenation.

As demonstrated in Figure 2 in[192], the multimodal Transformer models have another architecture taxonomy based on the computational size of the components.

In this section we survey multimodal Transformers based on the application scenarios.
We consider two important paradigms:
(1) Transformers for multimodal pretraining (Section4.1, including both task-agnostic (Section4.1.1) and task-specific (Section4.1.2) multimodal pretraining),
and
(2) Transformers for specific multimodal tasks (Section4.2).

Inspired by the great success of Transformer based pretraining in NLP community,
Transformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging.
Recent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models[7,104,102,105,106,110,103]clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability.
These superiorities have led Transformer-based multimodal pretraining to become a hot topic, which
has two main directions,i.e., general pretraining for agnostic down-stream tasks (Section4.1.1), goal-oriented pretraining for specific down-stream tasks (Section4.1.2).

We focus on these key points:
(1) What trends are emerging?
(2) Where/how do the cross-modal interactions take place during pretraining?
(3) How to sort out and understand the pretraining pretext objectives?
How can they drive Transformers to learn the cross-modal interactions?

Recently Transformer-oriented pretraining has been widely studied involving diverse modality combinations,e.g., video-text[7,107,117], image-text[102,104,103,193,194,195], acoustic-text[180].

Among existing work,
the following main trends are emerging:

(1)Vision-language pretraining (VLP) is a major research problem in this field. VLP is including both “image + language” and “video + language”, also termed visual-linguistic pretraining. A great deal of excellent work has been proposed,e.g.,
VideoBERT[7],
ViLBERT[102],
LXMERT[103],
VisualBERT[104],
VL-BERT[105],
UNITER[106],
CBT[107],
Unicoder-VL[108],
B2T2[109],
VLP[110],
12-in-1[111],
Oscar[112],
Pixel-BERT[113],
ActBERT[114],
ImageBERT[115],
HERO[116],
UniVL[117], SemVLP[196].

(2)Speech can be used as text. Thanks to recent advances in automatic speech recognition (ASR) techniques, in a multimodal context, speech can be converted to text by the
off-the-shelf speech
recognition tools.
For instance, VideoBERT[7]and CBT[107]make full use of speech rather than low-level sounds as a source of cross-modal supervision, by extracting high-level semantic text.

(3)Overly dependent on the well-aligned multimodal data. A majority of Transformer-based multimodal pretraining works in a self-supervised manner, however, it is overly dependent on the well-aligned multimodal sample pairs/tuples.
For instance, large amount of image-language pretraining Transformer models are pretrained on large-scale image-text pairs,e.g.,
VisualBERT[104], VL-BERT[105], ViLBERT[102],
LXMERT[103], UNITER[106].
For another example,
the instructional videos (e.g., cooking)333Note that instructional videos also have weakly aligned cases[197,198].are widely used as the pretraining corpora,e.g.,
HowToVQA69M[140],
HowTo100M[141],
as in general,
their visual clues/content and the spoken words have a higher probability to align with each other, if compared with other videos.
However, using cross-modal alignment as cross-modal supervision is costly for large-scale applications.
Thus, how to use the weakly-aligned or even unpaired/unaligned multimodal data as the pretraining corpora is still understudied. Some recent attempts[199,137]study the use of weakly-aligned cross-modal supervision to train Transformers to learn the cross-modal interactions.

(4)Most of the existing pretext tasks transfer well across modalities.
For instance, Masked Language Modelling (MLM) in the text domain has been applied to audio and image,e.g., Masked Acoustic Modelling[200,180], Masked Image Region Prediction[190],
while both Sentence Ordering Modelling (SOM)[201]in text domain and Frame Ordering Modelling (FOM)[116]in video domain share the same idea.
We will further discuss the pretext tasks for multimodal Transformer pretraining in the follows.

(5)Model structures are mainly in three categories. Essentially, in multimodal pretraining scenarios, Transformer models work based on those self-attention variants that are discussed in Section3.3.2. Thus, if considered from the perspective of model structures, the existing Transformers for multimodal pretraining are also mainly in three categories,i.e., single-stream, multi-stream, hybrid-stream.

(6)Cross-modal interactions can perform within various components/levels in the pretraining pipelines.
For Transformer based multimodal pretraining, the key is to drive the Transformer (encoder w/, w/o decoder) to learn the cross-modal interactions.
In the existing Transformer-based multimodal pretraining practices, the cross-modal interactions are flexible, which can perform within various components/levels in the pretraining pipelines.
In general, Transformer-based multimodal pretraining pipelines have three key components, from bottom to top,i.e., tokenization, Transformer representation, objective supervision.
For not only the multimodal pretraining but also the specific multimodal tasks,
the cross-modal interactions can perform within arbitrary component(s) of the three.
As discussed in Section3.3.2,
because self-attention models the embedding of an arbitrary token from an arbitrary modality as a node of a graph,
the existing pretraining pipelines
can, in general, be transferred independently across
modalities,
unless considered with modality-specific objectives.

DiscussionVision Language Pretraining (VLP) follows two general pipelines: two-stage (need object detector,e.g., Faster R-CNN[202]) (e.g., LXMERT[103], ViLBert[102], VL-Bert[105], UNITER[106]) and end-to-end (e.g., Pixel-Bert[113], SOHO[203], KD-VLP[204], Simvlm[199]).
Two-stage pipelines have a main advantage – object-aware perceiving, by using the supervised pre-trained visual detectors,
however these are based on a strong assumption that the visual representations can be fixed.

DiscussionHow to look for more corpora that intrinsically have well-aligned cross-modal supervision, such as instructional videos, is still an open problem.
However,
weakly-aligned cross-modal samples are popular in the real-life scenarios, for instance, enormous weakly aligned multimodal
data samples are emerging in e-commerce[137], due to fine-grained categories, complex combinations, and fuzzy correspondence.
Well labelled/aligned cross-modal datasets are very costly in collecting and annotating;
how to use weakly-aligned or even unaligned corpora crawled from the web is a promising question.
Some recently successful practice[9,205,199]used weakly aligned image-text pairs
to perform pretraining, and achieve both competitive performance and zero-shot learning capability for image classification, image-text retrieval, and open-ended visual question answering,etc.
Because these practices in weak supervision make full use of large-scale pretraining corpora, they yield greater promise of zero-shot generalization.

Pretext TasksIn Transformer based multimodal pretraining,
the pretraining tasks/objectives are also termed pretext tasks/objectives.
To date, various pretext tasks have been studied,e.g., masked language modelling (MLM)[137],
masked image region prediction/classification (also termed masked object classification (MOC))[190,137],
masked region regression (MRR)[115],
visual-linguistic matching (VLM) (e.g., image–text
matching (ITM)[188], image text matching (ITM), phrase-region alignment (PRA)[204], word-region alignment (WRA)[106], video-subtitle matching (VSM)[116]),
masked frame modelling (MFM)[116],
frame order modelling (FOM)[116],
next sentence prediction (NSP)[4,102,190],
masked sentence generation (MSG)[191],
masked group modelling (MGM)[188],
prefix language modelling (PrefixLM)[199],
video conditioned masked
language model[117],
text conditioned masked frame
model[117],
visual translation language modelling
(VTLM)[206], and
image-conditioned masked language modelling (also termed image-attended masked language modelling)[207].
These down-stream task -agnostic pretext pretraining is optional,
and the down-stream task objectives can be trained directly,
which will be discussed in Section4.1.2.
TableIIIprovides the common and representative pretext tasks for Transformer based multimodal pretraining.In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of[57], Table 2 of[58].

The pretext tasks have multiple taxonomies:

(1)Supervision.
The common multimodal pretraining Transformers use well-aligned, weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively.
Meanwhile,
if we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (e.g., masked language modelling (MLM)[7,137]) and supervised (e.g., image-text matching (ITM)[188][106,104,102,103,209]),etc.
Nowadays, self-supervised attempts are the majority.

(2)Modality. Considering the mathematical formulations, some pretexts are defined on single modality,e.g., masked language modelling[7], masked acoustic modelling[200], masked region regression (MRR)[115],
while other pretexts are defined on multiple modalities,e.g., image-conditioned masked language modelling (IMLM)[208], image-text matching (ITM)[188], video-subtitle matching (VSM)[116].
Thus, from this mathematical view, the pretext tasks can be divided into two categories,i.e., uni-modal and multimodal.

However, this classification is not really accurate.
It should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions,
(b) co-training with other pretexts that involve other modalities.
For instance, VL-BERT[105]uses two dual pretext tasks,i.e., masked language modelling and masked RoI classification.

(3)Motivation. If consider their motivations, the pretext tasks include masking, describing, matching, ordering,etc.

Some recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single- or dual- stream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks,e.g., Table 3 of[57], Table 2 of[58].
Different from these views,
in this survey,
we would propose our comparisons from some new perspectives.
Specifically:(1)The core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions.(2)Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically
work in a modality agnostic pipeline that is compatible
with various modalities by taking in the embedding of each
token as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains.(3)We suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top,i.e., tokenization, Transformer representation, objective supervision.

DiscussionIn spite of the recent advances,
multimodal pretraining Transformer methods still have some obvious bottlenecks.For instance,
as discussed by[208]in VLP field,
while the BERT-style cross-modal pretraining models produce excellent results on various down-stream vision-language tasks, they fail to be applied to generative tasks directly.As discussed in[208],
both VideoBERT[7]and CBT[107]have to train a separate video-to-text decoder for video captioning.
This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer.
Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved.
Again for instance,
common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by[137].

DiscussionAs discussed in[208],
the masked language
and region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks,e.g., VQA, image-text retrieval.

DiscussionHow to boost the performance for multimodal pretraining Transformers is an open problem.
Some practices demonstrate that
multi-task training (by adding auxiliary loss)[137,111]and adversarial training[210]improve multimodal pretraining Transformers to further boost the performance.
Meanwhile, overly compound
pretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization[199].
Moreover,
the difficulty of the pretexts is also worth discussing.
In general, if aim to learn more explicit object concepts, more complex pretext losses will be used[204]. However, for pretexts, whether more complexity is better remains a question.

In practices of multimodal Transformers,
the aforementioned down-stream task -agnostic pretraining is optional, not necessary,
and down-stream task specific pretraining is also widely studied[211,150,208,190].
The main reasons include:
(1) Limited by the existing technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications.
(2) There are non-negligible gaps among various down-stream applications,e.g., task logic, data form, making it difficult to transfer from pretraining to down-stream applications.

Therefore,
a large number of down-stream tasks still need tailor-made pretraining to improve the performance.
Guhuret al.[150]propose in-domain pretraining for vision-and-language navigation, as
the general VLP
focuses on learning vision-language correlations, not designed for sequential decision making as required
in embodied VLN.
Murahariet al.[190]present a visual dialogue oriented approach to leverage pretraining on general
vision-language datasets.
XGPT[208]is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pre-trained models fail to be applied
to generative tasks directly.
ERNIE-ViLG[211]is designed for bidirectional image-text generation with Transformers.

Special modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts.
GraphCodeBERT[44]uses
two structure-aware pretext tasks (i.e., predict where a variable is identified from, data flow edge
prediction between variables) for programming source code.
To learn from the spatial cues in360∘superscript360360^{\circ}video,
Morgadoet al.[145]propose to perform contrastive audio-visual spatial alignment of360∘superscript360360^{\circ}video and spatial audio.
Med-BERT[184]is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients.
Kaleido-BERT[212]is a VLP Transformer model tailor-made for the fashion domain.

Recent work has demonstrated that Transformer models can encode various multimodal inputs in both classical and novel discriminative applications,e.g., RGB & optical flow[46],
RGB & depth[213],
RGB & pointcloud[214],
RGB & LiDAR[215,216],
textual description & point cloud[31], acoustic & text[180], audio & visual observation for Audio-Visual Navigation[76],
speech query & schema of SQL database[25],
text question/query & the schema SQL database[24],
audio & tags[217],
multimodal representation for video[218,219],
text query & video[220],
audio & video for audio visual speech enhancement (AVSE)[179],
audio & video for Audio-Visual Video Parsing[173],
audio & video for audio-visual speech recognition[134],
video & text for Referring Video Object Segmentation (RVOS)[221],
source code & comment & data flow[44],
image & text for retrieval[222].

Meanwhile, Transformers also contribute to various multimodal generative tasks, including single-modality to single-modality (e.g., raw audio to 3D mesh sequence[39],
RGB to 3D scene[40],
single image to 3D human texture estimation[223],
RGB to scene graph[224,19,225,226],
graph to graph[33],
knowledge graph to text[227],
video to scene graph[228],
video to caption[229,230,231,232],
image to caption[233,234,235,236,237],
text to speech[238],
text to image[205,239],
text to shape[240],
RGB to 3D human pose and mesh[41],
music to dance[241]),
multimodality to single modality (e.g.,
image & text to scene graph[242],
Video Dialogue (text & audio & visual to text)[243],
Mono Audio & Depth to Binaural Audio[14],
music piece & seed 3D motion to long-range future 3D motions[146],
X-raying image & question to answer[244],
video & text & audio to text[245]),
and multimodality to multimodality (e.g.,[246]).

Complementing the application scenario taxonomy discussed in Section4,
we further survey prior work from the perspective of technical challenges.
We discuss seven challenges of Transformer based multimodal learning, including fusion, alignment, transferability,
efficiency,
robustness,
universalness, and interpretability.
This further extends the taxonomy introduced in[1]to tackle the higher diversity and wider scopes of
existing Transformer based MML works in recent years.

In general, MML Transformers fuse information across
multiple modalities primarily at three levels:
input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion).
Common early fusion based MML Transformer models[7,104,108]are also known as one-stream architecture,
allowing the adoption of the merits of BERT
due to minimal architectural modification.
The main difference between these one-stream models
is the usage of problem-specific modalities with variant masking techniques.
With attention operation, a noticeable fusion scheme
is introduced based on a notion of bottleneck tokens[175].
It applies for both early and middle fusion
by simply choosing to-be-fused layers.
We note that the simple prediction-based late fusion[247,248]is less adopted
in MML Transformers.
This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power.
For enhancing and interpreting the fusion of MML,
probing the interaction and measuring the fusion between modalities[249]would be an interesting direction to explore.

Cross-modal alignment is the key to
a number of real-world multimodal applications.
Transformer based cross-modal alignment has been studied for various tasks,e.g.,
speaker localization in multi-speaker videos[250],
speech translation[180],
text-to-speech alignment[251],
text-to-video retrieval[252,253,254],
and visual grounding of natural language[255,256,257,258,259].
Recently, Transformer based alignment[9,119,260,261,262]has led to a surge of leveraging large quantities of web data (e.g., image-text pairs) for vision and language tasks.

A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples.
The models based on this idea are often enormous in size
and expensive to optimize from millions or billions of training data.
Consequently, successive works mostly exploit pretrained models
for tackling various down-stream tasks[263,120,264,265,266].
These alignment models
have the ability of zero-shot transfer
particularly for image classification viaprompt engineering[267].
This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem
and zero-shot classification remains an unsolved challenge despite extensive research[268].
This has been studied
for more challenging and fine-grained tasks (e.g., object detection[269], visual question answering[263,106,112,103], and instance retrieval[263,222])
by
imposing region (semantic parts such as objects) level alignment.
Fine-grained alignment will however incur more computational costs
from explicit region detection
and how to eliminate this whilst keeping the region-level learning capability becomes a challenge.
Several ideas introduced recently include
random sampling[113],
learning concept dictionary[203],
uniform masking[270],
patch projection[192],
joint learning of a region detector[271], and
representation aligning before mask prediction[263].

Transferability is a major challenge for Transformer based multimodal learning, involving the question of how to transfer models across different datasets and applications.

Data augmentation and adversarial perturbation strategies help multimodal Transformers to improve the generalization ability. VILLA[210]is a two-stage strategy (task-agnostic adversarial pretraining, followed by task-specific adversarial finetuning) that improves VLP Transformers.

In practice,
the distribution gap between training data and practical data is noticeable.
For instance,
supervised data samples (well-labelled, well-aligned) are costly in practical applications, thus
how to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging[137].
CLIP[9]is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer
of the model to down-stream tasks.
The main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template “A photo of a {label}.” to bridge the distribution gap between training and test datasets.

Over-fitting is a major obstacle to transfer. Multimodal Transformers can be overly fitted to the dataset
biases during training, due to the large modelling capability.
Some recent practices exploit how to transfer the oracle model trained on noiseless dataset to real dataset.
For instance,
Kervadecet al.[272,273]explore
how transferable reasoning patterns are in VQA,
and demonstrate that for LXMERT[103]/BERT-like
reasoning patterns can be partially transferred from an ideal dataset to a real dataset.

Cross-task gap is another major obstacle to transfer[274,208], due to the different reasoning and input-output workflows,e.g.,
how to use multimodal datasets to finetune the language pretrained model is difficult[274].
In real applications,
multimodal pretrained Transformers sometimes need to handle the uni-modal data at inference stage due to the issue of missing modalities. One solution is using knowledge
distillation,e.g., distilling
from multimodal to uni-modal attention in Transformers[275], distilling from multiple uni-modal Transformer teachers to a shared Transformer encoder[276].
There is a huge gap across discriminative and generative multimodal tasks.
As discussed in[208],
the BERT-like encoder-only multimodal Transformers (e.g., VideoBERT[7], CBT[107]) need
separately to train decoders for generation tasks. This could create a pretrain-finetune discrepancy detrimental to the
generality.
Recently, more and more attempts study this issue further,e.g.,
GilBERT[222]is a generative VLP models for
a discriminative task,i.e., image-text retrieval.

Cross-lingual gap also should be considered for the transferability of Transformer based multimodal learning,e.g.,
universal cross-lingual generalization from English to non-English multimodal contexts[206,277].

Multimodal Transformers suffer from two major efficiency issues:
(1) Due to the large model parameter capacity, they are data hungry and thus dependent on huge scale training datasets.
(2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention.
In multimodal contexts,
calculation explosion will become worse due to jointly high dimension representations.
These two bottlenecks are interdependent and should be considered together.

To improve the training and/or inferring efficiency for multimodal Transformers,
recent efforts have attempted to find various solutions, to use fewer training data and/or parameters.
The main ideas can be summarized as the follows.

(1)Knowledge distillation.
Distill the knowledge from the trained larger Transformers to smaller Transformers[93].
Miechet al.[278]conduct distillation from a slower model (early concatenation based Transformers,𝒪​((N(A)+N(B))2)𝒪superscriptsubscript𝑁Asubscript𝑁B2\mathcal{O}((N_{(\texttt{A})}+N_{(\texttt{B})})^{2})) to a faster one (independently dual branch Transformers,𝒪​(N(A)2)𝒪superscriptsubscript𝑁A2\mathcal{O}(N_{(\texttt{A})}^{2})).

(2)Simplifying and compressing model.
Remove the components to simplify the pipelines.
Taking the VLP Transformer models as an example,
two-stage pipeline is costly as they need object detector.
One simplifying is processing the visual input in convolution-free manner,e.g., E2E-VLP[271], ViLT[192].
DropToken[174]reduces
the training complexity via
random dropping a portion of the video and audio tokens from input sequence during training.
DropToken can be treated as an implementation of dropout or adversarial training.
Weight-sharing is also a common practice for simplifying multimodal Transformer models.
Wenet al.[279]present a weight-sharing Transformer on top of the visual and
textual encoders to align text and image.
Leeet al.[280]propose a novel parameter sharing scheme based on low-rank approximation.

(3)Asymmetrical network structures.
Assign different model capacities and computational size properly for different modalities, to save parameters. See Figure 2 in[192].

(4)Improving utilization of training samples.
Liuet al.[281]train a simplified LXMERT by making full use of fewer samples at different granularities.
Liet al.[282]use fewer data to train CLIP by fully mining the potential self-supervised signals of (a) self-supervision within each modality, (b) multi-view supervision across modalities, and (c) nearest-neighbour supervision from other similar pairs.

(5)Compressing and pruning model.
Search the optimal sub-structures/sub-networks of multimodal Transformers,e.g.,
playing Lottery Tickets with the VLP Transformer models[283], adaptively freezing some layers during training[284].

(6)Optimizing the complexity of self-attention.
Transformers cost time and memory that grows quadratically with the input sequence length[285].
One potential solution is optimizing the𝒪​(N2)𝒪superscript𝑁2\mathcal{O}(N^{2})complexity,e.g.,
Childet al.[286]present sparse factorizations of the attention matrix to reduce the quadratical complexity to𝒪​(n​n)𝒪𝑛𝑛\mathcal{O}(n\sqrt{n}),
Transformer-LS[287]is an efficient Transformer for both language and vision long sequence, with linear computational and memory complexity.

(7)Optimizing the complexity of self-attention based multimodal interaction/fusion.
Nagraniet al.[175]propose Fusion via Attention Bottlenecks (FSN, fusion bottleneck) to improve the early concatenation based multimodal interaction.
FSN passes on the messages through a small number of bottleneck
latents, thus requiring the model to purify the most necessary information
from each modality for cross-modal sharing.
This strategy uses the fusion bottleneck as a bridge, and not only
improves fusion performance, but also reduces computational cost.

(8)Optimizing other strategies.
Use optimal strategies to perform the common Transformer based multimodal interactions.
Given the quadratic complexity of self-attention,
using early concatenation based multimodal interaction to synchronously fuse the inputs from multiple modalities/views is costly.
Yanet al.[288]present an efficient solution that sequentially fuses information between all pairs of
two adjacent views in ascending order of sequence length.
This is intrinsically a greedy strategy.

Multimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied.
This at least involves two key challenges,i.e., how to theoretically analyse the robustness, how to improve the robustness.

Although that recent attempts[289,99,182,290]study and evaluate how the Transformer components/sub-layers contribute to the robustness,
the main bottleneck is that the community lacks theoretical tools to analyse the Transformer family.
Recently, the common practices to analyse robustness are mainly based on experiment evaluations[291],e.g., cross-dataset evaluations, perturbation-based evaluations.
Thus, some multimodal datasets[292,130]are proposed for evaluating the robustness.

Recent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies[293,294],
(2) fine-grained loss functions[295].
For instance:
VILLA[210]is a generic adversarial training framework that can be applied to various multimodal Transformers.
Akulaet al.[292]empirically demonstrate that
ViLBERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multi-task learning.

Due to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models.
A large amount of recent attempts[296,117,297,298]study how to use as unified as possible pipelines to handle various modalities and multimodal tasks.
Ideally, the unified multimodal Transformers can be compatible with various data (e.g.,
aligned and unaligned, uni-modal and multimodal) and tasks (e.g., supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.

The currently unifying-oriented attempts mainly include:

(1)Unifying the pipelines for both uni-modal and multimodal inputs/tasks.
As discussed Section5.3, in practical scenarios,
multimodal Transformers need to
handle uni-modal data due to the issue of missing modalities.
Distilling multimodal knowledge into small models that are adaptable to uni-modal data and tasks is a successful practice[275,276].

(2)Unifying the pipelines for both multimodal understanding and generation.
In general, for multimodal Transformer pipelines,
understanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders.
Existing attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multi-task loss functions.
From the perspective of model structures, typical solutions include:
(a) encoder + decoder,e.g., E2E-VLP[271].
(b) separate encoders + cross encoder + decoder,e.g., UniVL[117], CBT[107].
(c) single unified/combined encoder-decoder,e.g., VLP[110].
(d)two-streamdecoupled design[191].

(3)Unifying and converting the tasks themselves,e.g., CLIP[9]converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model.

However, the aforementioned practices suffer some obvious challenges and bottlenecks, at least including:

(1)Due to modality and task gaps, universal models should consider the trade-off between universalness and cost. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant.

(2)Multi-task loss functions increase the complexity of training. How to co-train multiple objectives properly and effectively is challenging, due to that different objectives generally should be optimized in different strategies.

Why and how Transformers perform so well in multimodal learning has been investigated[299,106,300,301,302,303,304,305,306].
These attempts mainly use probing task and ablation study.
Caoet al.[299]design a set of probing tasks on UNITER[106]and LXMERT[103], to evaluate what patterns are learned in pretraining.
Hendrickset al.[301]probe the image–language Transformers by fine-grained image–sentence pairs, and find that verb understanding is harder than subject or object understanding.Chenet al.[106]examine the optimal combination of pretraining
tasks via ablation study, to compare how different pretexts contribute to the Transformers.Despite these attempts,
the interpretability of multimodal Transformers is still under-studied to date.

Designing the universal MML models to excel across all the unimodal and multimodal down-stream tasks with different characteristics simultaneously[115,299]is a non-trivial challenge.
For instance, two-stream architectures[9,263]are typically preferred over one-stream ones for cross-modal retrieval-like tasks in efficiency, since the representation of each modality can be pre-computed beforehand and reused repeatedly.
That being said, how to design task-agnostic MML architectures is still an open challenge, in addition to other design choices such as pretext and objective loss functions.
Furthermore, a clear gap remains
between the state-of-the-art and this ultimate goal.
In general, existing multimodal Transformer models[199,263,9]are superior only for specific MML tasks,
as they are designed specifically for only a subset of specific tasks[137,212,265,142,266,261,260,249].
Encouragingly, several recent studies towards  universal modality learning in terms of modality-agnostic network design[3]and more task-generic architecture design[307,308,309]have been introduced,
and it is hoped this will spark further investigation.
To that end, instead of exhaustively exploring the vast model design space, seeking in-depth understanding and interpretation of a MML model’s behaviour might be insightful for superior algorithm design, even though the interactions and synergy across different modalities are intrinsically complex and even potentially inconsistent over tasks[249].

For more fine-grained MML, it is widely acknowledged that discovering the latent semantic alignments across modalities is critical.
An intuitive strategy is to leverage semantic parts (e.g., objects) pre-extracted by an off-the-shelf detector for MML[310,106,112,103,104,105,204].
This, however, is not only complex and error-prone, but computationally costly[207].
Several remedies introduced recently include
random sampling[113],
learning concept dictionary[203],
jointly learning a region detector[271], and
representation aligning before mask prediction[263].
Given the scale of MML training data, exploring this direction
needs exhaustive computational costs, and
it is supposed that industrial research teams with rich resources are more likely to afford.
Ideally, a favourable MML method would leave fine-grained
semantic alignment across modalities to emerge on its own,
which is worthy of careful investigation in the future.

As the learning scale expands exponentially,
the training data become inevitably noisy and heterogeneous[9,263,199].
It has been recently shown that properly tackling the noise issue is useful[309,263].
Another related facet is training strategy,e.g., how many stages of training is superior over the common one-stage policy[115].
Further, the quadratic complexity with Transformers
becomes more acute for multimodal data due to longer input.
Despite extensive research on efficient variants[49],
dedicated efficiency study for MML is still underestimated even empirically and call for more investigation.

Identifying the strengths of Transformers for multimodal machine learning is a big open problem.
The following main points can be summarized from the literature:(1)Transformers can encode implicit knowledge[32].(2)The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model.
Ideally, multiple heads after training are good and different.
This is essentially a good practice of ensemble learning.(3)Transformers intrinsically have a nature of global aggregation that perceives the non-local patterns.(4)Thanks to the large model capacity, Transformer models handle the challenging domain gaps and shifts (e.g., linguistic and visual) better via effective pretraining on large-scale corpora[294].(5)Transformers can represent the inputs as graphs, which are
intrinsically compatible with more modalities,e.g., table and SQL.(6)For modelling series and sequence patterns (e.g., time-series), Transformers have better training and inference efficiency against RNN-based models,
thanks to their parallel computation in training and/or inference.
Transformers are inherently permutation invariant for processing a sequence of points,e.g., well-suited for point cloud learning[164].(7)Tokenization makes Transformers flexible to organize multimodal inputs, as discussed in Section3.1.1.

This survey focuses on multimodal machine learning with Transformers.
We reviewed the landscape by introducing the Transformer designs and training in the multimodal contexts.
We summarized the key challenges and solutions for this emerging and exciting field.
Moreover, we discussed open problems and potential research directions. We hope that this survey gives a helpful and detailed overview for
new researchers and practitioners, provides
a convenient reference for relevant experts (e.g., multimodal machine learning researchers, Transformer network designers), and encourages
future progress.

[图片: images\image_8.jpg]

[图片: images\image_9.jpg]

[图片: images\image_10.jpg]

Notations and AbbreviationsThroughout this survey,
unless specified otherwise, mathematical symbols and abbreviated terms follow the conventions in TableIV.

Special/Customized TokensIn both uni-modal and multimodal Transformers,
various special/customized tokens are semantically defined as place-holders in the token sequences.
Some common special tokens are summarized in TableV.

[图片: images\image_11.png]

[图片: images\image_12.png]

