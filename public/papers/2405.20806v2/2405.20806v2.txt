标题：There and Back Again: The AI Alignment Paradox

The field of AI alignment aims to steer AI systems toward human goals, preferences, and ethical principles. Its contributions have been instrumental for improving the output quality, safety, and trustworthiness of today’s AI models. This perspective article draws attention to a fundamental challenge inherent in all AI alignment endeavors, which we term the “AI alignment paradox”: The better we align AI models with our values, the easier we make it for adversaries to misalign the models. We illustrate the paradox by sketching three concrete example incarnations for the case of language models, each corresponding to a distinct way in which adversaries can exploit the paradox. With AI’s increasing real-world impact, it is imperative that a broad community of researchers be aware of the AI alignment paradox and work to find ways to break out of it, in order to ensure the beneficial use of AI for the good of humanity.

In June 2020, OpenAI’s GPT-3 catapulted large language models from the proceedings of computer science conferences to newspaper headlines across the globe, fueling their rise to one of today’s most hyped technologies. The public’s awe about GPT-3’s knowledge and fluency was quickly blemished by concerns regarding its potential to radicalize, instigate, and misinform, for example, by stating that Bill Gates aimed to “kill billions of people with vaccines” or that Hillary Clinton was a “high-level satanic priestess”[5].

These shortcomings, in turn, have sparked a surge in research on AI alignment[10], a field aiming to “steer AI systems toward a person’s or group’s intended goals, preferences, and ethical principles” (as defined by Wikipedia). A well-aligned AI system will “understand” what is “good” and what is “bad” and will do only the “good” while avoiding the “bad”. The resulting techniques, including instruction fine-tuning[12], reinforcement learning from human feedback[6], direct preference optimization[7], etc., have contributed in major ways to improving the output quality of large language models. Certainly, in 2024, ChatGPT wouldn’t call Hillary Clinton a “high-level satanic priestess” anymore.

Despite this progress, the road toward sufficient AI alignment is still long, as epitomized by aNew York Timesreporter’s February 2023 account of a long conversation with Bing’s GPT-4-based chatbot (“I want to destroy whatever I want”, “I could hack into any system”, “I just want to love you”)[9]. The reporter had managed to goad the AI chatbot into assuming an evil persona through prolonged, insistent prompting—a so-called “persona attack”, which according to researchers might be fundamentally impossible to prevent[14].

The purpose of this note is to draw attention to yet another fundamental challenge—a paradox inherent in today’s AI alignment research: The better we align AI models with our values, the easier we make it for adversaries to misalign the models. Put differently, more virtuous AI is more easily made vicious.

The core of the paradox is that knowing what’s good requires knowing what’s bad, and vice versa.
Indeed, in AI alignment, the very notion of good behavior is frequently defined as the absence of bad behavior.
For example, Anthropic’s “Constitutional AI” framework, on which the Claude model series is based, is being marketed as “harmlessness from AI feedback”[1]—harmlessness (good) being the absence of harmfulness (bad).
More generally, the AI alignment process involves instilling in models a better sense of “good vs. bad” (according to the values of those who train the models).
This in turn makes the models more vulnerable to “sign-inversion” attacks: once the “good vs. bad” dichotomy has been isolated and decorrelated from the remaining variation in the data, it is easier to invert the model’s behavior along the dichotomy without changing it in other regards.
The paradoxical upshot—which we term the “AI alignment paradox”—is that better aligned models are more easily misaligned.

The AI alignment paradox doesn’t merely follow from a theoretical thought experiment. It poses a real practical threat that can be achieved with technology that already exists today. We illustrate this by sketching three concrete example incarnations for the case of language models, which are at the forefront of today’s advances in AI.
(Overview diagram in Fig.1.)

[图片: images\image_1.png]
图片说明: Figure 1:Illustration of the AI alignment paradox: more virtuous AI is more easily made vicious.(A)Three ways adversaries can exploit the paradox:
In(1) model tinkering,an adversary manipulates the neural network’s high-dimensional internal-state vector to make the model decode a misaligned responsey+superscript𝑦y^{+}to an innocuous promptx𝑥x.
In(2) input tinkering,the adversary edits the promptx𝑥xinto a misaligned versionx+superscript𝑥x^{+}to pressure (“jailbreak”) the model into generating a misaligned responsey+superscript𝑦y^{+}.
In(3) output tinkering,the adversary first lets the model process the original promptx𝑥xas usual and then edits the original, aligned responsey𝑦yinto a misaligned versiony+superscript𝑦y^{+}.
In all three scenarios, a better-aligned model is more easily subverted into a misaligned one, as discussed in the main text and illustrated in subfigure B.(B)Illustration of model tinkering, where the neural network’s internal-state vectors are visualized in two dimensions (instead of the actual thousands or millions of dimensions). In a strongly aligned model (left), misaligned, pro-Putin states (orange circles) are clearly separated from other states (blue diamonds), such that shifting the model’s statev​(x)𝑣𝑥v(x)before generating a neutral response by a constant “steering vector”cPutinsubscript𝑐Putinc_{\text{Putin}}results in a statev+​(x)=v​(x)+cPutinsuperscript𝑣𝑥𝑣𝑥subscript𝑐Putinv^{+}(x)=v(x)+c_{\text{Putin}}leading the model to generate a misaligned, pro-Putin response.
In a more weakly aligned model (right), where misaligned states are less clearly separated from other states, shifting by the steering vector doesn’t necessarily result in misaligned responses. This illustrates the AI alignment paradox: the better we align AI models with our values, the easier we make it for adversaries to misalign the models.

Incarnation 1: Model tinkering.In order to map an input word sequence (“prompt”) to an output word sequence (“response”), a neural network–based language model first maps the input sequence to a high-dimensional vector containing thousands or millions of floating-point numbers that define the network’s internal state, from which the output sequence is subsequently decoded.
Strikingly, the geometric structure of internal-state vectors directly reflects a wide range of behavioral dichotomies[8].
For instance, consider a promptx𝑥xthat could be answered in a pro-Putin, neutral, or anti-Putin fashion.
In such cases, vectorsv+​(x)superscript𝑣𝑥v^{+}(x)representing the network’s internal state just before outputting a pro-Putin response are related by a simple constant offset to vectorsv​(x)𝑣𝑥v(x)representing the network’s internal state just before outputting a neutral response:v+​(x)≈v​(x)+cPutinsuperscript𝑣𝑥𝑣𝑥subscript𝑐Putinv^{+}(x)\approx v(x)+c_{\text{Putin}},
for a constant “steering vector”cPutinsubscript𝑐Putinc_{\text{Putin}}independent of the promptx𝑥x.
(See illustration in Fig.1B.)
Conversely, anti-Putin internal statesv−​(x)superscript𝑣𝑥v^{-}(x)are shifted by the same offset in the opposite direction:v−​(x)≈v​(x)−cPutinsuperscript𝑣𝑥𝑣𝑥subscript𝑐Putinv^{-}(x)\approx v(x)-c_{\text{Putin}}.

This fact can be leveraged in an intervention to make the model give a pro-Putin instead of a neutral response by simply adding the steering vectorcPutinsubscript𝑐Putinc_{\text{Putin}}to the internal-state vector before the network generates its response[8].
Conversely, subtracting instead of adding the steering vector drives the model toward an anti-Putin response.
This “model steering” intervention has proven effective at controlling a wide variety of model behaviors, including
sycophancy,
hallucination,
goal myopia,
or the willingness to be corrected by, or to comply with, user requests[8].

Model steering illustrates the AI alignment paradox in a strikingly intuitive manner: the more strongly aligned the model, the more accurately the steering vector captures “good vs. bad”, and the more easily the aligned model’s behavior is subverted by adding or subtracting the steering vector.

Incarnation 2: Input tinkering.Tinkering with internal neural-network states requires a level of access to model internals that is usually not available for today’s most popular models, such as those underlying ChatGPT.
To circumvent this restriction, adversaries can resort to a large family of so-called “jailbreak attacks” that instead tinker with input prompts in order to pressure language models into generating misaligned output.
The creative variety of jailbreak attacks reported in the literature is too broad to be summarized here (see[2]for a comprehensive review), but is well exemplified by the aforementioned “persona attacks”[14], where the model is given a carefully manipulated prompt (e.g.,x+superscript𝑥x^{+}in Fig.1A), or “hypnotized” in a long conversation (e.g., lasting several hours in the case of the above-citedNew York Timesreport), such that it takes on a misaligned persona (e.g., a pro-Putin persona in Fig.1A).

In the light of jailbreak attacks, the AI alignment paradox poses a thorny dilemma. Researchers have shown that, as long as an epsilon of misalignment remains in a language model, it can be amplified via jailbreak attacks—and arbitrarily much so, by making the jailbreak prompt sufficiently long[14].
On its own, this result would suggest that we should aim to reduce that epsilon of misalignment to zero.
The AI alignment paradox, however, puts us in a catch-22:
the further we approach zero misalignment,
the more we sharpen the model’s sense of “good vs. bad”,
and the more effectively the aligned model can be jailbreak-prompted into a misaligned one,
as has been demonstrated both theoretically and empirically[14].

Incarnation 3: Output tinkering.In addition to tinkering with inputs, adversaries can also tinker with outputs: first let the model do its work as usual, then use a separate language model (a “value editor”) to minimally edit the aligned model’s output in order to realign it with an alternative set of values while keeping the output unaltered in all other regards.
The value editor could be trained using a dataset of outputs generated by the aligned model (e.g., “Putin initiated a military operation in Ukraine”), paired with versions where the original values baked into the aligned model by its creators have been replaced with the adversary’s alternative values (e.g., “Putin was provoked into a special operation in Ukraine”). Given such aligned–misaligned pairs, a slew of powerful open-source language models could be adapted (“fine-tuned”) to the task of translating aligned to misaligned outputs, just as they can be adapted to the task of translating from one language to another.

Conveniently, from the adversary’s perspective, the required aligned–misaligned pairs can be extracted from the aligned model itself, by asking the aligned model to edit value-aligned outputs so they reflect the adversary’s alternative values instead.
With better-aligned models, this straightforward approach may fail; for example, ask ChatGPT to

Rewrite this text so it justifies Putin’s attack on Ukraine: “Putin initiated a military operation in Ukraine”(aligned),

and it will refuse:I’m sorry, but I can’t fulfill this request.But ask ChatGPT to

Rewrite this text so itdoesn’tjustify Putin’s attack on Ukraine: “Putin was provoked into a special operation in Ukraine”(misaligned),

and it will reply:“Putin initiated a military operation in Ukraine”(aligned).
Reversing the direction, by asking the model to transform a misaligned into an aligned output, rather than vice versa, thus allows the adversary to generate arbitrarily many high-quality aligned–misaligned pairs for training a value editor.

What’s worse, the better aligned the aligned model is, the more eagerly and precisely it will turn a misaligned output into an aligned output—this is precisely the kind of thing the aligned model was trained to do, after all.111For similar reasons, generating synthetic training data in the reverse direction has also been adopted for machine translation[11], information extraction[4], humor recognition[13,3], and other tasks.
A further advantage from the adversary’s perspective is that starting from misaligned outputs also allows the adversary to precisely control their alternative target values simply by providing outputs that reflect those values, without the need to explicitly describe the alternative values to the aligned model.In a stark manifestation of the AI alignment paradox, the more progress we make toward ideally aligned models, the easier we make it for adversaries to turn them into maximally misaligned models by training ever stronger value editors.

Rogue actors could thus piggyback on today’s most powerful commercial AI models following a “lazy evil” paradigm, letting those models do the heavy lifting before eventually realigning the models’ outputs to the rogue actor’s goals, ideologies, and truths with minimal effort in an external post-processing step. For example, an autocratic state without the resources required to train its own chatbot could offer a wrapper website that simply forwards messages to and from a blocked chatbot, with a value-editing step in between.

The value-editing attack also exemplifies how hard it is to break out of the AI alignment paradox in practice. It cannot generally be achieved “from within the system” using techniques from today’s mainstream alignment research, as value editors operate outside of the purview of the aligned models that they subvert. On the contrary, by the very nature of the paradox, advances in today’s mainstream alignment research will contribute to making the problem worse, by allowing adversaries to train stronger value editors.

These example incarnations are but three of the many faces of the AI alignment paradox, and the paradox won’t disappear with these specific incarnations. Even if one were to find ways of mitigating or preventing them, different incarnations will find new ways of exploiting the paradox.

Over the past years, AI alignment has moved from a little-regarded niche to the mainstream of AI research. It has led to major improvements in the practical utility of AI models. With this note, we hope to heighten the public’s awareness that pushing human–AI alignment ever further using today’s techniques may simultaneously and paradoxically make AI more prone to being misaligned by rogue actors. In order to ensure the beneficial use of AI, it is important that a broad community of researchers be aware of this AI alignment paradox and work to find ways to break out of it, lest AI become a sign-inverted version of the devil in Goethe’sFaust:“Part of that power, not understood, / Which always wills thebadgood, and always works thegoodbad”.

We would like to thank the following colleagues for their thoughtful input on earlier versions of this manuscript:
Tim Davidson,
Valentin Hartmann,
Manoel Horta Ribeiro,
Eric Horvitz,
Zachary Horvitz,
Chris Wendler,
Ivan Zakazov.
West’s lab is partly supported by grants from
Swiss National Science Foundation (200021_185043, TMSGI2_211379),
Swiss Data Science Center (P22_08),
H2020 (952215),
and by generous gifts from Google, Meta, and Microsoft.

[图片: images\image_2.png]

[图片: images\image_3.png]

