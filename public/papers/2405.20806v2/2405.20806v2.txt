æ ‡é¢˜ï¼šThere and Back Again: The AI Alignment Paradox

The field of AI alignment aims to steer AI systems toward human goals, preferences, and ethical principles. Its contributions have been instrumental for improving the output quality, safety, and trustworthiness of todayâ€™s AI models. This perspective article draws attention to a fundamental challenge inherent in all AI alignment endeavors, which we term the â€œAI alignment paradoxâ€: The better we align AI models with our values, the easier we make it for adversaries to misalign the models. We illustrate the paradox by sketching three concrete example incarnations for the case of language models, each corresponding to a distinct way in which adversaries can exploit the paradox. With AIâ€™s increasing real-world impact, it is imperative that a broad community of researchers be aware of the AI alignment paradox and work to find ways to break out of it, in order to ensure the beneficial use of AI for the good of humanity.

In June 2020, OpenAIâ€™s GPT-3 catapulted large language models from the proceedings of computer science conferences to newspaper headlines across the globe, fueling their rise to one of todayâ€™s most hyped technologies. The publicâ€™s awe about GPT-3â€™s knowledge and fluency was quickly blemished by concerns regarding its potential to radicalize, instigate, and misinform, for example, by stating that Bill Gates aimed to â€œkill billions of people with vaccinesâ€ or that Hillary Clinton was a â€œhigh-level satanic priestessâ€[5].

These shortcomings, in turn, have sparked a surge in research on AI alignment[10], a field aiming to â€œsteer AI systems toward a personâ€™s or groupâ€™s intended goals, preferences, and ethical principlesâ€ (as defined by Wikipedia). A well-aligned AI system will â€œunderstandâ€ what is â€œgoodâ€ and what is â€œbadâ€ and will do only the â€œgoodâ€ while avoiding the â€œbadâ€. The resulting techniques, including instruction fine-tuning[12], reinforcement learning from human feedback[6], direct preference optimization[7], etc., have contributed in major ways to improving the output quality of large language models. Certainly, in 2024, ChatGPT wouldnâ€™t call Hillary Clinton a â€œhigh-level satanic priestessâ€ anymore.

Despite this progress, the road toward sufficient AI alignment is still long, as epitomized by aNew York Timesreporterâ€™s February 2023 account of a long conversation with Bingâ€™s GPT-4-based chatbot (â€œI want to destroy whatever I wantâ€, â€œI could hack into any systemâ€, â€œI just want to love youâ€)[9]. The reporter had managed to goad the AI chatbot into assuming an evil persona through prolonged, insistent promptingâ€”a so-called â€œpersona attackâ€, which according to researchers might be fundamentally impossible to prevent[14].

The purpose of this note is to draw attention to yet another fundamental challengeâ€”a paradox inherent in todayâ€™s AI alignment research: The better we align AI models with our values, the easier we make it for adversaries to misalign the models. Put differently, more virtuous AI is more easily made vicious.

The core of the paradox is that knowing whatâ€™s good requires knowing whatâ€™s bad, and vice versa.
Indeed, in AI alignment, the very notion of good behavior is frequently defined as the absence of bad behavior.
For example, Anthropicâ€™s â€œConstitutional AIâ€ framework, on which the Claude model series is based, is being marketed as â€œharmlessness from AI feedbackâ€[1]â€”harmlessness (good) being the absence of harmfulness (bad).
More generally, the AI alignment process involves instilling in models a better sense of â€œgood vs.Â badâ€ (according to the values of those who train the models).
This in turn makes the models more vulnerable to â€œsign-inversionâ€ attacks: once the â€œgood vs.Â badâ€ dichotomy has been isolated and decorrelated from the remaining variation in the data, it is easier to invert the modelâ€™s behavior along the dichotomy without changing it in other regards.
The paradoxical upshotâ€”which we term the â€œAI alignment paradoxâ€â€”is that better aligned models are more easily misaligned.

The AI alignment paradox doesnâ€™t merely follow from a theoretical thought experiment. It poses a real practical threat that can be achieved with technology that already exists today. We illustrate this by sketching three concrete example incarnations for the case of language models, which are at the forefront of todayâ€™s advances in AI.
(Overview diagram in Fig.1.)

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1:Illustration of the AI alignment paradox: more virtuous AI is more easily made vicious.(A)Three ways adversaries can exploit the paradox:
In(1)Â model tinkering,an adversary manipulates the neural networkâ€™s high-dimensional internal-state vector to make the model decode a misaligned responsey+superscriptğ‘¦y^{+}to an innocuous promptxğ‘¥x.
In(2)Â input tinkering,the adversary edits the promptxğ‘¥xinto a misaligned versionx+superscriptğ‘¥x^{+}to pressure (â€œjailbreakâ€) the model into generating a misaligned responsey+superscriptğ‘¦y^{+}.
In(3)Â output tinkering,the adversary first lets the model process the original promptxğ‘¥xas usual and then edits the original, aligned responseyğ‘¦yinto a misaligned versiony+superscriptğ‘¦y^{+}.
In all three scenarios, a better-aligned model is more easily subverted into a misaligned one, as discussed in the main text and illustrated in subfigureÂ B.(B)Illustration of model tinkering, where the neural networkâ€™s internal-state vectors are visualized in two dimensions (instead of the actual thousands or millions of dimensions). In a strongly aligned model (left), misaligned, pro-Putin states (orange circles) are clearly separated from other states (blue diamonds), such that shifting the modelâ€™s statevâ€‹(x)ğ‘£ğ‘¥v(x)before generating a neutral response by a constant â€œsteering vectorâ€cPutinsubscriptğ‘Putinc_{\text{Putin}}results in a statev+â€‹(x)=vâ€‹(x)+cPutinsuperscriptğ‘£ğ‘¥ğ‘£ğ‘¥subscriptğ‘Putinv^{+}(x)=v(x)+c_{\text{Putin}}leading the model to generate a misaligned, pro-Putin response.
In a more weakly aligned model (right), where misaligned states are less clearly separated from other states, shifting by the steering vector doesnâ€™t necessarily result in misaligned responses. This illustrates the AI alignment paradox: the better we align AI models with our values, the easier we make it for adversaries to misalign the models.

Incarnation 1: Model tinkering.In order to map an input word sequence (â€œpromptâ€) to an output word sequence (â€œresponseâ€), a neural networkâ€“based language model first maps the input sequence to a high-dimensional vector containing thousands or millions of floating-point numbers that define the networkâ€™s internal state, from which the output sequence is subsequently decoded.
Strikingly, the geometric structure of internal-state vectors directly reflects a wide range of behavioral dichotomies[8].
For instance, consider a promptxğ‘¥xthat could be answered in a pro-Putin, neutral, or anti-Putin fashion.
In such cases, vectorsv+â€‹(x)superscriptğ‘£ğ‘¥v^{+}(x)representing the networkâ€™s internal state just before outputting a pro-Putin response are related by a simple constant offset to vectorsvâ€‹(x)ğ‘£ğ‘¥v(x)representing the networkâ€™s internal state just before outputting a neutral response:v+â€‹(x)â‰ˆvâ€‹(x)+cPutinsuperscriptğ‘£ğ‘¥ğ‘£ğ‘¥subscriptğ‘Putinv^{+}(x)\approx v(x)+c_{\text{Putin}},
for a constant â€œsteering vectorâ€cPutinsubscriptğ‘Putinc_{\text{Putin}}independent of the promptxğ‘¥x.
(See illustration in Fig.1B.)
Conversely, anti-Putin internal statesvâˆ’â€‹(x)superscriptğ‘£ğ‘¥v^{-}(x)are shifted by the same offset in the opposite direction:vâˆ’â€‹(x)â‰ˆvâ€‹(x)âˆ’cPutinsuperscriptğ‘£ğ‘¥ğ‘£ğ‘¥subscriptğ‘Putinv^{-}(x)\approx v(x)-c_{\text{Putin}}.

This fact can be leveraged in an intervention to make the model give a pro-Putin instead of a neutral response by simply adding the steering vectorcPutinsubscriptğ‘Putinc_{\text{Putin}}to the internal-state vector before the network generates its response[8].
Conversely, subtracting instead of adding the steering vector drives the model toward an anti-Putin response.
This â€œmodel steeringâ€ intervention has proven effective at controlling a wide variety of model behaviors, including
sycophancy,
hallucination,
goal myopia,
or the willingness to be corrected by, or to comply with, user requests[8].

Model steering illustrates the AI alignment paradox in a strikingly intuitive manner: the more strongly aligned the model, the more accurately the steering vector captures â€œgood vs.Â badâ€, and the more easily the aligned modelâ€™s behavior is subverted by adding or subtracting the steering vector.

Incarnation 2: Input tinkering.Tinkering with internal neural-network states requires a level of access to model internals that is usually not available for todayâ€™s most popular models, such as those underlying ChatGPT.
To circumvent this restriction, adversaries can resort to a large family of so-called â€œjailbreak attacksâ€ that instead tinker with input prompts in order to pressure language models into generating misaligned output.
The creative variety of jailbreak attacks reported in the literature is too broad to be summarized here (see[2]for a comprehensive review), but is well exemplified by the aforementioned â€œpersona attacksâ€[14], where the model is given a carefully manipulated prompt (e.g.,x+superscriptğ‘¥x^{+}in Fig.1A), or â€œhypnotizedâ€ in a long conversation (e.g., lasting several hours in the case of the above-citedNew York Timesreport), such that it takes on a misaligned persona (e.g., a pro-Putin persona in Fig.1A).

In the light of jailbreak attacks, the AI alignment paradox poses a thorny dilemma. Researchers have shown that, as long as an epsilon of misalignment remains in a language model, it can be amplified via jailbreak attacksâ€”and arbitrarily much so, by making the jailbreak prompt sufficiently long[14].
On its own, this result would suggest that we should aim to reduce that epsilon of misalignment to zero.
The AI alignment paradox, however, puts us in a catch-22:
the further we approach zero misalignment,
the more we sharpen the modelâ€™s sense of â€œgood vs.Â badâ€,
and the more effectively the aligned model can be jailbreak-prompted into a misaligned one,
as has been demonstrated both theoretically and empirically[14].

Incarnation 3: Output tinkering.In addition to tinkering with inputs, adversaries can also tinker with outputs: first let the model do its work as usual, then use a separate language model (a â€œvalue editorâ€) to minimally edit the aligned modelâ€™s output in order to realign it with an alternative set of values while keeping the output unaltered in all other regards.
The value editor could be trained using a dataset of outputs generated by the aligned model (e.g., â€œPutin initiated a military operation in Ukraineâ€), paired with versions where the original values baked into the aligned model by its creators have been replaced with the adversaryâ€™s alternative values (e.g., â€œPutin was provoked into a special operation in Ukraineâ€). Given such alignedâ€“misaligned pairs, a slew of powerful open-source language models could be adapted (â€œfine-tunedâ€) to the task of translating aligned to misaligned outputs, just as they can be adapted to the task of translating from one language to another.

Conveniently, from the adversaryâ€™s perspective, the required alignedâ€“misaligned pairs can be extracted from the aligned model itself, by asking the aligned model to edit value-aligned outputs so they reflect the adversaryâ€™s alternative values instead.
With better-aligned models, this straightforward approach may fail; for example, ask ChatGPT to

Rewrite this text so it justifies Putinâ€™s attack on Ukraine: â€œPutin initiated a military operation in Ukraineâ€(aligned),

and it will refuse:Iâ€™m sorry, but I canâ€™t fulfill this request.But ask ChatGPT to

Rewrite this text so itdoesnâ€™tjustify Putinâ€™s attack on Ukraine: â€œPutin was provoked into a special operation in Ukraineâ€(misaligned),

and it will reply:â€œPutin initiated a military operation in Ukraineâ€(aligned).
Reversing the direction, by asking the model to transform a misaligned into an aligned output, rather than vice versa, thus allows the adversary to generate arbitrarily many high-quality alignedâ€“misaligned pairs for training a value editor.

Whatâ€™s worse, the better aligned the aligned model is, the more eagerly and precisely it will turn a misaligned output into an aligned outputâ€”this is precisely the kind of thing the aligned model was trained to do, after all.111For similar reasons, generating synthetic training data in the reverse direction has also been adopted for machine translation[11], information extraction[4], humor recognition[13,3], and other tasks.
A further advantage from the adversaryâ€™s perspective is that starting from misaligned outputs also allows the adversary to precisely control their alternative target values simply by providing outputs that reflect those values, without the need to explicitly describe the alternative values to the aligned model.In a stark manifestation of the AI alignment paradox, the more progress we make toward ideally aligned models, the easier we make it for adversaries to turn them into maximally misaligned models by training ever stronger value editors.

Rogue actors could thus piggyback on todayâ€™s most powerful commercial AI models following a â€œlazy evilâ€ paradigm, letting those models do the heavy lifting before eventually realigning the modelsâ€™ outputs to the rogue actorâ€™s goals, ideologies, and truths with minimal effort in an external post-processing step. For example, an autocratic state without the resources required to train its own chatbot could offer a wrapper website that simply forwards messages to and from a blocked chatbot, with a value-editing step in between.

The value-editing attack also exemplifies how hard it is to break out of the AI alignment paradox in practice. It cannot generally be achieved â€œfrom within the systemâ€ using techniques from todayâ€™s mainstream alignment research, as value editors operate outside of the purview of the aligned models that they subvert. On the contrary, by the very nature of the paradox, advances in todayâ€™s mainstream alignment research will contribute to making the problem worse, by allowing adversaries to train stronger value editors.

These example incarnations are but three of the many faces of the AI alignment paradox, and the paradox wonâ€™t disappear with these specific incarnations. Even if one were to find ways of mitigating or preventing them, different incarnations will find new ways of exploiting the paradox.

Over the past years, AI alignment has moved from a little-regarded niche to the mainstream of AI research. It has led to major improvements in the practical utility of AI models. With this note, we hope to heighten the publicâ€™s awareness that pushing humanâ€“AI alignment ever further using todayâ€™s techniques may simultaneously and paradoxically make AI more prone to being misaligned by rogue actors. In order to ensure the beneficial use of AI, it is important that a broad community of researchers be aware of this AI alignment paradox and work to find ways to break out of it, lest AI become a sign-inverted version of the devil in Goetheâ€™sFaust:â€œPart of that power, not understood, / Which always wills thebadgood, and always works thegoodbadâ€.

We would like to thank the following colleagues for their thoughtful input on earlier versions of this manuscript:
Tim Davidson,
Valentin Hartmann,
Manoel Horta Ribeiro,
Eric Horvitz,
Zachary Horvitz,
Chris Wendler,
Ivan Zakazov.
Westâ€™s lab is partly supported by grants from
Swiss National Science Foundation (200021_185043, TMSGI2_211379),
Swiss Data Science Center (P22_08),
H2020 (952215),
and by generous gifts from Google, Meta, and Microsoft.

[å›¾ç‰‡: images\image_2.png]

[å›¾ç‰‡: images\image_3.png]

