æ ‡é¢˜ï¼šSome Insights into Lifelong Reinforcement Learning Systems

A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environmentover its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system.

Anagentis an abstraction of a decision-maker. At each time instancetğ‘¡t, it receives an observationotâˆˆOsubscriptğ‘œğ‘¡ğ‘‚o_{t}\in O, and outputs an actionatâˆˆAsubscriptğ‘ğ‘¡ğ´a_{t}\in Ato be carried out in the environment it lives in. Here,Oğ‘‚Ois the (finite) set of possible observations the agent can receive, andAğ´Ais the (finite) set of actions the agent can choose from. An agentâ€™s observationotsubscriptğ‘œğ‘¡o_{t}depends on the current environment statestâˆˆSsubscriptğ‘ ğ‘¡ğ‘†s_{t}\in Sthrough an agent observation functionSâ†’Oâ†’ğ‘†ğ‘‚S\to O, whereSğ‘†Sis the set of possible environment states. Theobservation historyhto=(o1,o2â€‹â€¦,ot)subscriptsuperscriptâ„ğ‘œğ‘¡subscriptğ‘œ1subscriptğ‘œ2â€¦subscriptğ‘œğ‘¡h^{o}_{t}=(o_{1},o_{2}...,o_{t})is the sequence of observations the agent has receivedtill timetğ‘¡t. LetHtosubscriptsuperscriptğ»ğ‘œğ‘¡H^{o}_{t}be the set of possible observation histories oflengthtğ‘¡t, thepolicyÏ€t:Htoâ†’A:subscriptğœ‹ğ‘¡â†’subscriptsuperscriptğ»ğ‘œğ‘¡ğ´\pi_{t}:H^{o}_{t}\to Aat timetğ‘¡tis defined as the mapping from an observation history of lengthtğ‘¡tto the action the agent will take. An agentâ€™s behavior can thus be fully specified by its policy across all timestepsÏ€=(Ï€1,Ï€2,â€¦,Ï€t,â€¦)ğœ‹subscriptğœ‹1subscriptğœ‹2â€¦subscriptğœ‹ğ‘¡â€¦\pi=(\pi_{1},\pi_{2},...,\pi_{t},...). Throughout the paper, it is assumed that an agent has a finite lifespanTğ‘‡T.

We are interested in agents that can achieve some goal. In reinforcement learning, a goal is expressed by a scalar signalrtâˆˆâ„subscriptğ‘Ÿğ‘¡â„r_{t}\in\mathbb{R}called thereward. The reward is dependent on the agentâ€™s observation history, and is assumed to be available to the agent at each timestep in addition to the observationotsubscriptğ‘œğ‘¡o_{t}. Our aim is to find policies that maximize the expected cumulative reward an agent receives over its lifetime:

Using the maximization of expected cumulative scalar reward to formulate the general notion of goal is a design choice in reinforcement learning, based on what is commonly known as thereward hypothesis(Sutton & Barto,2018), In Suttonâ€™s own words:

That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).

This design choice, however, is somewhat arbitrary. Among other things, the reward needs not be a scalar (e.g. multi-objective reinforcement learning(White,1982)), nor does it have to be a quantity whose cumulative sum is to be maximized (which we will come to shortly). Leaving aside the question of whether or not all goalscanbe formulated by Eq.1, I intend to show in this paper that the problem of lifelong reinforcement learning probablyshould notbe formulated as such.

Note that in Eq.1, I defined the reward in terms of the observation history, instead of the history of environment states as in most reinforcement learning literature. This reflects the view that reward signals are internal to the agent, as pointed out by Singh et al.(2004)in their work on intrinsic motivation. Since the observations are all that the agent has access to from the external environment, the intrinsic reward should depend on the environment stateonlythrough the agentâ€™s observation history.

Although the above reinforcement learning formulation recognizes the reward as a signal intrinsic to an agent, it focuses on learning across different generations111Usage of the word â€˜generationâ€™ here is only to emphasize that learning cannot be achieved within an agentâ€™s lifespan, and does not imply that evolution algorithms need to be used.of agents, as opposed to learning within an agentâ€™s lifespan. From an agentâ€™s point of view, the cumulative reward is known only when it reaches its end of life, by which time no learning can be done by the â€˜dyingâ€™ agent itself. The individual reward received at each timestep does not really matter, since the optimization objective is the cumulative sum (of reward). The information gathered by the agent, however, can be used to improve the policy of the next generation. In other words, with the conventional reinforcement learning formulation, learning can only happen at a level higher than the lives of individual agents (Figure1), with the goal that an optimal agent can eventually be found â€” the lifetime behavior of a particular agent is not of concern.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: Figure 1:Architecture of a traditional reinforcement learning system. At the beginning of an agentâ€™s life, it receives a policyÏ€i=(Ï€1i,Ï€2i,â€¦â€‹Ï€Ti)superscriptğœ‹ğ‘–subscriptsuperscriptğœ‹ğ‘–1subscriptsuperscriptğœ‹ğ‘–2â€¦subscriptsuperscriptğœ‹ğ‘–ğ‘‡\pi^{i}=(\pi^{i}_{1},\pi^{i}_{2},...\pi^{i}_{T})from the learning algorithm that carries out a mix of exploitation and exploration, where the superscriptiğ‘–iindicates that the agent belongs to theiğ‘–ith generation. The agent receives an observationotsubscriptğ‘œğ‘¡o_{t}at each timesteptğ‘¡t, and act according toÏ€tisubscriptsuperscriptğœ‹ğ‘–ğ‘¡\pi^{i}_{t}. At the end of the agentâ€™s life, the learning algorithm gathers the observation historyhTosubscriptsuperscriptâ„ğ‘œğ‘‡h^{o}_{T}and the cumulative rewardâˆ‘t=1Trâ€‹(hto)superscriptsubscriptğ‘¡1ğ‘‡ğ‘Ÿsubscriptsuperscriptâ„ğ‘œğ‘¡\sum_{t=1}^{T}{r(h^{o}_{t})}from the agent, and outputs the the next policyÏ€i+1superscriptğœ‹ğ‘–1\pi^{i+1}to be executed. The learning algorithm does not need to optimize the performance of any particularÏ€isuperscriptğœ‹ğ‘–\pi^{i}, as long as it is guaranteed to be able to eventually find the policies that maximize the expected cumulative reward.

In lifelong reinforcement learning, on the other hand, the focus is the agentâ€™s ability to learn and adapt to the environment throughout its lifetime. Intuitively, this implies that learning component of the learning system should reside within the agent.

To shed some lights on lifelong reinforcement learning, consider the Q-learning(Watkins & Dayan,1992)algorithm for the standard reinforcement learning problem formulated by Eq.1. For the purpose of this example only, it is further assumed that:

The reward depends only on the current observation. I.e.,râ€‹(hto)=râ€‹(ot)ğ‘Ÿsubscriptsuperscriptâ„ğ‘œğ‘¡ğ‘Ÿsubscriptğ‘œğ‘¡r(h^{o}_{t})=r(o_{t})

Observations are Markov with respect to past observations and actions. I.e.,Pâ€‹(ot|otâˆ’1,atâˆ’1,â€¦,o1,a1)=Pâ€‹(ot|otâˆ’1,atâˆ’1)ğ‘ƒconditionalsubscriptğ‘œğ‘¡subscriptğ‘œğ‘¡1subscriptğ‘ğ‘¡1â€¦subscriptğ‘œ1subscriptğ‘1ğ‘ƒconditionalsubscriptğ‘œğ‘¡subscriptğ‘œğ‘¡1subscriptğ‘ğ‘¡1P(o_{t}|o_{t-1},a_{t-1},...,o_{1},a_{1})=P(o_{t}|o_{t-1},a_{t-1})

These assumptions are only made so that Q-learning will find the solution to Eq.1, and are not essential for the general discussion. The (non-lifelong) learning system works as follows:

The agent receives its initial Q estimate from the past generation.

At each timesteptğ‘¡t, the agent takes anÏµitalic-Ïµ\epsilon-greedy action based on the current Q estimate, then does a Bellman update on the Q estimate:

When the agent dies, pass the updated Q estimate to the next generation.

At first sight, the fact that the Q estimate is updated every timestep seems to contradict my argument that learning only happens across generations. However, for Eq.2to be a valid update, the timesteptğ‘¡tneeds to be part of the observation â€” the observationotsubscriptğ‘œğ‘¡o_{t}here is in fact the raw observationotâˆ’subscriptsuperscriptğ‘œğ‘¡o^{-}_{t}augmented by timetğ‘¡t, i.e.,ot=(otâˆ’,t)subscriptğ‘œğ‘¡subscriptsuperscriptğ‘œğ‘¡ğ‘¡o_{t}=(o^{-}_{t},t). Since the timestep is part of the observation, no same observation will be experienced more than once throughout the agentâ€™s lifetime, and it makes no differenceto the agentwhether the Q estimate is updated every timestep, or after its life ends222The statement does not strictly hold true if function approximation is used. An update toQÎ¸â€‹(ot,a)subscriptğ‘„ğœƒsubscriptğ‘œğ‘¡ğ‘Q_{\theta}(o_{t},a)can potentially affect the Q estimate of all other observations. However, this is more a side effect than a desired property..

Itâ€™s clear that for an agent to exhibit any sensible behavior, the initial Q estimate it inherits from the past generation is vital. If the agent receives a random initial Q estimate, then itâ€™s lifelong behavior is bound to be random and meaningless. On the other side of the spectrum, if the agent receives the true Q function, then it will behave optimally. This suggests that if we care about the lifetime behaviour (which includes lifelong learning behavior)of a Q-learning agent, thenQâ€‹(ot,â‹…)ğ‘„subscriptğ‘œğ‘¡â‹…Q(o_{t},\cdot)is a fundamental signal the agent needs to receive in addition to the scalar reward. In a sense, if the signal represented by the scalar reward is a specification of what the goalis, then the signal represented by the Q estimate is the knowledge past generations have collected about what the goalmeansfor this type of agent. As an analogy, the pain associated with falling to the ground could be the former signal, while the innate fear of height could be the latter.

From a computational perspective, the separation of these two signals may not be necessary. Both signals can be considered as â€˜annotationsâ€™ for the observation history that the agent receives along with its observation, and can be incorporated into the concept of reward. The reward signals are no longer restricted scalars, nor are they necessarily quantities whose cumulative sum is to be maximized â€” they are just messages in somereward languagethat â€˜encodeâ€™ the knowledge pertaining to an agentâ€™s observation history â€” knowledge that enables the agent to learn continuously throughout its life. Such knowledge may include the goals of the agent, the subgoals that constitute these goals, the heuristics for achieving them, and so on. The reward is then â€˜decodedâ€™ by the learning algorithm, which defines how the agent responds to the reward given the observation history. The learning system should be designed such that by responding to the reward in its intended way, the agent will learn to achieve the goals implied by the reward before its end of life (Figure2).

To be precise, the rewardrâ€‹(hto)âˆˆÎ£ğ‘Ÿsubscriptsuperscriptâ„ğ‘œğ‘¡Î£r(h^{o}_{t})\in\Sigmanow belongs to somereward spaceÎ£Î£\Sigma. Thelearning algorithmis a mapping from reward histories to policies. Denoting the set of possible reward history of lengthtğ‘¡tasHtrsubscriptsuperscriptğ»ğ‘Ÿğ‘¡H^{r}_{t}, and the set of all possible policies at timetğ‘¡tasÎ tsubscriptÎ ğ‘¡\Pi_{t}, the learning algorithmmğ‘šmcan be represented bym=(m1,m2,â€¦,mt,â€¦,mT)ğ‘šsubscriptğ‘š1subscriptğ‘š2â€¦subscriptğ‘šğ‘¡â€¦subscriptğ‘šğ‘‡m=(m_{1},m_{2},...,m_{t},...,m_{T}), wheremt:Htrâ†’Î t:subscriptğ‘šğ‘¡â†’subscriptsuperscriptğ»ğ‘Ÿğ‘¡subscriptÎ ğ‘¡m_{t}:H^{r}_{t}\to\Pi_{t}. The formulation is general, and a learning system formulated as such is not automatically a lifelong learning system. In fact, it subsumes traditional reinforcement learning: the reward space is set to the real numbers (Î£=â„Î£â„\Sigma=\mathbb{R}), and the learning algorithm can be set to any algorithm that converges to a policy that maximizes the expected cumulative reward. Unfortunately, the reward in traditional reinforcement learning does not contain enough information for an agent to learnwithinits lifetime.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: Figure 2:Architecture of lifelong reinforcement learning system. In contrast to traditional reinforcement learning (Figure1), the learning algorithm resides inside the agent. The internal environment of the agent can be thought of as a built-in mechanism for the agent-designer to communicate with the agent (through the reward). At each timestep, the learning algorithm receives some message (encoded in the form of rewardrâ€‹(hto)ğ‘Ÿsubscriptsuperscriptâ„ğ‘œğ‘¡r(h^{o}_{t})) from the agentâ€™s internal environment, and outputs a policyÏ€tsubscriptğœ‹ğ‘¡\pi_{t}as a response.

Viewing the reward as a general language, and the learning algorithm as the response to the reward opens up the possibilities for principled ways to embed learning bias such as guidance and intrinsic motivation into the learning system, instead of relying solely on manipulating the scalar reward on an ad-hoc basis. In the rest of the paper, my focus remains on lifelong reinforcement learning, more specifically, what lifelong reinforcement learning requires of the reward language and the corresponding learning algorithm.

Although the term â€˜languageâ€™ used above can be understood in its colloquial sense, it can also be understood as the formal term in automata theory. To see this, consider the followingdeterministic finite automatonâŸ¨Î£,Q,Î´,q0,FâŸ©Î£ğ‘„ğ›¿subscriptğ‘0ğ¹\langle\Sigma,Q,\delta,q_{0},F\rangle, where:

Î£Î£\Sigmais thealphabetof the automaton, and is set to the reward space of the learning system. In other words, the alphabet of this automaton consists of all possible reward the agent can receive at any single timestep. Astringis a sequence of symbols chosen from some alphabet. For this particular automaton, a string is in fact a sequence of reward, so the notation for reward historyhtrsubscriptsuperscriptâ„ğ‘Ÿğ‘¡h^{r}_{t}is also used to denote a string of lengthtğ‘¡t. The set of all strings of lengthkğ‘˜koverÎ£Î£\Sigmais denoted asÎ£ksuperscriptÎ£ğ‘˜\Sigma^{k}, and the set of all strings (of any length) is denoted asÎ£âˆ—superscriptÎ£\Sigma^{*}.

Qğ‘„Qis the set of states of the automaton. Each state of this automaton is a possible pair of reward history and policies till some timesteptğ‘¡t. For example, members ofQğ‘„Qinclude:

for anyÏ€1âˆˆÎ 1subscriptğœ‹1subscriptÎ 1\pi_{1}\in\Pi_{1},Ï€2âˆˆÎ 2subscriptğœ‹2subscriptÎ 2\pi_{2}\in\Pi_{2}, â€¦,Ï€TâˆˆÎ Tsubscriptğœ‹ğ‘‡subscriptÎ ğ‘‡\pi_{T}\in\Pi_{T}, andht=1râˆˆÎ£1subscriptsuperscriptâ„ğ‘Ÿğ‘¡1superscriptÎ£1h^{r}_{t=1}\in\Sigma^{1},ht=2râˆˆÎ£2subscriptsuperscriptâ„ğ‘Ÿğ‘¡2superscriptÎ£2h^{r}_{t=2}\in\Sigma^{2}, â€¦,ht=TrâˆˆÎ£Tsubscriptsuperscriptâ„ğ‘Ÿğ‘¡ğ‘‡superscriptÎ£ğ‘‡h^{r}_{t=T}\in\Sigma^{T}. In addition, Q has a special â€˜emptyâ€™ memberq0subscriptğ‘0q_{0}, which corresponds to the initial state before any reward is received.

Î´:(QÃ—Î£)â†’Q:ğ›¿â†’ğ‘„Î£ğ‘„\delta:(Q\times\Sigma)\to Qis thetransition function. The transition function corresponds to the learning algorithm of the learning system, so we haveÎ´â€‹(âŸ¨htr,(Ï€1,â€¦,Ï€t)âŸ©,rt+1)=âŸ¨ht+1r,(Ï€1,â€¦,Ï€t,mt+1â€‹(ht+1r))âŸ©ğ›¿subscriptsuperscriptâ„ğ‘Ÿğ‘¡subscriptğœ‹1â€¦subscriptğœ‹ğ‘¡subscriptğ‘Ÿğ‘¡1subscriptsuperscriptâ„ğ‘Ÿğ‘¡1subscriptğœ‹1â€¦subscriptğœ‹ğ‘¡subscriptğ‘šğ‘¡1subscriptsuperscriptâ„ğ‘Ÿğ‘¡1\delta(\langle h^{r}_{t},(\pi_{1},...,\pi_{t})\rangle,\quad r_{t+1})=\langle h^{r}_{t+1},\quad(\pi_{1},...,\pi_{t},m_{t+1}(h^{r}_{t+1}))\rangle, whereht+1r=(htr,rt+1)subscriptsuperscriptâ„ğ‘Ÿğ‘¡1subscriptsuperscriptâ„ğ‘Ÿğ‘¡subscriptğ‘Ÿğ‘¡1h^{r}_{t+1}=(h^{r}_{t},r_{t+1}).

q0subscriptğ‘0q_{0}is the initial state of the automaton as explained above.

FâŠ‚Qğ¹ğ‘„F\subset Qis the set ofacceptingstates, which are the desired states of the automaton.

Itâ€™s not hard to see that this automaton is a model of the learning system described in Section1.2, with its desired property specified by the accepting statesFğ¹F. In this paper, the desired property is that the system be a lifelong learning system, so the accepting statesFğ¹Fare the set ofâŸ¨hTr,(Ï€1,Ï€2,â€¦,Ï€T)âŸ©subscriptsuperscriptâ„ğ‘Ÿğ‘‡subscriptğœ‹1subscriptğœ‹2â€¦subscriptğœ‹ğ‘‡\langle h^{r}_{T},\quad(\pi_{1},\pi_{2},...,\pi_{T})\ranglepairs that correspond to a lifelong learner333Recall that an agentâ€™s behavior is fully decided by its policyÏ€=(Ï€1,Ï€2,â€¦,Ï€T)ğœ‹subscriptğœ‹1subscriptğœ‹2â€¦subscriptğœ‹ğ‘‡\pi=(\pi_{1},\pi_{2},...,\pi_{T}). Therefore given a reward historyhTrsubscriptsuperscriptâ„ğ‘Ÿğ‘‡h^{r}_{T}, the policy is sufficient for us to tell whether the agent is a successful lifelong learner..

To specify learning objectives, each possible rewardrâˆˆÎ£ğ‘ŸÎ£r\in\Sigmais assigned some semantics. These semantics implicitly define the set of valid reward sequencesLâŠ‚Î£âˆ—ğ¿superscriptÎ£L\subset\Sigma^{*}. SinceLğ¿Lis a subset ofÎ£âˆ—superscriptÎ£\Sigma^{*}, it is a language overÎ£Î£\Sigma. We want to make sure that â€” for all reward sequences inLğ¿L, lifelong learning can be achieved by the learning system abstracted by this automaton, or equivalently, all reward sequences inLğ¿Llead to accepting statesFğ¹F.

Designing a lifelong reinforcement learning system involves designing the reward language and the learning algorithm holistically. Intuitively, the reward needs to contain enough information to control the relevant aspects of the learning algorithm, and the learning algorithm in turn needs to â€˜interpretâ€™ the reward signal in its intended way. In this section, I aim to provide some insights into the design process with a prototype lifelong reinforcement learning system.

The main reason lifelong learning is impossible in conventional reinforcement learning is that the learning objective in conventional reinforcement learning isglobal, in the sense that the goal of the agent is defined in terms of the observation history of its entire life. For a lifelong reinforcement learning agent, the learning objectives should instead belocal, meaning that the goals should be defined only for some smaller tasks that the agent can encounter multiple times during its lifetime. Once a local goalexpires, whether it is because the goal has been achieved or because the agent has failed to achieve it within a certain time limit, a new local goal (can potentially be another instantiation of the same goal) ensues. This way, the agent has the opportunity to gather knowledge for each of the goals, and improve upon them, all within one life. Local goals like this are ubiquitous for humans. For example, when a person is hungry, his main concern is probably not the global goal of being happy for the rest of his life â€” his goal is to have food. After the person is full, he might feel like taking a nap, which is another local goal. In fact, the local goals and the transition of them seems to embody what we mean by intrinsic motivation.

To be able to specify a series of local goals, the reward in this prototype learning system has two parts: thereward statertsâˆˆGsubscriptsuperscriptğ‘Ÿğ‘ ğ‘¡ğºr^{s}_{t}\in G, and thereward valuertvâˆˆâ„subscriptsuperscriptğ‘Ÿğ‘£ğ‘¡â„r^{v}_{t}\in\mathbb{R}, whereGğºGis the set of local goals the agent may have. This form of reward is inspired by thereward machine(Icarte etÂ al.,2018), a Mealy machine for specifying history-dependent reward, but the semantics we assign to the reward will be different. Also note that this Mealy machine bears no relation to the automaton we discussed in Section1.3â€” the reward machine models the reward, while the automaton in Section1.3models the learning system, and takes the reward as input. Each reward staterssuperscriptğ‘Ÿğ‘ r^{s}corresponds to a local goal. When a local goal (or equivalently, a reward state) expires, the agent receives a numerical reward valuervsuperscriptğ‘Ÿğ‘£r^{v}. For all other timesteps (other than the expiration of local goals), the reward value can be considered to take a specialNULLvalue, meaning that no reward value is received. The reward value is an evaluation of the agentâ€™s performance in an episode of a reward state, where anepisodeof a reward state is defined as the time period between the expiration of the previous reward state (exclusive) and the expiration of the reward state itself (inclusive). The reward state can potentially depend on the entire observation history, while the reward value can only depend on the observation history of the episode it is assessing. Overall, the reward is specified by(rts,rtv)=râ€‹(hto)subscriptsuperscriptğ‘Ÿğ‘ ğ‘¡subscriptsuperscriptğ‘Ÿğ‘£ğ‘¡ğ‘Ÿsubscriptsuperscriptâ„ğ‘œğ‘¡(r^{s}_{t},r^{v}_{t})=r(h^{o}_{t}).

The local goals described here are technically similar to subgoals inhierarchical reinforcement learning(Dietterich,2000; Sutton etÂ al.,1999; Parr & Russell,1997). However, the term â€˜subgoalâ€™ suggests that there is some higher-level goal that the agent needs to achieve, and that the higher-level goal is the true objective the agent needs to optimize. That is not the case here â€” although it is totally possible that the local goals are designed in such a way that some global goal can be achieved, the agent only needs to optimize the local goals.

The reward language in this prototype system makes two assumptions on the learning algorithm. As long as the two assumptions are met, the learning algorithm is considered to â€˜interpretâ€™ the reward correctly. The first assumption is that the learning algorithm only generates policies that areepisode-wise stationary, meaning thatÏ€t1=Ï€t2subscriptğœ‹subscriptğ‘¡1subscriptğœ‹subscriptğ‘¡2\pi_{t_{1}}=\pi_{t_{2}}for any timestepst1subscriptğ‘¡1t_{1}andt2subscriptğ‘¡2t_{2}in the same episode of a reward state, and thatÏ€t1:Oâ†’A:subscriptğœ‹subscriptğ‘¡1â†’ğ‘‚ğ´\pi_{t_{1}}:O\to A. This assumption is not particularly restrictive, because in cases where a local goal requires a more complex policy, we can always split the goal into multiple goals (by modifying the reward function) for which the policies are episode-wise stationary. With this assumption, we can use a single policyÏ€rs:Oâ†’A:subscriptğœ‹superscriptğ‘Ÿğ‘ â†’ğ‘‚ğ´\pi_{r^{s}}:O\to Ato represent the policies at all timesteps within an episode of reward staterssuperscriptğ‘Ÿğ‘ r^{s}. The second assumption is that the learning algorithm keeps a pool of â€˜eliteâ€™ policies for each reward state: a policy that led to high reward value in some episode has the opportunity to enter the pool, and a policy that consistently leads to higher reward value eventually dominates the policy pool. The exact criterion for selection into the pool (e.g., to use the expected reward value as the criterion, or to use the probability of the reward value being higher than a certain threshold, etc.) is not enforced, and is left up to the learning algorithm.

The learning algorithm in this prototype lifelong learning system is anevolutionary algorithm, adjusted to meet the assumptions made by the reward. The algorithm maintains a policy poolDrssubscriptğ·superscriptğ‘Ÿğ‘ D_{r^{s}}of maximum sizedğ‘‘dfor each reward statersâˆˆGsuperscriptğ‘Ÿğ‘ ğºr^{s}\in G. Each item in the pool is a two tupleâŸ¨Ï€,rÏ€vâŸ©ğœ‹subscriptsuperscriptğ‘Ÿğ‘£ğœ‹\langle\pi,r^{v}_{\pi}\ranglewhereÏ€ğœ‹\piis a policy andrÏ€vsubscriptsuperscriptğ‘Ÿğ‘£ğœ‹r^{v}_{\pi}is the reward value of the last episode in whichÏ€ğœ‹\piwas executed. Conceptually, the algorithm consists of three steps: policy generation, policy execution, and (policy) pool update, which are described below.

When an episode of reward staterssuperscriptğ‘Ÿğ‘ r^{s}starts, a policyÏ€rssubscriptğœ‹superscriptğ‘Ÿğ‘ \pi_{r^{s}}is generated from one of the following methods with probabilityp1subscriptğ‘1p_{1},p2subscriptğ‘2p_{2},p3subscriptğ‘3p_{3}, respectively:

Randomly sample a policy from the policy poolDrssubscriptğ·superscriptğ‘Ÿğ‘ D_{r^{s}}, and mutate the policy.

Randomly sample a policy fromDrssubscriptğ·superscriptğ‘Ÿğ‘ D_{r^{s}}and keep it as is. Remove the sampled policy fromDrssubscriptğ·superscriptğ‘Ÿğ‘ D_{r^{s}}. This is to re-evaluate a policy in the pool. Since the transition of observations might be stochastic, the same policy does not necessarily always result in the same reward value.

Randomly generate a new policyÏ€rs:Oâ†’A:subscriptğœ‹superscriptğ‘Ÿğ‘ â†’ğ‘‚ğ´\pi_{r^{s}}:O\to Afrom scratch. This is to keep the diversity of the policy pool.

p1subscriptğ‘1p_{1},p2subscriptğ‘2p_{2}andp3subscriptğ‘3p_{3}should sum up to111, and are hyper-parameters of the algorithm.

Execute the generated policyÏ€rssubscriptğœ‹superscriptğ‘Ÿğ‘ \pi_{r^{s}}until a numerical reward valuervsuperscriptğ‘Ÿğ‘£r^{v}is received.

If the policy pool is not full, insertâŸ¨Ï€rs,rvâŸ©subscriptğœ‹superscriptğ‘Ÿğ‘ superscriptğ‘Ÿğ‘£\langle\pi_{r^{s}},r^{v}\rangleinto the pool. Otherwise comparervsuperscriptğ‘Ÿğ‘£r^{v}with the minimum reward value in the pool. Ifrvsuperscriptğ‘Ÿğ‘£r^{v}is greater than or equal to the minimum reward value, replace the policy and reward value pair (that has the minimum reward value) withâŸ¨Ï€rs,rvâŸ©subscriptğœ‹superscriptğ‘Ÿğ‘ superscriptğ‘Ÿğ‘£\langle\pi_{r^{s}},r^{v}\rangle.

Learning biasin reinforcement learning systems refers to the explicit or implicit assumptions made by the learning algorithm about the policy. Our assumption that the policy is episode-wise stationary is an example of learning bias. Arguably, a good learning bias is as important as a good learning algorithm, therefore it is important that mechanisms are provided to embed learning bias into the learning system.

A straight-forward way to embed learning bias into the above lifelong learning system is through the policy generation process. This includes how existing policies are mutated, and what distribution new policies are sampled from. The learning bias provided this way does not depend on the agentâ€™s observation and reward history, and is sometimes implicit (e.g., the learning bias introduced by using a neural network of particular architecture).

Another type of learning bias common in reinforcement learning isguidance, the essence of which can be illustrated by Figure3. Suppose in some reward state, the agent starts from observationoğ‘œoand the goal is to reach444For sake of terminological convenience, we pretend that the observations here are environment states.observationoâ€²superscriptğ‘œâ€²o^{\prime}. Prior knowledge indicates that to reachoâ€²superscriptğ‘œâ€²o^{\prime}, visitingoâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}is a good heuristic, but reachingoâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}itself has little or no merit. In other words, we would like to encourage the agent to visit and explore aroundoâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}more frequently (than other parts of the observation space) until a reliable policy to reachoâ€²superscriptğ‘œâ€²o^{\prime}is found.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: Figure 3:A simplistic abstraction of guidance in reinforcement learning.

To provide guidance to the agent in the prototype lifelong learning system, we can utilize the property of the learning algorithm that policies leading to high reward values will enter the policy pool. Once a policy enters the pool, it has the opportunity to be sampled (possibly with mutation) and executed. Therefore, we just need to assign a higher reward value for reachingoâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}(before the expiration of the reward state) than reaching neitheroâ€²superscriptğ‘œâ€²o^{\prime}noroâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}. Also important is the ability to control the extent to which region aroundoâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}is explored. To achieve this, recall that the learning algorithm occasionally re-evaluates policies in the policy pool. If we assign a lower reward value for reachingoâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}with some probability, we can prevent the policy pool from being overwhelmed only by policies that lead tooâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}. In other words, the reward value for reachingoâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}should have multiple candidates. Letrvâ€‹({Oâˆ’{oâ€²,oâ€²â€²}})superscriptğ‘Ÿğ‘£ğ‘‚superscriptğ‘œâ€²superscriptğ‘œâ€²â€²r^{v}(\{O-\{o^{\prime},o^{\prime\prime}\}\})denote the reward value for an episode where the agent reaches neitheroâ€²superscriptğ‘œâ€²o^{\prime}noroâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime},rvâ€‹(oâ€²)superscriptğ‘Ÿğ‘£superscriptğ‘œâ€²r^{v}(o^{\prime})denote the reward value for reachingoâ€²superscriptğ‘œâ€²o^{\prime}, we can set the reward valuervâ€‹(oâ€²â€²)superscriptğ‘Ÿğ‘£superscriptğ‘œâ€²â€²r^{v}(o^{\prime\prime})for reachingoâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}as:

whereb<rvâ€‹({Oâˆ’{oâ€²,oâ€²â€²}})<a<rvâ€‹(oâ€²)ğ‘superscriptğ‘Ÿğ‘£ğ‘‚superscriptğ‘œâ€²superscriptğ‘œâ€²â€²ğ‘superscriptğ‘Ÿğ‘£superscriptğ‘œâ€²b<r^{v}(\{O-\{o^{\prime},o^{\prime\prime}\}\})<a<r^{v}(o^{\prime}). The probabilitypğ‘pcontrols the frequency region aroundoâ€²â€²superscriptğ‘œâ€²â€²o^{\prime\prime}is to be explored compared the other parts of the observation space555Note that the word â€˜probabilityâ€™ here should be interpreted as the â€˜long-run proportionâ€™, and therefore the reward value needs not be truly stochastic. E.g., we can imagine that the reward has a third component which is the state of a pseudo-random generator..

Now we evaluate the behaviour of the prototype lifelong reinforcement learning system. The source code of the experiments can be found athttps://gitlab.com/lifelong-rl/lifelongRL_gridworld

Consider a gridworld agent whose life revolves around getting food and taking the food back home for consumption. The agent lives in a111111by111111gridworld shown in Figure4. The shaded areas are barriers that the agent cannot go through. Some potential positions of interest are marked with letters:Fis the food source and is assumed to have infinite supply of food;His the agentâ€™s home. To get to the food source from home, and to carry the food home, the agent must pass through one of the two tunnels â€” the tunnel on the left is marked withLand the tunnel on the right is marked withR. At each timestep, the agent observes its position in the gridworld as well as a signal indicating whether it is in one of the four positions of interest (if yes, which), and chooses from one of the four actions:UP,RIGHT,DOWNandLEFT. Each action deterministically takes the agent to the adjacent grid in the corresponding direction, unless the destination is a barrier, in which case the agent remains in its original position. The agent starts from home at the beginning of its life, and needs to go to the food source to get food. Once it reaches the food source, it needs to carry the food back home. This process repeats until the agent dies. The lifespan of the agent is assumed to be100100100million timesteps. The agent is supposed to learn to reliably achieve these two local goals within its lifetime.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: Figure 4:Gridworld environment.

The reward state in this experiment is represented by the conjunction of Boolean variables. For example, if three Boolean variablesAğ´A,BğµBandCğ¶Care defined, then the reward state would be in the form ofrs=Aâˆ§Bâˆ§Csuperscriptğ‘Ÿğ‘ ğ´ğµğ¶r^{s}=A\land B\land Corrs=Aâˆ§Â¬Bâˆ§Csuperscriptğ‘Ÿğ‘ ğ´ğµğ¶r^{s}=A\land\neg B\land C, etc. At the bare minimum, one Boolean variableğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{GET\_FOOD}needs to be defined for this agent, whereğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{GET\_FOOD}being true corresponds to the local goal of going to the food source, andÂ¬ğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\neg\mathtt{GET\_FOOD}corresponds to the local goal of carrying the food home. The agent receives a reward value of+11+1ifğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{GET\_FOOD}is true and the agent reachesğ™µğ™µ\mathtt{F}, in which case the Boolean variableğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{GET\_FOOD}transitions to false. Similarly, the agent receives a reward value of+11+1ifÂ¬ğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\neg\mathtt{GET\_FOOD}is true and the agent reachesğ™·ğ™·\mathtt{H}, in which caseğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{GET\_FOOD}transitions to true. On top ofğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{GET\_FOOD}, we define another Boolean variableğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒ\mathtt{TIMED\_OUT}, which indicates whether the agent has exceeded a certain time limit for trying to get to the food source, or for trying to carry the food home. If the reward state isÂ¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§ğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\neg\mathtt{TIMED\_OUT}\land\mathtt{GET\_FOOD}, and the agent fails to reachğ™µğ™µ\mathtt{F}within the time limit, itreceives a reward value ofâˆ’11-1, and the reward state transition toğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§ğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{TIMED\_OUT}\land\mathtt{GET\_FOOD}. Fromğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§ğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{TIMED\_OUT}\land\mathtt{GET\_FOOD}, if the agent still fails to get toğ™µğ™µ\mathtt{F}within the time limit, it receives a reward value of00. The agent will remain inğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§ğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{TIMED\_OUT}\land\mathtt{GET\_FOOD}, until it reachesğ™µğ™µ\mathtt{F}, when the reward state transitions toÂ¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§Â¬ğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\neg\mathtt{TIMED\_OUT}\land\neg\mathtt{GET\_FOOD}(and receive a+11+1reward value as already mentioned). For the case whenğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{GET\_FOOD}is false, the reward transition is defined similarly. Throughout the experiments, the time limit is set to242424, which is enough for the agent to accomplish any of the local goals. We refer to this reward design as thebasecase.

Unfortunately, even for a toy problem like this, learning can be difficult if no proper learning bias is provided. Since there are444actions and747474possible positions, the number of possible episode-wise stationary policies is474superscript4744^{74}for each reward state. Among those policies, very few can achieve the local goals. If the policy generation and mutation is purely random, it will take a long time for the agent to find a good policy.

The first learning bias we consider isbiased policy, which is in contrast to theunbiased policycase where the policy generation and mutation is purely random. More specifically, we make the policy generation process biased towards policies that take the same action for similar observations. This would encourage policies that head consistently in one direction, and discourage those that indefinitely roam around between adjacent positions.

The second learning bias we consider is guidance based on the agentâ€™s progress. Different from the base case where the agent always receives a00(ifğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒ\mathtt{TIMED\_OUT}is true) orâˆ’11-1(ifğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒ\mathtt{TIMED\_OUT}is false) reward value when it fails to achieve the local goal within the time limit, the agent now has some probabilityp=0.8ğ‘0.8p=0.8of receiving a reward value proportional to the Manhattan distancedğ‘‘dit has traveled since the beginning of the episode. To be precise:

This way, policies leading to more progress (albeit not necessary towards the local goal) will be encouraged.

Finally, we consider a case of sub-optimal guidance that encourages the agent to explore a sub-optimal trajectory. As we have mentioned, both reaching the food source from home and carrying the food home require the agent to go through one of the two tunnels. However, if the agent goes through the left tunnel, it has to travel more distance. Suppose that we prefer the agent to take the shorter route, but we only know the route that goes through the left tunnel; and as a result, we sub-optimally encourage the agent to explore the left tunnel. To guide the agent to take the left tunnel, Boolean variableğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³â€‹_â€‹ğ™»ğ™´ğ™µğšƒğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³_ğ™»ğ™´ğ™µğšƒ\mathtt{VISITED\_LEFT}is introduced as an indicator of whetherLhas been visited since the last visitation ofForH. Now we have23=9superscript2392^{3}=9elements in the reward space, corresponding to999possible local goals. The reward transition is different from the base case in that if the agent has already visitedLwhen the local goalğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§Â¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§Â¬ğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³â€‹_â€‹ğ™»ğ™´ğ™µğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³_ğ™»ğ™´ğ™µğšƒ\mathtt{GET\_FOOD}\land\neg\mathtt{TIMED\_OUT}\land\neg\mathtt{VISITED\_LEFT}orÂ¬ğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§Â¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§Â¬ğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³â€‹_â€‹ğ™»ğ™´ğ™µğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³_ğ™»ğ™´ğ™µğšƒ\neg\mathtt{GET\_FOOD}\land\neg\mathtt{TIMED\_OUT}\land\neg\mathtt{VISITED\_LEFT}times out,ğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³â€‹_â€‹ğ™»ğ™´ğ™µğšƒğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³_ğ™»ğ™´ğ™µğšƒ\mathtt{VISITED\_LEFT}becomes true, and the agent will receive a reward value of+0.60.6+0.6with0.80.80.8probability, andâˆ’0.20.2-0.2with0.20.20.2probability. To express our preference for the shorter route, the agent receives a reward value of+0.80.8+0.8(instead of+11+1) when it reachesF(whenğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{GET\_FOOD}is true) orH(whenğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³ğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³\mathtt{GET\_FOOD}is false) through the left tunnel.

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: (a)unbiased policy, progressed-based guidance

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: (a)unbiased policy, progressed-based guidance

Figure5shows the learning curves for reward stateğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§Â¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒ\mathtt{GET\_FOOD}\land\neg\mathtt{TIMED\_OUT}with progress-based guidance. Thexğ‘¥x-axis is the timesteps (in million), and theyğ‘¦y-axis is the percentage of times the agent transitions into a particular next reward state starting fromğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§Â¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒ\mathtt{GET\_FOOD}\land\neg\mathtt{TIMED\_OUT}. A next reward state ofÂ¬ğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§Â¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒ\neg\mathtt{GET\_FOOD}\land\neg\mathtt{TIMED\_OUT}means that the agent successfully reachedFwithin the time limit, and a next reward state ofğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒ\mathtt{GET\_FOOD}\land\mathtt{TIMED\_OUT}means that the agent failed to do so. As we can see, with unbiased policy, it took the agent around252525million timesteps to achieve100%percent100100\%success rate; while with biased policy, this only took around888million timesteps.

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: (a)unbiased policy, sub-optimal guidance

[å›¾ç‰‡: images\image_8.png]
å›¾ç‰‡è¯´æ˜: (a)unbiased policy, sub-optimal guidance

Figure6shows the learning curves for reward stateğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§Â¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§Â¬ğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³â€‹_â€‹ğ™»ğ™´ğ™µğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³_ğ™»ğ™´ğ™µğšƒ\mathtt{GET\_FOOD}\land\neg\mathtt{TIMED\_OUT}\land\neg\mathtt{VISITED\_LEFT}with the sub-optimal guidance described in Section3.2. Similar to Figure5, thexğ‘¥x-axis is the timesteps (in million), and theyğ‘¦y-axis is the percentage of times the agent transitioned into a particular next reward state starting fromğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§Â¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§Â¬ğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³â€‹_â€‹ğ™»ğ™´ğ™µğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³_ğ™»ğ™´ğ™µğšƒ\mathtt{GET\_FOOD}\land\neg\mathtt{TIMED\_OUT}\land\neg\mathtt{VISITED\_LEFT}. A next reward state ofÂ¬ğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§Â¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§Â¬ğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³â€‹_â€‹ğ™»ğ™´ğ™µğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³_ğ™»ğ™´ğ™µğšƒ\neg\mathtt{GET\_FOOD}\land\neg\mathtt{TIMED\_OUT}\land\neg\mathtt{VISITED\_LEFT}means that the agent successfully reachedFwithin the time limit; a next reward state ofğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§ğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³â€‹_â€‹ğ™»ğ™´ğ™µğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³_ğ™»ğ™´ğ™µğšƒ\mathtt{GET\_FOOD}\land\mathtt{TIMED\_OUT}\land\mathtt{VISITED\_LEFT}means that the agent failed to reach the food source, but was able to find a way to the left tunnel; and a next reward state ofğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§Â¬ğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³â€‹_â€‹ğ™»ğ™´ğ™µğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³_ğ™»ğ™´ğ™µğšƒ\mathtt{GET\_FOOD}\land\mathtt{TIMED\_OUT}\land\neg\mathtt{VISITED\_LEFT}means that the agent was neither able to reach the left tunnel nor the food source within the time limit. As we can see, for both unbiased and biased policy, learning is much slower than progress-based guidance. This is likely due to the much sparser guidance signal â€” the agent receives guidance only when it reaches the left tunnel. For the unbiased policy case,100%percent100100\%success rate was not achieved within100100100million timesteps, but we can clearly see that exploration around the left tunnel was encouraged as intended. For the biased policy case, the agent was able to reach100%percent100100\%success rate after505050million timesteps. But was the agent able to figure out the optimal route, or did it only learn to take the sub-optimal route as guided? Recall that the agent receives a reward value of+11+1if it takes the optimal route, and a reward value of+0.80.8+0.8if it takes the sub-optimal route. As shown in Figure7, although the agent was taking the sub-optimal route by505050million timesteps when it just learned to reach the food source reliably, it was eventually able to figure out the optimal route by909090million timesteps.

[å›¾ç‰‡: images\image_9.png]
å›¾ç‰‡è¯´æ˜: Figure 7:Reward value forğ™¶ğ™´ğšƒâ€‹_â€‹ğ™µğ™¾ğ™¾ğ™³âˆ§Â¬ğšƒğ™¸ğ™¼ğ™´ğ™³â€‹_â€‹ğ™¾ğš„ğšƒâˆ§Â¬ğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³â€‹_â€‹ğ™»ğ™´ğ™µğšƒğ™¶ğ™´ğšƒ_ğ™µğ™¾ğ™¾ğ™³ğšƒğ™¸ğ™¼ğ™´ğ™³_ğ™¾ğš„ğšƒğš…ğ™¸ğš‚ğ™¸ğšƒğ™´ğ™³_ğ™»ğ™´ğ™µğšƒ\mathtt{GET\_FOOD}\land\neg\mathtt{TIMED\_OUT}\land\neg\mathtt{VISITED\_LEFT}(biased policy with sub-optimal guidance, averaged over202020runs).

Lifelong reinforcement learning is sometimes viewed as a multi-task reinforcement learning problem(Abel etÂ al.,2018), where the agent must learn to solve tasks sampled from some distributionğ’Ÿğ’Ÿ\mathcal{D}. The agent is expected to (explicitly or implicitly) discover the relation between tasks, and generalize its policy to unseen tasks fromğ’Ÿğ’Ÿ\mathcal{D}. The focus is therefore on the transfer learning(Taylor & Stone,2009)and continual learning(Ring,1998)aspects of lifelong reinforcement learning.

In this paper, I provided a systems view on lifelong reinforcement learning. In particular, I showed that the reward in a lifelong reinforcement learning system can be a general language, and that the language needs to be designed holistically with the learning algorithm. A prototype lifelong reinforcement learning system was given, with an emphasize on how learning bias can be embedded into the learning system through the synergy of the reward language and the learning algorithm.

The author would like to thank Gaurav Sharma (Borealis AI) for his comments on a draft of the paper.

[å›¾ç‰‡: images\image_10.png]

[å›¾ç‰‡: images\image_11.png]

