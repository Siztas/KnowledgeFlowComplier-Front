###奖励的语言化表征与终身智能体的构建路径

本文探讨了“终身强化学习系统（Lifelong Reinforcement Learning System）”的核心思想与传统强化学习范式之间的差异，并提出了对建构终身学习智能体的一些启发。作者指出，传统的强化学习框架主要关注于跨代的学习过程：即策略是在不同“代”之间逐步演化优化的，而非由单个智能体在其有限生命周期内完成。具体而言，在传统的强化学习系统中，每个智能体在出生时获得一个策略，然后在生命周期内按照该策略行动，并记录其经历与累积奖励，最后这一信息被用于优化下一代智能体的策略。换言之，学习行为是由外部学习算法主导的，而非嵌入于智能体之中。

作者进一步指出，这种基于累积奖励最大化的传统方法，其实是一种源自“奖励假设（reward hypothesis）”的设计选择。即，目标被视作一个累积标量奖励期望的最大化问题。然而，这种形式对于终身强化学习是不充分的，因为它不能很好地刻画智能体在其生命周期中逐步学习和适应环境的过程。特别地，个体在生命终点才能知晓其累积奖励，导致学习过程实际上依赖于代际更替而非个体自身的在线学习。

通过分析 Q-learning 算法，作者揭示了一个重要现象：尽管 Q-learning 允许每一步更新估计值，但若观察包含时间戳，则没有一个观察会被重复，因而实际上 Q-learning 的学习效果仍然类似于跨代优化。只有当初始 Q 值具备高度结构化或来自前代丰富经验时，当前智能体才能展现出有意义的行为。换句话说，除了奖励信号之外，Q 值本身应被视作一种经验信号或知识遗产，它表达了前代对“目标”含义的理解，而非仅仅是一个用于最大化的奖励指标。

因此，作者提出应当将奖励重新定义为一种“语言（reward language）”：不再仅是标量值，而是编码了关于目标、子目标、策略启发信息等在内的知识表达方式。相应地，学习算法则成为对这种语言的响应机制。该系统内部的学习行为不再依赖外部的策略更新器，而是内嵌在智能体自身的学习机制中，使其可以在生命周期内不断更新其策略。由此，终身学习系统的核心转向为一个映射函数：从奖励历史（即奖励语言字符串）映射到策略集合，即 $m_t: H^r_t \rightarrow \Pi_t$。

在形式化上，作者进一步使用自动机理论的术语构建了奖励语言的模型。其中，奖励空间 $\Sigma$ 被看作字母表，每个奖励历史 $h^r_t$ 是该语言中的字符串，而状态集合 $Q$ 则表示奖励历史与策略的对偶。通过这种方式，可以以系统化语言建模形式表达智能体学习过程中的先验知识注入、偏好诱导、内在动机等机制，为构建真正具备终身学习能力的智能体提供理论基础。

综上，本文通过对传统强化学习的批判性分析，引出了对终身强化学习系统架构的重新定义，即：奖励是信息语言，学习算法是对该语言的响应；学习应嵌入个体之内，而非在代际间进行；奖励应具备结构性、多模态、可解释性，以支撑终身学习的实现。
