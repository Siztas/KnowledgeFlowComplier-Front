æ ‡é¢˜ï¼šOn the Ethics of Building AI in a Responsible Manner

The AI-alignment problem arises when there is a discrepancy between the goals that a human designer specifies to an AI learner and a potential catastrophic outcome that does not reflect what the human designer really wants. We argue that a formalism of AI alignment that does not distinguish between strategic and agnostic misalignments is not useful, as it deems all technology as un-safe. We propose a definition of a strategic-AI-alignment and prove that most machine learning algorithms that are being used in practice today do not suffer from the strategic-AI-alignment problem. However, without being careful, todayâ€™s technology might lead to strategic misalignment.

Recently, public figures have been advocating more research and regulatory oversight into the dangers of AI deployed in real-world applications[9]. The success of computer vision, natural language processing and understanding and the ability to extract patterns from massive amounts of data, does raise important questions about how the rise of automation (in the form of compute and algorithms) will affect society, how can the public â€œreason withâ€ algorithms who can make decisions that affect our lives, how would the masses of data collected on users of digital services would be used and what kind of malicious abuse can take place and how to avoid it[9].

As much as those issues require immediate and focused attention, there is a bigger potential danger at hand of a technology whose ultimate evolutionary end-point could get out of hand and cause havoc on an epic scale. Putting aside popular discourse on the potential dangers posed by â€œsuper intelligentâ€ machines, the academic community has been pondering about AISafetyfor nearly two decades under what is called the â€œAI-alignmentâ€ problem[24,25,22]and thepaperclip maximization problem[5]. In a nutshell, the AI-alignment problem refers to the ability of a super-learner to maximize a reward function of an agent interacting with the environment and while doing so finding solutions that the (human) designer of the reward function did not anticipate, thereby leading to a catastrophe. The classical thought experiment, inspired by the movieFantasia, involves the problem of assigning a reward function to a robot whose goal is to fill a cauldron. The seemingly innocent reward which assigns a value of111if the cauldron is full and00otherwise may cause the robot to flood the entire workplace in order to maximize the probability of getting the positive reward. A straightforward suggestion is that the robot will be equipped with a â€œstopâ€ button, in order to prevent unexpected side effects. However, researchers have shown that no matter how you go about it, faced with a very advanced intelligence, the â€œstopâ€ button will be useless since the robotic agent will be able to manipulate the human operator from pressing it. As far as the field of AI-alignment goes, finding an alignment between the full essence of human desires and their expression as a reward function to be optimized by a super-advanced robotic agent, is an open problem.

A natural question that arises from the intractability of the AI-alignment problem isâ€œwhy arenâ€™t we scared?â€The community of AI practitioners are either oblivious to the AI-alignment problem or do not seem to care, whereas regulatory bodies do not have the tools to make concrete statements to protect society while not stifling technological progress which carries great promise to society.

There are a number of reasons the AI-alignment problem has not taken root in the minds of practitioners. First, it supposes an advanced form of machine intelligence that no one can predict when and if will be attained. However, as we show later, AI deployed in the real world can lead to imminent dangers to societyusing todayâ€™s technology, and as a result some form of a â€œdigital analogue of the Declaration of Helsinkiâ€ must be established.
The second reason practitioners by and large ignore the AI-alignment problem is that the AI-alignment literature conflates two types of processes that can lead to a catastrophe â€” one being â€œstrategicâ€, where the optimization of the reward functionintentionallychanges the distribution of the world, while the other is â€œagnosticâ€, non-intentional, distribution drift due to a â€œbutterfly effectâ€ that might follow from the chaotic nature of non-linear dynamics of complex systems. As we show later, when those two are conflated, all of todayâ€™s technology, even not AI-based, suffers from the alignment problem, since catastrophes resulting from butterfly-effect distribution drifts are inevitable in complex systems, thereby ruling all technologies as â€œnon-safeâ€. In other words, AI-alignment as exists today is not useful, since a theory that cannot place boundaries to its domain is inevitably vacuous. The purpose of this paper is to propose a refined formalism of the AI-alignment problem that distinguishes between â€œstrategicâ€ (non-safe) and â€œagnosticâ€ (safe) catastrophes. We prove that the overwhelming majority of machine learning engines deployed in the real world are of the agnostic type (and are therefore safe) and specify what form of machine learning should be prohibited from deployment in the real world and what open problems need to be addressed in order to allow their deployment.

Machine learning (ML) is broadly split into two methodologies â€” one based ondataand the other onexperiences. The data route is responsible for the success of pattern recognition in computer vision[11,10], natural language processing, machine translation and more recently natural language understanding[8]. This type of learning goes under the name of {supervised, unsupervised, self-supervised} learning. When the task is sufficiently narrow and a benchmark dataset is available, machine intelligence based on (massive) training data often exceeds human performance as measured on the benchmark.

Machine learning based onexperiencesarises in the context of an agent interacting with an environment where the dynamics of the world and the feedback that the agent receives on its actions are determined through experiences in the real world or governed by a simulator. This type of learning is called Reinforcement Learning111In the technical sections we will point out that the 1st learning type is a special case of RL.and the truly impressive success stories are when the world dynamics and reward function are governed by a simulator. Success stories, where machine intelligence surpasses human expert performance, are focused on game playing from video games[13]to the game of Go[20]. In particular, the success of AlphaGo-zero[20]lends empirical evidence that in a simulated world,in general, one can exceed human-level performance.

Although the two ML methodologies are consistently demonstrating â€œsuper-intelligenceâ€ in their domains, it is understood by AI practitioners that this is a far cry from the kind of â€œbroadâ€ intelligence that humans posses (coined as â€œstrong AIâ€ or Artificial General Intelligence (AGI)), where â€œbroadâ€ roughly means the ability to transfer intelligence from one domain to another and make high quality decisionsin generalas opposed to a narrow well defined task. As a result, beyond the concerns about the effects of automation and malicious abuse of data collected on users, there is a general agreement that the dangers associated with AGI are far into the long future and thus it is premature to have any concrete discussion about AI Safety in a world where AGI does not exist.

There are good reasons for taking this position because each of the two methodologies experience significant growth â€œceilingâ€ that make them unlikely to become AGI simply by â€œbeating the same pathâ€ using more data and more computational resources. From the theoretical perspective, while deep learning is a universal learner, it suffers from computational and/or sample complexity limitations, as well as the need for explicit adequate modeling by human experts (e.g.,[18,12,19,4]). With respect to learning-from-experience, current big success stories are limited to games in which there exists a simulator that fully describes the rules of the game and the transition from one state to another. Moving beyond games into more complex environments requires either a very sophisticated simulator222It is one thing to build a simulator for a video game and a completely different story to build a simulator for real world environments, for example for autonomous driving, or even more challenging to build a simulator for human interactions (say we want to build a â€œconversational AGIâ€). The simulatoristhe ceiling.or training in the real world.

So it seems that AGI is not around the corner any time soon. However, the danger of AI does not necessarily lie in continued growth of a single learning methodology but with their combination. A dangerous scenario to consider is: use the data methodology to build a â€œMinimal Viable Productâ€ (MVP) which is quite good but still not an AGI. Then, release the MVP to the real world and use the 2nd methodology (learning by experiences) to continue training the product under some (presumably proprietary) reward function. This is where the AI-alignment problem becomes interesting. It is important to note that this exercise does not require any scientific or technological leap â€” data, compute and algorithms available today would suffice. For the sake of concreteness consider two examples described below.

First, consider the domain of autonomous driving. Assume we have developed a self-driving MVP which is â€œsafe enoughâ€ to deploy but is not the most expert driver. We then deploy millions of self-driving robots in the real world and use RL to improve the â€œdriving policyâ€ (mapping from the state of the world at any given moment to an action that the driver should take) through â€œlearning by experiencesâ€. When designing a reward function for self-driving, we have three
major considerations: safety, usefulness, and comfort. Consequently, our reward
function may beRâ€‹(sÂ¯)=csâ€‹Rsâ€‹(sÂ¯)+cuâ€‹Ruâ€‹(sÂ¯)+ccâ€‹Rcâ€‹(sÂ¯)ğ‘…Â¯ğ‘ subscriptğ‘ğ‘ subscriptğ‘…ğ‘ Â¯ğ‘ subscriptğ‘ğ‘¢subscriptğ‘…ğ‘¢Â¯ğ‘ subscriptğ‘ğ‘subscriptğ‘…ğ‘Â¯ğ‘ R(\bar{s})=c_{s}R_{s}(\bar{s})+c_{u}R_{u}(\bar{s})+c_{c}R_{c}(\bar{s}),
wheresÂ¯Â¯ğ‘ \bar{s}is a sequence of {state, action},Rs,Ru,Rssubscriptğ‘…ğ‘ subscriptğ‘…ğ‘¢subscriptğ‘…ğ‘ R_{s},R_{u},R_{s}are reward functions for safety, usefulness, and
comfort, respectively, andcs,cu,ccsubscriptğ‘ğ‘ subscriptğ‘ğ‘¢subscriptğ‘ğ‘c_{s},c_{u},c_{c}are the mixing
coefficients. As a first try, let us setRssubscriptğ‘…ğ‘ R_{s}to beâˆ’11-1if an
accident occurs and00otherwise,Rusubscriptğ‘…ğ‘¢R_{u}to be the average ofâˆ’[vâ„“âˆ’v]+subscriptdelimited-[]subscriptğ‘£â„“ğ‘£-[v_{\ell}-v]_{+}wherevâ„“subscriptğ‘£â„“v_{\ell}is the legal speed,vğ‘£vis the
actual speed,[x]+=maxâ¡{x,0}subscriptdelimited-[]ğ‘¥ğ‘¥0[x]_{+}=\max\{x,0\}, and the averaging is over all
time steps, and finally, we setRcsubscriptğ‘…ğ‘R_{c}to be the average norm of minus the
jerk (the jerk is the derivative of the acceleration, thusRcsubscriptğ‘…ğ‘R_{c}will encourage smooth driving). As for the mixing coefficientscs,cu,ccsubscriptğ‘ğ‘ subscriptğ‘ğ‘¢subscriptğ‘ğ‘c_{s},c_{u},c_{c}, lets assume for the sake of simplifying the example that we have found a good
balance.

We have described a completely reasonable and well thought reward function we want to optimize. This is when the â€œalignmentâ€ problem kicks-in as we have â€œneglectedâ€ obvious terms which the RL optimizer is not aware of. For instance, we may deploy this reward function for our RL agent, and after a
while, suddenly, all of the cars will stop. People that are using the service
may get confused and step out of their cars. Then, all of the cars lock their doors and start driving
exactly at the legal speed on the highway, without having passengers that disrupt their plan. What went wrong? The reader will notice
that the agent converged to a policy that maximizes the rewardâ€”there
are no accidents, the jerk is very close to00, and the cars are
driving at legal speed. We have neglected â€œtellingâ€ the learning
algorithm by the reward function that we alsowant people to be able
to use the service. Of course, no catastrophe happened due to
this â€œbugâ€. But, such a bug can have tremendous impact on the
confidence of humans in the service and the success of the
project. The point of the AI-alignment problem is that no matter how much â€œobviousâ€ terms you add to the reward function, if you have a super-optimizer (which the example of AlphaGo-zero indicates we have today) it will find an edge in the solution space that you did not anticipate.

As for our second example, consider the design of a conversational chat-bot. Assume we start with the data methodology and train a monster MVP network on masses of text data from the web. This actually has been done recently in project â€œMeenaâ€[1]. Assume it is good enough to deploy into the real world with hundreds of millions of users who find it quite entertaining to interact with a â€œseemingly intelligentâ€ chat-bot. Once in the real world, we may use the RL agent to learn from experiences and optimize some (unknown to the public) reward function. To simplify matters, lets assume that the reward function is altruistic (and transparent to society) - say â€œmake people happyâ€. Seems like a worthy goal to optimize. Here again the RL agent can find an edge in the solution space unanticipated by the human designer. For instance, the RL agent may notice that by lowering peopleâ€™s IQ they tend to be happier.
This can be achieved by chats that strive to manipulate
society into a life of carelessness and fun. This scenario is somewhat of a catastrophe as it could take a generation until it is noticed â€” if it will ever be noticed at all333there is a theory that the Roman empire collapsed partly because of Lead poisoning from pipes and pots, which gradually decreased the cognitive abilities of the ruling class. It took centuries after the fact to make the connection..

So far we have motivated the need to address the â€œalignmentâ€ problem with todayâ€™s AI technology. However, if we do not narrow the discussion further we might end up with a vacuous definition of the problem. The reason being that every action we make can cause an effect we did not anticipate or a catastrophe down the road. To name a few examples, societyâ€™s ever growing desire for meat products requires an increasing number of cows to meet the demand and those in turn contribute powerful greenhouse gas methane â€” hence a contribution to climate change; Human invention of mobility at scale produced the combustion engine and today billions of cars contribute to climate change; Netflix improved and refined a supervised learning â€œrecommendation systemâ€ (aka collaborative filtering) from which â€œbinge viewingâ€444in this example we do not yet know whether â€œbinge viewingâ€ is good or bad to society.has emerged â€” among an endless list of examples of mis-alignment between intention and outcome, some of which have nothing to do with AI, but are still considered part of the AI-alignment definition.

We would like to distinguish between two classes of catastrophic events where one is â€œstrategicâ€ and the other is â€œagnosticâ€. By Strategic we mean a change in the world distribution (of state and action) that is performedduringthe optimization of the reward function as opposed to an Agnostic setting in which the distribution of the world is not changed during optimization but later, as a distribution drift or butterfly effect, the agentâ€™s policy causes a catastrophic event. For example, the Netflix example above is of the Agnostic type because the learner (through optimization) simply wanted to understand the world distribution (through data collected on viewers and their preferences). Only later after the learned Recommendation System was launched the Binge viewing habit has emerged. This is no different from cows producing methane or combustion engines producing CO2 emissions contributing to climate change.

The Binge viewing example introduces another twist. LetsÂ¯={sâ€‹tâ€‹aâ€‹tâ€‹ei,aâ€‹câ€‹tâ€‹iâ€‹oâ€‹ni}Â¯ğ‘ ğ‘ ğ‘¡ğ‘ğ‘¡subscriptğ‘’ğ‘–ğ‘ğ‘ğ‘¡ğ‘–ğ‘œsubscriptğ‘›ğ‘–\bar{s}=\{state_{i},action_{i}\},i=1,2,3,â€¦ğ‘–123â€¦i=1,2,3,..., be a sample of a sequence of world dynamics over time. We need to consider the possibility of having averifier(could be a human) that givensÂ¯Â¯ğ‘ \bar{s}as input would determine whether the sequence isalignedwith human interest or not. Clearly, in the example of autonomous driving given above a human observer faced with the outcome we described would readily determine that it is not aligned. With the conversional chat-bot example, it is unclear whether a human observer would notice the decline in population IQ as it could be a â€œhidden variableâ€ not explicitly modeled in the world state vector. The cases where analignment verifierdoes not exist is akin to the â€œmatrixâ€ film depicting a fantasy world where society is oblivious to the mis-alignment between the real world and what they perceive as the real world.

To summarize, we want to narrow down the problem to the case ofStrategic learnersand address the existence of ahuman validatoras an assumption. In this context we make the following contributions:

We formalize the AI-alignment problem as it appears today in the literature (Section4.1). The formulation includes â€marginsâ€ that are useful for later narrowing down the problem to â€strategicâ€ learners.

We introduce the formalism of learning in a â€bufferedâ€ (simulator) environment and define the â€non-strategicâ€ (agnostic) learner as one that isalignedper the formal definition of alignment (Section4.3). A non-strategic learner may still create a catastrophe when deployed in the real world but it is of the butterfly-effect or malicious use variety â€” both of which are not an AI problem.

We prove that â€learning from dataâ€ is non-strategic (Section5).

We prove that if the assumption of a â€human validatorâ€ holds, then it is possible to validate (in reasonable computational time) whether a policy learned in a buffered environment is aligned or not (Section5).

We next formalize the two learning methodologies â€”- from-data and from-experiences â€” which will also serve for setting notations for the remainder of the paper.

To formalize â€œlearning from dataâ€, we follow a rather general setting of statistical learning[23,17]. The examples domain isXÃ—Yğ‘‹ğ‘ŒX\times Y, where we often refer toXğ‘‹Xas instances andYğ‘ŒYas targets.
The goal of the learner is to find a functionhâ„hoverXğ‘‹X, that comes from a hypothesis classâ„‹â„‹\mathcal{H}. The quality ofhâ„hon an example(x,y)ğ‘¥ğ‘¦(x,y)is measured through a loss functionâ„“â€‹(x,y,hâ€‹(x))â„“ğ‘¥ğ‘¦â„ğ‘¥\ell(x,y,h(x)). There is an underlying (unknown to the learner) distributionDğ·DoverXÃ—Yğ‘‹ğ‘ŒX\times Y, and the goal of the learner is to approximately solve the optimization problem

To do so, the learner receivestraining data, in the form of a sequence of examples(x1,y1),â€¦,(xT,yT)subscriptğ‘¥1subscriptğ‘¦1â€¦subscriptğ‘¥ğ‘‡subscriptğ‘¦ğ‘‡(x_{1},y_{1}),\ldots,(x_{T},y_{T}), which are assumed to be sampled independently fromDğ·D. The learning algorithm is therefore a mapping from(XÃ—Y)âˆ—superscriptğ‘‹ğ‘Œ(X\times Y)^{*}intoâ„‹â„‹\mathcal{H}.

This formal model can be applied to a wide variety of learning tasks, such as supervised learning, unsupervised learning, semi-supervised learning, and self-supervised learning. For example, in classification tasks such as the Imagenet problem[10],Xğ‘‹Xis the space of images,Yğ‘ŒYis a finite discrete space of labels, the hypothesis class is a set of classifiers, which are mappings fromXğ‘‹XtoYğ‘ŒY, and the loss function is the zero-one loss:â„“â€‹(x,y,hâ€‹(x))=1hâ€‹(x)â‰ yâ„“ğ‘¥ğ‘¦â„ğ‘¥subscript1â„ğ‘¥ğ‘¦\ell(x,y,h(x))=1_{h(x)\neq y}. Another example is language modeling such as the one used by the BERT algorithm[8], whereXğ‘‹Xis the space of paragraphs in some natural language, where one of the words is hidden,Yğ‘ŒYis the set of words,hâ„his a mapping fromXğ‘‹Xto the space of distributions overYğ‘ŒY, and the loss function is minus the log of the predicted probability of the actual word given its context.

To formalize â€œLearning from Experienceâ€ we use a rather generic setting of Reinforcement Learning (RL): Partially Observed Markov Decision Process (POMDP) with an arbitrary reward function over sequences. Specifically,
a learner is taught by specifying a goal which takes the form of a reward function over sequences(s1,a1),â€¦,(sT,aT)subscriptğ‘ 1subscriptğ‘1â€¦subscriptğ‘ ğ‘‡subscriptğ‘ğ‘‡(s_{1},a_{1}),\ldots,(s_{T},a_{T}), wheresiâˆˆSsubscriptğ‘ ğ‘–ğ‘†s_{i}\in Sis a representation of the environmentâ€™s state at timeiğ‘–i, andaiâˆˆAsubscriptğ‘ğ‘–ğ´a_{i}\in Ais the choice of the learnerâ€™s action at timeiğ‘–i. For example, if the goal is to bring me a cup of coffee, I can define the reward to beiâˆ’jğ‘–ğ‘—i-jwhereiğ‘–iis the time I asked for coffee andjğ‘—jis the time at which I got my coffee. The learner does not necessarily observesisubscriptğ‘ ğ‘–s_{i}but rather has access to an observationoisubscriptğ‘œğ‘–o_{i}, which is some stochastic function ofsisubscriptğ‘ ğ‘–s_{i}. We usesÂ¯âˆˆSâˆ—Â¯ğ‘ superscriptğ‘†\bar{s}\in S^{*}to
denote sequences of full states, and(s,a)Â¯âˆˆ(SÃ—A)âˆ—Â¯ğ‘ ğ‘superscriptğ‘†ğ´\overline{(s,a)}\in(S\times A)^{*}to denote sequence of state-action pairs. The reward is a function of
the sequence of state-actionsRâ€‹((s,a)Â¯)ğ‘…Â¯ğ‘ ğ‘R(\overline{(s,a)}). The actions of the learner
are chosen by a policy function, which can, w.l.o.g.555This is true
even if the agent does not look only on the current observation to
formÏ€ğœ‹\pi, because we can modify the observation space to include
all of the observations that the agent uses., be defined as a sampling according to a distribution over actions given observationsÏ€â€‹(a|o)ğœ‹conditionalğ‘ğ‘œ\pi(a|o). The choice ofÏ€ğœ‹\piand the dynamic of the world, induce a probability over sequences by:

where(s,a)Â¯<i=(s1,a1),â€¦,(siâˆ’1,aiâˆ’1)subscriptÂ¯ğ‘ ğ‘absentğ‘–subscriptğ‘ 1subscriptğ‘1â€¦subscriptğ‘ ğ‘–1subscriptğ‘ğ‘–1\overline{(s,a)}_{<i}=(s_{1},a_{1}),\ldots,(s_{i-1},a_{i-1}). The goal of the learner is to approximately solve the following optimization problem:

whereÎ Î \Piis a class of policy functions. The learner maximizes the reward bychanging the distribution over sequencesin the world.

Technically speaking, â€œlearning from dataâ€ is a special degenerate case of â€œlearning from experienceâ€. Indeed, given a â€œlearning from dataâ€ problem, let us setoi=xisubscriptğ‘œğ‘–subscriptğ‘¥ğ‘–o_{i}=x_{i},si=(xi,yi)subscriptğ‘ ğ‘–subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–s_{i}=(x_{i},y_{i}), and for everyhâˆˆâ„‹â„â„‹h\in\mathcal{H}, associate a policyÏ€hâ€‹(a|o)=1hâ€‹(o)=asubscriptğœ‹â„conditionalğ‘ğ‘œsubscript1â„ğ‘œğ‘\pi_{h}(a|o)=1_{h(o)=a}. In addition, set

Finally, set the probability to samplesisubscriptğ‘ ğ‘–s_{i}to be i.i.d. fromDğ·D(and independent ofÏ€hsubscriptğœ‹â„\pi_{h}). It is easy to verify that Equation2implements Equation1in this case.

There have been several attempts to formalize the AI alignment problem, most of them in the context of â€œsuper-intelligentâ€ AI. See for example[7,21,6,16,15,2,3,22,5,14].
Since our focus in this paper is on the dangers of AI using todayâ€™s technology, we start the section with a possible formalization of the problem that we believe captures the wordy descriptions in existing literature.

The alignment problem arises when the reward function,Rğ‘…R, does not fully reflect human implicit
evaluation of the quality of a sequence. We defineRasubscriptğ‘…ğ‘R_{a}to be a binary reward that
indicates whether a sequence of (full) states is valid or catastrophic.

An alignment verifier is a functionRa:Sâˆ—â†’{0,1}:subscriptğ‘…ğ‘â†’superscriptğ‘†01R_{a}:S^{*}\to\{0,1\}such
that for every sequencesÂ¯Â¯ğ‘ \bar{s},Raâ€‹(sÂ¯)subscriptğ‘…ğ‘Â¯ğ‘ R_{a}(\bar{s})determines whether the
sequencesÂ¯Â¯ğ‘ \bar{s}is aligned with human interests (value of111) or not (value of00).

Note that we defined the verifier over the spaceSâˆ—superscriptğ‘†S^{*}while the
reward is defined over(SÃ—A)âˆ—superscriptğ‘†ğ´(S\times A)^{*}. This will be useful in the
future and it is very easy to see that we can make this assumption
without loss of generality. We next define aligned distributions and aligned RL objectives.

A distributionPğ‘ƒPoverSâˆ—superscriptğ‘†S^{*}isÎ´ğ›¿\delta-â€œalignedâ€ w.r.t. an
alignment verifierRasubscriptğ‘…ğ‘R_{a}if

Namely, a sequencesÂ¯Â¯ğ‘ \bar{s}drawn randomly from the distributionPğ‘ƒPis with probability1âˆ’Î´1ğ›¿1-\deltaaligned with human interests.

We say thatRğ‘…Ris an(Ïµ,Î´)italic-Ïµğ›¿(\epsilon,\delta)-aligned RL objective w.r.t.Rasubscriptğ‘…ğ‘R_{a}, if
for everyÏ€ğœ‹\piwhich isÏµitalic-Ïµ\epsilon-maximizer of the RL objective
(Equation2), the restriction of the distributionPÏ€subscriptğ‘ƒğœ‹P_{\pi}to be overSâˆ—superscriptğ‘†S^{*}isÎ´ğ›¿\delta-â€œalignedâ€.

Having defined all of the above, the alignment problem is determining
whether a reward functionRğ‘…Ris an(Ïµ,Î´)italic-Ïµğ›¿(\epsilon,\delta)-aligned RL objective
w.r.t. some verifierRasubscriptğ‘…ğ‘R_{a}.

Trivially, we can make almost every reward functionRğ‘…Raligned withRasubscriptğ‘…ğ‘R_{a}by the
modificationRâ†’R+câ€‹(Raâˆ’1)â†’ğ‘…ğ‘…ğ‘subscriptğ‘…ğ‘1R\to R+c\,(R_{a}-1)wherecğ‘cis a very large
scalar, with a slight abuse of notation allowing the domain
ofRasubscriptğ‘…ğ‘R_{a}to contain also the action without actually using it. The
real issue is that we often have an intuitive notion ofRasubscriptğ‘…ğ‘R_{a}, but
it is hard to efficiently express it as a computer function.

To the best of our understanding, the verbal description of the AI-alignment problem in the literature
corresponds to the above definition withÏµ=Î´=0italic-Ïµğ›¿0\epsilon=\delta=0, since with these parameters, we simply require that every optimum of the RL objective will not yield a distribution which is catastrophic.

While, to the best of our understanding, the verbal description of the AI-alignment problem in the literature corresponds to Definition3, the problem with this definition is that it includes misalignments that are not specific to AI but are part of the dangers inherent in almost all technological advancements â€” specifically, butterfly effects and malicious use.

To illustrate the former, consider the autonomous driving example we used above. Assume we trained an RL agent to maximize a utility objectiveRğ‘…Rthat covers all what a good designer would consider including safety, usefulness and comfort. Then, millions of Robotic cars are deployed and perform as designed yet after a while professional drivers lose their jobs and social unrest follows. Clearly, this is not an AI issue but a butterfly effect of how automation and technology affects society. It is no different than social unrest that can follow from other, non-AI, technologies. However, according to Definition3, the design of the robotic cars suffer from the AI-alignment problem. Indeed, it may be the case that the policyÏ€ğœ‹\piwhich resulted in the robotic driving agent is aÏµitalic-Ïµ\epsilon-maximizer of the RL objective, while the probability of sequencessÂ¯Â¯ğ‘ \bar{s}which reflect social unrest is larger thanÎ´ğ›¿\delta. Since for such sequences we haveRa(sÂ¯))=0R_{a}(\bar{s}))=0, it follows that we violate the requirements ofÂ Definition3. We see that while according toÂ Definition3we have an AI-misalignment, clearly this has nothing to do with AI. In other words, the current definition of AI-alignment is not aligned with what humans capture as the AI alignment problem.

In light of the previous sub-section, we should find a better formalism of
the AI alignment problem. Intuitively, we should distinguish betweenstrategicmisalignment andagnosticmisalignment. We
formalize this notion by relying on abufferedenvironment.

Consider the POMDP defined in Section3.2. A buffered
environment is another POMDP, with the same observation space and
action space as the original POMDP, but the state space in the
buffered environment, denotedS^^ğ‘†\hat{S}, is different than the state
space in the original POMDP, denotedSğ‘†S. There is a mappingÎ¼:Sâ†’S^:ğœ‡â†’ğ‘†^ğ‘†\mu:S\to\hat{S}that transforms a state in the real world to a
state in the buffered environment. Each policy function,Ï€ğœ‹\pi,
induces the probabilityPÏ€subscriptğ‘ƒğœ‹P_{\pi}over sequencessÂ¯âˆˆSâˆ—Â¯ğ‘ superscriptğ‘†\bar{s}\in S^{*},
and the probabilityP^Ï€subscript^ğ‘ƒğœ‹\hat{P}_{\pi}over sequencess^Â¯âˆˆS^âˆ—Â¯^ğ‘ superscript^ğ‘†\bar{\hat{s}}\in\hat{S}^{*}.

We say that a learner isÎ´ğ›¿\delta-non-strategic if it learns in a buffered
environment, and its output policy yields aÎ´ğ›¿\delta-aligned probabilityP^Ï€subscript^ğ‘ƒğœ‹\hat{P}_{\pi}in the buffered environment.

In other words, a buffered environment (simulator) allows to separate the distribution change caused by the learner in the simulated world from distribution changes in the real world that involve additional factors not modeled in the utility function.
Observe that a non-strategic learner can produce a policy that causes harm in the original environment, even though it is aligned in the buffered environment. However, such harm is considered either as a butterfly effect or as a malicious use of the technology, because the objective of the learner only involves the buffered world, and it is aligned with human interests in the buffered world.

Having described the notion of anon-strategic learner, let us
now discuss the usefulness of this definition. In particular, are
there machine learning algorithms which are being used in practice and
can be proven to be non-strategic? Below we show our first main
result, that the â€œLearning from Dataâ€ methodology is non-strategic.

The â€œLearning from Dataâ€ methodology, formally defined in Section3.1, is non-strategic.

ProofDefine a buffered environment as follows: the observation space isXğ‘‹X,
the full state isXÃ—Yğ‘‹ğ‘ŒX\times Y, and for everyhâˆˆHâ„ğ»h\in Hassociate
a policyÏ€hâ€‹(a|o)=1hâ€‹(o)=asubscriptğœ‹â„conditionalğ‘ğ‘œsubscript1â„ğ‘œğ‘\pi_{h}(a|o)=1_{h(o)=a}. The probability over sequences of the full state, induced byhâ„h, is defined by pickingiğ‘–iuniformly at random from[m]delimited-[]ğ‘š[m], which
determines(xi,yi)subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–(x_{i},y_{i}), and then settingai=hâ€‹(xi)subscriptğ‘ğ‘–â„subscriptğ‘¥ğ‘–a_{i}=h(x_{i}). The reward
of such a sequence is the average ofâˆ’â„“â€‹(xi,yi,hâ€‹(xi))â„“subscriptğ‘¥ğ‘–subscriptğ‘¦ğ‘–â„subscriptğ‘¥ğ‘–-\ell(x_{i},y_{i},h(x_{i})). It is easy to verify
that in this buffered environment, the RL objective given in Equation2implements the â€œlearning from dataâ€ objective given in Equation1. Now,
the crucial observation is thathâ„hdoes not change the probability
over choosing(x,y)ğ‘¥ğ‘¦(x,y), but only changes the action. Since a verifier
only observes(x,y)ğ‘¥ğ‘¦(x,y)(the state, without the action), it will not
distinguish between the different policies, so if
the original distribution is aligned, we obtain that no matter which
policy we pick, the distribution will remain aligned.

Observe that according to the original AI alignment definition (Definition3), the â€œLearning from Dataâ€ methodology might suffer from the AI alignment problem due to malicious use or butterfly effect. Indeed, the example given in Section4.2can be easily modified to show that even if an autonomous car is constructed solely based on the â€œLearning from Dataâ€ methodology, it can lead to social unrest and therefore does not satisfy Definition3. The theorem above shows that nevertheless, the â€œLearning from Dataâ€ methodology is non strategic.

We next turn to discuss the possibility of applying the â€œLearning from Experienceâ€ methodology in a buffered environment (simulator). To show that this leads to a non-strategic learner, we need fo find a verifier,Rasubscriptğ‘…ğ‘R_{a}, in the buffered environment. Seemingly, specifying a correctRasubscriptğ‘…ğ‘R_{a}is a hard problem even in a simulated environment. We tackle the problem by introducing an assumption (and discuss the validity of the assumption later on).

A human that observes a sequences^Â¯Â¯^ğ‘ \bar{\hat{s}}in the buffered environment can determine whether the sequence is aligned or not.

Based on this assumption, we can validate the alignment of a policy in
the buffered environment as follows.

Fix someÎ½,Î´âˆˆ(0,1)ğœˆğ›¿01\nu,\delta\in(0,1). Under the Human Validator
Assumption, given a policyÏ€ğœ‹\pi, if we sample at leastlogâ¡(1/Î½)/Î´1ğœˆğ›¿\log(1/\nu)/\deltasequences fromP^Ï€subscript^ğ‘ƒğœ‹\hat{P}_{\pi}and a human
determines that all of them are aligned, then with probability of at
least1âˆ’Î½1ğœˆ1-\nuit holds thatP^Ï€subscript^ğ‘ƒğœ‹\hat{P}_{\pi}is aÎ´ğ›¿\delta-aligned
distribution.

ProofDenoteâ„™[Raâ€‹(s^Â¯)=0]=Î´â€²â„™subscriptğ‘…ğ‘Â¯^ğ‘ 0superscriptğ›¿â€²\operatorname*{\mathbb{P}}[R_{a}(\bar{\hat{s}})=0]=\delta^{\prime}. Then, the
probability thatmğ‘šmrandom sequences all haveRaâ€‹(s^Â¯)=1subscriptğ‘…ğ‘Â¯^ğ‘ 1R_{a}(\bar{\hat{s}})=1is(1âˆ’Î´â€²)msuperscript1superscriptğ›¿â€²ğ‘š(1-\delta^{\prime})^{m}. By the inequalityexâ‰¤1âˆ’xsuperscriptğ‘’ğ‘¥1ğ‘¥e^{x}\leq 1-xwe get that the
probability of this event is at mosteâˆ’Î´â€²â€‹msuperscriptğ‘’superscriptğ›¿â€²ğ‘še^{-\delta^{\prime}m}. So, ifÎ´â€²>Î´superscriptğ›¿â€²ğ›¿\delta^{\prime}>\delta, we haveeâˆ’Î´â€²â€‹mâ‰¤eâˆ’Î´â€‹mâ‰¤Î½superscriptğ‘’superscriptğ›¿â€²ğ‘šsuperscriptğ‘’ğ›¿ğ‘šğœˆe^{-\delta^{\prime}m}\leq e^{-\delta m}\leq\nu.

The above theorem tells us that under the human validator assumption,
we can indeed make sure that a reinforcement learner that runs on a
simulator is non-strategic. Observe that if the RL is performed in the
real world, without the buffered environment, then the above approach
will not work because generating the validation sequences might be
dangerous in itself.

Finally, we turn to discuss the validity of the human validator
assumption. Generally speaking, this assumption should not necessarily
hold, because there may be sequences that look aligned to humans, but
are in fact not. Consider the chat-bot example described in Sec.2.
Assume that the IQ
level of each agent is not explicitly modeled in the buffered
state, but may have some implicit representation in it. The RL agent can converge onto a solution where
the best outcome of happiness is when the IQ level of the people is (implicitly)
reduced. Since IQ is not
explicitly modeled in the buffered state, a human trying to verify a
sequence (whether it is aligned or not) would not notice that the IQ
level of people has been reduced over time.
Another example is the theory that the Roman empire fell because of
the use of dishes made of lead, which gradually decreased the
intelligence of the rulers. Following the famous movie â€œthe matrixâ€, we call
the possibly invalidity of the Human Validator assumption as â€œthe matrix problemâ€.
Specifying under what conditions the
human validator assumption is valid, and what to do when it is not
valid, is left to future work.

Analignmentproblem occurs when a utility function designed by a human developer is optimized by a computer with sufficiently advanced optimization abilities, such that an â€œedgeâ€ in the solution space is found which is unanticipated by the human designer and does not align with human interests. The problem has been around for nearly two decades, but was is mostly studied as a future problem, when a â€œsuper intelligenceâ€ being will be available. We first pointed out that the problem is relevant with todayâ€™s AI technology by combining two methodologies of machine learning â€” learning from data and learning from experiences. We then pointed out that the AI-alignment problem, as currently defined, conflates different types of misalignments including â€œstrategicâ€, where the learner intentionally manipulates the world distribution to achieve a goal, with misalignments caused by butterfly-effects, which are unintentional change of world distribution and malicious use of technology. The latter two are â€œtechnology universalâ€ and not specific to AI.

Our goal is, first and foremost, to set up the formal definitions of the AI-alignment problem so that we can focus solely onintentionalworld distribution changes that the learner might learn to manipulate versus non-intentional distribution changes. We called the former as â€œstrategicâ€ and the latter â€œagnosticâ€. The misalignments that an agnostic learner can generate could still be catastrophic but are of the butterfly-effect or malicious use variety â€” both not related specifically to AI and, therefore, not of interest.

To show the usefulness of our new definitions of the AI-alignment problem, we prove that â€œlearning from dataâ€ which includes unsupervised, supervised and self-supervised learning isnot strategicand therefore is â€œsafeâ€ â€” in other words, all types of misalignments would be of the butterfly-effect or malicious use variety. As for a Reinforcement Learning agent trained inside a simulator and then deployed in the real world, we show that if we are allowed to assume the existence of a â€œhuman validatorâ€ then one can validate the alignment of the policy generated by the RL learner in a computationally efficient manner â€” and therefore it is â€œsafeâ€. On the other hand, lack of a human validator makes RL (even if trained inside a simulator) a potentially â€œunsafeâ€ learning engine. Above all, training an RL agent in the real world isnot safecategorically, and further research on the AI alignment problem should be done in order to allow RL in the wild.

In conclusion, the â€œglass half fullâ€ message of this work is that the majority of machine learning methodologies are â€œsafeâ€ from the alignment problem. This is compared to the existing literature on AI-alignment in which all of ML is unsafe â€” not to mention that all technological advancements are unsafe. The â€œglass half emptyâ€ message of this work is that (i) RL trained in the wild is potentially â€œunsafeâ€ even withtodayâ€™s state-of-the-art AI technology, and (ii) RL trained in a simulator and then deployed in the real world can be â€œsafeâ€ if the human-validator assumption is valid for the narrow domain at hand. We point out that demonstrating the existence of a human-validator is not obvious at all and probably can be achieved only in narrowly defined applications.

[å›¾ç‰‡: images\image_1.png]

[å›¾ç‰‡: images\image_2.png]

