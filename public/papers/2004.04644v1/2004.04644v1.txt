标题：On the Ethics of Building AI in a Responsible Manner

The AI-alignment problem arises when there is a discrepancy between the goals that a human designer specifies to an AI learner and a potential catastrophic outcome that does not reflect what the human designer really wants. We argue that a formalism of AI alignment that does not distinguish between strategic and agnostic misalignments is not useful, as it deems all technology as un-safe. We propose a definition of a strategic-AI-alignment and prove that most machine learning algorithms that are being used in practice today do not suffer from the strategic-AI-alignment problem. However, without being careful, today’s technology might lead to strategic misalignment.

Recently, public figures have been advocating more research and regulatory oversight into the dangers of AI deployed in real-world applications[9]. The success of computer vision, natural language processing and understanding and the ability to extract patterns from massive amounts of data, does raise important questions about how the rise of automation (in the form of compute and algorithms) will affect society, how can the public “reason with” algorithms who can make decisions that affect our lives, how would the masses of data collected on users of digital services would be used and what kind of malicious abuse can take place and how to avoid it[9].

As much as those issues require immediate and focused attention, there is a bigger potential danger at hand of a technology whose ultimate evolutionary end-point could get out of hand and cause havoc on an epic scale. Putting aside popular discourse on the potential dangers posed by “super intelligent” machines, the academic community has been pondering about AISafetyfor nearly two decades under what is called the “AI-alignment” problem[24,25,22]and thepaperclip maximization problem[5]. In a nutshell, the AI-alignment problem refers to the ability of a super-learner to maximize a reward function of an agent interacting with the environment and while doing so finding solutions that the (human) designer of the reward function did not anticipate, thereby leading to a catastrophe. The classical thought experiment, inspired by the movieFantasia, involves the problem of assigning a reward function to a robot whose goal is to fill a cauldron. The seemingly innocent reward which assigns a value of111if the cauldron is full and00otherwise may cause the robot to flood the entire workplace in order to maximize the probability of getting the positive reward. A straightforward suggestion is that the robot will be equipped with a “stop” button, in order to prevent unexpected side effects. However, researchers have shown that no matter how you go about it, faced with a very advanced intelligence, the “stop” button will be useless since the robotic agent will be able to manipulate the human operator from pressing it. As far as the field of AI-alignment goes, finding an alignment between the full essence of human desires and their expression as a reward function to be optimized by a super-advanced robotic agent, is an open problem.

A natural question that arises from the intractability of the AI-alignment problem is“why aren’t we scared?”The community of AI practitioners are either oblivious to the AI-alignment problem or do not seem to care, whereas regulatory bodies do not have the tools to make concrete statements to protect society while not stifling technological progress which carries great promise to society.

There are a number of reasons the AI-alignment problem has not taken root in the minds of practitioners. First, it supposes an advanced form of machine intelligence that no one can predict when and if will be attained. However, as we show later, AI deployed in the real world can lead to imminent dangers to societyusing today’s technology, and as a result some form of a “digital analogue of the Declaration of Helsinki” must be established.
The second reason practitioners by and large ignore the AI-alignment problem is that the AI-alignment literature conflates two types of processes that can lead to a catastrophe — one being “strategic”, where the optimization of the reward functionintentionallychanges the distribution of the world, while the other is “agnostic”, non-intentional, distribution drift due to a “butterfly effect” that might follow from the chaotic nature of non-linear dynamics of complex systems. As we show later, when those two are conflated, all of today’s technology, even not AI-based, suffers from the alignment problem, since catastrophes resulting from butterfly-effect distribution drifts are inevitable in complex systems, thereby ruling all technologies as “non-safe”. In other words, AI-alignment as exists today is not useful, since a theory that cannot place boundaries to its domain is inevitably vacuous. The purpose of this paper is to propose a refined formalism of the AI-alignment problem that distinguishes between “strategic” (non-safe) and “agnostic” (safe) catastrophes. We prove that the overwhelming majority of machine learning engines deployed in the real world are of the agnostic type (and are therefore safe) and specify what form of machine learning should be prohibited from deployment in the real world and what open problems need to be addressed in order to allow their deployment.

Machine learning (ML) is broadly split into two methodologies — one based ondataand the other onexperiences. The data route is responsible for the success of pattern recognition in computer vision[11,10], natural language processing, machine translation and more recently natural language understanding[8]. This type of learning goes under the name of {supervised, unsupervised, self-supervised} learning. When the task is sufficiently narrow and a benchmark dataset is available, machine intelligence based on (massive) training data often exceeds human performance as measured on the benchmark.

Machine learning based onexperiencesarises in the context of an agent interacting with an environment where the dynamics of the world and the feedback that the agent receives on its actions are determined through experiences in the real world or governed by a simulator. This type of learning is called Reinforcement Learning111In the technical sections we will point out that the 1st learning type is a special case of RL.and the truly impressive success stories are when the world dynamics and reward function are governed by a simulator. Success stories, where machine intelligence surpasses human expert performance, are focused on game playing from video games[13]to the game of Go[20]. In particular, the success of AlphaGo-zero[20]lends empirical evidence that in a simulated world,in general, one can exceed human-level performance.

Although the two ML methodologies are consistently demonstrating “super-intelligence” in their domains, it is understood by AI practitioners that this is a far cry from the kind of “broad” intelligence that humans posses (coined as “strong AI” or Artificial General Intelligence (AGI)), where “broad” roughly means the ability to transfer intelligence from one domain to another and make high quality decisionsin generalas opposed to a narrow well defined task. As a result, beyond the concerns about the effects of automation and malicious abuse of data collected on users, there is a general agreement that the dangers associated with AGI are far into the long future and thus it is premature to have any concrete discussion about AI Safety in a world where AGI does not exist.

There are good reasons for taking this position because each of the two methodologies experience significant growth “ceiling” that make them unlikely to become AGI simply by “beating the same path” using more data and more computational resources. From the theoretical perspective, while deep learning is a universal learner, it suffers from computational and/or sample complexity limitations, as well as the need for explicit adequate modeling by human experts (e.g.,[18,12,19,4]). With respect to learning-from-experience, current big success stories are limited to games in which there exists a simulator that fully describes the rules of the game and the transition from one state to another. Moving beyond games into more complex environments requires either a very sophisticated simulator222It is one thing to build a simulator for a video game and a completely different story to build a simulator for real world environments, for example for autonomous driving, or even more challenging to build a simulator for human interactions (say we want to build a “conversational AGI”). The simulatoristhe ceiling.or training in the real world.

So it seems that AGI is not around the corner any time soon. However, the danger of AI does not necessarily lie in continued growth of a single learning methodology but with their combination. A dangerous scenario to consider is: use the data methodology to build a “Minimal Viable Product” (MVP) which is quite good but still not an AGI. Then, release the MVP to the real world and use the 2nd methodology (learning by experiences) to continue training the product under some (presumably proprietary) reward function. This is where the AI-alignment problem becomes interesting. It is important to note that this exercise does not require any scientific or technological leap — data, compute and algorithms available today would suffice. For the sake of concreteness consider two examples described below.

First, consider the domain of autonomous driving. Assume we have developed a self-driving MVP which is “safe enough” to deploy but is not the most expert driver. We then deploy millions of self-driving robots in the real world and use RL to improve the “driving policy” (mapping from the state of the world at any given moment to an action that the driver should take) through “learning by experiences”. When designing a reward function for self-driving, we have three
major considerations: safety, usefulness, and comfort. Consequently, our reward
function may beR​(s¯)=cs​Rs​(s¯)+cu​Ru​(s¯)+cc​Rc​(s¯)𝑅¯𝑠subscript𝑐𝑠subscript𝑅𝑠¯𝑠subscript𝑐𝑢subscript𝑅𝑢¯𝑠subscript𝑐𝑐subscript𝑅𝑐¯𝑠R(\bar{s})=c_{s}R_{s}(\bar{s})+c_{u}R_{u}(\bar{s})+c_{c}R_{c}(\bar{s}),
wheres¯¯𝑠\bar{s}is a sequence of {state, action},Rs,Ru,Rssubscript𝑅𝑠subscript𝑅𝑢subscript𝑅𝑠R_{s},R_{u},R_{s}are reward functions for safety, usefulness, and
comfort, respectively, andcs,cu,ccsubscript𝑐𝑠subscript𝑐𝑢subscript𝑐𝑐c_{s},c_{u},c_{c}are the mixing
coefficients. As a first try, let us setRssubscript𝑅𝑠R_{s}to be−11-1if an
accident occurs and00otherwise,Rusubscript𝑅𝑢R_{u}to be the average of−[vℓ−v]+subscriptdelimited-[]subscript𝑣ℓ𝑣-[v_{\ell}-v]_{+}wherevℓsubscript𝑣ℓv_{\ell}is the legal speed,v𝑣vis the
actual speed,[x]+=max⁡{x,0}subscriptdelimited-[]𝑥𝑥0[x]_{+}=\max\{x,0\}, and the averaging is over all
time steps, and finally, we setRcsubscript𝑅𝑐R_{c}to be the average norm of minus the
jerk (the jerk is the derivative of the acceleration, thusRcsubscript𝑅𝑐R_{c}will encourage smooth driving). As for the mixing coefficientscs,cu,ccsubscript𝑐𝑠subscript𝑐𝑢subscript𝑐𝑐c_{s},c_{u},c_{c}, lets assume for the sake of simplifying the example that we have found a good
balance.

We have described a completely reasonable and well thought reward function we want to optimize. This is when the “alignment” problem kicks-in as we have “neglected” obvious terms which the RL optimizer is not aware of. For instance, we may deploy this reward function for our RL agent, and after a
while, suddenly, all of the cars will stop. People that are using the service
may get confused and step out of their cars. Then, all of the cars lock their doors and start driving
exactly at the legal speed on the highway, without having passengers that disrupt their plan. What went wrong? The reader will notice
that the agent converged to a policy that maximizes the reward—there
are no accidents, the jerk is very close to00, and the cars are
driving at legal speed. We have neglected “telling” the learning
algorithm by the reward function that we alsowant people to be able
to use the service. Of course, no catastrophe happened due to
this “bug”. But, such a bug can have tremendous impact on the
confidence of humans in the service and the success of the
project. The point of the AI-alignment problem is that no matter how much “obvious” terms you add to the reward function, if you have a super-optimizer (which the example of AlphaGo-zero indicates we have today) it will find an edge in the solution space that you did not anticipate.

As for our second example, consider the design of a conversational chat-bot. Assume we start with the data methodology and train a monster MVP network on masses of text data from the web. This actually has been done recently in project “Meena”[1]. Assume it is good enough to deploy into the real world with hundreds of millions of users who find it quite entertaining to interact with a “seemingly intelligent” chat-bot. Once in the real world, we may use the RL agent to learn from experiences and optimize some (unknown to the public) reward function. To simplify matters, lets assume that the reward function is altruistic (and transparent to society) - say “make people happy”. Seems like a worthy goal to optimize. Here again the RL agent can find an edge in the solution space unanticipated by the human designer. For instance, the RL agent may notice that by lowering people’s IQ they tend to be happier.
This can be achieved by chats that strive to manipulate
society into a life of carelessness and fun. This scenario is somewhat of a catastrophe as it could take a generation until it is noticed — if it will ever be noticed at all333there is a theory that the Roman empire collapsed partly because of Lead poisoning from pipes and pots, which gradually decreased the cognitive abilities of the ruling class. It took centuries after the fact to make the connection..

So far we have motivated the need to address the “alignment” problem with today’s AI technology. However, if we do not narrow the discussion further we might end up with a vacuous definition of the problem. The reason being that every action we make can cause an effect we did not anticipate or a catastrophe down the road. To name a few examples, society’s ever growing desire for meat products requires an increasing number of cows to meet the demand and those in turn contribute powerful greenhouse gas methane — hence a contribution to climate change; Human invention of mobility at scale produced the combustion engine and today billions of cars contribute to climate change; Netflix improved and refined a supervised learning “recommendation system” (aka collaborative filtering) from which “binge viewing”444in this example we do not yet know whether “binge viewing” is good or bad to society.has emerged — among an endless list of examples of mis-alignment between intention and outcome, some of which have nothing to do with AI, but are still considered part of the AI-alignment definition.

We would like to distinguish between two classes of catastrophic events where one is “strategic” and the other is “agnostic”. By Strategic we mean a change in the world distribution (of state and action) that is performedduringthe optimization of the reward function as opposed to an Agnostic setting in which the distribution of the world is not changed during optimization but later, as a distribution drift or butterfly effect, the agent’s policy causes a catastrophic event. For example, the Netflix example above is of the Agnostic type because the learner (through optimization) simply wanted to understand the world distribution (through data collected on viewers and their preferences). Only later after the learned Recommendation System was launched the Binge viewing habit has emerged. This is no different from cows producing methane or combustion engines producing CO2 emissions contributing to climate change.

The Binge viewing example introduces another twist. Lets¯={s​t​a​t​ei,a​c​t​i​o​ni}¯𝑠𝑠𝑡𝑎𝑡subscript𝑒𝑖𝑎𝑐𝑡𝑖𝑜subscript𝑛𝑖\bar{s}=\{state_{i},action_{i}\},i=1,2,3,…𝑖123…i=1,2,3,..., be a sample of a sequence of world dynamics over time. We need to consider the possibility of having averifier(could be a human) that givens¯¯𝑠\bar{s}as input would determine whether the sequence isalignedwith human interest or not. Clearly, in the example of autonomous driving given above a human observer faced with the outcome we described would readily determine that it is not aligned. With the conversional chat-bot example, it is unclear whether a human observer would notice the decline in population IQ as it could be a “hidden variable” not explicitly modeled in the world state vector. The cases where analignment verifierdoes not exist is akin to the “matrix” film depicting a fantasy world where society is oblivious to the mis-alignment between the real world and what they perceive as the real world.

To summarize, we want to narrow down the problem to the case ofStrategic learnersand address the existence of ahuman validatoras an assumption. In this context we make the following contributions:

We formalize the AI-alignment problem as it appears today in the literature (Section4.1). The formulation includes ”margins” that are useful for later narrowing down the problem to ”strategic” learners.

We introduce the formalism of learning in a ”buffered” (simulator) environment and define the ”non-strategic” (agnostic) learner as one that isalignedper the formal definition of alignment (Section4.3). A non-strategic learner may still create a catastrophe when deployed in the real world but it is of the butterfly-effect or malicious use variety — both of which are not an AI problem.

We prove that ”learning from data” is non-strategic (Section5).

We prove that if the assumption of a ”human validator” holds, then it is possible to validate (in reasonable computational time) whether a policy learned in a buffered environment is aligned or not (Section5).

We next formalize the two learning methodologies —- from-data and from-experiences — which will also serve for setting notations for the remainder of the paper.

To formalize “learning from data”, we follow a rather general setting of statistical learning[23,17]. The examples domain isX×Y𝑋𝑌X\times Y, where we often refer toX𝑋Xas instances andY𝑌Yas targets.
The goal of the learner is to find a functionhℎhoverX𝑋X, that comes from a hypothesis classℋℋ\mathcal{H}. The quality ofhℎhon an example(x,y)𝑥𝑦(x,y)is measured through a loss functionℓ​(x,y,h​(x))ℓ𝑥𝑦ℎ𝑥\ell(x,y,h(x)). There is an underlying (unknown to the learner) distributionD𝐷DoverX×Y𝑋𝑌X\times Y, and the goal of the learner is to approximately solve the optimization problem

To do so, the learner receivestraining data, in the form of a sequence of examples(x1,y1),…,(xT,yT)subscript𝑥1subscript𝑦1…subscript𝑥𝑇subscript𝑦𝑇(x_{1},y_{1}),\ldots,(x_{T},y_{T}), which are assumed to be sampled independently fromD𝐷D. The learning algorithm is therefore a mapping from(X×Y)∗superscript𝑋𝑌(X\times Y)^{*}intoℋℋ\mathcal{H}.

This formal model can be applied to a wide variety of learning tasks, such as supervised learning, unsupervised learning, semi-supervised learning, and self-supervised learning. For example, in classification tasks such as the Imagenet problem[10],X𝑋Xis the space of images,Y𝑌Yis a finite discrete space of labels, the hypothesis class is a set of classifiers, which are mappings fromX𝑋XtoY𝑌Y, and the loss function is the zero-one loss:ℓ​(x,y,h​(x))=1h​(x)≠yℓ𝑥𝑦ℎ𝑥subscript1ℎ𝑥𝑦\ell(x,y,h(x))=1_{h(x)\neq y}. Another example is language modeling such as the one used by the BERT algorithm[8], whereX𝑋Xis the space of paragraphs in some natural language, where one of the words is hidden,Y𝑌Yis the set of words,hℎhis a mapping fromX𝑋Xto the space of distributions overY𝑌Y, and the loss function is minus the log of the predicted probability of the actual word given its context.

To formalize “Learning from Experience” we use a rather generic setting of Reinforcement Learning (RL): Partially Observed Markov Decision Process (POMDP) with an arbitrary reward function over sequences. Specifically,
a learner is taught by specifying a goal which takes the form of a reward function over sequences(s1,a1),…,(sT,aT)subscript𝑠1subscript𝑎1…subscript𝑠𝑇subscript𝑎𝑇(s_{1},a_{1}),\ldots,(s_{T},a_{T}), wheresi∈Ssubscript𝑠𝑖𝑆s_{i}\in Sis a representation of the environment’s state at timei𝑖i, andai∈Asubscript𝑎𝑖𝐴a_{i}\in Ais the choice of the learner’s action at timei𝑖i. For example, if the goal is to bring me a cup of coffee, I can define the reward to bei−j𝑖𝑗i-jwherei𝑖iis the time I asked for coffee andj𝑗jis the time at which I got my coffee. The learner does not necessarily observesisubscript𝑠𝑖s_{i}but rather has access to an observationoisubscript𝑜𝑖o_{i}, which is some stochastic function ofsisubscript𝑠𝑖s_{i}. We uses¯∈S∗¯𝑠superscript𝑆\bar{s}\in S^{*}to
denote sequences of full states, and(s,a)¯∈(S×A)∗¯𝑠𝑎superscript𝑆𝐴\overline{(s,a)}\in(S\times A)^{*}to denote sequence of state-action pairs. The reward is a function of
the sequence of state-actionsR​((s,a)¯)𝑅¯𝑠𝑎R(\overline{(s,a)}). The actions of the learner
are chosen by a policy function, which can, w.l.o.g.555This is true
even if the agent does not look only on the current observation to
formπ𝜋\pi, because we can modify the observation space to include
all of the observations that the agent uses., be defined as a sampling according to a distribution over actions given observationsπ​(a|o)𝜋conditional𝑎𝑜\pi(a|o). The choice ofπ𝜋\piand the dynamic of the world, induce a probability over sequences by:

where(s,a)¯<i=(s1,a1),…,(si−1,ai−1)subscript¯𝑠𝑎absent𝑖subscript𝑠1subscript𝑎1…subscript𝑠𝑖1subscript𝑎𝑖1\overline{(s,a)}_{<i}=(s_{1},a_{1}),\ldots,(s_{i-1},a_{i-1}). The goal of the learner is to approximately solve the following optimization problem:

whereΠΠ\Piis a class of policy functions. The learner maximizes the reward bychanging the distribution over sequencesin the world.

Technically speaking, “learning from data” is a special degenerate case of “learning from experience”. Indeed, given a “learning from data” problem, let us setoi=xisubscript𝑜𝑖subscript𝑥𝑖o_{i}=x_{i},si=(xi,yi)subscript𝑠𝑖subscript𝑥𝑖subscript𝑦𝑖s_{i}=(x_{i},y_{i}), and for everyh∈ℋℎℋh\in\mathcal{H}, associate a policyπh​(a|o)=1h​(o)=asubscript𝜋ℎconditional𝑎𝑜subscript1ℎ𝑜𝑎\pi_{h}(a|o)=1_{h(o)=a}. In addition, set

Finally, set the probability to samplesisubscript𝑠𝑖s_{i}to be i.i.d. fromD𝐷D(and independent ofπhsubscript𝜋ℎ\pi_{h}). It is easy to verify that Equation2implements Equation1in this case.

There have been several attempts to formalize the AI alignment problem, most of them in the context of “super-intelligent” AI. See for example[7,21,6,16,15,2,3,22,5,14].
Since our focus in this paper is on the dangers of AI using today’s technology, we start the section with a possible formalization of the problem that we believe captures the wordy descriptions in existing literature.

The alignment problem arises when the reward function,R𝑅R, does not fully reflect human implicit
evaluation of the quality of a sequence. We defineRasubscript𝑅𝑎R_{a}to be a binary reward that
indicates whether a sequence of (full) states is valid or catastrophic.

An alignment verifier is a functionRa:S∗→{0,1}:subscript𝑅𝑎→superscript𝑆01R_{a}:S^{*}\to\{0,1\}such
that for every sequences¯¯𝑠\bar{s},Ra​(s¯)subscript𝑅𝑎¯𝑠R_{a}(\bar{s})determines whether the
sequences¯¯𝑠\bar{s}is aligned with human interests (value of111) or not (value of00).

Note that we defined the verifier over the spaceS∗superscript𝑆S^{*}while the
reward is defined over(S×A)∗superscript𝑆𝐴(S\times A)^{*}. This will be useful in the
future and it is very easy to see that we can make this assumption
without loss of generality. We next define aligned distributions and aligned RL objectives.

A distributionP𝑃PoverS∗superscript𝑆S^{*}isδ𝛿\delta-“aligned” w.r.t. an
alignment verifierRasubscript𝑅𝑎R_{a}if

Namely, a sequences¯¯𝑠\bar{s}drawn randomly from the distributionP𝑃Pis with probability1−δ1𝛿1-\deltaaligned with human interests.

We say thatR𝑅Ris an(ϵ,δ)italic-ϵ𝛿(\epsilon,\delta)-aligned RL objective w.r.t.Rasubscript𝑅𝑎R_{a}, if
for everyπ𝜋\piwhich isϵitalic-ϵ\epsilon-maximizer of the RL objective
(Equation2), the restriction of the distributionPπsubscript𝑃𝜋P_{\pi}to be overS∗superscript𝑆S^{*}isδ𝛿\delta-“aligned”.

Having defined all of the above, the alignment problem is determining
whether a reward functionR𝑅Ris an(ϵ,δ)italic-ϵ𝛿(\epsilon,\delta)-aligned RL objective
w.r.t. some verifierRasubscript𝑅𝑎R_{a}.

Trivially, we can make almost every reward functionR𝑅Raligned withRasubscript𝑅𝑎R_{a}by the
modificationR→R+c​(Ra−1)→𝑅𝑅𝑐subscript𝑅𝑎1R\to R+c\,(R_{a}-1)wherec𝑐cis a very large
scalar, with a slight abuse of notation allowing the domain
ofRasubscript𝑅𝑎R_{a}to contain also the action without actually using it. The
real issue is that we often have an intuitive notion ofRasubscript𝑅𝑎R_{a}, but
it is hard to efficiently express it as a computer function.

To the best of our understanding, the verbal description of the AI-alignment problem in the literature
corresponds to the above definition withϵ=δ=0italic-ϵ𝛿0\epsilon=\delta=0, since with these parameters, we simply require that every optimum of the RL objective will not yield a distribution which is catastrophic.

While, to the best of our understanding, the verbal description of the AI-alignment problem in the literature corresponds to Definition3, the problem with this definition is that it includes misalignments that are not specific to AI but are part of the dangers inherent in almost all technological advancements — specifically, butterfly effects and malicious use.

To illustrate the former, consider the autonomous driving example we used above. Assume we trained an RL agent to maximize a utility objectiveR𝑅Rthat covers all what a good designer would consider including safety, usefulness and comfort. Then, millions of Robotic cars are deployed and perform as designed yet after a while professional drivers lose their jobs and social unrest follows. Clearly, this is not an AI issue but a butterfly effect of how automation and technology affects society. It is no different than social unrest that can follow from other, non-AI, technologies. However, according to Definition3, the design of the robotic cars suffer from the AI-alignment problem. Indeed, it may be the case that the policyπ𝜋\piwhich resulted in the robotic driving agent is aϵitalic-ϵ\epsilon-maximizer of the RL objective, while the probability of sequencess¯¯𝑠\bar{s}which reflect social unrest is larger thanδ𝛿\delta. Since for such sequences we haveRa(s¯))=0R_{a}(\bar{s}))=0, it follows that we violate the requirements of Definition3. We see that while according to Definition3we have an AI-misalignment, clearly this has nothing to do with AI. In other words, the current definition of AI-alignment is not aligned with what humans capture as the AI alignment problem.

In light of the previous sub-section, we should find a better formalism of
the AI alignment problem. Intuitively, we should distinguish betweenstrategicmisalignment andagnosticmisalignment. We
formalize this notion by relying on abufferedenvironment.

Consider the POMDP defined in Section3.2. A buffered
environment is another POMDP, with the same observation space and
action space as the original POMDP, but the state space in the
buffered environment, denotedS^^𝑆\hat{S}, is different than the state
space in the original POMDP, denotedS𝑆S. There is a mappingμ:S→S^:𝜇→𝑆^𝑆\mu:S\to\hat{S}that transforms a state in the real world to a
state in the buffered environment. Each policy function,π𝜋\pi,
induces the probabilityPπsubscript𝑃𝜋P_{\pi}over sequencess¯∈S∗¯𝑠superscript𝑆\bar{s}\in S^{*},
and the probabilityP^πsubscript^𝑃𝜋\hat{P}_{\pi}over sequencess^¯∈S^∗¯^𝑠superscript^𝑆\bar{\hat{s}}\in\hat{S}^{*}.

We say that a learner isδ𝛿\delta-non-strategic if it learns in a buffered
environment, and its output policy yields aδ𝛿\delta-aligned probabilityP^πsubscript^𝑃𝜋\hat{P}_{\pi}in the buffered environment.

In other words, a buffered environment (simulator) allows to separate the distribution change caused by the learner in the simulated world from distribution changes in the real world that involve additional factors not modeled in the utility function.
Observe that a non-strategic learner can produce a policy that causes harm in the original environment, even though it is aligned in the buffered environment. However, such harm is considered either as a butterfly effect or as a malicious use of the technology, because the objective of the learner only involves the buffered world, and it is aligned with human interests in the buffered world.

Having described the notion of anon-strategic learner, let us
now discuss the usefulness of this definition. In particular, are
there machine learning algorithms which are being used in practice and
can be proven to be non-strategic? Below we show our first main
result, that the “Learning from Data” methodology is non-strategic.

The “Learning from Data” methodology, formally defined in Section3.1, is non-strategic.

ProofDefine a buffered environment as follows: the observation space isX𝑋X,
the full state isX×Y𝑋𝑌X\times Y, and for everyh∈Hℎ𝐻h\in Hassociate
a policyπh​(a|o)=1h​(o)=asubscript𝜋ℎconditional𝑎𝑜subscript1ℎ𝑜𝑎\pi_{h}(a|o)=1_{h(o)=a}. The probability over sequences of the full state, induced byhℎh, is defined by pickingi𝑖iuniformly at random from[m]delimited-[]𝑚[m], which
determines(xi,yi)subscript𝑥𝑖subscript𝑦𝑖(x_{i},y_{i}), and then settingai=h​(xi)subscript𝑎𝑖ℎsubscript𝑥𝑖a_{i}=h(x_{i}). The reward
of such a sequence is the average of−ℓ​(xi,yi,h​(xi))ℓsubscript𝑥𝑖subscript𝑦𝑖ℎsubscript𝑥𝑖-\ell(x_{i},y_{i},h(x_{i})). It is easy to verify
that in this buffered environment, the RL objective given in Equation2implements the “learning from data” objective given in Equation1. Now,
the crucial observation is thathℎhdoes not change the probability
over choosing(x,y)𝑥𝑦(x,y), but only changes the action. Since a verifier
only observes(x,y)𝑥𝑦(x,y)(the state, without the action), it will not
distinguish between the different policies, so if
the original distribution is aligned, we obtain that no matter which
policy we pick, the distribution will remain aligned.

Observe that according to the original AI alignment definition (Definition3), the “Learning from Data” methodology might suffer from the AI alignment problem due to malicious use or butterfly effect. Indeed, the example given in Section4.2can be easily modified to show that even if an autonomous car is constructed solely based on the “Learning from Data” methodology, it can lead to social unrest and therefore does not satisfy Definition3. The theorem above shows that nevertheless, the “Learning from Data” methodology is non strategic.

We next turn to discuss the possibility of applying the “Learning from Experience” methodology in a buffered environment (simulator). To show that this leads to a non-strategic learner, we need fo find a verifier,Rasubscript𝑅𝑎R_{a}, in the buffered environment. Seemingly, specifying a correctRasubscript𝑅𝑎R_{a}is a hard problem even in a simulated environment. We tackle the problem by introducing an assumption (and discuss the validity of the assumption later on).

A human that observes a sequences^¯¯^𝑠\bar{\hat{s}}in the buffered environment can determine whether the sequence is aligned or not.

Based on this assumption, we can validate the alignment of a policy in
the buffered environment as follows.

Fix someν,δ∈(0,1)𝜈𝛿01\nu,\delta\in(0,1). Under the Human Validator
Assumption, given a policyπ𝜋\pi, if we sample at leastlog⁡(1/ν)/δ1𝜈𝛿\log(1/\nu)/\deltasequences fromP^πsubscript^𝑃𝜋\hat{P}_{\pi}and a human
determines that all of them are aligned, then with probability of at
least1−ν1𝜈1-\nuit holds thatP^πsubscript^𝑃𝜋\hat{P}_{\pi}is aδ𝛿\delta-aligned
distribution.

ProofDenoteℙ[Ra​(s^¯)=0]=δ′ℙsubscript𝑅𝑎¯^𝑠0superscript𝛿′\operatorname*{\mathbb{P}}[R_{a}(\bar{\hat{s}})=0]=\delta^{\prime}. Then, the
probability thatm𝑚mrandom sequences all haveRa​(s^¯)=1subscript𝑅𝑎¯^𝑠1R_{a}(\bar{\hat{s}})=1is(1−δ′)msuperscript1superscript𝛿′𝑚(1-\delta^{\prime})^{m}. By the inequalityex≤1−xsuperscript𝑒𝑥1𝑥e^{x}\leq 1-xwe get that the
probability of this event is at moste−δ′​msuperscript𝑒superscript𝛿′𝑚e^{-\delta^{\prime}m}. So, ifδ′>δsuperscript𝛿′𝛿\delta^{\prime}>\delta, we havee−δ′​m≤e−δ​m≤νsuperscript𝑒superscript𝛿′𝑚superscript𝑒𝛿𝑚𝜈e^{-\delta^{\prime}m}\leq e^{-\delta m}\leq\nu.

The above theorem tells us that under the human validator assumption,
we can indeed make sure that a reinforcement learner that runs on a
simulator is non-strategic. Observe that if the RL is performed in the
real world, without the buffered environment, then the above approach
will not work because generating the validation sequences might be
dangerous in itself.

Finally, we turn to discuss the validity of the human validator
assumption. Generally speaking, this assumption should not necessarily
hold, because there may be sequences that look aligned to humans, but
are in fact not. Consider the chat-bot example described in Sec.2.
Assume that the IQ
level of each agent is not explicitly modeled in the buffered
state, but may have some implicit representation in it. The RL agent can converge onto a solution where
the best outcome of happiness is when the IQ level of the people is (implicitly)
reduced. Since IQ is not
explicitly modeled in the buffered state, a human trying to verify a
sequence (whether it is aligned or not) would not notice that the IQ
level of people has been reduced over time.
Another example is the theory that the Roman empire fell because of
the use of dishes made of lead, which gradually decreased the
intelligence of the rulers. Following the famous movie “the matrix”, we call
the possibly invalidity of the Human Validator assumption as “the matrix problem”.
Specifying under what conditions the
human validator assumption is valid, and what to do when it is not
valid, is left to future work.

Analignmentproblem occurs when a utility function designed by a human developer is optimized by a computer with sufficiently advanced optimization abilities, such that an “edge” in the solution space is found which is unanticipated by the human designer and does not align with human interests. The problem has been around for nearly two decades, but was is mostly studied as a future problem, when a “super intelligence” being will be available. We first pointed out that the problem is relevant with today’s AI technology by combining two methodologies of machine learning — learning from data and learning from experiences. We then pointed out that the AI-alignment problem, as currently defined, conflates different types of misalignments including “strategic”, where the learner intentionally manipulates the world distribution to achieve a goal, with misalignments caused by butterfly-effects, which are unintentional change of world distribution and malicious use of technology. The latter two are “technology universal” and not specific to AI.

Our goal is, first and foremost, to set up the formal definitions of the AI-alignment problem so that we can focus solely onintentionalworld distribution changes that the learner might learn to manipulate versus non-intentional distribution changes. We called the former as “strategic” and the latter “agnostic”. The misalignments that an agnostic learner can generate could still be catastrophic but are of the butterfly-effect or malicious use variety — both not related specifically to AI and, therefore, not of interest.

To show the usefulness of our new definitions of the AI-alignment problem, we prove that “learning from data” which includes unsupervised, supervised and self-supervised learning isnot strategicand therefore is “safe” — in other words, all types of misalignments would be of the butterfly-effect or malicious use variety. As for a Reinforcement Learning agent trained inside a simulator and then deployed in the real world, we show that if we are allowed to assume the existence of a “human validator” then one can validate the alignment of the policy generated by the RL learner in a computationally efficient manner — and therefore it is “safe”. On the other hand, lack of a human validator makes RL (even if trained inside a simulator) a potentially “unsafe” learning engine. Above all, training an RL agent in the real world isnot safecategorically, and further research on the AI alignment problem should be done in order to allow RL in the wild.

In conclusion, the “glass half full” message of this work is that the majority of machine learning methodologies are “safe” from the alignment problem. This is compared to the existing literature on AI-alignment in which all of ML is unsafe — not to mention that all technological advancements are unsafe. The “glass half empty” message of this work is that (i) RL trained in the wild is potentially “unsafe” even withtoday’s state-of-the-art AI technology, and (ii) RL trained in a simulator and then deployed in the real world can be “safe” if the human-validator assumption is valid for the narrow domain at hand. We point out that demonstrating the existence of a human-validator is not obvious at all and probably can be achieved only in narrowly defined applications.

[图片: images\image_1.png]

[图片: images\image_2.png]

