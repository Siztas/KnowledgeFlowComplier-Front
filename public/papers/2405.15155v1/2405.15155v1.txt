æ ‡é¢˜ï¼šCLIP model is an Efficient Online Lifelong Learner

Online Lifelong Learning (OLL) addresses the challenge of learning from continuous and non-stationary data streams. Existing online lifelong learning methods based on image classification models often require preset conditions such as the total number of classes or maximum memory capacity, which hinders the realization of real never-ending learning and renders them impractical for real-world scenarios. In this work, we propose that vision-language models, such as Contrastive Language-Image Pretraining (CLIP), are more suitable candidates for online lifelong learning. We discover that maintaining symmetry between image and text is crucial during Parameter-Efficient Tuning (PET) for CLIP model in online lifelong learning. To this end, we introduce the Symmetric Image-Text (SIT) tuning strategy. We conduct extensive experiments on multiple lifelong learning benchmark datasets and elucidate the effectiveness of SIT through gradient analysis. Additionally, we assess the impact of lifelong learning on generalizability of CLIP and found that tuning the image encoder is beneficial for lifelong learning, while tuning the text encoder aids in zero-shot learning.

The deep neural networks in practical applications typically follows a supervised training paradigm on pre-collected datasets. This paradigm has proven remarkably effective in closed or constrained environments where the data distribution remains relatively stable. However, in other scenarios, models are expected to learn from data streams and its distribution evolves over time, as depicted in Figure1a, so the traditional training paradigm faces significant challenges. Furthermore, due to constraints such as storage or privacy, it is impractical to retain all data. The direct training of models in such an incremental stream of data can lead to drastic performance degradation. This phenomenon is known as catastrophic forgetting[1,2].

To mitigate the challenges posed by non-stationary data distributions, a multitude of continual learning methods have been proposed, aiming to strike a balance between model stability and plasticity. Early continual learning approaches involve segmenting continuous data streams into distinct tasks and employing task identifiers to select task-specific components for classification, which is known as task-incremental learning (TIL)[3]. Subsequent developments led to class-incremental learning (CIL)[4,5,6,7], which addresses the scenario where task identifiers are unavailable, necessitating inference based on class information alone. Futhermore, task-agnostic continual learning (TACL)[8,9]operates without explicit task boundaries, enabling models to be trained online with anytime inference. Despite these advancements, traditional image classification models are often designed for closed-set scenarios and encounter various limitations in lifelong learning. These models typically require continuous modifications to their architecture to accommodate new data[6], face mismatches between image features and prototypes[10], or necessitate the predefinition of a maximum class count[11]. Such constraints impede their ability to adapt to the ever-evolving nature of real-world data, where new classes and examples are continuously introduced without prior knowledge of their existence. The inability to dynamically evolve with new information challenges the notion that these models are capable of never-ending learning or lifelong learning, thereby restricting their applicability in practical scenarios where the learning process must be sustained and uninterrupted. Consequently, there is a pressing need for models that can transcend these limitations and provide a more flexible, scalable, and realistic approach to learning in open and dynamic environments.

In this paper, we explore the potential of the vision-language models such as Contrastive Language-Image Pretraining (CLIP) model in the scenario of Online Lifelong Learning (OLL), offering a unique approach to classification that transcends traditional model architecture and class count limitations. Unlike conventional classifiers, the CLIP model achieves categorization by matching images to textual descriptions in the form of "A bad photo of CLASS". This mechanism allows a flexible learning process that is not confined by a predefined model structure or the total number of classes, thereby facilitating realistic OLL. The pre-trained CLIP model, with its generalizable representations and robust zero-shot performance, presents a significant advantage for OLL scenarios.

To enhance the performance of CLIP in OLL scenarios, we adopt Parameter-Efficient Tuning (PET), an approach that optimizes the model without a significant increase in parameters. Our experiments reveal that the asymmetry between image and text during tuning, where text from all seen classes is matched with images from the current time step, can lead to severe catastrophic forgetting. To address this issue, we introduce the Symmetric Image-Text (SIT) tuning strategy, a straightforward yet potent method that ensures a balanced update of the knowledge of CLIP model. We conduct comprehensive experiments on various lifelong learning benchmark datasets and elucidate the effectiveness of SIT through gradient analysis, providing insights into how symmetric tuning mitigates the loss of previously acquired knowledge. Additionally, we assess the impact of lifelong learning on generalizability of CLIP model and discover that tuning the image encoder is conducive to incremental learning in OLL, while tuning the text encoder enhances zero-shot learning capabilities. These findings underscore the importance of SIT in maintaining the balance between adapting to new information and retaining existing knowledge, thereby optimizing the performance of CLIP model in the complex and ever-changing landscape of OLL.

Lifelong Learning.To alleviate catastrophic forgetting, various lifelong learning methods have been proposed. Existing lifelong methods can be categorized into three types[12]: model-based, data-based[4,8], and algorithm-based[5,13,14,15]methods. Model-based methods dynamically expand the network for different tasks.Douillard etÂ al. [6]assigns specialized decoder networks and classifiers to each task.Yan etÂ al. [16]adds a new learnable module composed of a feature extractor and a linear classifier to accommodate new classes at each step. Recently, pioneering works[17,7]utilize pre-trained models and introduce prompt-tuning to balance the stability and plasticity of the model, demonstrating the superiority of pre-trained models in CL. However, most CL research requires a predefined maximum number of classes to determine the output dimension of the classifier and mainly focuses on the offline scenario, which has clear task boundaries, unlike in the real world. Therefore, online learning with blurry task boundaries[8,9]is proposed, where clear task boundaries are often absent and learned samples are unavailable. Though later[18]proposes a more realistic setting to further tackle the problem of class emerging, recurring and disappearing in the real world, each of its tasks has the same number of new classes and the same ratio between new and blurry classes. To address this,Moon etÂ al. [19]randomly assigns each blurry class and disjoint class to each blurry task and disjoint task by the disjoint class ratio and every training task is composed of blurry task and disjoint task with a stochastic blurry task boundary, better aligning with the needs of real-world scenarios.

Parameter-Efficient Tuning in CLIP.Trained with abundantly available data from the web, the vision-language model CLIP[20]has demonstrate great advantage in a wide variety of tasks including few-shot and zero-shot visual recognition. However, how to efficiently adapt them to downstream tasks still remains a challenge. To solve this problem, several parameter-efficient tuning methods have been proposed, roughly categorized into LoRA[21], Adapter[22]prompt-tuning[23]and prefix-tuning[24]. Inspired by prompt learning in NLP, many works tune CLIP through the learnable prompt.Zhou etÂ al. [25]applies prompt learning-based approach to CLIP for the first time and shows exceptional performance in downstream transfer learning.Zhou etÂ al. [26]extendsZhou etÂ al. [25]by learning a lightweight network that generates an input-conditional token for each image to further solve the generalization issue.[27]improve the alignment between two modalities by projecting textual prompt into visual prompt and embedding them into corresponding encoders. Adapter approaches insert small modules, called adapters, in the pre-trained model.Gao etÂ al. [28]introduces CLIP-Adapter, adding a fully connected layer adapter and then merging the additional knowledge with zero-shot prediction based on residual connections.Jiang etÂ al. [29]andZhang etÂ al. [30]share adapter weights between visual and textual modalities to enhance the cross-modal interaction. LoRA approaches, on the other hand, inject trainable low-rank matrices into the pre-trained model to realize the tuning of the model.dos Santos etÂ al. [31]employs LoRA in CLIP model for language adaptation for the first time, considerably reducing the number of trainable parameters.

[å›¾ç‰‡: images\image_1.png]
å›¾ç‰‡è¯´æ˜: (a)US Extreme Weather Search Interest.

[å›¾ç‰‡: images\image_2.png]
å›¾ç‰‡è¯´æ˜: (a)US Extreme Weather Search Interest.

[å›¾ç‰‡: images\image_3.png]
å›¾ç‰‡è¯´æ˜: (a)US Extreme Weather Search Interest.

[å›¾ç‰‡: images\image_4.png]
å›¾ç‰‡è¯´æ˜: (a)US Extreme Weather Search Interest.

Lifelong learning aims to train a unified modelâ„±Î¸:ğ’³â†’ğ’´:subscriptâ„±ğœƒâ†’ğ’³ğ’´\mathcal{F}_{\theta}:\mathcal{X}\rightarrow\mathcal{Y}parameterized byÎ¸ğœƒ\thetathat makes good predictions for all seen classes.
In classic class-incremental learning setting, given a sequence of tasksğ’¯={ğ’¯1,ğ’¯2,â€¦,ğ’¯T}ğ’¯subscriptğ’¯1subscriptğ’¯2â€¦subscriptğ’¯ğ‘‡\mathcal{T}=\{\mathcal{T}_{1},\mathcal{T}_{2},...,\mathcal{T}_{T}\}, the training set of thettâ€‹hsuperscriptğ‘¡ğ‘¡â„t^{th}taskğ’¯tsubscriptğ’¯ğ‘¡\mathcal{T}_{t}isğ’Ÿt=(ğ’™it,yit)i=1Nsubscriptğ’Ÿğ‘¡subscriptsuperscriptsubscriptsuperscriptğ’™ğ‘¡ğ‘–superscriptsubscriptğ‘¦ğ‘–ğ‘¡ğ‘ğ‘–1\mathcal{D}_{t}={(\bm{x}^{t}_{i},y_{i}^{t})}^{N}_{i=1}, whereğ’™itâˆˆğ’³subscriptsuperscriptğ’™ğ‘¡ğ‘–ğ’³\bm{x}^{t}_{i}\in\mathcal{X}andyitâˆˆğ’´superscriptsubscriptğ‘¦ğ‘–ğ‘¡ğ’´y_{i}^{t}\in\mathcal{Y}denote the input sample and its corresponding label respectively. We define the output space for all observed class labelsğ’´(t)âŠ‚ğ’´(t+1)superscriptğ’´ğ‘¡superscriptğ’´ğ‘¡1\mathcal{Y}^{(t)}\subset\mathcal{Y}^{(t+1)}. The labelğ’´(t)superscriptğ’´ğ‘¡\mathcal{Y}^{(t)}between any two tasks are disjoint, i.e.ğ’´(t)âˆ©ğ’´(tâ€²)=âˆ…superscriptğ’´ğ‘¡superscriptğ’´superscriptğ‘¡â€²\mathcal{Y}^{(t)}\cap\mathcal{Y}^{(t^{{}^{\prime}})}=\emptyset.

In online lifelong learning, the training data is partitioned into batchesâ„¬={â„¬1,â„¬2,â€¦}â„¬subscriptâ„¬1subscriptâ„¬2â€¦\mathcal{B}=\{\mathcal{B}_{1},\mathcal{B}_{2},...\}, the training set of thettâ€‹hsuperscriptğ‘¡ğ‘¡â„t^{th}batchâ„¬tsubscriptâ„¬ğ‘¡\mathcal{B}_{t}isğ’Ÿt=(ğ’™it,yit)i=1Nsubscriptğ’Ÿğ‘¡subscriptsuperscriptsubscriptsuperscriptğ’™ğ‘¡ğ‘–superscriptsubscriptğ‘¦ğ‘–ğ‘¡ğ‘ğ‘–1\mathcal{D}_{t}={(\bm{x}^{t}_{i},y_{i}^{t})}^{N}_{i=1}. Each batchâ„¬isubscriptâ„¬ğ‘–\mathcal{B}_{i}is allowed to be seen only once. Here we still use taskğ’¯ğ’¯\mathcal{T}to represent abrupt changes in data distribution, but the learner is unaware of the task alteration during training. For Si-Blurry setting[19], we split all classes into two groups, whereNğ‘N% of the classes are selected as disjoint classes and the remaining 100-Nğ‘N% of the classes are selected as blurryMğ‘€Mclasses, whereMğ‘€Mis the blurry level[18]. We randomly assign a certain number of disjoint and blurry classes to each task, as shown in Figure1d, the blurry classes of each task may overlap, thus blurring the clear task boundaries. In particular, whenN=0ğ‘0N=0, all classes are disjoint classes, representing the classical CIL setting, as illustrated in Figure1b. Conversely, when the ratio of disjoint classes to blurry classes remains constant in each task, it corresponds to the i-Blurry setting[18], as depicted in Figure1.

The CLIP model represents a significant advancement in multimodal machine learning. The CLIP model is designed to map images and text into a shared feature space, with the pre-training objective of maximizing the similarity between feature vectors of image-text pairs. This approach contrasts with traditional methods that focus on minimizing the similarity between non-matching pairs.
Consider aKğ¾K-class image classification problem, CLIP maps an unidentified imageğ’™âˆˆğ’³ğ’™ğ’³\bm{x}\in\mathcal{X}to its corresponding feature vector through the image encoderğ’—=ğ„visualâ€‹(ğ±)ğ’—subscriptğ„visualğ±\bm{v}=\mathbf{E}_{\texttt{visual}}(\mathbf{x}). Class labelyâˆˆğ’´ğ‘¦ğ’´y\in\mathcal{Y}is prepended by a hand-crafted prompt templateğ©â†’â†’ğ©absent\mathbf{p}\rightarrowa bad photo of a{class} to form a class-specific text inputğ²={ğ©;y}ğ²ğ©ğ‘¦\mathbf{y}=\{\mathbf{p};y\}, which is then encoded into a text feature vectorğ’•ğ’•\bm{t}by the text encoderğ’•=ğ„textâ€‹(ğ²)ğ’•subscriptğ„textğ²\bm{t}=\mathbf{E}_{\texttt{text}}(\mathbf{y}). The prediction probability can be denoted by

wheresimâ€‹(â‹…)simâ‹…\text{sim}(\cdot)denotes the cosine similarity.

Traditional image classification models are constrained by a predefined set of classes, which necessitates model retraining or adjustments when novel classes are introduced in lifelong learning. In contrast, the design of CLIP model overcomes these limitations by employing a text-based approach. This approach allows CLIP to dynamically adapt to new classes without altering the model architecture. The incorporation of textual descriptions as classifiers provides a flexible and scalable solution for lifelong learning. This capability is particularly advantageous in environments where the set of classes is continuously expanding.

The vast dataset provides CLIP with a broad understanding of visual and textual patterns, enabling it to generalize well across a wide range of classes. However, when facing the case where the data is not covered by the pre-trained dataset, the performance can falter. Therefore, it is imperative to develop PET strategies for the CLIP model that enhance its performance on specialized tasks while maintaining its robust zero-shot learning abilities, ensuring that the model remains versatile and adaptable in a lifelong learning environment. After adding adapter or prefix to the CLIP model, it is typical to tune with the InfoNCE loss

whereğ’—ğ’Šsubscriptğ’—ğ’Š\bm{v_{i}}andğ’•+superscriptğ’•\bm{t^{+}}are the positive sample pairs,Vbsubscriptğ‘‰ğ‘V_{b}is the image feature vectors of the current batch, andTğ‘‡Tis the text feature vectors of all seen classes.
Unlike the pre-training phase of CLIP model, online lifelong learning scenarios limit the access to the current batch of images only, with text features drawn from all previously seen classes. Our experiments in4.3.1have revealed that during tuning, the asymmetry between image and text causes the gradient of the negative samples to be smaller than the gradient of the positive samples, causing the model to tend to predict the old class as the new class, resulting in a significant performance degradation. To counteract this issue, we propose a straightforward yet effective method, called Symmetric Image-Text (SIT) tuning strategy, that restores the symmetry between image and text features during training. Specifically, we reformulate the loss function

whereTbsubscriptğ‘‡ğ‘T_{b}is the text feature vectors of classes in the current batch.
By doing so, we effectively mitigate catastrophic forgetting, allowing the model to maintain its zero-shot learning capabilities while adapting to new classes in an online lifelong learning context.

Datasets.We conducted comprehensive experiments on several benchmark datasets to evaluate the performance of our strategy. For OLL and CIL, we selected datasets such as CIFAR-100[33], Tiny-ImageNet[34], and ImageNet-R[35], which consist of 100, 200, and 200 classes. In the OOL scenario, we employ the Si-blurry setting, and set a disjoint class ratioN=50ğ‘50N=50, a blurry levelM=10ğ‘€10M=10and a task numberT=5ğ‘‡5T=5. To assess the impact of OLL on the generalization of CLIP model, we tested on five distinct datasets. These include the generic-objects dataset Caltech101[36], fine-grained datasets such as Flowers102[37], OxfordPets[38], Food101[39], and a satellite image dataset EuroSAT[40].

Implementation Details.We conducted experiments on a pre-trained ViT-B/16 CLIP model, wheredl=512subscriptğ‘‘ğ‘™512d_{l}=512,dv=768subscriptğ‘‘ğ‘£768d_{v}=768anddvâ€‹l=512subscriptğ‘‘ğ‘£ğ‘™512d_{vl}=512. The prompt template utilizes "a bad photo of a{class}". For the implementation of LoRA[21]and Adapters[22], we referred toHe etÂ al. [41]. The Adapters featured a down-projection dimension of 64, while the rank of LoRA is set to 4. Unless specified otherwise, both Adapters and LoRA are integrated into each transformer layer of the Text and Image encoders. We employ the Adam optimizer for tuning the CLIP model with either Adapters or LoRA, using a learning rate of5â€‹eâˆ’45ğ‘’45e-4and training for 3 iterations per batch. For the implementation of MaPLe[27], we set the prompt depthJğ½Jto 9 and standardized the lengths of both the language and vision prompts to 2. MaPLe is optimized using the SGD optimizer with a learning rate of1â€‹eâˆ’41ğ‘’41e-4, also over 3 iterations per batch.

Evaluation Metrics.We record the top-1 accuracyğ’œtsubscriptğ’œğ‘¡\mathcal{A}_{t}of the model on the test set after finishing training at incremental steptğ‘¡tand present it as a curve, where the test set contains all classes the model has ever seen. We denote the accuracy at the end of the last taskğ’œlastsubscriptğ’œlast\mathcal{A}_{\text{last}}as a metric for overall accuracy and use the average of the test accuracy across all incremental stepsğ’œavg=1Tâ€‹âˆ‘t=0Tâˆ’1ğ’œtsubscriptğ’œavg1ğ‘‡superscriptsubscriptğ‘¡0ğ‘‡1subscriptğ’œğ‘¡\mathcal{A}_{\text{avg}}=\frac{1}{T}\sum_{t=0}^{T-1}\mathcal{A}_{t}to assess the performance of all tasks. Furthermore, to evaluate the online learning ability of the model, we also useğ’œaucsubscriptğ’œauc\mathcal{A}_{\text{auc}}[18]to measure the performance of anytime inference, which assumes that inference queries can be made anytime during training.ğ’œauc=âˆ‘i=1kfAâ€‹(iâ‹…Î´â€‹n)â‹…Î´â€‹nsubscriptğ’œaucsubscriptsuperscriptğ‘˜ğ‘–1â‹…subscriptğ‘“ğ´â‹…ğ‘–ğ›¿ğ‘›ğ›¿ğ‘›\mathcal{A}_{\text{auc}}=\sum^{k}_{i=1}{f_{A}(i\cdot\delta n)}\cdot\delta n, whereÎ´â€‹nğ›¿ğ‘›\delta nis the number of seen samples during the evaluation andfAâ€‹(â‹…)subscriptğ‘“ğ´â‹…f_{A}(\cdot)is the accuracy curve.

In this experiments, we utilized the Si-Blurry setting to evaluate our proposed method on CIFAR-100, TinyImageNet, and ImageNet-R datasets. Specifically, we set the disjoint class ratioN=50ğ‘50N=50, blurry levelM=10ğ‘€10M=10and task numberT=5ğ‘‡5T=5. Our method is compared with several state-of-the-art OLL approaches, including replay-based methods ER[42], RM[8], and CLIB[18], regularization-based method LwF[13], a combination of replay-regularization in EWC++[5], and prompt-based methods L2P[17], DualPrompt[7], MVP and MVP-R[19]. Additionally, we considered Continual-CLIP, which leverages the zero-shot learning capability of CLIP model, and set finetuning and linear probing as our lower-bound benchmarks. All methods employ a pre-trained Vision Transformer (ViT-B/16) as the backbone model. For replay-based methods ER, RM, CLIB, and MVP-R, the buffer size is consistently set to 2000 to ensure a fair comparison. The results, as depicted in Table1, reveal that CLIB and MVP, designed specifically for boundary-blurred scenarios, perform well on general datasets like CIFAR-100 and TinyImageNet. However, even with replay data, these classical classification models only marginally outperform Continual-CLIP and significantly underperform on the domain-shift designed ImageNet-R. Furthermore, we found that applying the SIT strategy to tune CLIP model via PET can substantially enhance the performance of CLIP model without the need for replay, knowledge distillation, or other auxiliary techniques. Our findings underscore the effectiveness of our proposed method, which not only matches but also surpasses the performance of existing methods, highlighting its potential as a robust solution for lifelong learning in blurred and dynamic environments.

We evaluated the performance of our proposed SIT against several state-of-the-art CILmethods, including EWC[5], LwF[13], iCaRL[4], EEIL[46], UCIR[14], BiC[44], WA[10], PODNet[45], MUC[47], DER[16], PASS[15], and DyTox[6]on CIFAR-100 and TinyImagenet. These CIL methods adhere to the traditional setup where the model has access to the entire dataset of the current task. For Continual-CLIP[43]that do not require tuning, we recorded the test results at the end of each task. In contrast, our SIT strategy follows an online CIL setting, where the model is restricted to access the data in the current batch only, simulating a more challenging and realistic incremental learning scenario. The experimental results for the compared CIL methods are sourced fromThengane etÂ al. [43], providing a reliable benchmark for our evaluation. As illustrated in Table2, it is evident that our SIT-LoRA method outperforms all other methods in both task settings. Specifically, in the more challenging 20 tasks setting, SIT-LoRA is impressive with anğ’œavgsubscriptğ’œavg\mathcal{A}_{\text{avg}}of 86.88 and anğ’œlastsubscriptğ’œlast\mathcal{A}_{\text{last}}of 80.38. Table3shows that the proposed SIT methods outperform the baseline and other state-of-the-art methods in this comparison. Specifically, SIT-Adapter shows the highestğ’œavgsubscriptğ’œavg\mathcal{A}_{\text{avg}}across all task increments, with 81.93 for 5 tasks, 81.31 for 10 tasks, and 80.50 for 20 tasks. It also achieves the highestğ’œlastsubscriptğ’œlast\mathcal{A}_{\text{last}}for the 20 tasks setting with 72.62. The results demonstrate the efficacy of our SIT strategy, showcasing its potential to compete with or even surpass established CIL methods while adhering to a more stringent online learning constraint.

As discussed in Section3.3, the asymmetry between image and text can lead to a degradation in the performance of OLL, a phenomenon we investigated through comparative experiments on CIFAR-100, TinyImageNet, and ImageNet-R, with results summarized in Table4. In our experiments, the Asymmetric Image-Text training strategy (AIT) involved matching all previously seen classes with images at each time step, which, when compared to its baseline, led to a significant reduction inğ’œlastsubscriptğ’œlast\mathcal{A}_{\text{last}}while theğ’œaucsubscriptğ’œauc\mathcal{A}_{\text{auc}}is slightly lower than the baseline. Notably, as depicted in Figures2aand2d, AIT resulted in the predictions bias towards new classes, indicative of catastrophic forgetting when contrasted with the SIT. To explore the reasons behind this phenomenon, we conducted an analysis of the gradients during training. Here, we considered the text encoder and text features collectively as a classifier and tallied the gradients of positive and negative samples. For Figures2band2e, we designated negative samples from the classes present at the current batch as symmetric negative samples, while those from classes not present are termed asymmetric negative samples. It is observed that for SIT, the gradients for both positive and negative samples are largely maintained at the same order of magnitude. In contrast, for AIT, the total gradient for negative samples across the OLL process is greater than that for positive samples for blurry classes, whereas for disjoint classes, the gradient for negative samples is less than that for positive samples. Moreover, the gradient for negative samples corresponding to new classes progressively diminishes. Figures2cand2fillustrate the temporal evolution of the gradients. For AIT, the gradients on negative samples tend to become uniform, which hinder the CLIP model to effectively distinguish between new and old classes. The gradient analysis aligns with the observations in Figure2a, where the model is inclined to predict classes as new. Overall, compared to AIT, SIT demonstrates the ability to selectively update knowledge, effectively mitigating catastrophic forgetting and preserving the generalization of CLIP model.

[å›¾ç‰‡: images\image_5.png]
å›¾ç‰‡è¯´æ˜: (a)Confusion matrix (AIT).

[å›¾ç‰‡: images\image_6.png]
å›¾ç‰‡è¯´æ˜: (a)Confusion matrix (AIT).

[å›¾ç‰‡: images\image_7.png]
å›¾ç‰‡è¯´æ˜: (a)Confusion matrix (AIT).

[å›¾ç‰‡: images\image_8.png]
å›¾ç‰‡è¯´æ˜: (a)Confusion matrix (AIT).

[å›¾ç‰‡: images\image_9.png]
å›¾ç‰‡è¯´æ˜: (a)Confusion matrix (AIT).

[å›¾ç‰‡: images\image_10.png]
å›¾ç‰‡è¯´æ˜: (a)Confusion matrix (AIT).

In this experiments, we conducted a comparative analysis of various PET methods to evaluate their impact on the performance and generalizability in OLL. As evident from Tables1and5, MaPLe, which is based on prompt tuning, shows minimal improvement over the baseline in OLL but also has the least detrimental effect on generalization compared to other methods. Both LoRA and Adapter demonstrate significant performance in OLL. LoRA has a relatively smaller impact on generalization, which has fewer parameters. Furthermore, we compared the effects of only tuning the text encoder versus the image encoder. Table5indicates that tuning the image encoder outperform only tuning the text encoder in OLL, implying that the model needs to adapt to different image distributions throughout lifelong learning. In contrast, for zero-shot learning, only tuning the text encoder is more effective than the image encoder, suggesting that zero-shot learning benefits more from the rich semantic information contained in language. These findings underscore the importance of selecting the appropriate tuning strategy based on the learning paradigm. In OLL, where the model encounters a continuous stream of new data, it is crucial to focus on the image encoder to adapt to varying image distributions. Conversely, in zero-shot learning, leveraging the capacity of text encoder to extract semantic information is more beneficial.

The objective of this experiment is to determine how variations in batch size affects the efficiency and effectiveness of the learning process in an OLL scenario. We conducted a series of experiments with batch sizes ranging from 8 to 128 and use the SIT-LoRA within the Si-Blurry setting on the tinyImagenet dataset. The results, as indicated in Table6, demonstrate that the overall impact of batch size on OLL performance is relatively minor, with theğ’œaucsubscriptğ’œauc\mathcal{A}_{\text{auc}}differing by only 6.39% between the smallest and largest batch sizes tested. Notably, the final performance in OLL converges when the batch size exceeds 16, suggesting that beyond this point, increasing the batch size does not yield significant improvement in performance. Interestingly, for zero-shot testing, an increase in batch size during OLL is found to mitigate the reduction in the generalizability of CLIP model, indicating that larger batches may contribute to better retention of knowledge about novel classes. These findings suggest that while batch size does play a role in OLL, the optimal size for maintaining a balance between performance and generalizability may not need to be exceedingly large.

Online lifelong learning entails the capability of models to learn from data streams without preset constraints such as the total number of classes or maximum memory capacity, and to apply and evaluate model performance at any given moment. In this paper, we proposed that vision-language models such as CLIP are more suitable online lifelong learners compared to traditional image classification models. Through gradient analysis, we discovered that asymmetry between text and image during PET in OLL can lead to catastrophic forgetting. To address this, we introduced a simple yet effective strategy known as the Symmetric Image-Text tuning strategy, which matches images and class labels within the current batch only during online learning. We conducted extensive experiments in both OLL and CIL scenarios and evaluated the impact of lifelong learning on generalizability of CLIP. Our future work will explore Mixture of Experts (MoE) to accumulate knowledge gained throughout lifelong learning while avoiding catastrophic forgetting of vision-language models.

[å›¾ç‰‡: images\image_11.png]

[å›¾ç‰‡: images\image_12.png]

