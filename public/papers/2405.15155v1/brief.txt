### CLIP模型：一种高效的在线终身学习者

在线终身学习（OLL）面临着从持续且非平稳的数据流中学习的挑战。现有的基于图像分类模型的在线终身学习方法通常需要预设条件，如类别总数或最大内存容量，这限制了其实现真正的永续学习，难以应用于真实世界的场景。本文提出，视觉-语言模型（如对比语言-图像预训练模型CLIP）更适合在线终身学习任务。我们发现，在CLIP模型的参数高效调优（PET）过程中，保持图像和文本之间的对称性至关重要。为此，我们提出了对称图像-文本（SIT）调优策略。通过在多个终身学习基准数据集上的实验，我们通过梯度分析阐明了SIT策略的有效性。同时，我们评估了终身学习对CLIP模型泛化能力的影响，发现调优图像编码器有助于终身学习，而调优文本编码器则促进了零-shot学习。

在实际应用中，深度神经网络通常采用基于监督学习的训练范式，这在数据分布相对稳定的封闭环境中非常有效。然而，在数据流不断变化的情况下，传统的训练方式面临着严重的挑战。更糟糕的是，由于存储和隐私等限制，无法保存所有数据。直接对这种增量数据流进行训练可能导致性能显著下降，这种现象被称为灾难性遗忘（catastrophic forgetting）。为了应对不断变化的数据分布，提出了多种持续学习方法，旨在平衡模型的稳定性和可塑性。早期的持续学习方法通过将连续的数据流分割为不同的任务，使用任务标识符选择任务特定的组件进行分类，这种方法被称为任务增量学习（TIL）。随后的发展则提出了类增量学习（CIL），它解决了没有任务标识符的场景，只能根据类信息进行推断。进一步的发展是任务无关的持续学习（TACL），它在没有明确任务边界的情况下进行在线训练，允许随时进行推断。尽管这些方法取得了进展，传统的图像分类模型仍然设计为闭集场景，存在许多限制，特别是在终身学习中。它们通常需要不断修改架构以适应新数据，面临图像特征与原型不匹配的问题，或者需要预定义最大类别数量等限制。这些约束阻碍了它们适应真实世界中不断变化的环境，因为新类和新样本不断被引入，而事先无法知道这些类别的存在。因此，现有方法在真正的终身学习或永续学习上存在局限，不能在实际场景中长时间不间断地进行学习。

为了解决这一问题，本文探索了CLIP模型在在线终身学习中的潜力，提出了一种超越传统模型架构和类别数量限制的分类方法。与传统分类器不同，CLIP通过将图像与文本描述进行匹配来进行分类，文本格式为“A bad photo of CLASS”。这种机制使得学习过程更加灵活，不受预定义模型结构或类别总数的限制，因此更适合实现真实的终身学习。CLIP的预训练模型具有很强的泛化能力和零-shot学习能力，使其在终身学习场景中具有显著优势。

为了提升CLIP在终身学习中的表现，本文采用了参数高效调优（PET）方法，这种方法在优化模型的同时不会显著增加参数量。我们的实验表明，在调优过程中，图像和文本之间的非对称性，尤其是文本来自所有已见类别与图像来自当前时间步的数据匹配，可能导致灾难性遗忘。为此，我们提出了一种简单而有效的对称图像-文本（SIT）调优策略，确保CLIP模型的知识更新保持平衡。通过对多个终身学习基准数据集的综合实验和梯度分析，验证了SIT的有效性，表明对称调优可以有效减缓已经学习的知识丢失。此外，我们还评估了终身学习对CLIP模型泛化能力的影响，发现调优图像编码器有助于增量学习，而调优文本编码器有助于提升零-shot学习能力。这些发现强调了SIT策略在维持适应新信息和保留现有知识之间平衡的重要性，从而优化了CLIP模型在复杂和不断变化的终身学习环境中的表现。

通过这些研究，本文展示了CLIP模型在真实世界的终身学习中的潜力，提出了更灵活、可扩展且更适应动态环境的学习方法。CLIP的设计使得它在数据流持续变化的环境中能够不断适应新类别的加入，克服了传统方法面临的诸多局限，具有广泛的应用前景。
