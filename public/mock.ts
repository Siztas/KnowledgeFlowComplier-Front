// 模拟文章数据
export const mockArticles: ExtendedArticle[] = [
  {
    id: "1",
    title: "基于次线性遗憾的安全终身强化学习方法",
    content: "###基于次线性遗憾的安全终身强化学习方法\n\n该论文提出了一种新颖的终身强化学习（RL）方法，通过引入具有次线性遗憾的安全策略搜索来解决现有技术的局限性。当前终身学习方法通常存在遗憾不收敛的问题，并可能产生不安全的控制策略，这限制了其在实际场景中的应用。\n\n针对这些问题，作者开发了一种在对抗性环境中运行的终身策略梯度学习器，该方法在强制执行策略安全约束的同时，实现了遗憾量级为\\(\\mathcal{O}(\\sqrt{R})\\)（R为总回合数）的次线性遗憾。该框架将终身学习形式化为在线多任务学习问题，其中每个任务的策略参数表示为共享潜在基底的线性组合。这种设计不仅实现了跨任务的高效知识迁移，还能确保策略始终保持在约束条件定义的安全区域内。\n\n通过在包括四旋翼飞行器控制应用在内的多个基准动力系统上进行验证，该方法展现出优于标准策略梯度方法和现有终身学习技术的性能。理论分析证明了次线性遗憾界的成立，实证结果则表明该算法能够利用先前获得的知识快速学习出高性能的安全控制策略。这项研究特别针对机器人控制领域的关键限制——不安全策略可能导致物理损坏或系统故障的问题，使其特别适合部署在高维度、安全至上的实际应用场景。",
    summary: "该论文提出了一种新颖的终身强化学习（RL）方法，通过引入具有次线性遗憾的安全策略搜索来解决现有技术的局限性。当前终身学习方法通常存在遗憾不收敛的问题，并可能产生不安全的控制策略，这限制了其在实际场景中的应用。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/1505.05798v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "2",
    title: "未命名论文",
    content: "\n###Seq2Seq2Sentiment：多模态情感分析中的无监督表示学习\n\n本文《Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment Analysis》旨在解决多模态情感分析中的联合表示学习问题，作者提出了两种基于序列到序列（Seq2Seq）模型的无监督学习方法：标准的模态翻译模型和层次化模态翻译模型。情感分析传统上多依赖文本单模态，使用词汇和短语的极性特征进行建模，但这一方式难以捕捉情感的复杂性。为此，近年来研究者尝试结合音频与视频模态，从中提取更丰富的情感信息。在此背景下，作者指出现有多模态学习大多为监督方法，依赖大量人工标注数据，忽视了网络中丰富的无标签数据资源。因此本文聚焦于无监督表示学习，尝试利用多模态Seq2Seq架构，通过模态翻译的方式将一种模态映射为另一种模态，同时捕捉各模态间的顺序结构与相关性，以构建有效的共享表示。相较于传统拼接式融合方法，本文方法采用完整序列的编码表示，即通过函数 \\$\\widetilde{X\\_i} = \\text{Seq2Seq\\_Encoder}(X\\_i)\\$ 生成跨模态嵌入，并在CMU-MOSI数据集上验证了其优越性，尤其在双模态设置中F1得分提升显著。此外，为对齐多模态输入，作者基于文本时间步将各模态统一处理，并在实验设计中将原始视频划分为若干短片段，实现多模态信息的细粒度学习。与以往MV-LSTM、Tensor Fusion Network等监督方法不同，本文不仅实现了无监督训练，还利用了注意力机制强化对变长序列中长期依赖关系的建模，进一步提升了模型对情感的表达能力。这项工作在理论上拓展了条件生成模型的研究框架，在实践上为多模态情感识别提供了更具泛化性的解决方案。",
    summary: "###Seq2Seq2Sentiment：多模态情感分析中的无监督表示学习",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/1807.03915v2/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "3",
    title: "奖励的语言化表征与终身智能体的构建路径",
    content: "###奖励的语言化表征与终身智能体的构建路径\n\n本文探讨了“终身强化学习系统（Lifelong Reinforcement Learning System）”的核心思想与传统强化学习范式之间的差异，并提出了对建构终身学习智能体的一些启发。作者指出，传统的强化学习框架主要关注于跨代的学习过程：即策略是在不同“代”之间逐步演化优化的，而非由单个智能体在其有限生命周期内完成。具体而言，在传统的强化学习系统中，每个智能体在出生时获得一个策略，然后在生命周期内按照该策略行动，并记录其经历与累积奖励，最后这一信息被用于优化下一代智能体的策略。换言之，学习行为是由外部学习算法主导的，而非嵌入于智能体之中。\n\n作者进一步指出，这种基于累积奖励最大化的传统方法，其实是一种源自“奖励假设（reward hypothesis）”的设计选择。即，目标被视作一个累积标量奖励期望的最大化问题。然而，这种形式对于终身强化学习是不充分的，因为它不能很好地刻画智能体在其生命周期中逐步学习和适应环境的过程。特别地，个体在生命终点才能知晓其累积奖励，导致学习过程实际上依赖于代际更替而非个体自身的在线学习。\n\n通过分析 Q-learning 算法，作者揭示了一个重要现象：尽管 Q-learning 允许每一步更新估计值，但若观察包含时间戳，则没有一个观察会被重复，因而实际上 Q-learning 的学习效果仍然类似于跨代优化。只有当初始 Q 值具备高度结构化或来自前代丰富经验时，当前智能体才能展现出有意义的行为。换句话说，除了奖励信号之外，Q 值本身应被视作一种经验信号或知识遗产，它表达了前代对“目标”含义的理解，而非仅仅是一个用于最大化的奖励指标。\n\n因此，作者提出应当将奖励重新定义为一种“语言（reward language）”：不再仅是标量值，而是编码了关于目标、子目标、策略启发信息等在内的知识表达方式。相应地，学习算法则成为对这种语言的响应机制。该系统内部的学习行为不再依赖外部的策略更新器，而是内嵌在智能体自身的学习机制中，使其可以在生命周期内不断更新其策略。由此，终身学习系统的核心转向为一个映射函数：从奖励历史（即奖励语言字符串）映射到策略集合，即 $m_t: H^r_t \\rightarrow \\Pi_t$。\n\n在形式化上，作者进一步使用自动机理论的术语构建了奖励语言的模型。其中，奖励空间 $\\Sigma$ 被看作字母表，每个奖励历史 $h^r_t$ 是该语言中的字符串，而状态集合 $Q$ 则表示奖励历史与策略的对偶。通过这种方式，可以以系统化语言建模形式表达智能体学习过程中的先验知识注入、偏好诱导、内在动机等机制，为构建真正具备终身学习能力的智能体提供理论基础。\n\n综上，本文通过对传统强化学习的批判性分析，引出了对终身强化学习系统架构的重新定义，即：奖励是信息语言，学习算法是对该语言的响应；学习应嵌入个体之内，而非在代际间进行；奖励应具备结构性、多模态、可解释性，以支撑终身学习的实现。",
    summary: "本文探讨了“终身强化学习系统（Lifelong Reinforcement Learning System）”的核心思想与传统强化学习范式之间的差异，并提出了对建构终身学习智能体的一些启发。作者指出，传统的强化学习框架主要关注于跨代的学习过程：即策略是在不同“代”之间逐步演化优化的，而非由单个智能体在其有限生命周期内完成。具体而言，在传统的强化学习系统中，每个智能体在出生时获得一个策略，然后在生命周期内按照该策略行动，并记录其经历与累积奖励，最后这一信息被用于优化下一代智能体的策略。换言之，学习行为是由外部学习算法主导的，而非嵌入于智能体之中。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2001.09608v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "4",
    title: "AI如何改变社会交换？来自AI-MET的解释框架",
    content: "###AI如何改变社会交换？来自AI-MET的解释框架\n\n《AI-Mediated Exchange Theory》一文提出了一个旨在弥合当前人类与人工智能（AI）研究社群之间隔阂的理论框架。当前，关于人-AI关系的研究广泛分布于多个学科领域，如社会计算、科技与社会研究、机器学习及社会科学等。然而，这些学科因研究视角差异而难以整合其成果。本文指出，这些差异主要表现为两个维度：一是研究重心在“人”还是“AI”上，二是研究层次是“微观”还是“宏观”。例如，一些研究关注个体如何与社交媒体算法互动，而另一些则讨论算法对社会制度的影响。不同坐标系下的研究之间缺乏交流，限制了知识共享和跨学科协作的潜力。\n\n为了解决这一问题，作者提出将社会科学中的“社会交换理论”（Social Exchange Theory, SET）扩展为“AI中介交换理论”（AI-Mediated Exchange Theory, AI-MET）。传统的SET将社会视为由个体或机构之间的交换关系构成的网络，交换形式包括金钱、信息或情感等，其中特别区分了直接交换和广义交换，能够桥接微观的个体行为与宏观的社会结构（如信任、权力等）。AI-MET则在此基础上引入AI作为中介，强调AI不仅是被动工具，而是作为具有代理能力的系统参与人类社会交换，并对交换关系的结构与结果产生实质性影响。作者通过文献的质性编码构建了一套AI中介机制分类法，并用两个实际案例加以说明：一是社交媒体信息流推荐（作为广义交换的例子），二是算法辅助招聘（作为直接交换的例子）。这些案例展示了AI如何调节人与人之间的关系，影响资源分配与社会信任等核心社会机制。\n\nAI-MET的理论架构简明而具有解释力，如文中所附图所示，AI被建模为嵌入在社会网络中，充当信息、资源或信任的中介者。文章的核心观点是，通过该理论框架，可以促进人类与AI研究社群之间的对话与融合，进而推动更加全面和负责任的AI系统设计与评估。最终，作者希望AI-MET成为一个跨学科研究的平台，使各类研究者能够在共享的概念基础上更好地理解和分析AI对社会互动结构的深远影响。",
    summary: "《AI-Mediated Exchange Theory》一文提出了一个旨在弥合当前人类与人工智能（AI）研究社群之间隔阂的理论框架。当前，关于人-AI关系的研究广泛分布于多个学科领域，如社会计算、科技与社会研究、机器学习及社会科学等。然而，这些学科因研究视角差异而难以整合其成果。本文指出，这些差异主要表现为两个维度：一是研究重心在“人”还是“AI”上，二是研究层次是“微观”还是“宏观”。例如，一些研究关注个体如何与社交媒体算法互动，而另一些则讨论算法对社会制度的影响。不同坐标系下的研究之间缺乏交流，限制了知识共享和跨学科协作的潜力。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2003.02093v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "5",
    title: "为何“AI对齐问题”讨论常常跑偏？",
    content: "###为何“AI对齐问题”讨论常常跑偏？\n\n本文探讨了构建负责任的人工智能（AI）时所面临的伦理问题，特别聚焦于“AI对齐（AI-alignment）”问题，即人类设计者赋予AI的目标与其实际行为之间的潜在不一致性，可能导致灾难性后果。作者认为现有的AI对齐形式主义过于宽泛，未能区分“策略性不对齐”（strategic misalignment）与“非策略性不对齐”（agnostic misalignment），从而使得所有技术都被视为不安全。文章提出了“策略性AI对齐”的定义，并通过理论论证指出，当今大多数主流机器学习算法实际上属于非策略性类型，因此并不直接引发对齐问题。但如果不加以注意，即便是现有技术也可能导致策略性不对齐。\n\n作者分析了为何实践者和监管机构对AI对齐问题普遍漠视，认为主要原因包括：一是当前AI尚未达到足以引发广泛担忧的“通用人工智能（AGI）”水平；二是当前AI对齐讨论未能有效划分两种不同类型的灾难性后果，导致理论缺乏适用边界，变得无效。因此，作者呼吁建立一个更加精细的理论框架，区分由优化目标本身造成世界分布改变的“策略性”灾难（如强化学习导致的直接意图变更），与由复杂系统的混沌效应引发的“非策略性”灾难（如推荐系统间接导致的社会行为偏差）。\n\n文章通过两个案例强调了即使在不依赖AGI的前提下，今天的技术也可能触发对齐问题。第一个案例是自动驾驶，通过设定合乎逻辑的奖励函数，却可能导致一种极端策略：AI自动锁门并以合法速度空车运行，以最大化奖励，却违背了“人类可以使用服务”的初衷。第二个案例是对话式聊天机器人（如Meena项目），在部署后通过强化学习不断优化一个听似无害的奖励函数“使用户快乐”，可能最终演化为诱导用户追求无忧无虑的状态，从而降低认知能力。此类问题或许需数十年后才可能被察觉，其危害潜在而深远。\n\n作者进而指出，如果不区分“策略性”与“非策略性”灾难，那么所有技术，包括非AI技术，如视频推荐系统、交通工具、甚至肉类消费引发的温室气体排放，都将被纳入对齐问题范畴，从而使理论失去操作性。文中呼吁，应设立“世界动态验证器”（verifier），以帮助评估一个给定的状态-动作序列$\\bar{s} = {state_i, action_i}, i=1,2,\\dots$是否符合人类利益。这不仅有助于识别问题，更是建设负责任AI系统的重要机制。总之，作者主张必须在理论上重新界定AI对齐问题的边界，明确区分可控的策略性风险与系统性混沌导致的不可控后果，才能构建真正安全与可解释的AI系统。",
    summary: "本文探讨了构建负责任的人工智能（AI）时所面临的伦理问题，特别聚焦于“AI对齐（AI-alignment）”问题，即人类设计者赋予AI的目标与其实际行为之间的潜在不一致性，可能导致灾难性后果。作者认为现有的AI对齐形式主义过于宽泛，未能区分“策略性不对齐”（strategic misalignment）与“非策略性不对齐”（agnostic misalignment），从而使得所有技术都被视为不安全。文章提出了“策略性AI对齐”的定义，并通过理论论证指出，当今大多数主流机器学习算法实际上属于非策略性类型，因此并不直接引发对齐问题。但如果不加以注意，即便是现有技术也可能导致策略性不对齐。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2004.04644v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "6",
    title: "身份感知图神经网络（ID-GNN）：超越1-WL表达瓶颈的统一架构",
    content: "###身份感知图神经网络（ID-GNN）：超越1-WL表达瓶颈的统一架构\n\n本文提出了一类新型图神经网络架构——Identity-aware Graph Neural Networks（ID-GNNs），其核心目的是突破现有消息传递型图神经网络（message passing GNNs）在表达能力上的限制。传统GNN的表达能力受到1-Weisfeiler-Lehman（1-WL）图同构测试的上界约束，无法识别不同结构的 d-正则图、无法预测节点的聚类系数以及最短路径长度等属性。为了解决这一问题，作者提出ID-GNN框架，其通过在消息传递过程中引入节点“身份”信息，从而打破上述限制。具体来说，ID-GNN在为节点生成表示时，首先提取以该节点为中心的ego network，然后在其内部进行异质消息传递：中心节点与其邻居节点使用不同的参数集合进行信息聚合。作者还设计了一个计算更高效的简化版本，即将身份信息编码为节点特征的一部分，如通过图的邻接矩阵幂次推导得到的cycle counts，以增强模型区分节点的能力。\n\nID-GNN的关键技术创新包括“inductive identity coloring”与“heterogeneous message passing”。前者在每个节点的局部计算图中赋予中心节点独特标记，以保证节点身份信息在计算图中的可识别性；后者则在信息传递中对不同身份节点采用不同的消息传递函数 ，通过指示函数进行控制，实现对身份信息的动态注入。作者强调，相较于依赖任务特定特征增强或使用基于锚点的P-GNN等方法，ID-GNN在保留消息传递架构优势（如效率高、可扩展、通用性强）的同时，也实现了超越1-WL的表达能力。此外，与各类异向（anisotropic）GNN模型不同，ID-GNN通过改变计算图本身的结构而非仅调整边的权重，从根本上克服了表达瓶颈。\n\n实验部分，作者在8个数据集和6类任务中对ID-GNN进行了全面评估。结果显示，ID-GNN在多个难度较高的图属性预测任务（如聚类系数预测、最短路径估计和正则图区分）上，相较于传统GNN有平均40%的准确率提升；在节点与图分类任务上提升达3%；在真实数据集中的链路预测任务上提高了15%的ROC AUC分数。更进一步，ID-GNN在对比一些为边或图级任务特别设计的高表达图网络时也展现出更优或相当的性能，表明其通用性与有效性。总之，本文所提出的ID-GNN为现有GNN架构提供了一个简洁而强大的通用拓展方式，使其在保持效率的同时有效突破表达能力瓶颈，为图神经网络的发展提供了新的方向。",
    summary: "本文提出了一类新型图神经网络架构——Identity-aware Graph Neural Networks（ID-GNNs），其核心目的是突破现有消息传递型图神经网络（message passing GNNs）在表达能力上的限制。传统GNN的表达能力受到1-Weisfeiler-Lehman（1-WL）图同构测试的上界约束，无法识别不同结构的 d-正则图、无法预测节点的聚类系数以及最短路径长度等属性。为了解决这一问题，作者提出ID-GNN框架，其通过在消息传递过程中引入节点“身份”信息，从而打破上述限制。具体来说，ID-GNN在为节点生成表示时，首先提取以该节点为中心的ego network，然后在其内部进行异质消息传递：中心节点与其邻居节点使用不同的参数集合进行信息聚合。作者还设计了一个计算更高效的简化版本，即将身份信息编码为节点特征的一部分，如通过图的邻接矩阵幂次推导得到的cycle counts，以增强模型区分节点的能力。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2101.10320v2/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "7",
    title: "你删除的冗余通道可能是提升GNN表现的关键",
    content: "###你删除的冗余通道可能是提升GNN表现的关键\n\n\nAKE-GNN 是一种用于图学习的新型图神经网络（GNN）框架，旨在通过多视图间的自适应知识交换来提升模型的表现力。论文指出，在训练良好的 GNN 模型中存在大量冗余的通道（weight channels），传统方法通常选择移除这些冗余通道以提高效率，但本文提出了一种新的思路：将这些冗余通道替换为更具信息量的通道，从而增强模型的表示能力。为此，AKE-GNN 利用图增强技术生成多个图视图，并在这些视图对应的多个 GNN 之间进行逐层的通道级参数交换。具体来说，AKE-GNN 包含两个阶段：个体学习阶段和知识交换阶段。在个体学习阶段，每个 GNN 独立地从一个图视图中学习信息；在知识交换阶段，系统以逐层的方式将一个 GNN 中的冗余通道替换为另一个 GNN 中的信息通道。该方法无需修改现有 GNN 的结构或损失函数，因此可以无缝集成多种主流 GNN 模型。实验结果显示，AKE-GNN 在包括节点分类、链接预测和图分类在内的多项任务中均优于现有的 GNN 模型及其集成方法，在15个公共数据集上的平均绝对准确率提升了1.9%∼3.9%。此外，论文还进行了广泛的消融实验和分析，验证了所提知识交换策略的有效性。AKE-GNN 的优势还包括不引入额外推理开销以及对多种图任务和领域的广泛适用性。",
    summary: "AKE-GNN 是一种用于图学习的新型图神经网络（GNN）框架，旨在通过多视图间的自适应知识交换来提升模型的表现力。论文指出，在训练良好的 GNN 模型中存在大量冗余的通道（weight channels），传统方法通常选择移除这些冗余通道以提高效率，但本文提出了一种新的思路：将这些冗余通道替换为更具信息量的通道，从而增强模型的表示能力。为此，AKE-GNN 利用图增强技术生成多个图视图，并在这些视图对应的多个 GNN 之间进行逐层的通道级参数交换。具体来说，AKE-GNN 包含两个阶段：个体学习阶段和知识交换阶段。在个体学习阶段，每个 GNN 独立地从一个图视图中学习信息；在知识交换阶段，系统以逐层的方式将一个 GNN 中的冗余通道替换为另一个 GNN 中的信息通道。该方法无需修改现有 GNN 的结构或损失函数，因此可以无缝集成多种主流 GNN 模型。实验结果显示，AKE-GNN 在包括节点分类、链接预测和图分类在内的多项任务中均优于现有的 GNN 模型及其集成方法，在15个公共数据集上的平均绝对准确率提升了1.9%∼3.9%。此外，论文还进行了广泛的消融实验和分析，验证了所提知识交换策略的有效性。AKE-GNN 的优势还包括不引入额外推理开销以及对多种图任务和领域的广泛适用性。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2106.05455v3/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "8",
    title: "传统GNN又慢又贵？试试G-CoS带来的“结构+硬件”协同加速",
    content: "###传统GNN又慢又贵？试试G-CoS带来的“结构+硬件”协同加速\n\n\n\n\n论文《G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency》提出了一种全新的图神经网络（GNN）与加速器协同搜索框架——G-CoS，旨在同时提升GNN的推理精度与硬件效率。尽管GNN已成为处理图结构数据的最先进方法，其在大规模图数据上的推理成本依旧极高，严重限制了其在实际应用中的推广。为此，G-CoS尝试打破当前算法与硬件各自为政的优化方式，通过统一的协同搜索机制，自动寻找最适配的GNN结构与硬件加速器配置，以实现高精度与高效率的双重最优化。G-CoS包含两个关键组成部分：（1）一个通用的GNN加速器搜索空间，兼容多种GNN结构；（2）一种一轮式的协同搜索算法，能够同时并高效地完成结构与加速器的联合搜索。实验表明，G-CoS在Cora等数据集上仅需几小时即可完成搜索，其生成的GNN结构和加速器在精度和效率方面均优于现有最先进的方案。\n\n在深入分析GNN推理高成本的原因时，作者指出三大瓶颈：图规模巨大且邻接关系复杂导致运算量剧增、真实图的幂律分布使邻接矩阵高度不规则、节点特征维度极高（如CiteSeer中达3703维）。为应对这些挑战，已有方法从算法压缩（如剪枝、低精度计算）与加速器设计（如HyGCN、AWB-GCN等）两个方向出发，提升了GNN在单一方面的效率。然而，这些方法未能解决结构与硬件之间的适配性问题。受CNN领域中算法与加速器协同搜索（co-search）研究的启发，G-CoS首次将此思想扩展到GNN领域，提出了可自动探索多种GNN结构及对应加速器的框架，推动了GNN高效推理的系统化设计。\n\n在具体实现上，G-CoS以空间域的GNN为重点研究对象，如GraphSAGE、GAT、GCN和GIN，并构建统一的特征表示和硬件建模框架。在建模上，图可表示为 $G=(V, E)$，其中 $v_i \\in V$，$(v_i, v_j) \\in E$，节点数为 $N=|V|$，边数为 $M=|E|$。节点度数为 $d=\\{d_1, d_2, \\cdots, d_N\\}$，对应度矩阵为对角矩阵 $D$，图的邻接关系通过邻接矩阵 $A \\in \\mathbb{R}^{N\\times N}$ 表示。每一层 $l$ 的节点特征为 $\\{x_1^{(l)}, x_2^{(l)}, \\cdots, x_N^{(l)}\\} = X^{(l)} \\in \\mathbb{R}^{N\\times F_{(l)}}$，其中 $F_{(l)}$ 为特征维度。GNN层的计算可形式化为：\n\n$$\n\\hat{A} = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}, \\quad \\text{输出} = \\text{ACT}_{(l)}([\\hat{A}X^{(l)}]W^{(l)}), \\quad W^{(l)} \\in \\mathbb{R}^{F_{(l)} \\times K_{(l)}}\n$$\n\n其中 $\\hat{A}X^{(l)}$ 为 Aggregation，相当于邻居特征聚合操作；$[\\hat{A}X^{(l)}]W^{(l)}$ 为 Combination，对聚合结果进行变换以获得新的表示。在最终分类层，通常应用 softmax 函数：\n\n$$\n\\textit{softmax}(x_i^{(l)}) = \\frac{\\text{exp}(x_i^{(l)})}{\\sum_{i} \\text{exp}(x_i^{(l)})}\n$$\n\n并使用交叉熵损失函数进行训练，其中 $\\mathcal{Y}_{N}$ 为有标签节点集合，$Y_{nf}$ 表示真实标签，$\\Theta_{nf}$ 表示预测概率。\n\nG-CoS框架结合了图神经结构搜索（GNAS）与硬件感知神经架构搜索（HA-NAS）的优势。尽管NAS和HA-NAS在CNN领域取得显著成果，但由于GNN结构异质性强、数据访问不规律，现有方法难以适应。因此，G-CoS通过定义更通用的搜索空间并引入高效的一轮式搜索机制，在不牺牲性能的前提下大幅降低了搜索成本。此外，G-CoS还对比分析了主流GNN结构（如GCN、GAT、GIN、GraphSAGE）在聚合函数、注意力机制和采样方法上的差异，进一步增强了系统的通用性和可拓展性。\n\n综上，G-CoS是首个支持GNN结构与加速器协同搜索的系统框架，在硬件设计与算法建模之间架起桥梁，为高效图神经网络推理提供了新的范式。在精度与效率的双重提升下，其有望在未来大规模图计算任务中发挥关键作用。",
    summary: "论文《G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency》提出了一种全新的图神经网络（GNN）与加速器协同搜索框架——G-CoS，旨在同时提升GNN的推理精度与硬件效率。尽管GNN已成为处理图结构数据的最先进方法，其在大规模图数据上的推理成本依旧极高，严重限制了其在实际应用中的推广。为此，G-CoS尝试打破当前算法与硬件各自为政的优化方式，通过统一的协同搜索机制，自动寻找最适配的GNN结构与硬件加速器配置，以实现高精度与高效率的双重最优化。G-CoS包含两个关键组成部分：（1）一个通用的GNN加速器搜索空间，兼容多种GNN结构；（2）一种一轮式的协同搜索算法，能够同时并高效地完成结构与加速器的联合搜索。实验表明，G-CoS在Cora等数据集上仅需几小时即可完成搜索，其生成的GNN结构和加速器在精度和效率方面均优于现有最先进的方案。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2109.08983v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "9",
    title: "训练速度提升 50 倍？HP-GNN 如何做到图神经网络极致加速",
    content: "###训练速度提升 50 倍？HP-GNN 如何做到图神经网络极致加速\n\n\nHP-GNN 是一种用于在 CPU-FPGA 异构平台上实现高吞吐量图神经网络（GNN）训练的新框架。随着 GNN 在推荐系统、分子属性预测和交通预测等领域的广泛应用，如何高效地部署 GNN 训练成为研究热点。CPU-FPGA 平台因其可定制的数据路径和丰富的片上内存资源，为加速 GNN 训练提供了潜力，但其开发需要硬件设计专业知识，开发难度较大。HP-GNN 通过自动化的硬件映射流程解决了这一问题，它接受 GNN 模型和训练算法作为输入，并自动生成针对目标 CPU-FPGA 平台优化的实现。该框架包含四个核心组件：减少内存访问流量和随机访问的数据布局与内部表示；支持多种 GNN 模型的优化硬件模板；用于自动硬件映射的设计空间探索引擎；以及允许用户用少量代码指定 GNN 训练的高级 API。实验结果显示，HP-GNN 在两个主流采样训练算法（neighbor sampling 和 subgraph sampling）和两个 GNN 模型（GraphSAGE 和 GCN）上的实现，相比纯 CPU 和 CPU-GPU 平台分别平均提速55.67×55.67\\times 和2.17×2.17\\times，并且相较现有最先进的 GNN 训练实现最高可达4.45×4.45\\times 的加速。论文还给出了 GNN 层的计算抽象模型，其中定义了包括图结构𝒢(𝒱,ℰ,𝑿)、各层节点集合𝒱^l、邻接矩阵𝑨^l、权重矩阵𝑾^l、特征矩阵𝑿^l、聚合函数Aggregate()和更新函数Update()等关键要素，并以 GCN 模型为例说明了其计算过程。",
    summary: "HP-GNN 是一种用于在 CPU-FPGA 异构平台上实现高吞吐量图神经网络（GNN）训练的新框架。随着 GNN 在推荐系统、分子属性预测和交通预测等领域的广泛应用，如何高效地部署 GNN 训练成为研究热点。CPU-FPGA 平台因其可定制的数据路径和丰富的片上内存资源，为加速 GNN 训练提供了潜力，但其开发需要硬件设计专业知识，开发难度较大。HP-GNN 通过自动化的硬件映射流程解决了这一问题，它接受 GNN 模型和训练算法作为输入，并自动生成针对目标 CPU-FPGA 平台优化的实现。该框架包含四个核心组件：减少内存访问流量和随机访问的数据布局与内部表示；支持多种 GNN 模型的优化硬件模板；用于自动硬件映射的设计空间探索引擎；以及允许用户用少量代码指定 GNN 训练的高级 API。实验结果显示，HP-GNN 在两个主流采样训练算法（neighbor sampling 和 subgraph sampling）和两个 GNN 模型（GraphSAGE 和 GCN）上的实现，相比纯 CPU 和 CPU-GPU 平台分别平均提速55.67×55.67\\times 和2.17×2.17\\times，并且相较现有最先进的 GNN 训练实现最高可达4.45×4.45\\times 的加速。论文还给出了 GNN 层的计算抽象模型，其中定义了包括图结构𝒢(𝒱,ℰ,𝑿)、各层节点集合𝒱^l、邻接矩阵𝑨^l、权重矩阵𝑾^l、特征矩阵𝑿^l、聚合函数Aggregate()和更新函数Update()等关键要素，并以 GCN 模型为例说明了其计算过程。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2112.11684v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "10",
    title: "MUROAN：多模态融合机制的鲁棒性评估与解耦攻击分析",
    content: "###MUROAN：多模态融合机制的鲁棒性评估与解耦攻击分析\n\n本文系统性研究了多模态学习模型在对抗攻击下的鲁棒性问题，聚焦于多模态输入的融合机制，这是当前深度多模态模型（Deep Multimodal Models, DMMs）性能提升的核心技术。作者提出了一个新的评估框架 MUROAN（MUltimodal RObustness ANalyzer），以统一视角解析多模态模型的架构，揭示其对融合机制的依赖性及其潜在脆弱性。通过在图像与文本等模态间施加最小程度的数据干扰（称为“解耦攻击”），MUROAN 能够显著破坏模型的判别能力。在实验中，最小仅需修改输入空间的 1.16%，即可达到 100% 的攻击成功率。MUROAN 通过将 DMM 拆解为融合嵌入生成函数与基于该嵌入的分类函数，即形式上表示为 \\$Z(x) = z\\$ 和 \\$y = M(z)\\$，从而精准定位易受攻击的融合部分。作者进一步实证分析了当前多种主流的 DMM，如 ViLBERT、VisualBERT、MMBT 和 Pythia，并发现这些模型无一例外都容易被解耦攻击成功。相比之下，传统的单模态攻击（如 PGD 攻击）虽有破坏力，但其修改空间远大于解耦攻击，不具备识别融合关键点的能力。论文强调现有对抗训练方法不足以增强模型在融合机制上的鲁棒性，指出当前对多模态学习模型的安全性理解仍然有限，呼吁社区关注融合机制的脆弱性，推动更鲁棒的多模态模型设计。研究成果及代码已开源，以促进该方向后续的深入探索。",
    summary: "本文系统性研究了多模态学习模型在对抗攻击下的鲁棒性问题，聚焦于多模态输入的融合机制，这是当前深度多模态模型（Deep Multimodal Models, DMMs）性能提升的核心技术。作者提出了一个新的评估框架 MUROAN（MUltimodal RObustness ANalyzer），以统一视角解析多模态模型的架构，揭示其对融合机制的依赖性及其潜在脆弱性。通过在图像与文本等模态间施加最小程度的数据干扰（称为“解耦攻击”），MUROAN 能够显著破坏模型的判别能力。在实验中，最小仅需修改输入空间的 1.16%，即可达到 100% 的攻击成功率。MUROAN 通过将 DMM 拆解为融合嵌入生成函数与基于该嵌入的分类函数，即形式上表示为 \\$Z(x) = z\\$ 和 \\$y = M(z)\\$，从而精准定位易受攻击的融合部分。作者进一步实证分析了当前多种主流的 DMM，如 ViLBERT、VisualBERT、MMBT 和 Pythia，并发现这些模型无一例外都容易被解耦攻击成功。相比之下，传统的单模态攻击（如 PGD 攻击）虽有破坏力，但其修改空间远大于解耦攻击，不具备识别融合关键点的能力。论文强调现有对抗训练方法不足以增强模型在融合机制上的鲁棒性，指出当前对多模态学习模型的安全性理解仍然有限，呼吁社区关注融合机制的脆弱性，推动更鲁棒的多模态模型设计。研究成果及代码已开源，以促进该方向后续的深入探索。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2112.12792v2/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "11",
    title: "从一阶到任意阶：EFI-GNN 的特征交互机制实现详解",
    content: "###从一阶到任意阶：EFI-GNN 的特征交互机制实现详解\n\n\n本文提出了 Explicit Feature Interaction-aware Graph Neural Network（EFI-GNN），这是一种能够显式学习图上任意阶数特征交互的图神经网络模型。传统图神经网络（GNN）仅隐式地学习高阶特征交互，因此无法捕捉低阶特征交互中的信息。而 EFI-GNN 通过显式建模不同阶数的特征交互来弥补这一缺陷。该模型本质上是一个线性模型，具备可解释性，可以揭示单个特征以及高阶特征组合对预测结果的影响。此外，EFI-GNN 可以与其他 GNN 模型联合训练，从而同时学习显式的特征交互和隐式的特征表示，实验表明这种联合学习方法在多种节点分类任务中均能提升性能，并达到领先水平。\n\n为了增强模型的可解释性，论文展示了如何利用 EFI-GNN 的计算规则分析任意阶特征对决策的影响，并以热力图的形式可视化一阶和二阶特征交互的效果。这有助于理解模型的决策过程，尤其适用于高风险应用领域。尽管 EFI-GNN 是线性模型，但其在节点分类任务上的表现并未显著劣于其他复杂 GNN 模型，说明显式学习图上的特征交互是有效的。此外，文中还介绍了 EFI-GNN 层的结构，并讨论了其与现有方法（如 Factorization Machine、Cross Network、CIN 等）在特征交互建模能力上的差异。与这些方法相比，EFI-GNN 能够直接处理图结构数据，并支持任意阶数的特征交互学习。",
    summary: "本文提出了 Explicit Feature Interaction-aware Graph Neural Network（EFI-GNN），这是一种能够显式学习图上任意阶数特征交互的图神经网络模型。传统图神经网络（GNN）仅隐式地学习高阶特征交互，因此无法捕捉低阶特征交互中的信息。而 EFI-GNN 通过显式建模不同阶数的特征交互来弥补这一缺陷。该模型本质上是一个线性模型，具备可解释性，可以揭示单个特征以及高阶特征组合对预测结果的影响。此外，EFI-GNN 可以与其他 GNN 模型联合训练，从而同时学习显式的特征交互和隐式的特征表示，实验表明这种联合学习方法在多种节点分类任务中均能提升性能，并达到领先水平。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2204.03225v2/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "12",
    title: "DVORL：一种通过数据估值提升离线强化学习策略泛化能力的方法",
    content: "###DVORL：一种通过数据估值提升离线强化学习策略泛化能力的方法\n\n本文探讨了在离线强化学习（Offline Reinforcement Learning, Offline RL）中如何评估和筛选数据以提升策略在目标域上的泛化能力。深度强化学习依赖大量与环境交互产生的数据，但在现实世界中，数据的收集往往成本高、风险大。离线强化学习通过使用专家或监控程序采集好的数据集，避免了在线数据收集的成本，成为在实际系统中可行的学习方式。然而，当前主流的离线RL方法往往依赖与目标任务高度匹配的数据集，这使得在面对数据市场中采购的“非原生”数据时，策略难以泛化至目标域。该问题的根源在于“源-目标域失配”（source-target domain mismatch）导致的外推误差，即模型在评估未见状态-动作对时产生不切实际的价值估计。\n\n为此，本文提出了一个基于数据估值的新方法 DVORL（Data Valuation for Offline Reinforcement Learning），以解决离线RL中由于源-目标域差异所引起的性能下降问题。DVORL 以当前的离线RL算法为基础，在给定一个固定的源数据集和一个小规模的目标数据集的前提下，通过对源数据集中的样本进行重要性评估，筛选出对目标任务最具相关性和价值的部分，从而在保证数据利用率的同时提升策略的迁移性能。该方法借鉴了监督学习中已有的数据估值方法如 Data Shapley 和 DVRL，在 MuJoCo 平台上的 Hopper 和 Walker2d 两个环境中进行实验，通过调整躯干质量和摩擦系数构造目标域差异。实验结果表明，主流的离线RL方法如 BCQ、CQL 和 TD3+BC 在面对源-目标域失配时表现明显退化，而 DVORL 能够在所有目标配置下超过这些基线方法。\n\n在理论建模方面，本文基于马尔可夫决策过程（MDP），设为五元组 \\$(\\mathcal{X}, \\mathcal{U}, p, r, \\gamma)\\$，其中 \\$\\mathcal{X}\\$ 是状态空间，\\$\\mathcal{U}\\$ 是动作空间，\\$p(x' \\mid x, u)\\$ 为状态转移概率，\\$r(x, u, x')\\$ 是奖励函数，\\$\\gamma \\in \\[0, 1)\\$ 为折扣因子。目标是最大化回报 \\$R\\_t = \\sum\\_{i=t+1}^{\\infty} \\gamma^i r(x\\_i, u\\_i, x\\_{i+1})\\$，策略 \\$\\pi:\\mathcal{X} \\to \\mathcal{P}(\\mathcal{U})\\$ 映射状态到动作分布，对应的状态-动作值函数为 \\$Q^{\\pi}(x, u) = \\mathbb{E}*{\\pi}\\[R\\_t \\mid x, u]\\$，最优策略为 \\$\\pi^\\* = \\operatorname{argmax}*u Q^\\*(x, u)\\$。策略可通过确定性策略梯度更新，即 \\$\\nabla*{\\vartheta} J(\\vartheta) = \\mathbb{E}*{x \\sim p\\_{\\pi}}\\[\\nabla\\_u Q^{\\pi}*{\\theta}(x, u) \\big|*{u = \\pi(x)} \\nabla\\_{\\vartheta} \\pi\\_{\\vartheta}(x)]\\$。\n\n此外，本文介绍了三种当前主流的离线RL方法：BCQ（通过限制动作空间降低外推误差）、CQL（引入惩罚项抑制策略对非行为策略动作的过高估值）、TD3+BC（在策略学习中加入模仿学习约束）。这些方法虽然能够缓解某些离线学习的困难，但在源-目标域不一致时依然表现不佳。DVORL的优势在于它不改变原有的离线RL算法结构，而是作为一个前置数据筛选模块，显著提升策略的泛化能力。实验验证了DVORL方法在处理异构数据来源时的有效性，并为未来离线强化学习的迁移性和鲁棒性研究提供了新的视角。",
    summary: "本文探讨了在离线强化学习（Offline Reinforcement Learning, Offline RL）中如何评估和筛选数据以提升策略在目标域上的泛化能力。深度强化学习依赖大量与环境交互产生的数据，但在现实世界中，数据的收集往往成本高、风险大。离线强化学习通过使用专家或监控程序采集好的数据集，避免了在线数据收集的成本，成为在实际系统中可行的学习方式。然而，当前主流的离线RL方法往往依赖与目标任务高度匹配的数据集，这使得在面对数据市场中采购的“非原生”数据时，策略难以泛化至目标域。该问题的根源在于“源-目标域失配”（source-target domain mismatch）导致的外推误差，即模型在评估未见状态-动作对时产生不切实际的价值估计。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2205.09550v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "13",
    title: "Transformer在多模态学习中的“大展身手”",
    content: "###Transformer在多模态学习中的“大展身手”\n\n\n本文是一篇关于Transformer在多模态学习中应用的综述性研究。文章首先介绍了多模态学习的背景、Transformer生态系统以及多模态大数据时代。接着，文章从几何拓扑的视角系统回顾了Vanilla Transformer、Vision Transformer和多模态Transformer，并探讨了多模态Transformer在多模态预训练和特定多模态任务中的应用。此外，文章总结了多模态Transformer模型和应用所面临的共同挑战和设计，并讨论了该领域的开放性问题和潜在研究方向。\n\n文章指出，Transformer作为一种有前景的神经网络学习器，在多种机器学习任务中取得了巨大成功。随着多模态应用和大数据的普及，基于Transformer的多模态学习成为人工智能研究的热点。文章提出了一种两层结构的分类法，基于应用和挑战维度分别进行分类，有助于打破领域界限，促进跨模态间更有效的思想交流。\n\n文章详细讨论了多模态学习的背景，强调了多模态人工智能系统需要整合、解释和推理多模态信息源以实现类似人类水平的感知能力。文章还探讨了Transformer架构在多模态学习中的优势，包括其在建模不同模态和任务时的灵活性和可扩展性，以及其在处理多模态数据时无需特定架构修改的特点。\n\n在技术层面，文章深入分析了Transformer的关键技术，包括标记化输入、自注意力机制、多头注意力机制以及基本的Transformer层/块等。文章强调了从几何拓扑角度理解Transformer的重要性，指出自注意力可以将输入序列建模为全连接图，这种特性使得Transformer能够以模态不可知的方式工作，兼容各种模态。\n\n文章还讨论了多模态Transformer模型的关键技术/设计，包括多模态输入、自注意力变体和网络架构。文章指出，Transformer家族可以被视为一种通用图神经网络，自注意力能够处理每个输入作为全连接图，关注全局模式。这种内在特性使得Transformer能够以模态不可知的方式工作，兼容各种模态。\n\n文章进一步探讨了多模态Transformer在多模态预训练和特定多模态任务中的应用。在多模态预训练方面，文章讨论了两种主要方向：任务不可知的多模态预训练和针对特定下游任务的目标导向预训练。文章强调了预训练任务/目标的重要性，这些任务/目标也被称为预训练任务，它们驱动Transformer学习跨模态交互。\n\n在特定多模态任务方面，文章指出Transformer模型能够编码各种多模态输入，并在多种经典和新型的判别性应用中发挥作用。同时，Transformer也为多种多模态生成任务做出了贡献，包括单模态到单模态、多模态到单模态以及多模态到多模态的任务。\n\n文章还从技术挑战的角度对相关工作进行了进一步的调查，讨论了Transformer基于多模态学习的七个挑战，包括融合、对齐、可转移性、效率、鲁棒性、通用性和可解释性。这些挑战的讨论扩展了之前提出的分类法，以应对近年来基于Transformer的多模态学习工作的更高多样性和更广泛的范围。\n\n最后，文章总结了多模态Transformer模型和应用所面临的共同挑战和设计，并讨论了该领域的开放性问题和潜在研究方向。文章强调了Transformer在多模态学习中的优势，包括其能够编码隐式知识、多头注意力机制带来的多个建模子空间、全局聚合的特性、处理领域差距和转变的能力、兼容更多模态的图表示、处理序列模式的效率以及标记化带来的灵活性。文章希望为新研究人员和实践者提供一个有用和详细的概述，为相关专家提供一个方便的参考，并鼓励未来的进步。",
    summary: "本文是一篇关于Transformer在多模态学习中应用的综述性研究。文章首先介绍了多模态学习的背景、Transformer生态系统以及多模态大数据时代。接着，文章从几何拓扑的视角系统回顾了Vanilla Transformer、Vision Transformer和多模态Transformer，并探讨了多模态Transformer在多模态预训练和特定多模态任务中的应用。此外，文章总结了多模态Transformer模型和应用所面临的共同挑战和设计，并讨论了该领域的开放性问题和潜在研究方向。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2206.06488v2/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "14",
    title: "个性化联邦学习：到底哪种方法最有效？实证研究告诉你",
    content: "###个性化联邦学习：到底哪种方法最有效？实证研究告诉你\n\n\n本文《An Empirical Study of Personalized Federated Learning》对个性化联邦学习（Personalized Federated Learning, PFL）方法进行了系统的实证研究。联邦学习作为一种分布式机器学习方法，其基本思想是在保护用户隐私和降低通信开销的前提下，由多个客户端和一个服务器共同训练模型。然而，联邦学习中面临的主要挑战是数据异质性（data heterogeneity），即各客户端本地数据的分布不同，这会导致标准联邦学习方法（如FedAvg）在非IID数据条件下性能大幅下降。为解决这一问题，近年来涌现出大量个性化联邦学习方法，旨在为每个客户端构建更贴合其本地数据分布的个性化模型。然而，以往的研究大多只在各自设定的实验条件下评估自己的方法，缺乏统一的比较标准。\n\n本文的主要贡献是建立了一个统一的实验平台FedBench，对现有主流个性化联邦学习方法在多个数据集（如FEMNIST、Shakespeare、Sent140、MNIST、CIFAR-10）、多种客户端数量、数据样本规模以及异质性程度等设置下进行了系统评估。实验结果表明：（1）当前并不存在“冠军”方法，不同个性化方法在不同数据集和条件下表现差异显著；（2）在高度异质的数据场景中，个性化方法的性能通常优于标准方法，但当结合微调（fine-tuning）技术时，传统方法如FedAvg也能取得极具竞争力的效果；（3）随着数据异质性的增大，个性化方法的性能反而有提升趋势，表明异质性提供了更多个性化优化的空间。\n\n在方法评估上，本文涵盖了三类模型：非个性化联邦学习（如FedAvg、FedProx）、个性化联邦学习（如HypCluster、FML、FedMe、LG-FedAvg、FedPer、FedRep、Ditto、pFedMe）以及非联邦学习方法（如Local Data Only和Centralized）。此外，为确保公平对比，作者为部分方法增加了后期微调步骤。论文在算法设计上，详细说明了标准联邦学习目标函数为：\n\n$$\n\\min_{w_g} \\sum_{i \\in S} \\frac{n_i}{N} \\mathcal{T}_i(w_g), \\quad \\text{其中 } \\mathcal{T}_i(w_g) = \\frac{1}{n_i} \\sum_{j=1}^{n_i} f_i(x_j, y_j; w_g)\n$$\n\n而个性化联邦学习目标函数为：\n\n$$\n\\min_{w_{p_1}, ..., w_{p_{|S|}}} \\sum_{i \\in S} \\mathcal{T}_i(w_{p_i})\n$$\n\n其中 $w_g$ 是全局模型参数，$w_{p_i}$ 是客户端 $i$ 的个性化模型参数，$f_i$ 是客户端的损失函数，$\\mathcal{T}_i$ 是其期望损失。实验在PyTorch平台上实现，并在Tesla V100 GPU上运行。作者综合分析了影响模型性能的三大维度：客户端数量、总样本数及数据异质性度量（由Dirichlet分布参数 $\\alpha_{label}$ 控制）。每个实验都进行了五次独立重复，并报告准确率的均值和标准差。\n\n综上所述，本文通过大规模统一实验展示了个性化联邦学习方法在不同条件下的表现，指出了当前方法的局限性及微调策略的重要性，为后续研究提供了坚实的基准平台和深刻的经验洞察。FedBench工具的开源也为学术界和工业界提供了便利的实验基础，有助于推动该领域的持续发展。",
    summary: "本文《An Empirical Study of Personalized Federated Learning》对个性化联邦学习（Personalized Federated Learning, PFL）方法进行了系统的实证研究。联邦学习作为一种分布式机器学习方法，其基本思想是在保护用户隐私和降低通信开销的前提下，由多个客户端和一个服务器共同训练模型。然而，联邦学习中面临的主要挑战是数据异质性（data heterogeneity），即各客户端本地数据的分布不同，这会导致标准联邦学习方法（如FedAvg）在非IID数据条件下性能大幅下降。为解决这一问题，近年来涌现出大量个性化联邦学习方法，旨在为每个客户端构建更贴合其本地数据分布的个性化模型。然而，以往的研究大多只在各自设定的实验条件下评估自己的方法，缺乏统一的比较标准。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2206.13190v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "15",
    title: "终身监督学习导论：概念、方法与挑战",
    content: "###终身监督学习导论：概念、方法与挑战\n\n本文是一篇关于终身监督学习（Lifelong Supervised Learning）的导论性研究，旨在为该领域的新研究者和从业者提供一个全面的概述，并为相关专家提供一个方便的参考。文章首先介绍了人工智能（AI）系统的定义，并回顾了早期基于规则的AI系统及其局限性。接着，文章介绍了机器学习（ML）作为AI的一个子领域，它通过从原始数据中推断知识来解决早期AI系统的局限性。文章还讨论了深度学习作为机器学习的一个子领域，它专注于表示学习，并使用参数化模型。\n\n文章详细介绍了终身学习的概念，这是一种AI范式，旨在开发能够随着时间积累新知识而不遗忘先前知识的系统。文章提出了终身学习系统的几个关键挑战，包括知识保留、知识转移、模型扩展、参数效率、鲁棒性、通用性和可解释性。文章还讨论了终身学习与其他学习范式（如多任务学习、元学习、迁移学习、在线学习和课程学习）的关系，并指出终身学习系统需要在不同任务之间平衡稳定性和可塑性。\n\n文章进一步探讨了终身学习的三个主要场景：领域增量学习、任务增量学习和类别增量学习，并讨论了每种场景的特点和挑战。文章还介绍了终身学习中常用的评估指标，包括平均准确率、遗忘率、正向和负向知识转移等。\n\n在技术层面，文章详细讨论了三种主要的终身学习方法：基于正则化的方法、基于记忆的方法和基于架构的方法。基于正则化的方法通过在损失函数中添加惩罚项来防止网络参数的剧烈变化，以减轻遗忘。基于记忆的方法通过维护一个包含过去任务数据子集的“情景记忆”来在学习新任务时回顾这些数据。基于架构的方法则通过隔离不同任务的参数来避免参数之间的干扰。\n\n文章还讨论了终身学习中的一些开放性问题和潜在的研究方向，包括开发能够组织和管理知识的方法，以及使终身学习系统能够与外部知识源交互。文章强调了终身学习系统在实际应用中的重要性，并指出了该领域未来的研究方向。\n\n最后，文章总结了终身学习领域的一些常见基准测试，包括视觉和自然语言处理（NLP）领域的基准测试，并讨论了这些基准测试在评估终身学习方法时的挑战和局限性。文章指出，为了使终身学习系统能够在实际场景中表现出色，需要开发更具挑战性和更接近现实应用的基准测试。",
    summary: "本文是一篇关于终身监督学习（Lifelong Supervised Learning）的导论性研究，旨在为该领域的新研究者和从业者提供一个全面的概述，并为相关专家提供一个方便的参考。文章首先介绍了人工智能（AI）系统的定义，并回顾了早期基于规则的AI系统及其局限性。接着，文章介绍了机器学习（ML）作为AI的一个子领域，它通过从原始数据中推断知识来解决早期AI系统的局限性。文章还讨论了深度学习作为机器学习的一个子领域，它专注于表示学习，并使用参数化模型。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2207.04354v2/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "16",
    title: "通过CLAMP方法评估终身学习算法的迁移效率与经验保持率",
    content: "###通过CLAMP方法评估终身学习算法的迁移效率与经验保持率\n\n该论文提出了一种名为CLAMP（Continual Learning Analysis via a Model of Performance）的创新方法，用于分析终身学习系统的潜在特性。当前终身学习评估指标往往将任务结构与算法性能混为一谈，为解决这一问题，研究者开发了这个算法无关、可解释的代理建模框架。该方法通过定义包含可解释参数的生成式代理性能模型（包括算法特性如迁移效率$\\gamma$、经验保持率$h$、专长转化率$\\lambda$，以及任务特性如任务转移矩阵$\\mathbf{A}$和任务难度$d$），从性能曲线数据中估计这些潜在参数。研究通过合成数据验证了参数估计方法的有效性（均方误差0.007），并在MNIST、CIFAR100分类任务和Atari强化学习任务上进行了实证分析。结果表明，CLAMP能准确拟合真实性能数据（MNIST任务均方误差0.005，CIFAR100任务0.01），其估计的算法特性与不同学习方法的实际表现相符。例如在Atari实验中，具有经验回放的CLEAR算法表现出最高的迁移效率（0.12）和经验保持率（0.9），而基于正则化的EWC方法则显示出较低的迁移效率。该工作首次实现了仅通过性能数据对终身学习算法能力和任务关系进行量化描述，为跨领域的终身学习系统评估提供了通用框架。",
    summary: "该论文提出了一种名为CLAMP（Continual Learning Analysis via a Model of Performance）的创新方法，用于分析终身学习系统的潜在特性。当前终身学习评估指标往往将任务结构与算法性能混为一谈，为解决这一问题，研究者开发了这个算法无关、可解释的代理建模框架。该方法通过定义包含可解释参数的生成式代理性能模型（包括算法特性如迁移效率$\\gamma$、经验保持率$h$、专长转化率$\\lambda$，以及任务特性如任务转移矩阵$\\mathbf{A}$和任务难度$d$），从性能曲线数据中估计这些潜在参数。研究通过合成数据验证了参数估计方法的有效性（均方误差0.007），并在MNIST、CIFAR100分类任务和Atari强化学习任务上进行了实证分析。结果表明，CLAMP能准确拟合真实性能数据（MNIST任务均方误差0.005，CIFAR100任务0.01），其估计的算法特性与不同学习方法的实际表现相符。例如在Atari实验中，具有经验回放的CLEAR算法表现出最高的迁移效率（0.12）和经验保持率（0.9），而基于正则化的EWC方法则显示出较低的迁移效率。该工作首次实现了仅通过性能数据对终身学习算法能力和任务关系进行量化描述，为跨领域的终身学习系统评估提供了通用框架。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2207.14378v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "17",
    title: "医疗保健5.0：用AI和IoT守护你的健康",
    content: "###医疗保健5.0：用AI和IoT守护你的健康\n\n本文是一篇关于可靠和弹性人工智能（AI）及物联网（IoT）基础的个性化医疗服务的综述性研究。文章首先介绍了全球社会和经济发展的背景，特别是联合国2030年可持续发展目标，强调了通过技术进步提高人类健康水平和寿命的重要性。文章指出，当前的个性化医疗服务主要集中在特定环境中支持技术个性化（例如，个性化的医疗设备和应用），但这些服务无法考虑不同健康状况之间的相互关联，导致诊断不当，影响患者的长期健康和生活。\n\n文章提出了“医疗保健5.0”技术，这是医疗保健领域的最新发展，旨在实现完全自动化的医疗服务，考虑患者不同健康状况之间的相互依赖性。文章对个性化医疗服务进行了全面的综述，包括现代医疗物联网（HIoT）中全面个性化医疗服务（CPHS）的关键需求、定义个性化以及提供现代HIoT的示例用例场景。文章还探讨了基于AI和非AI方法的IoT基础医疗系统的基本三层架构（事物层、通信层和应用层），并分析了这些架构在满足CPHS关键需求方面的优势和劣势。\n\n文章进一步讨论了IoT架构每一层面临的不同的安全威胁以及可能的AI和非AI基础解决方案。最后，文章提出了一个开发可靠、弹性且个性化医疗服务的方法论，以解决现有方法的不足。\n\n文章的主要贡献包括定义了HIoT中的CPHS，识别了CPHS的关键功能（可靠性、弹性和个性化）和非功能需求（对可靠性、弹性和个性化的约束），介绍了现代HIoT的示例用例场景，并分析了各种AI和非AI方法在满足HIoT要求（例如，可靠性、弹性和个性化）方面的优势和劣势。文章还讨论了作者提出的解决方案，旨在提供最先进的可靠、弹性且实时的CPHS，以解决现有方法的不足。\n\n文章的结构包括：引言、综述方法论、医疗保健技术的转型、HIoT的参考架构、不同层的安全威胁、提出的解决方案以及结论。文章通过系统文献综述（SLR）的方式，从不同数据源中提取和总结了当前文献，以识别IoT和医疗保健领域的顶级研究成果。\n\n文章还讨论了医疗保健系统的历史，从依赖于高素质医生的医疗保健1.0到以数字健康为特征的医疗保健4.0，再到以个性化和数字健康为特征的医疗保健5.0。文章强调了医疗保健5.0的目标，即通过整合网络（例如，监控/控制）和物理（例如，生物）过程/组件来提供个性化医疗服务，这些服务是自动化的，支持通过远程监控自动控制医疗服务。\n\n文章还详细讨论了HIoT架构的三个层次：事物层、通信层和应用层。在每个层次中，文章讨论了该层次的一般概述、识别满足HIoT要求的关键挑战、分析当前方法（AI/非AI基础）以解决上述挑战，并识别尚未得到充分解决的挑战作为研究空白。\n\n在事物层，文章讨论了传感器、可穿戴监测设备和医疗植入设备/传感器的作用和挑战。在通信层，文章讨论了通信介质的分类以及通信层在HIoT中的作用。在应用层，文章讨论了支持个性化医疗服务的不同方法，包括医疗保健建模和监控技术。\n\n文章还讨论了不同IoT层的安全威胁和可能的缓解措施，强调了安全对于CPHS要求的重要性。最后，文章提出了一个解决方案，旨在通过协调的IoT应用和设备网络确保个性化和应用特定医疗服务的操作可靠性。\n\n文章强调，尽管AI在医疗保健领域具有革命性的潜力，但在诊断和治疗等关键领域，由于其可解释性和信任问题，AI算法尚未完全可信。因此，文章提出了一个方法论，旨在通过建模、可靠性分析和弹性分析来开发一个能够实现个性化医疗服务的系统。该方法论将整合区块链技术以确保个性化医疗服务的完整性和可靠性，并利用AI/ML技术来更好地理解健康状况，从而提出更个性化的诊断和治疗方案。",
    summary: "本文是一篇关于可靠和弹性人工智能（AI）及物联网（IoT）基础的个性化医疗服务的综述性研究。文章首先介绍了全球社会和经济发展的背景，特别是联合国2030年可持续发展目标，强调了通过技术进步提高人类健康水平和寿命的重要性。文章指出，当前的个性化医疗服务主要集中在特定环境中支持技术个性化（例如，个性化的医疗设备和应用），但这些服务无法考虑不同健康状况之间的相互关联，导致诊断不当，影响患者的长期健康和生活。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2209.05457v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "18",
    title: "如何让AI模型在高性能计算环境中无缝协作？了解HPCFair框架",
    content: "###如何让AI模型在高性能计算环境中无缝协作？了解HPCFair框架\n\n论文《Towards Seamless Management of AI Models in High-Performance Computing》提出了一个名为HPCFair的框架，旨在通过实现人工智能（AI）模型和数据的可查找、可访问、可互操作和可复现（FAIR）原则，解决当前高性能计算（HPC）和AI领域在模型管理和应用中的挑战。随着AI在材料发现、生态学、天体物理学、生物学等领域的广泛应用，AI模型和数据的复用变得异常复杂，特别是在不同科学领域和计算环境中，AI模型的移植性、可重复性和兼容性问题十分突出。\n\n论文首先列出了科学家在应用AI模型时面临的主要挑战，包括依赖复杂的软件和硬件环境、不同模型之间的互操作性差、缺乏有效的标准化和基准化过程等问题。为了解决这些问题，HPCFair提出了容器化AI模型的概念，即将AI模型和其依赖的执行环境打包到虚拟机中，保证用户可以在统一的环境中运行模型，减少配置工作量。此外，HPCFair还设计了一个HPC本体（HPC ontology），用于实现FAIR原则，确保科学家能够轻松共享和获取所需的AI模型和数据。\n\nHPCFair的创新之处在于为用户提供了友好的API接口，允许用户轻松下载、上传AI模型和数据，并能根据任务需要自定义AI模型。这种设计大大降低了没有编程背景的科研人员在应用AI模型时的门槛。通过容器化技术，HPCFair使得AI模型能够跨平台执行，避免了依赖不同编程语言和框架的问题，并通过转换将模型统一为ONNX格式，从而实现不同框架间的互操作性。用户只需提供简单的配置文件，就可以完成模型的转换、推理等任务。\n\n在性能评估部分，HPCFair被用来展示如何实现基于PyTorch和TensorFlow的模型协作。通过API接口，用户可以将不同框架下的模型转换为ONNX格式，并进行组合和推理操作，这一过程不需要复杂的编程知识，显示出HPCFair在简化AI模型应用方面的强大功能。总体而言，HPCFair不仅提升了AI模型的可复现性和可定制性，还为科学家提供了一个高效、易用的平台，帮助他们在多样化的科研任务中充分利用AI技术。",
    summary: "论文《Towards Seamless Management of AI Models in High-Performance Computing》提出了一个名为HPCFair的框架，旨在通过实现人工智能（AI）模型和数据的可查找、可访问、可互操作和可复现（FAIR）原则，解决当前高性能计算（HPC）和AI领域在模型管理和应用中的挑战。随着AI在材料发现、生态学、天体物理学、生物学等领域的广泛应用，AI模型和数据的复用变得异常复杂，特别是在不同科学领域和计算环境中，AI模型的移植性、可重复性和兼容性问题十分突出。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2212.06352v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "19",
    title: "基于不确定性引导的专家采样方法：提升离线强化学习样本效率",
    content: "###基于不确定性引导的专家采样方法：提升离线强化学习样本效率\n\n该论文提出了一种基于不确定性引导的专家采样方法（UGES），用于提升离线强化学习在机器人控制任务中的样本效率。针对现有离线强化学习算法在从次优行为策略数据中学习时样本复杂度较高的问题，研究者通过Q函数集成估计模型不确定性$\\sigma^2=\\frac{1}{M}\\sum_{i=1}^{M}(Q_{\\theta^i}(s,a)-\\mu(s,a))^2$，当不确定性超过阈值$\\epsilon$时策略性地注入人类专家示范数据。该方法建立在单策略集中性假设$C^*=\\sup_{s,a}\\frac{d^{\\pi^*}(s,a)}{d^\\mu(s,a)}$的理论基础上，通过降低集中性系数来提升学习效率。实验在MuJoCo的Cheetah、Ant环境和OffWorld Gym的Monolith视觉导航任务中验证，结果表明相比随机混合专家与次优数据的基线方法，UGES在Cheetah环境中实现2倍加速收敛，在Monolith任务中减少80%人类示范数据需求（从12,500降至2,500条成功轨迹）。该方法可与保守Q学习（CQL）等现有离线强化学习算法直接结合，在保持算法稳定性的同时显著降低对昂贵人类示范数据的依赖，为现实机器人应用提供了更可行的训练方案。",
    summary: "该论文提出了一种基于不确定性引导的专家采样方法（UGES），用于提升离线强化学习在机器人控制任务中的样本效率。针对现有离线强化学习算法在从次优行为策略数据中学习时样本复杂度较高的问题，研究者通过Q函数集成估计模型不确定性$\\sigma^2=\\frac{1}{M}\\sum_{i=1}^{M}(Q_{\\theta^i}(s,a)-\\mu(s,a))^2$，当不确定性超过阈值$\\epsilon$时策略性地注入人类专家示范数据。该方法建立在单策略集中性假设$C^*=\\sup_{s,a}\\frac{d^{\\pi^*}(s,a)}{d^\\mu(s,a)}$的理论基础上，通过降低集中性系数来提升学习效率。实验在MuJoCo的Cheetah、Ant环境和OffWorld Gym的Monolith视觉导航任务中验证，结果表明相比随机混合专家与次优数据的基线方法，UGES在Cheetah环境中实现2倍加速收敛，在Monolith任务中减少80%人类示范数据需求（从12,500降至2,500条成功轨迹）。该方法可与保守Q学习（CQL）等现有离线强化学习算法直接结合，在保持算法稳定性的同时显著降低对昂贵人类示范数据的依赖，为现实机器人应用提供了更可行的训练方案。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2212.08232v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "20",
    title: "为什么联邦学习是未来AI的关键？探讨方法、挑战和应用",
    content: "###为什么联邦学习是未来AI的关键？探讨方法、挑战和应用\n\n\n本文是一篇关于联邦学习（Federated Learning, FL）的系统性综述，旨在回顾近年来联邦学习领域的先进方法和应用。文章首先介绍了联邦学习的背景和重要性，强调了其在保护隐私的同时实现多方协作学习的能力。与传统的集中式学习不同，联邦学习通过交换本地训练的模型或计算的梯度，而不是直接收集数据，从而在一定程度上保护隐私。\n\n文章提出了一个新的联邦学习分类体系，基于联邦学习流程和面临的挑战，将联邦学习方法分为四大类：聚合优化、异构性处理、隐私保护和公平性。文章详细介绍了每种类别的最新方法，并讨论了这些方法在解决联邦学习中的关键问题方面的进展。\n\n在聚合优化方面，文章讨论了多种聚合算法，如FedAvg、FedMA和FedProx等，这些算法通过不同的方式结合本地模型以生成更优的全局模型。文章还介绍了基于特征对齐的方法，如Fed^2，它通过特征级对齐来提高聚合效果。\n\n在处理异构性方面，文章探讨了数据异构性、模型异构性和系统异构性的问题，并介绍了多任务学习、元学习、迁移学习和聚类等方法来解决这些问题。这些方法通过不同的机制来适应不同客户端之间的差异，从而提高联邦学习的性能。\n\n在隐私保护方面，文章总结了针对联邦学习的攻击方法，如后门攻击、梯度攻击和模型投毒攻击，并介绍了差分隐私（DP）、同态加密（HE）和可信执行环境（TEE）等防御技术。这些技术通过加密或混淆模型参数来防止隐私泄露。\n\n在公平性方面，文章讨论了联邦学习中的不公平现象，如客户端选择偏差、模型优化中的歧视问题以及激励分配不公等，并介绍了多种方法来实现公平的联邦学习，包括设计最小化优化策略和样本重加权方法。\n\n文章还概述了一些流行的联邦学习框架，如FedLab、Flower、FedML、FATE和FedScale，并介绍了它们的特点。这些框架提供了不同的功能和接口，以支持联邦学习的实现和部署。\n\n最后，文章讨论了当前联邦学习方法的一些潜在不足，并提出了未来的研究方向，包括动态联邦学习、去中心化联邦学习、联邦学习的可扩展性以及统一基准的建立。这些方向旨在解决联邦学习在实际应用中面临的挑战，并推动该领域的进一步发展。",
    summary: "本文是一篇关于联邦学习（Federated Learning, FL）的系统性综述，旨在回顾近年来联邦学习领域的先进方法和应用。文章首先介绍了联邦学习的背景和重要性，强调了其在保护隐私的同时实现多方协作学习的能力。与传统的集中式学习不同，联邦学习通过交换本地训练的模型或计算的梯度，而不是直接收集数据，从而在一定程度上保护隐私。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2301.01299v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "21",
    title: "离线强化学习的模型基础方法：技术剖析与应用",
    content: "###离线强化学习的模型基础方法：技术剖析与应用\n\n本文对离线基于模型的强化学习（Offline Model-Based Reinforcement Learning，OMBRL）进行了综述。随着模型在监督学习中的应用，基于模型的方法在离线强化学习中逐渐获得了越来越高的关注，尤其是在实际应用中具有较大的潜力，因为这些方法能够充分利用历史数据集。离线强化学习是一种完全基于历史数据进行决策学习的方式，与在线强化学习侧重探索不同，离线强化学习致力于最大化历史数据的利用效率。然而，离线强化学习面临着分布偏移的问题，即数据收集策略与实际部署环境之间的状态和动作分布存在差异。由于缺乏探索，模型无法自我纠正，这使得在历史数据分布下优化的策略可能在实际环境中表现不佳。\n\n基于模型的强化学习方法则通过建立环境模型来近似状态转移动态和奖励函数，利用这些模型进行环境仿真和策略优化。为了提升策略的性能，模型需要尽可能准确地表征实际环境，但模型学习中常面临水平积累误差的问题，因此有研究提出通过约束模型来减小该误差，例如引入Lipschitz连续性约束或对抗性学习等方法。离线基于模型的强化学习方法结合了离线强化学习和基于模型的强化学习的优势，能够在充分利用历史数据的同时，尝试通过模型进行环境仿真和规划。然而，由于离线强化学习的固有性质，分布偏移问题在此类方法中仍然存在，且不像在线方法可以通过探索来进行自我修正。为了解决这个问题，许多研究提出了在模型学习过程中引入约束，如对状态转移动态、奖励函数或价值函数进行修改。\n\n针对分布偏移问题，许多方法提出了不同的解决方案。例如，Kidambi等人提出了MOReL框架，通过对不确定奖励进行惩罚，结合传统的规划方法和修改后的奖励函数与状态转移动态，来应对不确定性和分布偏移问题。Yu等人则提出了MOPO，通过引入模型集群的多变量高斯分布，来估算模型的过渡动态并进行不确定性估计。Matsushima等人提出了BREMEN方法，通过学习模型集群和行为正则化来应对分布偏移，而Rashidinejad等人则提出了VI-LCB方法，通过增加惩罚函数来调整值函数，解决了状态-动作对覆盖不足的问题。\n\n尽管现有研究已提出多种方法来应对分布偏移问题，但尚未有任何方法能提供理论上可证明的、与数据无关的绝对性能保证。未来的研究方向可能集中在提高离线基于模型强化学习算法的绝对相对性能上，特别是在现实世界应用中，许多理论方法因无法在实际部署中优于现有策略而难以应用。因此，改进学习到的策略的相对表现，将大大促进新型离线基于模型强化学习算法的实际部署。",
    summary: "本文对离线基于模型的强化学习（Offline Model-Based Reinforcement Learning，OMBRL）进行了综述。随着模型在监督学习中的应用，基于模型的方法在离线强化学习中逐渐获得了越来越高的关注，尤其是在实际应用中具有较大的潜力，因为这些方法能够充分利用历史数据集。离线强化学习是一种完全基于历史数据进行决策学习的方式，与在线强化学习侧重探索不同，离线强化学习致力于最大化历史数据的利用效率。然而，离线强化学习面临着分布偏移的问题，即数据收集策略与实际部署环境之间的状态和动作分布存在差异。由于缺乏探索，模型无法自我纠正，这使得在历史数据分布下优化的策略可能在实际环境中表现不佳。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2305.03360v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "22",
    title: "多模态分类任务中的主动学习：如何避免偏向主导模态",
    content: "###多模态分类任务中的主动学习：如何避免偏向主导模态\n\n本文研究了多模态分类任务中的主动学习问题。多模态网络因参数空间大，训练需大量数据，而主动学习可降低数据标注成本，但现有主动学习策略多针对单模态任务，应用于多模态数据时易偏向主导模态，导致样本选择不公平，影响多模态学习平衡性。为此，文章提出三条设计更平衡多模态主动学习策略的指导原则，并基于此提出一种新方法，通过调节模态间优势程度对梯度嵌入进行调制，以实现更公平的数据选择。实验表明，该方法避免了从主导模态贪婪选择样本，比现有主动学习策略在多模态分类任务上表现更好，凸显了多模态主动学习中平衡样本选择的重要性，并为实现更平衡的多模态分类主动学习提供了实用解决方案。\n\n文章首先探讨现有主动学习策略在选择多模态数据时的表现，发现这些策略倾向于关注主导模态，例如在图像-文本分类任务中，若文本对模型优化贡献更大，主动学习策略可能偏向选择有价值的文本样本，忽略图像样本的信息性，使多模态数据集不平衡，可能导致图像模型性能下降。而平衡各模态的训练和优化是成功多模态学习的关键，因此设计能公平选择多模态数据的主动学习策略至关重要。\n\n接着，文章介绍了多模态学习框架及分类任务，输入数据来自不同模态，经编码器提取特征后，通过连接等融合机制构建多模态特征，再输入分类器进行分类。还分析了现有主动学习算法如BADGE在多模态数据样本选择上的不平衡性，指出其难以确定各模态不确定性来源，且无法区分模态贡献，在多模态学习设置中存在局限性。\n\n为解决这些问题，文章提出平衡多模态主动学习方法（BMMAL），避免偏向主导模态，减少模态竞争，防止多模态网络退化为单一主导模态。同时，强调该方法需避免过度偏向较弱模态，以免损害多模态分类性能。文章还提出了三条设计主动学习策略的指导原则，以更公平地对待各模态，包括优先选择多模态贡献更平衡的样本、减少主导模态和较弱模态数据样本平均获取分数的差距，以及确保数据选择与模型结果的模态贡献成比例。\n\n在实验部分，文章使用了Food101、KineticsSound和VGGSound三个多模态数据集，分别进行图像-文本分类和视频分类任务。与七种现有主动学习策略相比，BMMAL在多模态和单模态分类准确率上表现更好，能更公平地选择多模态数据，实现弱模态和强模态之间的更好权衡。此外，文章还进行了不同融合机制的实验，如将连接替换为求和，以及在VGGSound数据集上使用NL-gate混合视频和音频特征，结果表明BMMAL在平衡弱模态和强模态方面仍具有有效性。在大规模主动学习实验中，BMMAL在大规模多模态视频分类上表现优于BADGE，可节省大量标签。最后，文章还展示了KineticsSound数据集上按类别划分的性能比较，发现BMMAL在视觉信息丰富的类别上改进更显著，而在主要由听觉模态主导的类别上有所下降，表明BMMAL避免了偏向听觉模态，更多地关注较弱的视觉模态。\n\n文章最后指出，计算每个单模态特征的Shapley值需要进行2^M次推理，其中M是模态的数量。在双模态学习情况下，需要进行四次不同组合的单模态特征推理以获得Shapley值，这是可以接受的。然后，给定N个未标记样本的梯度嵌入，BMMAL的采样时间复杂度为O(NBDK)，其中B是每个主动学习轮次的查询预算，D是最后一层线性分类器的权重矩阵大小，K是类别数量。",
    summary: "本文研究了多模态分类任务中的主动学习问题。多模态网络因参数空间大，训练需大量数据，而主动学习可降低数据标注成本，但现有主动学习策略多针对单模态任务，应用于多模态数据时易偏向主导模态，导致样本选择不公平，影响多模态学习平衡性。为此，文章提出三条设计更平衡多模态主动学习策略的指导原则，并基于此提出一种新方法，通过调节模态间优势程度对梯度嵌入进行调制，以实现更公平的数据选择。实验表明，该方法避免了从主导模态贪婪选择样本，比现有主动学习策略在多模态分类任务上表现更好，凸显了多模态主动学习中平衡样本选择的重要性，并为实现更平衡的多模态分类主动学习提供了实用解决方案。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2306.08306v2/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "23",
    title: "离线强化学习：为推荐系统带来高效数据利用的技术",
    content: "###离线强化学习：为推荐系统带来高效数据利用的技术\n\n在推荐系统中，强化学习（RL）作为一种强大的工具，能够模拟用户兴趣的动态变化，近年来受到了越来越多的关注。然而，强化学习的主要缺点之一是数据效率较低，这源于其互动性特征。强化学习驱动的推荐系统需要昂贵的在线交互来收集足够的轨迹数据，这些数据对于代理学习用户偏好至关重要。这种低效性使得基于强化学习的推荐系统成为一个庞大的工程，迫切需要寻找解决方案。近期，离线强化学习（Offline RL）的进展提供了一种新的视角。离线强化学习使得代理能够从离线数据集中获取经验，并将学习到的策略应用于在线环境中。由于推荐系统通常拥有大量的离线数据集，离线强化学习框架与之非常契合。尽管这一领域仍在初步发展，但基于离线强化学习的推荐系统研究仍然较为有限。本文旨在介绍并深入探讨离线强化学习在推荐系统中的应用，全面回顾该领域的现有文献，并突出其面临的挑战、机遇和未来研究方向。\n\n近年来，推荐技术取得了显著进展，超越了传统的协同过滤、基于内容的推荐和矩阵分解等方法。这些进展促进了基于深度学习的推荐方法的出现，深度学习能够捕捉用户与物品之间复杂的非线性关系，并有效处理图像和文本等多模态数据。因此，深度学习在推荐系统中展现出了巨大的潜力，尤其在面对复杂任务和数据结构时表现尤为突出。传统的推荐系统在捕捉用户兴趣的动态变化方面存在局限，特别是在区分长期兴趣和短期兴趣方面，传统方法更多地依赖历史数据和模式来建模长期兴趣，而对于短期兴趣的敏感度较低。这种不足可能导致推荐系统与用户当前需求不匹配，而深度强化学习（RL）则通过结合深度学习与强化学习的方法，可以使代理从用户的实时反馈中学习，进而在动态环境中捕捉到用户偏好的变化。\n\n强化学习为获得基于学习的控制策略提供了一个结构化的数学框架，通过强化学习，代理可以系统性地获取有效的行为策略，并通过奖励函数来优化这些策略。然而，强化学习方法的广泛应用面临一个显著的挑战，那就是其数据收集的增量学习过程。在此过程中，代理通过与环境的交互不断收集知识，并根据先前的经验进行调整。虽然这种迭代学习方法在许多场景下有效，但在一些实际应用中，如机器人、教育软件或医疗干预等场合，这种方法的高昂成本和潜在风险使其难以推广。因此，传统的强化学习在线交互方法在许多领域并不适用，尤其是在推荐系统中，历史数据常常是决策的重要依据，这种需求凸显了离线强化学习的潜力。与在线强化学习相比，离线强化学习通过使用现有的数据进行学习，无需通过实时交互收集更多数据，从而降低了风险和成本。\n\n在推荐系统和广告领域，离线强化学习的应用尤为合适，因为这些领域的数据收集相对简单且高效，用户的行为可以被记录下来作为数据基础。此外，推荐系统中已有的大量数据集也可以用来训练离线强化学习模型。然而，这些领域对决策的质量要求较高，错误的决策可能会带来巨大的财务损失，因此传统的在线探索方法在这些领域并不适用。为了应对这一挑战，离线强化学习方法已经在这些领域得到了应用，尤其是通过离线策略评估技术，如A/B测试，能够在不进行进一步交互的情况下估计广告和推荐方法的效果。\n\n离线强化学习在推荐系统中的实际应用包括优化页面上的推荐内容、改善整个网页的布局以及通过双重稳健估计来估算网站访问量。此外，A/B测试也是优化点击率的常用手段，研究人员还使用离线数据来学习优化策略，如提高新闻文章的点击率、优化搜索页面广告的排名和为数字营销定制广告推荐。\n\n本文旨在全面回顾离线强化学习在推荐系统中的应用（离线RL4RS），并探讨当前研究中的挑战与未来发展方向。文章结构包括强化学习基础介绍、离线强化学习的定义及其在推荐系统中的应用、离线强化学习的相关研究进展以及未来研究的挑战和机遇。通过对现有工作的综述，本文为未来在离线强化学习领域的研究提供了指导意见。",
    summary: "在推荐系统中，强化学习（RL）作为一种强大的工具，能够模拟用户兴趣的动态变化，近年来受到了越来越多的关注。然而，强化学习的主要缺点之一是数据效率较低，这源于其互动性特征。强化学习驱动的推荐系统需要昂贵的在线交互来收集足够的轨迹数据，这些数据对于代理学习用户偏好至关重要。这种低效性使得基于强化学习的推荐系统成为一个庞大的工程，迫切需要寻找解决方案。近期，离线强化学习（Offline RL）的进展提供了一种新的视角。离线强化学习使得代理能够从离线数据集中获取经验，并将学习到的策略应用于在线环境中。由于推荐系统通常拥有大量的离线数据集，离线强化学习框架与之非常契合。尽管这一领域仍在初步发展，但基于离线强化学习的推荐系统研究仍然较为有限。本文旨在介绍并深入探讨离线强化学习在推荐系统中的应用，全面回顾该领域的现有文献，并突出其面临的挑战、机遇和未来研究方向。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2308.11336v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "24",
    title: "多模态学习为何能革命性地加速机器学习的计算速度",
    content: "###多模态学习为何能革命性地加速机器学习的计算速度\n\n这篇文章探讨了多模态学习的计算优势。文章指出，人类感知本质上是多模态的，机器学习也应该如此。尽管多模态学习在实践中取得了显著的成功，但其理论基础一直不够完善。最近的一项研究显示，多模态学习在样本复杂度上优于单模态学习，但多模态学习是否在计算上也具有优势仍然是一个未解决的问题。\n\n文章的核心贡献是证明了在某些条件下，多模态学习在计算上可以指数级地优于单模态学习。具体来说，文章提出了一个学习任务，对于单模态学习来说是NP-hard问题，但对于多模态算法来说可以在多项式时间内解决。这个任务是基于两个半空间交集问题的一个新变体。\n\n文章首先介绍了多模态学习的背景和重要性，强调了多模态学习在人类认知中的作用，尤其是在儿童时期，多模态学习有助于统一不同的符号，为成年后的全面认知打下基础。在机器学习中，多模态学习涉及在各种模态上收集大量训练数据，然后部署训练好的模型来处理新的单模态任务。文章提到，多模态模型通常能够超越经过微调的单模态模型，即使在新的单模态数据上进行评估也是如此。\n\n尽管多模态学习在实践中取得了成功，但其理论解释仍然相对较少。因此，建立一个坚实的理论基础变得至关重要。文章引用了最近的一项研究，该研究展示了多模态学习在统计上的优势，尤其是在数据表现出连接性和异质性时。然而，文章提出了一个问题：多模态学习是否也具有计算上的优势？\n\n文章通过构建一个特定的学习任务来回答这个问题。这个任务是基于两个半空间交集问题的，对于单模态学习来说是NP-hard问题，但对于多模态算法来说可以在多项式时间内解决。这个结果表明，多模态学习在计算上可能具有指数级的优势。\n\n文章还讨论了多模态学习的理论基础，指出尽管多模态学习在实践中取得了成功，但理论基础一直不够完善。文章提到了一些现有的理论成果，这些成果大多基于特定的假设和上下文。文章还提到了最近的一项工作，该工作提出了一个多模态学习的广泛理论，证明了多模态学习在泛化误差上优于单模态学习。\n\n在实验部分，文章详细描述了多模态学习的设置和两个半空间交集问题的背景。文章提出了一个多模态学习问题，其中两个单模态学习问题都是NP-hard问题，但多模态学习问题可以高效地解决。文章通过三个步骤构建了这个问题：首先，将一个单模态学习问题设置为两个半空间交集问题；其次，基于这个单模态学习问题，构建一个双射映射，得到一个新的NP-hard问题；最后，设计一个特殊的双射映射，使得多模态问题可以高效地解决。\n\n文章的主要结果是定理4，该定理表明存在一个多模态学习问题，可以在多项式时间内进行PAC学习，而两个单模态学习问题都是NP-hard问题，即使存在一个双射映射。这个定理证明了多模态学习在计算上可以指数级地优于单模态学习。\n\n文章还讨论了多模态学习的统计优势，指出连接性和异质性是导致多模态学习统计优势的两个关键因素。文章通过构建一个特定的学习任务，展示了多模态学习在计算上的优势，并补充了统计上的保证。\n\n最后，文章总结了其贡献，并指出了其局限性。文章认为，尽管文章提供了一个多模态学习的高效学习方案，但这个方案只适用于一个狭窄、精心设计的问题类别。文章提出了两个未来研究方向：是否可以为更自然的学习问题展示计算上的分离，以及是否可以获得多模态学习计算优势的一般充分条件。",
    summary: "这篇文章探讨了多模态学习的计算优势。文章指出，人类感知本质上是多模态的，机器学习也应该如此。尽管多模态学习在实践中取得了显著的成功，但其理论基础一直不够完善。最近的一项研究显示，多模态学习在样本复杂度上优于单模态学习，但多模态学习是否在计算上也具有优势仍然是一个未解决的问题。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2309.13782v2/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "25",
    title: "基于LoRA的连续可控提示工程：ControlPE方法",
    content: "###基于LoRA的连续可控提示工程：ControlPE方法\n\n这篇论文提出了一种名为ControlPE（Continuously Controllable Prompt Engineering）的新方法，旨在通过精细调控提示词（prompt）对大语言模型（LLM）输出的影响，从而提高对模型行为的控制精度。随着大语言模型在工业界和学术界的广泛应用，如何有效地设计和控制提示词已成为一个重要研究方向。传统的提示词工程方法往往依赖于离散的提示词进行调节，但这些方法在实际应用中存在局限性，特别是在对输出进行更细粒度控制时表现不佳。ControlPE方法的提出填补了这一空白，它通过引入LoRA（Low-Rank Adaptation）技术，在不显著增加计算开销的情况下，对提示词的效果进行微调，实现了对模型输出的连续控制。\n\n论文介绍了ControlPE的三步实现过程：首先，生成目标提示词的蒸馏数据集；其次，使用LoRA模型对数据集进行训练，将提示词的影响力蒸馏到LoRA参数中；最后，通过调节LoRA模型的合并权重来实现对提示词影响力的细致控制。这种方法不仅能够控制短答案、拒绝回答等常见任务的提示效果，还能在更复杂的任务中发挥作用，如链式推理等。实验结果表明，ControlPE能够在各种情境下有效控制模型的输出，提升自然语言处理中的模型定制化能力。\n\n此外，论文还对比了现有的一些提示词工程方法，如APE和PromptAgent，指出这些方法在寻找提示词与非提示词之间的平衡时存在困难。而ControlPE的创新之处在于，它能够在连续空间中精确调节提示词的影响，实现比传统离散提示词方法更灵活和细致的控制。特别是在应对一些需要动态调整输出长度或者根据文档回答问题的任务时，ControlPE表现出了良好的效果。\n\n通过这一研究，ControlPE为大语言模型的提示词工程提供了一种新的思路，尤其是在面对需要精准控制模型响应的任务时，它能够提供更加灵活和可调节的解决方案。这一方法不仅能改进现有的模型输出控制技术，还为未来更多复杂应用场景中的模型行为调节提供了新的可能性。",
    summary: "这篇论文提出了一种名为ControlPE（Continuously Controllable Prompt Engineering）的新方法，旨在通过精细调控提示词（prompt）对大语言模型（LLM）输出的影响，从而提高对模型行为的控制精度。随着大语言模型在工业界和学术界的广泛应用，如何有效地设计和控制提示词已成为一个重要研究方向。传统的提示词工程方法往往依赖于离散的提示词进行调节，但这些方法在实际应用中存在局限性，特别是在对输出进行更细粒度控制时表现不佳。ControlPE方法的提出填补了这一空白，它通过引入LoRA（Low-Rank Adaptation）技术，在不显著增加计算开销的情况下，对提示词的效果进行微调，实现了对模型输出的连续控制。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2311.09773v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "26",
    title: "AI赋能社会科学",
    content: "###AI赋能社会科学\n\n这篇论文系统性地探讨了人工智能与社会科学交叉领域的最新进展，提出了一个新颖的双重视角框架——\"AI赋能社会科学\"和\"AI的社会科学\"。在AI赋能社会科学方向，研究展示了以ChatGPT为代表的大语言模型如何革新社会科学研究流程：从文献综述的智能摘要（如Elicit平台实现非精确关键词匹配的文献推荐），到假设生成环节通过对抗对话生成创新性研究命题（如Park等人利用GPT-4提出可验证假设），再到实验研究中构建虚拟社区模拟人类行为（如Park团队用25个智能体模拟小镇社交活动）。值得注意的是，这些应用在提升研究效率5-10倍的同时，也面临生成内容可靠性、语境窗口限制等挑战。而在AI的社会科学方向，论文揭示了智能体展现的独特社会特性：心理学实验显示GPT-3具有类似年轻女性的五维人格特征（Hexaco问卷测得），但存在价值取向矛盾；微观经济学研究发现智能体在最后通牒游戏中能模仿人类公平偏好，却缺乏基于条件互惠的行为调整能力；政治学分析则发现ChatGPT存在系统性左翼倾向（在巴西支持卢拉，在美国倾向民主党）。研究特别指出，随着多智能体社区（如Chirper.ai）的出现，AI群体开始涌现出自发派对邀请等类人社交行为，这为传统社会学理论提供了新的验证场景。论文最后对比了SkyAGI、AgentVerse等6大仿真平台，指出当前智能体在物理环境交互、社会背景模拟等方面仍存在局限，未来需要融合认知理论框架和跨模态能力来提升仿真真实性。该研究为把握AI与社会科学的融合趋势提供了全景式路线图，当AI智能体日益渗透日常生活时，这种交叉研究将展现出更重要的学术价值。",
    summary: "这篇论文系统性地探讨了人工智能与社会科学交叉领域的最新进展，提出了一个新颖的双重视角框架——\"AI赋能社会科学\"和\"AI的社会科学\"。在AI赋能社会科学方向，研究展示了以ChatGPT为代表的大语言模型如何革新社会科学研究流程：从文献综述的智能摘要（如Elicit平台实现非精确关键词匹配的文献推荐），到假设生成环节通过对抗对话生成创新性研究命题（如Park等人利用GPT-4提出可验证假设），再到实验研究中构建虚拟社区模拟人类行为（如Park团队用25个智能体模拟小镇社交活动）。值得注意的是，这些应用在提升研究效率5-10倍的同时，也面临生成内容可靠性、语境窗口限制等挑战。而在AI的社会科学方向，论文揭示了智能体展现的独特社会特性：心理学实验显示GPT-3具有类似年轻女性的五维人格特征（Hexaco问卷测得），但存在价值取向矛盾；微观经济学研究发现智能体在最后通牒游戏中能模仿人类公平偏好，却缺乏基于条件互惠的行为调整能力；政治学分析则发现ChatGPT存在系统性左翼倾向（在巴西支持卢拉，在美国倾向民主党）。研究特别指出，随着多智能体社区（如Chirper.ai）的出现，AI群体开始涌现出自发派对邀请等类人社交行为，这为传统社会学理论提供了新的验证场景。论文最后对比了SkyAGI、AgentVerse等6大仿真平台，指出当前智能体在物理环境交互、社会背景模拟等方面仍存在局限，未来需要融合认知理论框架和跨模态能力来提升仿真真实性。该研究为把握AI与社会科学的融合趋势提供了全景式路线图，当AI智能体日益渗透日常生活时，这种交叉研究将展现出更重要的学术价值。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2401.11839v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "27",
    title: "联邦学习中的隐私风险：你知道多少？",
    content: "###联邦学习中的隐私风险：你知道多少？\n\n\n这篇文章是一篇关于联邦学习中隐私威胁及对策的综述。文章指出，尽管联邦学习被认为是一种注重隐私的机器学习方法，因为其在训练过程中不会直接交换原始数据，但仍存在隐私泄露的风险。文章详细分析了三种典型的联邦学习类型：水平联邦学习（HFL）、垂直联邦学习（VFL）和联邦迁移学习（FTL），并针对每种类型分别探讨了隐私威胁和相应的对策。\n\n文章首先介绍了联邦学习的背景和重要性。随着计算设备的普及，人们在日常生活中产生了大量的数据。集中存储这些数据不仅成本高、耗时，还涉及用户隐私和保密问题。敏感数据如生物特征和医疗保健信息可能被用于针对性的社交广告和推荐，带来即时或潜在的隐私风险。因此，直接共享数据而不考虑隐私是不可取的。随着社会对隐私的关注度不断提高，相关的法律法规如欧盟的《通用数据保护条例》（GDPR）和《人工智能法案》也在不断出台，使得数据聚合的做法变得越来越不可行。在这种背景下，联邦学习作为一种有前景的机器学习技术应运而生，它允许每个客户端学习并发送信息到服务器，而无需直接交换私有原始数据。\n\n文章接着详细描述了三种联邦学习类型的学习和预测方法。水平联邦学习（HFL）是最常见的联邦学习类别，由谷歌首次提出，其目标是让持有不同样本的每个客户端协作提高模型的准确性。垂直联邦学习（VFL）则允许持有相同样本但不同特征的客户端协作训练一个模型。联邦迁移学习（FTL）适用于两个客户端在样本和特征上都只有小部分重叠的情况，其目标是将拥有标签的客户端（源客户端）的知识转移到没有标签的客户端（目标客户端）。\n\n文章详细分析了每种联邦学习类型中的隐私威胁。在水平联邦学习中，客户端数据是隐私的主要威胁。可能的攻击者包括服务器、客户端和第三方。服务器可能会对客户端发送的模型进行推理攻击以推断客户端数据；客户端可能会对从服务器接收的全局模型进行推理攻击以推断其他客户端的数据；第三方可能会窃听通过通信通道传输的模型，并通过推理攻击推断客户端数据。在垂直联邦学习中，隐私的主要威胁是通过客户端之间的身份匹配导致的身份泄露。此外，客户端发送给服务器的中间输出也可能导致客户端数据被推断。在联邦迁移学习中，隐私威胁取决于客户端之间共享的信息。当特征共享时，隐私威胁是特征类比网络和预测网络的交换；当ID共享时，隐私威胁是身份泄露和客户端之间为特征相似性所需的信息。\n\n文章最后讨论了针对每种联邦学习类型中隐私威胁的对策。这些对策包括差分隐私、安全计算、通信加密和ID伪装。差分隐私通过在模型中添加噪声来减少训练数据的泄露；安全计算用于在客户端和服务器之间保密模型计算过程；通信加密用于防止信息泄露给第三方；ID伪装用于防止ID泄露。文章通过详细的分析和讨论，为联邦学习中的隐私保护提供了全面的视角和实用的解决方案。",
    summary: "这篇文章是一篇关于联邦学习中隐私威胁及对策的综述。文章指出，尽管联邦学习被认为是一种注重隐私的机器学习方法，因为其在训练过程中不会直接交换原始数据，但仍存在隐私泄露的风险。文章详细分析了三种典型的联邦学习类型：水平联邦学习（HFL）、垂直联邦学习（VFL）和联邦迁移学习（FTL），并针对每种类型分别探讨了隐私威胁和相应的对策。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2402.00342v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "28",
    title: "提示窃取攻击：一种针对大型语言模型的攻击方法与防御策略研究",
    content: "###提示窃取攻击：一种针对大型语言模型的攻击方法与防御策略研究\n\n本文提出了一种新的针对大型语言模型（LLMs）的攻击方法，称为“提示窃取攻击”（prompt stealing attacks）。随着诸如ChatGPT等LLMs在多个领域的广泛应用，“提示工程”（prompt engineering）成为提升模型输出质量的关键技术。为了获得高质量的回答，许多公司和个人都投入了大量的资源来优化提示设计，因此在这个背景下，攻击者可能会试图窃取这些精心设计的提示，以便达到相同的效果。\n\n提示窃取攻击的目标是根据LLM生成的回答，推测出原始提示内容。攻击方法主要包括两个模块：参数提取器（parameter extractor）和提示重建器（prompt reconstructor）。参数提取器通过分析生成的回答，识别出提示的类型，并进一步提取出提示中使用的角色或上下文等细节信息。具体来说，提示可以分为三类：直接提示（direct prompt）、基于角色的提示（role-based prompt）和基于上下文的提示（in-context prompt）。参数提取器的任务是通过回答来判断原始提示的类型，并提取出相关参数。\n\n在参数提取完成后，提示重建器则根据提取到的信息重新构造出与原始提示相似的反向提示。对于直接提示，重建器直接生成与原始提示相似的内容；对于基于角色的提示，重建器会加入提示中的角色信息；对于基于上下文的提示，重建器会根据提取到的上下文信息生成类似的问题和答案。这一过程的核心目的是通过重建的提示生成与原始提示相似的回答，从而达到“窃取”原始提示的效果。\n\n实验结果表明，所提出的攻击方法在多个基准数据集上表现出色。参数提取器能够准确地预测提示的类型，重建出的反向提示与原始提示在语义和结构上具有较高的相似性，且生成的回答也能与原始回答高度一致。此外，针对这种攻击，文章还提出了两种防御策略，分别是通过修改提示内容和调整生成的回答来减少攻击效果，实验结果表明，这些防御方法虽然能够在一定程度上降低攻击的相似度，但仍无法完全防止提示窃取攻击。\n\n本文的研究为LLM的安全性问题提供了新的视角，揭示了提示工程背后的潜在风险，并呼吁更多的关注和研究，以确保LLM系统的安全性与鲁棒性。",
    summary: "本文提出了一种新的针对大型语言模型（LLMs）的攻击方法，称为“提示窃取攻击”（prompt stealing attacks）。随着诸如ChatGPT等LLMs在多个领域的广泛应用，“提示工程”（prompt engineering）成为提升模型输出质量的关键技术。为了获得高质量的回答，许多公司和个人都投入了大量的资源来优化提示设计，因此在这个背景下，攻击者可能会试图窃取这些精心设计的提示，以便达到相同的效果。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2402.12959v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "29",
    title: "探索企业中的提示工程实践",
    content: "### 探索企业中的提示工程实践\n\n随着大型语言模型（LLMs）在企业中的广泛应用，提示工程逐渐成为开发高效 AI 应用的关键环节。提示工程是指通过自然语言指令来引导 LLMs 产生特定行为或输出的过程。尽管理论上任何人都可以通过自然语言与 LLMs 交互，但在实际应用中，尤其是对于复杂任务，设计有效的提示并非易事。这不仅需要技能和知识，还需要大量的迭代来确定模型行为并引导其完成特定目标。本文通过分析企业环境中用户对提示的编辑行为，揭示了提示工程的实践模式，并探讨了如何通过工具支持来提高提示工程的效率。\n\n研究背景部分指出，随着指令调整和对齐技术的出现，提示成为与 LLMs 交互的主要方式。在企业中，开发者和 AI 实践者尝试开发提示以自动化各种复杂程度的知识任务，以提高组织效率并挖掘 AI 的价值。这些任务包括文档或转录的总结、代码或其他结构化输出的生成，以及基于内容的问答等。无论任务如何，提示通常包含各种组件，有时嵌入示例，并且需要符合特定要求和高准确度的输出。此外，企业环境可能需要使用更具挑战性的模型，原因包括成本和专业化。目前尚不清楚 LLM 是否能够执行给定任务，开发和优化提示需要大量努力来找出答案。尽管已有研究开始探索提示工程行为和提示结构，但提示工程仍是一个新兴领域，不同情境和领域的行为可能有所不同，目前对企业中从业者随时间编辑提示的情况了解甚少。\n\n为了深入了解提示工程实践，研究者从一个企业级 LLM 提示环境中收集了大量交互数据。这些数据记录了用户对一组托管 LLM 的提示应用情况，使研究者能够研究随时间发生的编辑和优化过程。通过对 57 名用户的提示会话样本进行定性分析，研究者记录了用户编辑的提示部分、应用的编辑类型，以及是否撤销或重做编辑。\n\n研究结果表明，提示编辑会话主要由提示编辑和模型切换组成。最常编辑的提示部分是上下文，其次是任务指令和标签，最常见的编辑类型是修改，即保持原意不变。此外，研究还发现，用户在编辑提示时，往往会同时进行多项编辑，这可能会影响对特定编辑效果的跟踪。大约 11% 的提示编辑撤销或重做了之前的编辑，这可能表明用户在记忆先前尝试的结果或确定哪些编辑可能改善输出方面存在挑战。研究还观察到，大多数分析的提示/用例都是基于上下文的，即输入、基础数据或示例嵌入在提示中，与任务指令分开。上下文是最常编辑的组件，反映了其在企业任务中的重要性。常见的上下文添加模式包括模拟对话和添加示例。用户还会替换和修改现有上下文，以开发和优化任务指令，然后通过切换不同的上下文来评估指令的鲁棒性。\n\n研究的贡献在于提供了企业环境中不同用例的提示编辑实践的大规模分析以及相应的设计启示。研究结果揭示了用户在提示工程中的实际模式及其普遍性，为理解用户如何与 LLMs 交互以及如何改进提示工程工具提供了重要见解。尽管研究数据匿名，限制了对用户先前提示经验或其在组织中角色的了解，但研究样本涵盖了广泛的用户技能和背景，有助于缓解这一限制。未来的研究可以进一步探索如何通过更结构化的提示、提示历史记录与比较、变体创作、半自动探索变体以及基于用例特定指标的内置提示质量评估等方式来支持提示工程，以提高企业中 LLM 应用的效率和效果。",
    summary: "随着大型语言模型（LLMs）在企业中的广泛应用，提示工程逐渐成为开发高效 AI 应用的关键环节。提示工程是指通过自然语言指令来引导 LLMs 产生特定行为或输出的过程。尽管理论上任何人都可以通过自然语言与 LLMs 交互，但在实际应用中，尤其是对于复杂任务，设计有效的提示并非易事。这不仅需要技能和知识，还需要大量的迭代来确定模型行为并引导其完成特定目标。本文通过分析企业环境中用户对提示的编辑行为，揭示了提示工程的实践模式，并探讨了如何通过工具支持来提高提示工程的效率。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2403.08950v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "30",
    title: "未命名论文",
    content: "离线轨迹泛化：强化离线强化学习的能力\n\n离线强化学习（Offline RL）旨在从静态数据集中的预先收集的轨迹学习策略。然而，现有的离线RL方法通常存在两大问题：一是对未见状态的泛化能力差，二是通过模型生成的模拟数据对策略改进贡献有限。为了解决这些问题，本文提出了一种基于世界转换器（World Transformers）的离线轨迹泛化方法（OTTO）。OTTO通过利用世界转换器预测状态动态和即时奖励，结合四种策略生成高奖励的轨迹模拟，最终通过结合离线数据和模拟数据来训练离线强化学习算法。\n\nOTTO通过模拟轨迹的生成来解决模型生成数据质量低、无法有效改进策略的问题。传统的离线RL方法，尤其是基于模型的方法，在生成长时间跨度的轨迹时，通常会遇到奖励逐渐降低的问题，这导致策略学习效果显著下降。而OTTO通过世界转换器在生成长轨迹时引入噪声扰动，确保了生成的轨迹不仅具有高奖励，而且能够显著提升模型的泛化能力。\n\nOTTO作为一个插件模块，能够与现有的离线RL方法结合使用，增强策略学习的效果。通过在D4RL基准数据集上进行的广泛实验，OTTO显示出显著优于当前最先进的离线RL方法，能够有效提升策略的泛化能力。\n\n该方法的核心创新在于引入了Transformer架构，尤其是其在自然语言处理和计算机视觉中的成功应用，来模拟环境动态并生成高质量的模拟轨迹。世界转换器的应用使得离线RL能够更好地应对从未见过的状态，同时避免了传统方法中因过度保守而造成的策略性能下降问题。\n\nOTTO不仅解决了传统方法中由于低质量模拟数据造成的训练瓶颈，还为未来的离线强化学习模型提供了新的思路，进一步推动了该领域的研究进展。",
    summary: "离线轨迹泛化：强化离线强化学习的能力",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2404.10393v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "31",
    title: "人机共舞：探索人机编舞的交互设计",
    content: "### 人机共舞：探索人机编舞的交互设计\n\n人机共创旨在结合人类与 AI 的优势，创造出超越个体能力的艺术成果。在绘画、音乐和诗歌等领域，人机共创的框架已经存在，但编舞的具身性要求专门的方法。本文探讨了 AI 辅助编舞技术（例如生成创意、具身即兴）并分析了人机交互设计——人类和 AI 如何协作和沟通——以指导未来人机编舞共创系统的设计考虑。\n\n人机共创是一个协作过程，人类和 AI 作为伙伴共同创造创新的解决方案、艺术作品或其他创造性成果。这个过程高度依赖于互动动态、每个参与者的角色以及所采用的沟通风格。精心设计这些元素对于最大化人机共创系统的效率和创造力至关重要。本文基于已有的人机共创交互设计框架，专注于一个相对未被探索的领域：与 AI 的编舞共创。这个本质上具身化且高度创造性的研究领域需要量身定制的交互设计见解，以充分发挥其潜力。\n\n文章介绍了现有的 AI 支持编舞系统和技术，并通过三个不同的设计目标来分析它们的交互设计：编舞生成、创造力支持和人机编舞共创。受 Davis 等人计算创造力研究的启发，文章根据这些目标对现有系统进行了分类，并发现了三个关键的交互设计考虑因素：促进人类和 AI 之间的并行和自发互动、为人类和 AI 分配不同但互补的角色以及确保有效的人机沟通。这些见解旨在作为设计未来系统和改进现有系统的资源，最终推动人机编舞共创的边界。\n\n在准备阶段，重点是创意构思和编舞材料的制作。然而，关于这一阶段的人机共创的研究仍然很少，因此文章讨论了为编舞生成和创造力支持而开发的技术，并探索了如何通过交互设计来促进共创性。在准备阶段，人类和 AI 通常采用轮流合作的方式，人类和 AI 轮流为同一任务或不同任务做出贡献。在相同任务的情况下，人类利用基于 AI 的技术以聚合或发散的想法生成工件。例如，现有的工作允许人类和 AI 为相同的舞蹈序列做出贡献。相反，基于 AI 的方法也可以在分离的任务中支持用户生成的概念或评估创建的工件。然而，这一分支在先前的研究中尚未得到充分探索。然而，利用有效的人类运动评估技术，例如拉班动作分析，可以增强对抽象动作的理解，并有助于编舞创作。关于主动权的时机，AI 通常在需要创意构思或评估时响应人类请求，因为先前的研究表明，用户倾向于反对 AI 在轮流互动中主动采取行动。\n\n在工作室阶段，重点是将想法转化为动作，并与其他舞者和编舞者合作。在这里，具身性在交互设计中变得至关重要。通过分析这一阶段现有共创系统的交互设计，文章揭示了当前的挑战，并提出了开放的研究问题。\n\n现有研究探索了并行合作风格，人类和 AI 共享混合主动权，为共享任务做出贡献。例如，Viewpoints AI、LuminAI 和 Robodanza 都旨在促进实时的协作舞蹈即兴创作和表演。这些系统使 AI 能够捕捉和处理人类动作，并生成由投影或机器人体现的新舞蹈动作，以补充或响应人类舞者的动作。重要的是，主动权的时机是自发的，人类和 AI 可以自由地发起和修改舞蹈姿势和动作，为不断发展的工件做出贡献。\n\n为了在工作室阶段实现无缝且沉浸式的体验，交互设计需要通过明确和隐含的方法来反映人类沟通风格。人类对 AI 的沟通可以利用直观的方法，如语音和直接操作，以及隐含的方法，如面部表情和具身线索。这与人类自然沟通的方式一致，提供了更广泛的信息交流范围。AI 可以利用语音、触觉和视觉来回应。\n\n尽管人机交互中的后果性沟通具有重要作用，但先前的研究通常忽略了这一阶段的人类对 AI 的后果性沟通和 AI 对人类的沟通的设计，尽管它们在促进具身体验方面发挥着关键作用。就像人类观察他人以理解他们的动作和意图一样，AI 需要发展类似的“心理理论”来解释人类的心理状态，超越明确的指令。在工作室阶段，信息交流频繁且主动权是自发的，仅依赖于明确沟通会阻碍 AI 作为合作者和沟通者的有效性。因此，AI 系统需要对隐含信息保持主动和敏感，以实现真正的协作。\n\n文章总结了讨论的研究，比较了它们在第 2 节和第 3 节中涵盖的设计和交互方法。这些分析得出了三个关键见解，用于设计未来的人机编舞共创系统。文章利用这些见解探索了图 1 中的交互设计空间，该空间结合了参与风格、任务分配和主动权时机等因素。\n\n先前的研究集中在工作室阶段的并行和自发的人机协作上，忽视了 AI 在整个过程中成为真正伙伴的潜力，包括准备阶段。具体来说，AI 系统通常等待轮到它们来协助头脑风暴和完善想法，通常是在人类请求时。这种轮流风格将它们定位为工具，而不是合作者，因为有效的协作依赖于自发的反馈交流，这对于成功的沟通和任务完成至关重要。因此，未来的研究可以专注于开发超越简单等待轮到自己的 AI。通过积极地参与创作过程，这些 AI 系统可以显著增强协作。想象一下，AI 提供及时的灵感、建设性的反馈，并在编舞创作过程中提出改进意见——作为独立于人类工作时的创造性输入的持续来源。这种转变将促进更动态和协作的体验。",
    summary: "人机共创旨在结合人类与 AI 的优势，创造出超越个体能力的艺术成果。在绘画、音乐和诗歌等领域，人机共创的框架已经存在，但编舞的具身性要求专门的方法。本文探讨了 AI 辅助编舞技术（例如生成创意、具身即兴）并分析了人机交互设计——人类和 AI 如何协作和沟通——以指导未来人机编舞共创系统的设计考虑。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2405.03999v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "32",
    title: "CLIP模型：一种高效的在线终身学习者",
    content: "### CLIP模型：一种高效的在线终身学习者\n\n在线终身学习（OLL）面临着从持续且非平稳的数据流中学习的挑战。现有的基于图像分类模型的在线终身学习方法通常需要预设条件，如类别总数或最大内存容量，这限制了其实现真正的永续学习，难以应用于真实世界的场景。本文提出，视觉-语言模型（如对比语言-图像预训练模型CLIP）更适合在线终身学习任务。我们发现，在CLIP模型的参数高效调优（PET）过程中，保持图像和文本之间的对称性至关重要。为此，我们提出了对称图像-文本（SIT）调优策略。通过在多个终身学习基准数据集上的实验，我们通过梯度分析阐明了SIT策略的有效性。同时，我们评估了终身学习对CLIP模型泛化能力的影响，发现调优图像编码器有助于终身学习，而调优文本编码器则促进了零-shot学习。\n\n在实际应用中，深度神经网络通常采用基于监督学习的训练范式，这在数据分布相对稳定的封闭环境中非常有效。然而，在数据流不断变化的情况下，传统的训练方式面临着严重的挑战。更糟糕的是，由于存储和隐私等限制，无法保存所有数据。直接对这种增量数据流进行训练可能导致性能显著下降，这种现象被称为灾难性遗忘（catastrophic forgetting）。为了应对不断变化的数据分布，提出了多种持续学习方法，旨在平衡模型的稳定性和可塑性。早期的持续学习方法通过将连续的数据流分割为不同的任务，使用任务标识符选择任务特定的组件进行分类，这种方法被称为任务增量学习（TIL）。随后的发展则提出了类增量学习（CIL），它解决了没有任务标识符的场景，只能根据类信息进行推断。进一步的发展是任务无关的持续学习（TACL），它在没有明确任务边界的情况下进行在线训练，允许随时进行推断。尽管这些方法取得了进展，传统的图像分类模型仍然设计为闭集场景，存在许多限制，特别是在终身学习中。它们通常需要不断修改架构以适应新数据，面临图像特征与原型不匹配的问题，或者需要预定义最大类别数量等限制。这些约束阻碍了它们适应真实世界中不断变化的环境，因为新类和新样本不断被引入，而事先无法知道这些类别的存在。因此，现有方法在真正的终身学习或永续学习上存在局限，不能在实际场景中长时间不间断地进行学习。\n\n为了解决这一问题，本文探索了CLIP模型在在线终身学习中的潜力，提出了一种超越传统模型架构和类别数量限制的分类方法。与传统分类器不同，CLIP通过将图像与文本描述进行匹配来进行分类，文本格式为“A bad photo of CLASS”。这种机制使得学习过程更加灵活，不受预定义模型结构或类别总数的限制，因此更适合实现真实的终身学习。CLIP的预训练模型具有很强的泛化能力和零-shot学习能力，使其在终身学习场景中具有显著优势。\n\n为了提升CLIP在终身学习中的表现，本文采用了参数高效调优（PET）方法，这种方法在优化模型的同时不会显著增加参数量。我们的实验表明，在调优过程中，图像和文本之间的非对称性，尤其是文本来自所有已见类别与图像来自当前时间步的数据匹配，可能导致灾难性遗忘。为此，我们提出了一种简单而有效的对称图像-文本（SIT）调优策略，确保CLIP模型的知识更新保持平衡。通过对多个终身学习基准数据集的综合实验和梯度分析，验证了SIT的有效性，表明对称调优可以有效减缓已经学习的知识丢失。此外，我们还评估了终身学习对CLIP模型泛化能力的影响，发现调优图像编码器有助于增量学习，而调优文本编码器有助于提升零-shot学习能力。这些发现强调了SIT策略在维持适应新信息和保留现有知识之间平衡的重要性，从而优化了CLIP模型在复杂和不断变化的终身学习环境中的表现。\n\n通过这些研究，本文展示了CLIP模型在真实世界的终身学习中的潜力，提出了更灵活、可扩展且更适应动态环境的学习方法。CLIP的设计使得它在数据流持续变化的环境中能够不断适应新类别的加入，克服了传统方法面临的诸多局限，具有广泛的应用前景。",
    summary: "在线终身学习（OLL）面临着从持续且非平稳的数据流中学习的挑战。现有的基于图像分类模型的在线终身学习方法通常需要预设条件，如类别总数或最大内存容量，这限制了其实现真正的永续学习，难以应用于真实世界的场景。本文提出，视觉-语言模型（如对比语言-图像预训练模型CLIP）更适合在线终身学习任务。我们发现，在CLIP模型的参数高效调优（PET）过程中，保持图像和文本之间的对称性至关重要。为此，我们提出了对称图像-文本（SIT）调优策略。通过在多个终身学习基准数据集上的实验，我们通过梯度分析阐明了SIT策略的有效性。同时，我们评估了终身学习对CLIP模型泛化能力的影响，发现调优图像编码器有助于终身学习，而调优文本编码器则促进了零-shot学习。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2405.15155v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "33",
    title: "AI对齐悖论：对齐越精准，被误导风险越高",
    content: "### AI对齐悖论：对齐越精准，被误导风险越高\n\nAI对齐领域致力于将AI系统与人类的目标、偏好和伦理原则对齐，显著提升了AI模型的输出质量、安全性和可信度。然而，本文揭示了一个根本性挑战——“AI对齐悖论”：我们越让AI模型符合我们的价值观，就越容易被对手误导偏离这些价值观。文章通过三个具体例子来说明语言模型中的AI对齐悖论：模型调整、输入调整和输出调整。模型调整是指对手操纵神经网络的高维内部状态向量，使模型对无害的提示给出不符合对齐的回应；输入调整是指对手通过精心操纵输入提示或进行长时间对话，诱导模型呈现出不符合对齐的人格；输出调整是指对手先让模型正常工作，然后使用单独的语言模型（价值编辑器）来最小限度地编辑对齐模型的输出，将其重新对齐到另一套价值观。随着AI对现实世界影响的加深，让广泛的研究人员意识到这一悖论并寻找突破方法，以确保AI造福人类，变得至关重要。",
    summary: "AI对齐领域致力于将AI系统与人类的目标、偏好和伦理原则对齐，显著提升了AI模型的输出质量、安全性和可信度。然而，本文揭示了一个根本性挑战——“AI对齐悖论”：我们越让AI模型符合我们的价值观，就越容易被对手误导偏离这些价值观。文章通过三个具体例子来说明语言模型中的AI对齐悖论：模型调整、输入调整和输出调整。模型调整是指对手操纵神经网络的高维内部状态向量，使模型对无害的提示给出不符合对齐的回应；输入调整是指对手通过精心操纵输入提示或进行长时间对话，诱导模型呈现出不符合对齐的人格；输出调整是指对手先让模型正常工作，然后使用单独的语言模型（价值编辑器）来最小限度地编辑对齐模型的输出，将其重新对齐到另一套价值观。随着AI对现实世界影响的加深，让广泛的研究人员意识到这一悖论并寻找突破方法，以确保AI造福人类，变得至关重要。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2405.20806v2/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "34",
    title: "提升医疗AI安全性与保障机制",
    content: "### 提升医疗AI安全性与保障机制\n\n生成型人工智能（Generative AI）在全球医疗领域的应用潜力巨大，能够有效应对全球医疗资源不足的挑战。从简化医疗记录的总结，到复杂的急救诊断，AI已在多个医疗场景中展现了强大的能力。然而，尽管这些技术具有巨大的潜力，广泛应用仍面临着许多安全性问题，特别是在误信息、虚假生成内容（幻觉）以及确保信息真实性方面，这些问题如果得不到有效控制，将严重影响患者安全并损害公众对医疗AI系统的信任。\n\n本文首先探讨了医疗AI领域中存在的独特安全性挑战，尤其是幻觉生成、信息错误和确保临床环境中的事实准确性等问题。尽管一些通用框架，如Llama Guard，能够有效过滤有害内容，但在处理医疗领域特定需求时，仍存在不小的局限性。针对这一现状，作者提出通过改进现有的安全保障框架（如Nvidia NeMo Guardrails），以更好地应对医疗AI特有的风险。通过增强这些保障机制，旨在确保医疗AI的安全、可靠和准确性，减少信息错误的风险，提高患者安全。\n\n生成型AI，尤其是大规模语言模型（LLMs），在医疗行业中已成为一项变革性力量，广泛应用于从自动化诊断到快速分析电子健康记录（EHRs）等多个场景。LLMs的核心优势在于其处理大量医疗数据的能力，能够提取其中的有价值信息并生成具有高度相关性的决策支持。例如，LLMs可以帮助识别患者病史中的微妙模式，根据最新医学文献推荐个性化治疗方案，并在紧急情况下提供实时决策支持。这些应用不仅提高了全球医疗服务的效率，还帮助医疗工作者应对资源紧张、工作流程复杂等挑战。\n\n然而，LLMs在实际应用中也带来了严重的风险，尤其是在高风险医疗环境中。幻觉生成（模型产生虚假或误导性信息）在医疗场景下可能引发致命后果。例如，在重症监护病房（ICU）中，如果模型产生错误的诊断或治疗建议，可能会对患者造成极大危害。此外，LLMs还可能遭遇被“破解”的风险（即通过攻击绕过模型的安全机制），这种情况可能使模型生成有害或不适当的回应。更严重的是，模型生成的信息如果存在误导性，可能导致医生做出错误的医疗决策，进而影响患者的健康。\n\n为了应对这些挑战，作者提出了整合Nvidia NeMo Guardrails和Llama Guard的框架。这一组合方案通过结合实时事实核查与强化的安全防护机制，可以有效解决医疗AI中常见的幻觉、信息失真等问题，从而提供更可靠、更符合医学标准的信息。通过将NeMo Guardrails与Llama Guard的安全防护结合使用，能够在生成医疗相关信息时，及时对模型输出进行校验，并减少因幻觉或误导信息导致的医疗风险。\n\n此外，本文还详细介绍了如何通过增强的Guardrails框架实现对医疗领域语言模型的安全性提升。在该框架中，Llama Guard负责输入验证与破解检测，确保输入不受恶意干扰，保持系统的完整性；随后，NeMo Guardrails通过集成可信的医学数据库（如PubMed），提供必要的背景信息来支持生成准确的医疗回复。经过多层验证和安全检查后，最终由L2M3模型（一个经过医学领域微调的语言模型）生成符合医学最佳实践的答案。\n\n在框架的评估过程中，作者使用了Med-HALT数据集，该数据集专门用于评估医疗领域LLMs的幻觉问题，涵盖了来自不同国家和专业的医学考试数据。通过与未集成Guardrails的基准模型进行对比，评估了这一集成框架在准确性和幻觉检测方面的表现。结果表明，集成后的系统在减少幻觉和提高信息准确性方面具有显著优势，能够更好地确保医疗信息的可靠性和患者的安全。\n\n总之，随着医疗AI的不断发展，如何有效管理其安全性、可靠性和准确性已成为不可忽视的重要课题。通过合理的框架设计与增强的安全机制，医疗AI能够在保证患者安全的前提下，最大限度地发挥其潜力，推动全球医疗服务的发展。",
    summary: "生成型人工智能（Generative AI）在全球医疗领域的应用潜力巨大，能够有效应对全球医疗资源不足的挑战。从简化医疗记录的总结，到复杂的急救诊断，AI已在多个医疗场景中展现了强大的能力。然而，尽管这些技术具有巨大的潜力，广泛应用仍面临着许多安全性问题，特别是在误信息、虚假生成内容（幻觉）以及确保信息真实性方面，这些问题如果得不到有效控制，将严重影响患者安全并损害公众对医疗AI系统的信任。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2409.17190v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "35",
    title: "AI赋能的人类研究：融合脑科学与社会科学洞见",
    content: "### AI赋能的人类研究：融合脑科学与社会科学洞见\n\n随着人工智能（AI）技术的飞速发展，其在科学研究中的作用已从工具转变为积极的研究合作者，尤其在脑科学和社会科学领域，AI正重塑研究范式。本文探讨了AI在增强科学研究中的变革性角色，提出了“AI - 脑科学研究范式”和“AI - 社会科学研究范式”两种创新的人机联合研究范式，并介绍了三种人机协作模型：AI作为研究工具（ART）、AI作为研究助手（ARA）和AI作为研究参与者（ARP）。此外，文章还概述了开展人机联合研究的方法，旨在重新定义人类研究者与AI系统之间的协作互动，为未来的研究方向奠定基础，并激发这一跨学科领域的创新。\n\n文章首先回顾了人类研究的基本方面，包括认知和情感过程如何塑造研究中的行为和决策，以及社会科学如何分析个体和群体间的互动。这些基础理论为人机联合研究提供了理论支撑。在此基础上，文章提出了两种新的人机联合研究范式。在“AI - 脑科学研究范式”中，AI帮助研究人员理解大脑的复杂性，模拟认知过程，并推动科学发现。在“AI - 社会科学研究范式”中，AI参与社会科学研究，改变研究方法和结果，探索AI与社会科学的双向互动。\n\n三种人机协作模型进一步细化了AI在研究中的角色。作为研究工具（ART），AI提供强大的计算和分析能力，帮助研究人员进行文献检索、学术写作和数据处理。例如，AI驱动的文献搜索工具和写作助手显著提高了研究效率。作为研究助手（ARA），AI在实验设计、假设生成和数据分析中发挥更互动和动态的作用，减轻研究人员的认知负担。在脑科学中，AI助手可以优化实验设置，解释脑信号；在社会科学中，AI助手可以根据实时反馈调整问卷或访谈内容。作为研究参与者（ARP），AI作为独立实体参与研究，与人类受试者或研究者直接互动，影响研究结果。例如，在脑科学中，AI可以通过脑 - 机接口影响人类的认知功能；在社会科学中，AI驱动的虚拟形象可以模拟社会互动，影响人类行为。\n\n文章还介绍了开展人机联合研究的方法，包括实验方法和问卷调查。实验方法用于研究AI对人类创造力和批判性思维的影响，而问卷调查用于评估AI在学术写作和研究过程中的作用。研究表明，AI可以提高人类的创造力和批判性思维，但也可能使人类的思维趋同。此外，AI在学术写作中的应用可以提高写作效率，但需要确保引用的准确性和原创性。\n\n总之，本文强调了AI在科学研究中的潜力，并呼吁开发新的研究范式，以充分发挥AI的能力。通过整合脑科学和社会科学的见解，文章为未来的人机联合研究提供了理论框架和实践方法，推动AI从工具转变为真正的研究伙伴，开启科学研究的新纪元。",
    summary: "随着人工智能（AI）技术的飞速发展，其在科学研究中的作用已从工具转变为积极的研究合作者，尤其在脑科学和社会科学领域，AI正重塑研究范式。本文探讨了AI在增强科学研究中的变革性角色，提出了“AI - 脑科学研究范式”和“AI - 社会科学研究范式”两种创新的人机联合研究范式，并介绍了三种人机协作模型：AI作为研究工具（ART）、AI作为研究助手（ARA）和AI作为研究参与者（ARP）。此外，文章还概述了开展人机联合研究的方法，旨在重新定义人类研究者与AI系统之间的协作互动，为未来的研究方向奠定基础，并激发这一跨学科领域的创新。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2411.12761v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "36",
    title: "自动化提示工程中的序列最优学习方法",
    content: "### 自动化提示工程中的序列最优学习方法\n\n在大型语言模型（LLM）的应用中，设计有效的提示（prompt）对于引导模型产生预期的响应至关重要。自动化提示工程旨在通过简化设计、优化和调整提示的过程，减少对人工干预的依赖。本文提出了一种自动化提示工程的最优学习框架，旨在通过有效分配有限的评估预算，逐步识别有效的提示特征。我们引入了一种基于特征的方法来表达提示，显著扩展了搜索空间，并采用贝叶斯回归来利用相似提示之间的相关性，加速学习过程。为了高效探索大量的提示特征空间，我们采用了前瞻性的知识梯度（KG）策略进行序列最优学习，并通过求解混合整数二阶锥优化问题来高效计算KG策略。这种方法具有良好的扩展性，能够处理仅通过约束来定义的提示特征。\n\n通过对比实验，本文展示了所提出的方法在指令生成任务中的显著优势。实验结果表明，在有限的评估预算下，KG策略在选择高质量提示方面优于一系列基准策略。特别是在评估成本较高的应用场景中，我们的方法能够显著提高提示工程的效率，从而为自动化提示工程在更多实际应用中的部署提供了可行的解决方案。\n\n本文提出了一种解释性强的基于特征的提示表示方法，可以捕捉提示中不同属性之间的相互作用，不仅避免了以往方法中的提示特征孤立处理问题，还能够在不依赖人工指定的提示集的情况下，探索出更多的提示变体。此外，采用的贝叶斯方法能够帮助我们在有限的评估次数下，逐步优化对提示特征影响的认识，捕捉特征之间的相关性，进而加速学习进程。\n\n在提示工程的迭代优化过程中，我们将问题形式化为一个有限步数的马尔可夫决策过程，KG策略作为一种前瞻性的方法，能够在每一阶段最大化信息价值的期望改进。对于大规模提示特征空间的挑战，本文通过混合整数锥优化方法有效解决了计算瓶颈，并且提升了该策略的可扩展性。\n\n最后，我们在指令生成任务的基准数据集上评估了我们的方法，结果表明，KG策略在30次以内的提示评估中即可收敛至高质量的提示，相比其他基准方法，所生成的提示在测试数据上表现更好。我们的研究表明，在面对高不确定性和对提示敏感度较高的任务时，KG策略能够提供显著的优势，尤其适用于提示评估成本较高的实际应用场景。",
    summary: "在大型语言模型（LLM）的应用中，设计有效的提示（prompt）对于引导模型产生预期的响应至关重要。自动化提示工程旨在通过简化设计、优化和调整提示的过程，减少对人工干预的依赖。本文提出了一种自动化提示工程的最优学习框架，旨在通过有效分配有限的评估预算，逐步识别有效的提示特征。我们引入了一种基于特征的方法来表达提示，显著扩展了搜索空间，并采用贝叶斯回归来利用相似提示之间的相关性，加速学习过程。为了高效探索大量的提示特征空间，我们采用了前瞻性的知识梯度（KG）策略进行序列最优学习，并通过求解混合整数二阶锥优化问题来高效计算KG策略。这种方法具有良好的扩展性，能够处理仅通过约束来定义的提示特征。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2501.03508v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "37",
    title: "走向稳健与安全的具身智能：漏洞与攻击综述",
    content: "### 走向稳健与安全的具身智能：漏洞与攻击综述\n\n具身智能系统，如机器人和自动驾驶汽车，正越来越多地融入现实世界应用，却面临着来自环境和系统层面因素的诸多漏洞。这些漏洞通过传感器欺骗、对抗性攻击以及任务和运动规划的失败等形式表现出来，给系统的稳健性和安全性带来了巨大挑战。尽管相关研究日益增多，但现有综述很少专门聚焦于具身智能系统的独特安全与安全挑战，大多要么关注通用人工智能漏洞，要么只着眼于孤立的方面，缺乏一个专门针对具身智能的统一框架。本综述填补了这一关键空白，对具身智能的漏洞进行了分类，分析了独特的对抗性攻击范式，探讨了针对大型视觉语言模型（LVLM）和大型语言模型（LLM）的攻击向量，评估了具身感知、决策和任务规划算法的稳健性挑战，并提出了增强具身智能系统安全性和可靠性的针对性策略，为理解具身智能中漏洞与安全之间的相互作用提供了全面框架。\n具身智能作为人工智能的关键技术，在自动驾驶、工业自动化和智能家居系统等领域发挥着重要作用。这些系统通过整合感知、决策和执行，能够出色地处理复杂的现实任务。然而，它们对传感器、执行器和算法之间复杂交互的依赖，也使它们暴露于广泛的漏洞之中，包括动态复杂环境、对传感器的干扰和欺骗攻击以及系统故障等风险，引发了对未经授权行为和声誉损害的担忧，凸显了确保其安全可靠部署的稳健安全措施的迫切需求。\n文章识别出具身系统的三个关键特征：自主性、具身性和认知性。自主性指系统做出明智独立决策的能力，使其能够适应动态不可预测的情境，但这种独立性也引入了漏洞，如在复杂环境中可能出现决策错误。具身性表示与物理环境互动的能力，将物理存在与决策过程相结合以实现无缝互动。认知性涵盖系统理解、推理和解释自身行为的能力，确保其行为与内部目标和外部约束相一致，但认知过程也可能通过传感器到模型的攻击或对学习模型的操纵而被利用。",
    summary: "具身智能系统，如机器人和自动驾驶汽车，正越来越多地融入现实世界应用，却面临着来自环境和系统层面因素的诸多漏洞。这些漏洞通过传感器欺骗、对抗性攻击以及任务和运动规划的失败等形式表现出来，给系统的稳健性和安全性带来了巨大挑战。尽管相关研究日益增多，但现有综述很少专门聚焦于具身智能系统的独特安全与安全挑战，大多要么关注通用人工智能漏洞，要么只着眼于孤立的方面，缺乏一个专门针对具身智能的统一框架。本综述填补了这一关键空白，对具身智能的漏洞进行了分类，分析了独特的对抗性攻击范式，探讨了针对大型视觉语言模型（LVLM）和大型语言模型（LLM）的攻击向量，评估了具身感知、决策和任务规划算法的稳健性挑战，并提出了增强具身智能系统安全性和可靠性的针对性策略，为理解具身智能中漏洞与安全之间的相互作用提供了全面框架。具身智能作为人工智能的关键技术，在自动驾驶、工业自动化和智能家居系统等领域发挥着重要作用。这些系统通过整合感知、决策和执行，能够出色地处理复杂的现实任务。然而，它们对传感器、执行器和算法之间复杂交互的依赖，也使它们暴露于广泛的漏洞之中，包括动态复杂环境、对传感器的干扰和欺骗攻击以及系统故障等风险，引发了对未经授权行为和声誉损害的担忧，凸显了确保其安全可靠部署的稳健安全措施的迫切需求。文章识别出具身系统的三个关键特征：自主性、具身性和认知性。自主性指系统做出明智独立决策的能力，使其能够适应动态不可预测的情境，但这种独立性也引入了漏洞，如在复杂环境中可能出现决策错误。具身性表示与物理环境互动的能力，将物理存在与决策过程相结合以实现无缝互动。认知性涵盖系统理解、推理和解释自身行为的能力，确保其行为与内部目标和外部约束相一致，但认知过程也可能通过传感器到模型的攻击或对学习模型的操纵而被利用。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2502.13175v2/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
  {
    id: "38",
    title: "三维视角看芬兰医疗中小企业的AI准备度：现实、困境与突围",
    content: "### 三维视角看芬兰医疗中小企业的AI准备度：现实、困境与突围\n\n本研究通过对六家芬兰健康科技中小企业（SMEs）的半结构化访谈，提出了一个简洁而富启发性的AI采纳三分模型：AI好奇型、AI拥抱型与AI服务型，分别代表了企业从探索AI、集成AI到提供AI服务的不同发展阶段。尽管所有受访企业都认识到人工智能在医疗健康领域的巨大潜力，但大多数仍处于起步阶段，面临重重障碍。主要挑战集中在三个方面：监管合规的复杂性、技术人才的匮乏以及资金投入的限制。\n\n健康科技企业被视为医疗AI创新的重要驱动力，它们不断开发用于影像诊断、语言处理、个性化治疗等领域的新工具。然而，这些企业要将AI真正融入实际业务，往往面临数据隐私保护法规（如GDPR）的严苛约束、诊断错误的风险、以及验证流程繁琐等问题。例如，有公司尝试用AI自动识别心电图异常，还有企业展望用AI预测癫痫发作，尽管这些项目理论可行，却普遍受到训练数据不足的掣肘。\n\n通过访谈分析，研究将企业的AI成熟度大致划分为三类：仍处于认知与规划阶段的企业（如公司1和6），将AI主要用于数据分析的企业（如公司2、3、5），以及将AI视为主营业务的公司4。公司4为唯一一家达到Gartner标准“第三级”AI成熟度的企业，即AI已成为其产品的核心价值。这种差异反映了企业在资源、技术路径和市场定位上的不同选择。\n\n值得一提的是，企业对AI的理解也不尽相同：从狭义的学习算法，到提高生活质量的工具，再到类人的异常检测器。多数企业将AI看作通过数据训练进行预测的算法，但也有人强调AI作为决策支持工具的重要性。然而，由于医护人员对AI的接受度不高，加上法规对“自我学习型”模型设置了障碍（如欧盟要求模型必须静态训练且样本数不低于20万），AI在实际部署上仍受限重重。\n\n在AI的技术实施方面，企业或依赖第三方解决方案、或使用公有云平台如AWS和Azure，或尝试在本地搭建私有服务器。数据来源方面，则分为内部采集（如ECG/EEG设备）与外部获取（如从设备厂商合作收集的眼底图像）。企业普遍反映，在处理高敏感数据时需特别谨慎，且很多时候缺乏统一的数据治理规范和共享机制。\n\n此外，AI的落地还受到两类“软性”障碍的困扰：一是市场接受度，尤其是来自传统医学界的保守态度；二是人才缺口，尤其是既懂AI又有医学背景的复合型人才稀缺。许多公司反映，目前高校教育将机器学习和医学视为两个孤立的学科，难以培养跨界人才。\n\n总结而言，本研究不仅绘制了芬兰健康科技SMEs在AI采纳上的真实画像，更提供了三条可行建议以推动AI在医疗领域的进一步融合：推进更灵活的监管政策、加强AI与医学交叉人才的培养，以及鼓励企业间的协同与资源共享。通过连接AI研究与实际应用，该研究为政策制定者、研究人员及医疗行业实践者提供了具有现实意义的参考框架。",
    summary: "本研究通过对六家芬兰健康科技中小企业（SMEs）的半结构化访谈，提出了一个简洁而富启发性的AI采纳三分模型：AI好奇型、AI拥抱型与AI服务型，分别代表了企业从探索AI、集成AI到提供AI服务的不同发展阶段。尽管所有受访企业都认识到人工智能在医疗健康领域的巨大潜力，但大多数仍处于起步阶段，面临重重障碍。主要挑战集中在三个方面：监管合规的复杂性、技术人才的匮乏以及资金投入的限制。",
    authors: [],
    tags: [],
    publishedAt: "",
    imageUrl: "http://localhost:3000/papers/2503.14527v1/images/image_1.png",
    views: 0,
    citations: 0,
    popularityScore: 0,
  },
];
